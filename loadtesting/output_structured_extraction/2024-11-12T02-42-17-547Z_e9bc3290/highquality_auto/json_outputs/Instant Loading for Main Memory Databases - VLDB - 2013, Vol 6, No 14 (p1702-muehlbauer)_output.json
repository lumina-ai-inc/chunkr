{
  "file_name": "Instant Loading for Main Memory Databases - VLDB - 2013, Vol 6, No 14 (p1702-muehlbauer).pdf",
  "task_id": "dac75bef-a10f-4340-ae25-759b50d5202b",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "5bb00824-64e3-4a31-a6cc-d5cb8876f556",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Instant Loading for Main Memory Databases\r\nTobias Muhlbauer ¨\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nmuehlbau@in.tum.de\r\nWolf Rodiger ¨\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nroediger@in.tum.de\r\nRobert Seilbeck\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nseilbeck@in.tum.de\r\nAngelika Reiser\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nreiser@in.tum.de\r\nAlfons Kemper\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nkemper@in.tum.de\r\nThomas Neumann\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nneumann@in.tum.de\r\nABSTRACT\r\neScience and big data analytics applications are facing the\r\nchallenge of efficiently evaluating complex queries over vast\r\namounts of structured text data archived in network storage\r\nsolutions. To analyze such data in traditional disk-based\r\ndatabase systems, it needs to be bulk loaded, an operation\r\nwhose performance largely depends on the wire speed of the\r\ndata source and the speed of the data sink, i.e., the disk.\r\nAs the speed of network adapters and disks has stagnated\r\nin the past, loading has become a major bottleneck. The\r\ndelays it is causing are now ubiquitous as text formats are\r\na preferred storage format for reasons of portability.\r\nBut the game has changed: Ever increasing main mem\u0002ory capacities have fostered the development of in-memory\r\ndatabase systems and very fast network infrastructures are\r\non the verge of becoming economical. While hardware limi\u0002tations for fast loading have disappeared, current approaches\r\nfor main memory databases fail to saturate the now available\r\nwire speeds of tens of Gbit/s. With Instant Loading, we con\u0002tribute a novel CSV loading approach that allows scalable\r\nbulk loading at wire speed. This is achieved by optimizing all\r\nphases of loading for modern super-scalar multi-core CPUs.\r\nLarge main memory capacities and Instant Loading thereby\r\nfacilitate a very efficient data staging processing model con\u0002sisting of instantaneous load-work-unload cycles across data\r\narchives on a single node. Once data is loaded, updates and\r\nqueries are efficiently processed with the flexibility, security,\r\nand high performance of relational main memory databases.\r\n1. INTRODUCTION\r\nThe volume of data archived in structured text formats\r\nlike comma-separated values (CSV) has grown rapidly and\r\ncontinues to do so at an unprecedented rate. Scientific data\r\nsets such as the Sloan Digital Sky Survey and Pan-STARRS\r\nare stored as image files and, for reasons of portability and\r\ndebugability, as multi-terabyte archives of derived CSV files\r\nthat are frequently loaded to databases to evaluate complex\r\nPermission to make digital or hard copies of all or part of this work for\r\npersonal or classroom use is granted without fee provided that copies are\r\nnot made or distributed for profit or commercial advantage and that copies\r\nbear this notice and the full citation on the first page. To copy otherwise, to\r\nrepublish, to post on servers or to redistribute to lists, requires prior specific\r\npermission and/or a fee. Articles from this volume were invited to present\r\ntheir results at The 39th International Conference on Very Large Data Bases,\r\nAugust 26th - 30th 2013, Riva del Garda, Trento, Italy.\r\nProceedings of the VLDB Endowment, Vol. 6, No. 14\r\nCopyright 2013 VLDB Endowment 2150-8097/13/14... $ 10.00.\r\n1 GbE HDD SSD\r\n∼4 Gbit/s\r\n10 GbE Infiniband\r\n4×QDR\r\nDDR3-1600\r\n(2 channels)\r\n0 %\r\n25 %\r\n50 %\r\n75 %\r\n100 %\r\n10×\r\nimproved\r\nestimated Instant Loading /w more cores\r\nInstant Loading\r\n/w indexes\r\nInstant Loading\r\nCurrent CSV\r\nbulk loading\r\nwire speed of CSV source in terms of I/O devices\r\nwire speed saturation\r\n4 CPU cores (8 threads)\r\nFigure 1: Pushing the envelope: wire speed satura\u0002tion of current bulk loading vs. Instant Loading.\r\nqueries [27, 26]. Other big data analytics and business ap\u0002plications are equally faced with the need to analyze similar\r\narchives of CSV and CSV-like data [25, 26]. These archives\r\nare usually stored externally from the database server in a\r\nnetwork-attached storage (NAS) or distributed file system\r\n(DFS) or locally in a SSD/RAID storage.\r\nTo efficiently analyze CSV archives, traditional databases\r\ncan do little to overcome the premise of loading. The cost\r\nof parsing, deserializing, validating, and indexing structured\r\ntext data needs to be paid either up front during a bulk load\r\nor lazily during query processing on external tables. The\r\nperformance of loading largely depends on the wire speed of\r\nthe data source and the speed of the data sink, i.e., the disk.\r\nAs the speed of network adapters and disks has stagnated\r\nin the past, loading has become a major bottleneck and the\r\ndelays it is causing are now ubiquitous.\r\nBut the game has changed: Ever increasing main mem\u0002ory capacities have fostered the development of in-memory\r\ndatabase systems and modern network infrastructures as\r\nwell as faster disks are on the verge of becoming economical.\r\nServers with 1 TB of main memory and a 10 GbE adapter\r\n(10 Gbit/s ≈ 1.25 GB/s wire speed) already retail for less\r\nthan $30,000. On this modern hardware, the loading source\r\nand sink are no longer the bottleneck. Rather, current load\u0002ing approaches for main memory databases fail to saturate\r\nthe now available wire speeds. With Instant Loading, we\r\ncontribute a novel CSV loading approach that allows scal\u0002able bulk loading at wire speed (see Fig. 1). This makes the\r\ndelays caused by loading unobtrusive and relational main\r\nmemory databases attractive for a very efficient data stag\u0002ing processing model consisting of instantaneous load-work\u0002unload cycles across CSV data archives on a single node.\r\n1702",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/5bb00824-64e3-4a31-a6cc-d5cb8876f556.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c4e9cc51b15574fc3cd8f4a7799537928d8f6d28419cb5872ba09eaca1698239",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 855
      },
      {
        "segments": [
          {
            "segment_id": "5bb00824-64e3-4a31-a6cc-d5cb8876f556",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Instant Loading for Main Memory Databases\r\nTobias Muhlbauer ¨\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nmuehlbau@in.tum.de\r\nWolf Rodiger ¨\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nroediger@in.tum.de\r\nRobert Seilbeck\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nseilbeck@in.tum.de\r\nAngelika Reiser\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nreiser@in.tum.de\r\nAlfons Kemper\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nkemper@in.tum.de\r\nThomas Neumann\r\nTechnische Universit¨at M¨unchen\r\nMunich, Germany\r\nneumann@in.tum.de\r\nABSTRACT\r\neScience and big data analytics applications are facing the\r\nchallenge of efficiently evaluating complex queries over vast\r\namounts of structured text data archived in network storage\r\nsolutions. To analyze such data in traditional disk-based\r\ndatabase systems, it needs to be bulk loaded, an operation\r\nwhose performance largely depends on the wire speed of the\r\ndata source and the speed of the data sink, i.e., the disk.\r\nAs the speed of network adapters and disks has stagnated\r\nin the past, loading has become a major bottleneck. The\r\ndelays it is causing are now ubiquitous as text formats are\r\na preferred storage format for reasons of portability.\r\nBut the game has changed: Ever increasing main mem\u0002ory capacities have fostered the development of in-memory\r\ndatabase systems and very fast network infrastructures are\r\non the verge of becoming economical. While hardware limi\u0002tations for fast loading have disappeared, current approaches\r\nfor main memory databases fail to saturate the now available\r\nwire speeds of tens of Gbit/s. With Instant Loading, we con\u0002tribute a novel CSV loading approach that allows scalable\r\nbulk loading at wire speed. This is achieved by optimizing all\r\nphases of loading for modern super-scalar multi-core CPUs.\r\nLarge main memory capacities and Instant Loading thereby\r\nfacilitate a very efficient data staging processing model con\u0002sisting of instantaneous load-work-unload cycles across data\r\narchives on a single node. Once data is loaded, updates and\r\nqueries are efficiently processed with the flexibility, security,\r\nand high performance of relational main memory databases.\r\n1. INTRODUCTION\r\nThe volume of data archived in structured text formats\r\nlike comma-separated values (CSV) has grown rapidly and\r\ncontinues to do so at an unprecedented rate. Scientific data\r\nsets such as the Sloan Digital Sky Survey and Pan-STARRS\r\nare stored as image files and, for reasons of portability and\r\ndebugability, as multi-terabyte archives of derived CSV files\r\nthat are frequently loaded to databases to evaluate complex\r\nPermission to make digital or hard copies of all or part of this work for\r\npersonal or classroom use is granted without fee provided that copies are\r\nnot made or distributed for profit or commercial advantage and that copies\r\nbear this notice and the full citation on the first page. To copy otherwise, to\r\nrepublish, to post on servers or to redistribute to lists, requires prior specific\r\npermission and/or a fee. Articles from this volume were invited to present\r\ntheir results at The 39th International Conference on Very Large Data Bases,\r\nAugust 26th - 30th 2013, Riva del Garda, Trento, Italy.\r\nProceedings of the VLDB Endowment, Vol. 6, No. 14\r\nCopyright 2013 VLDB Endowment 2150-8097/13/14... $ 10.00.\r\n1 GbE HDD SSD\r\n∼4 Gbit/s\r\n10 GbE Infiniband\r\n4×QDR\r\nDDR3-1600\r\n(2 channels)\r\n0 %\r\n25 %\r\n50 %\r\n75 %\r\n100 %\r\n10×\r\nimproved\r\nestimated Instant Loading /w more cores\r\nInstant Loading\r\n/w indexes\r\nInstant Loading\r\nCurrent CSV\r\nbulk loading\r\nwire speed of CSV source in terms of I/O devices\r\nwire speed saturation\r\n4 CPU cores (8 threads)\r\nFigure 1: Pushing the envelope: wire speed satura\u0002tion of current bulk loading vs. Instant Loading.\r\nqueries [27, 26]. Other big data analytics and business ap\u0002plications are equally faced with the need to analyze similar\r\narchives of CSV and CSV-like data [25, 26]. These archives\r\nare usually stored externally from the database server in a\r\nnetwork-attached storage (NAS) or distributed file system\r\n(DFS) or locally in a SSD/RAID storage.\r\nTo efficiently analyze CSV archives, traditional databases\r\ncan do little to overcome the premise of loading. The cost\r\nof parsing, deserializing, validating, and indexing structured\r\ntext data needs to be paid either up front during a bulk load\r\nor lazily during query processing on external tables. The\r\nperformance of loading largely depends on the wire speed of\r\nthe data source and the speed of the data sink, i.e., the disk.\r\nAs the speed of network adapters and disks has stagnated\r\nin the past, loading has become a major bottleneck and the\r\ndelays it is causing are now ubiquitous.\r\nBut the game has changed: Ever increasing main mem\u0002ory capacities have fostered the development of in-memory\r\ndatabase systems and modern network infrastructures as\r\nwell as faster disks are on the verge of becoming economical.\r\nServers with 1 TB of main memory and a 10 GbE adapter\r\n(10 Gbit/s ≈ 1.25 GB/s wire speed) already retail for less\r\nthan $30,000. On this modern hardware, the loading source\r\nand sink are no longer the bottleneck. Rather, current load\u0002ing approaches for main memory databases fail to saturate\r\nthe now available wire speeds. With Instant Loading, we\r\ncontribute a novel CSV loading approach that allows scal\u0002able bulk loading at wire speed (see Fig. 1). This makes the\r\ndelays caused by loading unobtrusive and relational main\r\nmemory databases attractive for a very efficient data stag\u0002ing processing model consisting of instantaneous load-work\u0002unload cycles across CSV data archives on a single node.\r\n1702",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/5bb00824-64e3-4a31-a6cc-d5cb8876f556.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c4e9cc51b15574fc3cd8f4a7799537928d8f6d28419cb5872ba09eaca1698239",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 855
      },
      {
        "segments": [
          {
            "segment_id": "bb34cafe-1c96-4c0b-8fb7-d2a136299293",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "queries\r\nhigh-speed\r\nNAS/DFS or SSD/RAID\r\nu\r\nnload\r\n... ... CSV CSV or binary\r\nLoad CSV 2 Work: OLTP and OLAP in 3\r\nfull-featured database\r\nUnload\r\nwindow of interest\r\nupdates\r\nloading/unloading at wire speed load\r\nmain\r\nm\r\ne\r\nm\r\nory\r\n1\r\nFigure 2: Instant Loading for data staging process\u0002ing: load-work-unload cycles across CSV data.\r\nContributions. To achieve instantaneous loading, we\r\noptimize CSV bulk loading for modern super-scalar multi\u0002core CPUs by task- and data-parallelizing all phases of load\u0002ing. In particular, we propose a task-parallel CSV process\u0002ing pipeline and present generic high-performance parsing,\r\ndeserialization, and input validation methods based on SSE\r\n4.2 SIMD instructions. While these already improve load\u0002ing time significantly, other phases of loading become the\r\nbottleneck. We thus further show how copying deserialized\r\ntuples into the storage backend can be sped up and how in\u0002dex creation can efficiently be interleaved with parallelized\r\nbulk loading using merge-able index structures (e.g., hash\u0002ing with chaining and the adaptive radix tree (ART) [20]).\r\nTo prove the feasibility of our generic Instant Loading\r\napproach, we integrate it in our main memory database sys\u0002tem HyPer [19] and evaluate our implementation using the\r\nindustry-standard TPC benchmarks. Results show improve\u0002ments of up to a factor of 10 on a quad-core commodity ma\u0002chine compared to current CSV bulk loading in main mem\u0002ory databases like MonetDB [4] and Vectorwise. Our imple\u0002mentation of the Instant Loading approach aims at highest\r\nperformance in an in-memory computation setting where\r\nraw CPU costs dominate. We therefore strive for good code\r\nand data locality and use light-weight synchronization prim\u0002itives such as atomic instructions. As the proportion of se\u0002quential code is minimized, we expect our approach to scale\r\nwith faster data sources and CPUs with ever more cores.\r\nInstant Loading in action: the (lwu)* data staging\r\nprocessing model. Servers with 1 TB of main memory and\r\nmore offer enough space to facilitate an in-memory analysis\r\nof large sets of structured text data. However, currently the\r\nadoption of databases for such analysis tasks is hindered\r\nby the inefficiency of bulk loading (cf., Sect. 3.1). With\r\nInstant Loading we remove this obstacle and allow a novel\r\ndata staging processing model consisting of instantaneous\r\nload-work-unload cycles (lwu)* across windows of interest.\r\nData staging workflows exist in eScience (e.g., astronomy\r\nand genetics [27, 26]) and other big data analytics appli\u0002cations. For example, Netflix, a popular on-demand media\r\nstreaming service, reported that they are collecting 0.6 TB\r\nof CSV-like log data in a DFS per day [11]. Each hour,\r\nthe last hour’s structured log data is loaded to a 50+ node\r\nHadoop/Hive-based data warehouse, which is used for the\r\nextraction of performance indicators and for ad-hoc queries.\r\nOur vision is to use Instant Loading in a single-node main\r\nmemory database for these kinds of recurring load-work\u0002unload workflows. Fig. 2 illustrates our three-step (lwu)*\r\napproach. 1 : A window of interest of hot CSV files is loaded\r\nfrom a NAS/DFS or a local high-performance SSD/RAID\r\n1,Africa\\n 2,Antarctica\\n 3,Asia\\n 4,Australia\\n 5,Europe\\n 6,North America\\n 7,South America\\n (a) CSV\r\nid name 1 Africa 2 Antarctica 3 Asia 4 Australia 5 Europe 6 North America 7 South America (b) relational 1 Africa 2 Antarctica 3 Asia 4 Australia vector chunk\r\nPartition 1 Partition 2\r\n5 Europe 6 North America 7 South America (c) physical (chunk-based column-store)\r\nFigure 3: Continent names in three representations:\r\n(a) CSV, (b) relational, and (c) physical.\r\nto a main memory database at wire speed. The window of\r\ninterest can even be bigger than the size of the main memory\r\nas selection predicates can be pushed into the loading pro\u0002cess. Further, data can be compressed at load time. 2 : The\r\nfull set of features of a relational main memory database—\r\nincluding efficient support for queries (OLAP) and transac\u0002tional updates (OLTP)—can then be used by multiple users\r\nto work on the window of interest. 3 : Prior to loading new\r\ndata, the potentially modified data is unloaded in either a\r\n(compressed) binary format or, for portability and debuga\u0002bility, as CSV. Instant Loading is the essential backbone\r\nthat facilitates the (lwu)* approach.\r\nComparison to MapReduce approaches. Google’s\r\nMapReduce [5] (MR) and its open-source implementation\r\nHadoop brought along new analysis approaches for struc\u0002tured text files. While we focus on analyzing such files on\r\na single node, these approaches scale jobs out to a clus\u0002ter of nodes. By working on raw files, MR requires no ex\u0002plicit loading like relational databases. On the downside,\r\na comparison of databases and MR [23] has shown that\r\ndatabases are, in general, much easier to query and sig\u0002nificantly faster at data analysis. Extensions of MR and\r\nHadoop like Hive [28] and HAIL [7] try to close this gap\r\nby, e.g., adding support for declarative query languages, in\u0002dexes, and data preprocessing. As for comparison of MR\r\nwith our approach, Instant Loading in its current state aims\r\nat accelerating bulk loading on a single database node—\r\nthat could be part of a cluster of servers. We see scaleout of\r\nquery and transaction processing as an orthogonal direction\r\nof research. Nevertheless, MR-based systems can as well\r\nprofit from the generic high-performance CSV parsing and\r\ndeserialization methods proposed in this work.\r\n2. DATA REPRESENTATIONS\r\nAn important part of bulk loading is the transformation\r\nand reorganization of data from one format into another.\r\nThis paper focuses on the comma separated values (CSV),\r\nrelational, and common physical representations in main\r\nmemory database systems; Fig. 3 illustrates these three.\r\nCSV representation. CSV is a simple, yet widely used\r\ndata format that represents tabular data as a sequence of\r\ncharacters in a human readable format. It is in many cases\r\nthe least common denominator of information exchange. As\r\nsuch, tera-scale archives of CSV and CSV-like data exist in\r\neScience and other big data analytics applications [27, 26,\r\n25]. Physically, each character is encoded in one or several\r\n1703",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/bb34cafe-1c96-4c0b-8fb7-d2a136299293.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8fd119bc6f7a8d87b69e72ae90e38057add3e85b852ba13f4bee3ad8ebc793dd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 958
      },
      {
        "segments": [
          {
            "segment_id": "bb34cafe-1c96-4c0b-8fb7-d2a136299293",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "queries\r\nhigh-speed\r\nNAS/DFS or SSD/RAID\r\nu\r\nnload\r\n... ... CSV CSV or binary\r\nLoad CSV 2 Work: OLTP and OLAP in 3\r\nfull-featured database\r\nUnload\r\nwindow of interest\r\nupdates\r\nloading/unloading at wire speed load\r\nmain\r\nm\r\ne\r\nm\r\nory\r\n1\r\nFigure 2: Instant Loading for data staging process\u0002ing: load-work-unload cycles across CSV data.\r\nContributions. To achieve instantaneous loading, we\r\noptimize CSV bulk loading for modern super-scalar multi\u0002core CPUs by task- and data-parallelizing all phases of load\u0002ing. In particular, we propose a task-parallel CSV process\u0002ing pipeline and present generic high-performance parsing,\r\ndeserialization, and input validation methods based on SSE\r\n4.2 SIMD instructions. While these already improve load\u0002ing time significantly, other phases of loading become the\r\nbottleneck. We thus further show how copying deserialized\r\ntuples into the storage backend can be sped up and how in\u0002dex creation can efficiently be interleaved with parallelized\r\nbulk loading using merge-able index structures (e.g., hash\u0002ing with chaining and the adaptive radix tree (ART) [20]).\r\nTo prove the feasibility of our generic Instant Loading\r\napproach, we integrate it in our main memory database sys\u0002tem HyPer [19] and evaluate our implementation using the\r\nindustry-standard TPC benchmarks. Results show improve\u0002ments of up to a factor of 10 on a quad-core commodity ma\u0002chine compared to current CSV bulk loading in main mem\u0002ory databases like MonetDB [4] and Vectorwise. Our imple\u0002mentation of the Instant Loading approach aims at highest\r\nperformance in an in-memory computation setting where\r\nraw CPU costs dominate. We therefore strive for good code\r\nand data locality and use light-weight synchronization prim\u0002itives such as atomic instructions. As the proportion of se\u0002quential code is minimized, we expect our approach to scale\r\nwith faster data sources and CPUs with ever more cores.\r\nInstant Loading in action: the (lwu)* data staging\r\nprocessing model. Servers with 1 TB of main memory and\r\nmore offer enough space to facilitate an in-memory analysis\r\nof large sets of structured text data. However, currently the\r\nadoption of databases for such analysis tasks is hindered\r\nby the inefficiency of bulk loading (cf., Sect. 3.1). With\r\nInstant Loading we remove this obstacle and allow a novel\r\ndata staging processing model consisting of instantaneous\r\nload-work-unload cycles (lwu)* across windows of interest.\r\nData staging workflows exist in eScience (e.g., astronomy\r\nand genetics [27, 26]) and other big data analytics appli\u0002cations. For example, Netflix, a popular on-demand media\r\nstreaming service, reported that they are collecting 0.6 TB\r\nof CSV-like log data in a DFS per day [11]. Each hour,\r\nthe last hour’s structured log data is loaded to a 50+ node\r\nHadoop/Hive-based data warehouse, which is used for the\r\nextraction of performance indicators and for ad-hoc queries.\r\nOur vision is to use Instant Loading in a single-node main\r\nmemory database for these kinds of recurring load-work\u0002unload workflows. Fig. 2 illustrates our three-step (lwu)*\r\napproach. 1 : A window of interest of hot CSV files is loaded\r\nfrom a NAS/DFS or a local high-performance SSD/RAID\r\n1,Africa\\n 2,Antarctica\\n 3,Asia\\n 4,Australia\\n 5,Europe\\n 6,North America\\n 7,South America\\n (a) CSV\r\nid name 1 Africa 2 Antarctica 3 Asia 4 Australia 5 Europe 6 North America 7 South America (b) relational 1 Africa 2 Antarctica 3 Asia 4 Australia vector chunk\r\nPartition 1 Partition 2\r\n5 Europe 6 North America 7 South America (c) physical (chunk-based column-store)\r\nFigure 3: Continent names in three representations:\r\n(a) CSV, (b) relational, and (c) physical.\r\nto a main memory database at wire speed. The window of\r\ninterest can even be bigger than the size of the main memory\r\nas selection predicates can be pushed into the loading pro\u0002cess. Further, data can be compressed at load time. 2 : The\r\nfull set of features of a relational main memory database—\r\nincluding efficient support for queries (OLAP) and transac\u0002tional updates (OLTP)—can then be used by multiple users\r\nto work on the window of interest. 3 : Prior to loading new\r\ndata, the potentially modified data is unloaded in either a\r\n(compressed) binary format or, for portability and debuga\u0002bility, as CSV. Instant Loading is the essential backbone\r\nthat facilitates the (lwu)* approach.\r\nComparison to MapReduce approaches. Google’s\r\nMapReduce [5] (MR) and its open-source implementation\r\nHadoop brought along new analysis approaches for struc\u0002tured text files. While we focus on analyzing such files on\r\na single node, these approaches scale jobs out to a clus\u0002ter of nodes. By working on raw files, MR requires no ex\u0002plicit loading like relational databases. On the downside,\r\na comparison of databases and MR [23] has shown that\r\ndatabases are, in general, much easier to query and sig\u0002nificantly faster at data analysis. Extensions of MR and\r\nHadoop like Hive [28] and HAIL [7] try to close this gap\r\nby, e.g., adding support for declarative query languages, in\u0002dexes, and data preprocessing. As for comparison of MR\r\nwith our approach, Instant Loading in its current state aims\r\nat accelerating bulk loading on a single database node—\r\nthat could be part of a cluster of servers. We see scaleout of\r\nquery and transaction processing as an orthogonal direction\r\nof research. Nevertheless, MR-based systems can as well\r\nprofit from the generic high-performance CSV parsing and\r\ndeserialization methods proposed in this work.\r\n2. DATA REPRESENTATIONS\r\nAn important part of bulk loading is the transformation\r\nand reorganization of data from one format into another.\r\nThis paper focuses on the comma separated values (CSV),\r\nrelational, and common physical representations in main\r\nmemory database systems; Fig. 3 illustrates these three.\r\nCSV representation. CSV is a simple, yet widely used\r\ndata format that represents tabular data as a sequence of\r\ncharacters in a human readable format. It is in many cases\r\nthe least common denominator of information exchange. As\r\nsuch, tera-scale archives of CSV and CSV-like data exist in\r\neScience and other big data analytics applications [27, 26,\r\n25]. Physically, each character is encoded in one or several\r\n1703",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/bb34cafe-1c96-4c0b-8fb7-d2a136299293.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8fd119bc6f7a8d87b69e72ae90e38057add3e85b852ba13f4bee3ad8ebc793dd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 958
      },
      {
        "segments": [
          {
            "segment_id": "f9f41b80-720c-4318-a375-e4eb193df1ed",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "bytes of a character encoding scheme, commonly ASCII or\r\nUTF-8. ASCII is a subset of UTF-8, where the 128 ASCII\r\ncharacters correspond to the first 128 UTF-8 characters.\r\nASCII characters are stored in a single byte where the high\r\nbit is not set. Other characters in UTF-8 are represented by\r\nsequences of up to 6 bytes where for each byte the high bit\r\nis set. Thus, an ASCII byte cannot be part of a multi-byte\r\nsequence that represents a UTF-8 character. Even though\r\nCSV is widely used, it has never been fully standardized. A\r\nfirst approach in this direction is the RFC 4180 [30] proposal\r\nwhich closely resembles our understanding of CSV. Data is\r\nstructured in records, which are separated by a record delim\u0002iter (usually ’\\n’ or \"\\r\\n\"). Each record contains fields,\r\nwhich are again separated by a field delimiter (e.g., ’,’).\r\nFields can be quoted, i.e., enclosed by a quotation character\r\n(e.g., ’\"’). Inside a quoted field, record and field delimiters\r\nare not treated as such. Quotation characters that are part\r\nof a quoted field have to be escaped by an escape charac\u0002ter (e.g., ’\\’). If the aforementioned special characters are\r\nuser-definable, the CSV format is highly portable. Due to\r\nits tabular form, it can naturally represent relations, where\r\ntuples and attribute values are mapped to records and fields.\r\nPhysical representations. Databases store relations\r\nin a storage backend that is optimized for efficient update\r\nand query processing. In our HyPer main memory database\r\nsystem, a relation can be stored in a row- or a column\u0002store backend. A storage backend is structured in par\u0002titions, which horizontally split the relation into disjoint\r\nsubsets. These partitons store the rows or columns in ei\u0002ther contiguous blocks of memory or are again horizontally\r\npartitioned into multiple chunks (chunked backend, cf., Fig\r\n3(c)), a technique first proposed by MonetDB/X100 [4]. The\r\ncombination of these options gives four possibile types of\r\nstorage backends: contiguous memory-based/chunked row-\r\n/column-store. Most, if not all, main memory database sys\u0002tems, including MonetDB, Vectorwise, and SAP HANA im\u0002plement similar storage backends. Instant Loading is de\u0002signed for all of the aforementioned types of storage back\u0002ends and is therefore a generic approach that can be inte\u0002grated into various main memory database systems.\r\nThis work focuses on bulk loading to uncompressed phys\u0002ical representations. Dictionary encoding can, however, be\r\nused in the CSV data or created on the fly at load time.\r\n3. INSTANT LOADING\r\n3.1 CSV Bulk Loading Analysis\r\nTo better understand how bulk loading of CSV data on\r\nmodern hardware can be optimized, we first analyzed why it\r\ncurrently cannot saturate available wire speeds. The stan\u0002dard single-threaded implementation of CSV bulk loading\r\nin our HyPer [19] main memory database system achieves a\r\nloading throughput of around 100 MB/s for 10 GB of CSV\r\ndata stored in an in-memory file system1. This is compara\u0002ble to the CSV loading throughput of other state of the art\r\nmain memory databases like MonetDB [4] and Vectorwise,\r\nwhich we also evaluated. The measured loading throughputs\r\nof 100 MB/s, however, do not saturate the available wire\r\nspeed of the in-memory file system. In fact, not even a SSD\r\n1For lack of a high-speed network-attached storage or dis\u0002tributed file system in our lab, we used the in-memory file\r\nsystem ramfs as the loading source to emulate a CSV source\r\nwire speed of multiple GB/s.\r\n(500 MB/s) or 1 GbE (128 MB/s) can be saturated. A perf\r\nanalysis shows that about 50% of CPU cycles are spent on\r\nparsing the input, 20% on deserialization, 10% on inserting\r\ntuples into the relation, and finally 20% on updating indexes.\r\nIn our standard approach, parsing is expensive as it is\r\nbased on a character at a time comparison of CSV input and\r\nspecial characters, where each comparison is implemented as\r\nan if-then conditional branch. Due to their pipelined ar\u0002chitecture, current general purpose CPUs try to predict the\r\noutcome of such branches. Thereby, a mispredicted branch\r\nrequires the entire pipeline to be flushed and ever deeper\r\npipelines in modern CPUs lead to huge branch miss penal\u0002ties [2]. For CSV parsing, however, the comparison branches\r\ncan hardly be predicted, which leads to almost one mispre\u0002diction per field and record delimiter of the CSV input.\r\nEach value found by the parser needs to be deserialized.\r\nThe deserialization method validates the string input and\r\ntransforms the string value into its data type representation\r\nin the database. Again, several conditional branches lead to\r\na significant number of branch miss penalties.\r\nParsed and deserialized tuples are inserted into the rela\u0002tion and are indexed in the relation’s indexes. Inserting and\r\nindexing of tuples accounts for 30% of loading time and is\r\nnot the bottleneck in our standard loading approach. In\u0002stead, our experiment revealed that the insertion and in\u0002dexing speed of HyPer’s partitioned column-store backend\r\nexceeds the speed at which standard parsing and deserial\u0002ization methods are able to produce new tuples.\r\n3.2 Design of the Instant Loading Pipeline\r\nThe aforementioned standard CSV bulk loading approach\r\nfollows a single-threaded execution model. To fully exploit\r\nthe performance of modern super-scalar multi-core CPUs,\r\napplications need to be highly parallelized [17]. Following\r\nAmdahl’s law the proportion of sequential code needs to be\r\nreduced to a minimum to achieve maximum speedup.\r\nWe base our implementation of Instant Loading on the\r\nprogramming model of the Intel Threading Building Blocks\r\n(TBB) [24] library. In TBB, parallelism is exposed by the\r\ndefinition of tasks rather than threads. Tasks are dynami\u0002cally scheduled and executed on available hardware threads\r\nby a run-time engine. The engine implements task stealing\r\nfor workload balancing and reuses threads to avoid initial\u0002ization overhead. Task-based programming allows to expose\r\nparallelism to a great extent.\r\nInstant Loading is designed for high scalability and pro\u0002ceeds in two steps (see Fig. 4). 1 st, CSV input is chunked\r\nand CSV chunks are processed by unsynchronized tasks.\r\nEach task parses and deserializes the tuples in its chunk.\r\nIt further determines a tuple’s corresponding partition (see\r\nSect. 2 for a description of our partitioned storage backend)\r\nand stores tuples that belong to the same partition in a com\u0002mon buffer which we refer to as a partition buffer. Partition\r\nbuffers have the same physical layout (e.g., row or colum\u0002nar) as the relation partition, such that no further transfor\u0002mation is necessary when inserting tuples from the buffer\r\ninto the relation partition. Additionally, tuples in partition\r\nbuffers are indexed according to the indexes defined for the\r\nrelation. In a 2 nd step, partition buffers are merged with\r\nthe corresponding relation partitions. This includes merg\u0002ing of tuples and indexes. While CSV chunk processing is\r\nperformed in parallel for each CSV chunk, merging with re\u0002lation partitions is performed in parallel for each partition.\r\n1704",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/f9f41b80-720c-4318-a375-e4eb193df1ed.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cc8a587d527eb5b4a884aa5a0eab543687afd199944f65b04c944f3b2df1c497",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1105
      },
      {
        "segments": [
          {
            "segment_id": "f9f41b80-720c-4318-a375-e4eb193df1ed",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "bytes of a character encoding scheme, commonly ASCII or\r\nUTF-8. ASCII is a subset of UTF-8, where the 128 ASCII\r\ncharacters correspond to the first 128 UTF-8 characters.\r\nASCII characters are stored in a single byte where the high\r\nbit is not set. Other characters in UTF-8 are represented by\r\nsequences of up to 6 bytes where for each byte the high bit\r\nis set. Thus, an ASCII byte cannot be part of a multi-byte\r\nsequence that represents a UTF-8 character. Even though\r\nCSV is widely used, it has never been fully standardized. A\r\nfirst approach in this direction is the RFC 4180 [30] proposal\r\nwhich closely resembles our understanding of CSV. Data is\r\nstructured in records, which are separated by a record delim\u0002iter (usually ’\\n’ or \"\\r\\n\"). Each record contains fields,\r\nwhich are again separated by a field delimiter (e.g., ’,’).\r\nFields can be quoted, i.e., enclosed by a quotation character\r\n(e.g., ’\"’). Inside a quoted field, record and field delimiters\r\nare not treated as such. Quotation characters that are part\r\nof a quoted field have to be escaped by an escape charac\u0002ter (e.g., ’\\’). If the aforementioned special characters are\r\nuser-definable, the CSV format is highly portable. Due to\r\nits tabular form, it can naturally represent relations, where\r\ntuples and attribute values are mapped to records and fields.\r\nPhysical representations. Databases store relations\r\nin a storage backend that is optimized for efficient update\r\nand query processing. In our HyPer main memory database\r\nsystem, a relation can be stored in a row- or a column\u0002store backend. A storage backend is structured in par\u0002titions, which horizontally split the relation into disjoint\r\nsubsets. These partitons store the rows or columns in ei\u0002ther contiguous blocks of memory or are again horizontally\r\npartitioned into multiple chunks (chunked backend, cf., Fig\r\n3(c)), a technique first proposed by MonetDB/X100 [4]. The\r\ncombination of these options gives four possibile types of\r\nstorage backends: contiguous memory-based/chunked row-\r\n/column-store. Most, if not all, main memory database sys\u0002tems, including MonetDB, Vectorwise, and SAP HANA im\u0002plement similar storage backends. Instant Loading is de\u0002signed for all of the aforementioned types of storage back\u0002ends and is therefore a generic approach that can be inte\u0002grated into various main memory database systems.\r\nThis work focuses on bulk loading to uncompressed phys\u0002ical representations. Dictionary encoding can, however, be\r\nused in the CSV data or created on the fly at load time.\r\n3. INSTANT LOADING\r\n3.1 CSV Bulk Loading Analysis\r\nTo better understand how bulk loading of CSV data on\r\nmodern hardware can be optimized, we first analyzed why it\r\ncurrently cannot saturate available wire speeds. The stan\u0002dard single-threaded implementation of CSV bulk loading\r\nin our HyPer [19] main memory database system achieves a\r\nloading throughput of around 100 MB/s for 10 GB of CSV\r\ndata stored in an in-memory file system1. This is compara\u0002ble to the CSV loading throughput of other state of the art\r\nmain memory databases like MonetDB [4] and Vectorwise,\r\nwhich we also evaluated. The measured loading throughputs\r\nof 100 MB/s, however, do not saturate the available wire\r\nspeed of the in-memory file system. In fact, not even a SSD\r\n1For lack of a high-speed network-attached storage or dis\u0002tributed file system in our lab, we used the in-memory file\r\nsystem ramfs as the loading source to emulate a CSV source\r\nwire speed of multiple GB/s.\r\n(500 MB/s) or 1 GbE (128 MB/s) can be saturated. A perf\r\nanalysis shows that about 50% of CPU cycles are spent on\r\nparsing the input, 20% on deserialization, 10% on inserting\r\ntuples into the relation, and finally 20% on updating indexes.\r\nIn our standard approach, parsing is expensive as it is\r\nbased on a character at a time comparison of CSV input and\r\nspecial characters, where each comparison is implemented as\r\nan if-then conditional branch. Due to their pipelined ar\u0002chitecture, current general purpose CPUs try to predict the\r\noutcome of such branches. Thereby, a mispredicted branch\r\nrequires the entire pipeline to be flushed and ever deeper\r\npipelines in modern CPUs lead to huge branch miss penal\u0002ties [2]. For CSV parsing, however, the comparison branches\r\ncan hardly be predicted, which leads to almost one mispre\u0002diction per field and record delimiter of the CSV input.\r\nEach value found by the parser needs to be deserialized.\r\nThe deserialization method validates the string input and\r\ntransforms the string value into its data type representation\r\nin the database. Again, several conditional branches lead to\r\na significant number of branch miss penalties.\r\nParsed and deserialized tuples are inserted into the rela\u0002tion and are indexed in the relation’s indexes. Inserting and\r\nindexing of tuples accounts for 30% of loading time and is\r\nnot the bottleneck in our standard loading approach. In\u0002stead, our experiment revealed that the insertion and in\u0002dexing speed of HyPer’s partitioned column-store backend\r\nexceeds the speed at which standard parsing and deserial\u0002ization methods are able to produce new tuples.\r\n3.2 Design of the Instant Loading Pipeline\r\nThe aforementioned standard CSV bulk loading approach\r\nfollows a single-threaded execution model. To fully exploit\r\nthe performance of modern super-scalar multi-core CPUs,\r\napplications need to be highly parallelized [17]. Following\r\nAmdahl’s law the proportion of sequential code needs to be\r\nreduced to a minimum to achieve maximum speedup.\r\nWe base our implementation of Instant Loading on the\r\nprogramming model of the Intel Threading Building Blocks\r\n(TBB) [24] library. In TBB, parallelism is exposed by the\r\ndefinition of tasks rather than threads. Tasks are dynami\u0002cally scheduled and executed on available hardware threads\r\nby a run-time engine. The engine implements task stealing\r\nfor workload balancing and reuses threads to avoid initial\u0002ization overhead. Task-based programming allows to expose\r\nparallelism to a great extent.\r\nInstant Loading is designed for high scalability and pro\u0002ceeds in two steps (see Fig. 4). 1 st, CSV input is chunked\r\nand CSV chunks are processed by unsynchronized tasks.\r\nEach task parses and deserializes the tuples in its chunk.\r\nIt further determines a tuple’s corresponding partition (see\r\nSect. 2 for a description of our partitioned storage backend)\r\nand stores tuples that belong to the same partition in a com\u0002mon buffer which we refer to as a partition buffer. Partition\r\nbuffers have the same physical layout (e.g., row or colum\u0002nar) as the relation partition, such that no further transfor\u0002mation is necessary when inserting tuples from the buffer\r\ninto the relation partition. Additionally, tuples in partition\r\nbuffers are indexed according to the indexes defined for the\r\nrelation. In a 2 nd step, partition buffers are merged with\r\nthe corresponding relation partitions. This includes merg\u0002ing of tuples and indexes. While CSV chunk processing is\r\nperformed in parallel for each CSV chunk, merging with re\u0002lation partitions is performed in parallel for each partition.\r\n1704",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/f9f41b80-720c-4318-a375-e4eb193df1ed.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cc8a587d527eb5b4a884aa5a0eab543687afd199944f65b04c944f3b2df1c497",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1105
      },
      {
        "segments": [
          {
            "segment_id": "61a2c4e9-6664-4c46-a74a-444535f382cf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "buffer 1\r\nchunk n\r\nRelation\r\nPartition 1 Partition m\r\nbuffer 1 ... buffer m ... chunk n ... chunk 1 chunk 2\r\nMerge buffers with relation partitions:\r\nmerge tuples and indexes\r\n(partition-parallel)\r\ntask 1 task m\r\n1\r\n2\r\nProcess CSV chunks:\r\ndetermine orphan and parse, deserialize, partition, index each tuple (chunk-parallel)\r\nchunk 1 chunk 2\r\ntask 1\r\nwidow orphan\r\nCSV input . \\n 3 , A s i a \\n 4 , A u s t r a l i a \\n 5 , ... ... ... task 2 task n\r\nbuffer 1 buffer m buffer m\r\nFigure 4: Schematic overview of Instant Loading: from CSV input to relation partitions.\r\n3.3 Task-Parallelization\r\nTo allow synchronization-free task-parallelization of pars\u0002ing, deserialization, partition classification, and indexing, we\r\nsplit CSV input into independent CSV chunks that can be\r\nprocessed in parallel. The choice of the chunk size granu\u0002larity is challenging and impacts the parallelizability of the\r\nbulk loading process. The smaller the chunk size, the more\r\nchunk processing and merge steps can be interleaved. How\u0002ever, chunks should not be too small, as otherwise the over\u0002head of dealing with incomplete tuples at chunk borders\r\nincreases. Instant Loading splits the input according to a\r\nsize for which it can at least be guaranteed that, assuming\r\nthe input is well-formed, one complete tuple fits into a CSV\r\nchunk. Otherwise, parallelized parsing would be hindered.\r\nTo identify chunk sizes that allow for high-performance load\u0002ing, we evaluated our Instant Loading implementation with\r\nvarying chunk sizes (see Fig. 13). The evaluation leads us to\r\nthe conclusion that on a CPU with a last-level cache of size l\r\nand n hardware threads, the highest loading throughput can\r\nbe achieved with a CSV chunk size in the range of 0.25×l/n\r\nto 1.0 × l/n. E.g., a good chunk size on a current Intel Ivy\r\nBridge CPU with a 8 MB L3 cache and 8 hardware threads\r\nis in the range of 256 kB to 1 MB. When loading from a local\r\nI/O device, we use madvise to advise the kernel to prefetch\r\nthe CSV chunks.\r\nChunking CSV input according to a fixed size produces\r\nincomplete tuples at CSV chunk borders. We refer to these\r\ntuples as widows and orphans (cf., Fig. 4):\r\nDefinition (Widow and orphan). “An orphan has\r\nno past, a widow has no future” is a famous mnemonic in\r\ntypesetting. In typesetting, a widow is a line that ends and\r\nan orphan is a line that opens a paragraph and is separated\r\nfrom the rest of the paragraph by a page break, respectively.\r\nChunking CSV input creates a similar effect. A widow of a\r\nCSV chunk is an incomplete tuple at the end of a chunk that\r\nis separated from the part that would make it complete, i.e.,\r\nthe orphan, by a chunk border.\r\nUnfortunately, if chunk borders are chosen according to\r\na fixed size, CSV chunk-processing tasks can no longer dis\u0002tinguish between real record delimiters and record delim\u0002iters inside quoted fields, which are allowed in the RFC pro\u0002posal [30]. It is thus impossible to determine the widow and\r\norphan of a CSV chunk only by analyzing the data in the\r\nchunk. However, under the restriction that record delim\u0002iters inside quoted fields need to be escaped, widows and\r\norphans can again be determined. In fact, as many appli\u0002cations produce CSV data that escapes the record delimiter\r\ninside quoted fields, we propose two loading options: a fast\r\nand a safe mode. The fast mode is intended for files that\r\nadhere to the restriction and splits the CSV input according\r\nto a fixed chunk size. A CSV chunk-processing task initially\r\nscans for the first unescaped record delimiter in its chunk2\r\nand starts processing the chunk data from there. When the\r\ntask reaches the end of its chunk, it continues processing\r\nby reading data from its subsequent chunk until it again\r\nfinds an unescaped record delimiter. In safe mode, a serial\r\ntask scans the CSV input and splits it into CSV chunks of at\r\nleast a certain chunk size. The task keeps track of quotation\r\nscopes and splits the input at record delimiters, such that no\r\nwidows and orphans are created. However, the performance\r\nof the safe mode is determined by the speed of the sequential\r\ntask. For our implementation, at a multiprogramming level\r\nof 8, the safe mode is 10% slower than the fast mode.\r\n3.4 Vectorization\r\nParsing, i.e., finding delimiters and other special charac\u0002ters, and input validation are commonly based on a char\u0002acter at a time comparison of CSV input with certain spe\u0002cial characters. These comparisons are usually implemented\r\nas if-then conditional branches. For efficient processing,\r\ncurrent general purpose CPUs need multiple instructions in\r\ntheir instruction pipeline. To fill this pipeline, the hardware\r\ntries to predict upcoming branches. However, in the case of\r\nparsing and deserialization, this is not efficiently possible,\r\nwhich leads to a significant number of branch miss penal\u0002ties [2]. It is thus desirable to reduce the number of control\r\nflow branches in the parsing and deserialization methods.\r\nOne such possibility is data-parallelization.\r\nModern general purpose CPUs are super-scalar multi-core\r\nprocessors that allow not only parallelization at the task\r\nlevel but also at the data level—via single instruction multi\u0002ple data (SIMD) instructions and dedicated execution units.\r\nData parallelization is also referred to as vectorization where\r\na single instruction is performed simultaneously on multiple\r\noperands, referred to as a vector. Vectorization in general\r\nbenefits performance and energy efficiency [15]. In the past,\r\nSIMD extensions of x86 CPUs like SSE and 3DNow! mostly\r\ntargeted multimedia and scientific computing applications.\r\nSSE 4.2 [15] adds additional byte-comparing instructions for\r\nstring and text processing.\r\nProgrammers can use vectorization instructions manually\r\nvia intrinsics. Modern compilers such as GCC also try to\r\nautomatically vectorize source code. This is, however, re\u0002stricted to specific code patterns. To the best of our knowl\u0002edge, no compiler can (yet) automatically vectorize code us\u0002ing SSE 4.2 instructions. This is due to the fact that using\r\nthese instructions requires non-trivial changes to the design\r\nof algorithms.\r\n2This might require reading data from the preceeding chunk.\r\n1705",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/61a2c4e9-6664-4c46-a74a-444535f382cf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f4f663a222cea4c8a3a0aef7d5a48d68017adb2a949d1295a108809b3bc73939",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1003
      },
      {
        "segments": [
          {
            "segment_id": "61a2c4e9-6664-4c46-a74a-444535f382cf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "buffer 1\r\nchunk n\r\nRelation\r\nPartition 1 Partition m\r\nbuffer 1 ... buffer m ... chunk n ... chunk 1 chunk 2\r\nMerge buffers with relation partitions:\r\nmerge tuples and indexes\r\n(partition-parallel)\r\ntask 1 task m\r\n1\r\n2\r\nProcess CSV chunks:\r\ndetermine orphan and parse, deserialize, partition, index each tuple (chunk-parallel)\r\nchunk 1 chunk 2\r\ntask 1\r\nwidow orphan\r\nCSV input . \\n 3 , A s i a \\n 4 , A u s t r a l i a \\n 5 , ... ... ... task 2 task n\r\nbuffer 1 buffer m buffer m\r\nFigure 4: Schematic overview of Instant Loading: from CSV input to relation partitions.\r\n3.3 Task-Parallelization\r\nTo allow synchronization-free task-parallelization of pars\u0002ing, deserialization, partition classification, and indexing, we\r\nsplit CSV input into independent CSV chunks that can be\r\nprocessed in parallel. The choice of the chunk size granu\u0002larity is challenging and impacts the parallelizability of the\r\nbulk loading process. The smaller the chunk size, the more\r\nchunk processing and merge steps can be interleaved. How\u0002ever, chunks should not be too small, as otherwise the over\u0002head of dealing with incomplete tuples at chunk borders\r\nincreases. Instant Loading splits the input according to a\r\nsize for which it can at least be guaranteed that, assuming\r\nthe input is well-formed, one complete tuple fits into a CSV\r\nchunk. Otherwise, parallelized parsing would be hindered.\r\nTo identify chunk sizes that allow for high-performance load\u0002ing, we evaluated our Instant Loading implementation with\r\nvarying chunk sizes (see Fig. 13). The evaluation leads us to\r\nthe conclusion that on a CPU with a last-level cache of size l\r\nand n hardware threads, the highest loading throughput can\r\nbe achieved with a CSV chunk size in the range of 0.25×l/n\r\nto 1.0 × l/n. E.g., a good chunk size on a current Intel Ivy\r\nBridge CPU with a 8 MB L3 cache and 8 hardware threads\r\nis in the range of 256 kB to 1 MB. When loading from a local\r\nI/O device, we use madvise to advise the kernel to prefetch\r\nthe CSV chunks.\r\nChunking CSV input according to a fixed size produces\r\nincomplete tuples at CSV chunk borders. We refer to these\r\ntuples as widows and orphans (cf., Fig. 4):\r\nDefinition (Widow and orphan). “An orphan has\r\nno past, a widow has no future” is a famous mnemonic in\r\ntypesetting. In typesetting, a widow is a line that ends and\r\nan orphan is a line that opens a paragraph and is separated\r\nfrom the rest of the paragraph by a page break, respectively.\r\nChunking CSV input creates a similar effect. A widow of a\r\nCSV chunk is an incomplete tuple at the end of a chunk that\r\nis separated from the part that would make it complete, i.e.,\r\nthe orphan, by a chunk border.\r\nUnfortunately, if chunk borders are chosen according to\r\na fixed size, CSV chunk-processing tasks can no longer dis\u0002tinguish between real record delimiters and record delim\u0002iters inside quoted fields, which are allowed in the RFC pro\u0002posal [30]. It is thus impossible to determine the widow and\r\norphan of a CSV chunk only by analyzing the data in the\r\nchunk. However, under the restriction that record delim\u0002iters inside quoted fields need to be escaped, widows and\r\norphans can again be determined. In fact, as many appli\u0002cations produce CSV data that escapes the record delimiter\r\ninside quoted fields, we propose two loading options: a fast\r\nand a safe mode. The fast mode is intended for files that\r\nadhere to the restriction and splits the CSV input according\r\nto a fixed chunk size. A CSV chunk-processing task initially\r\nscans for the first unescaped record delimiter in its chunk2\r\nand starts processing the chunk data from there. When the\r\ntask reaches the end of its chunk, it continues processing\r\nby reading data from its subsequent chunk until it again\r\nfinds an unescaped record delimiter. In safe mode, a serial\r\ntask scans the CSV input and splits it into CSV chunks of at\r\nleast a certain chunk size. The task keeps track of quotation\r\nscopes and splits the input at record delimiters, such that no\r\nwidows and orphans are created. However, the performance\r\nof the safe mode is determined by the speed of the sequential\r\ntask. For our implementation, at a multiprogramming level\r\nof 8, the safe mode is 10% slower than the fast mode.\r\n3.4 Vectorization\r\nParsing, i.e., finding delimiters and other special charac\u0002ters, and input validation are commonly based on a char\u0002acter at a time comparison of CSV input with certain spe\u0002cial characters. These comparisons are usually implemented\r\nas if-then conditional branches. For efficient processing,\r\ncurrent general purpose CPUs need multiple instructions in\r\ntheir instruction pipeline. To fill this pipeline, the hardware\r\ntries to predict upcoming branches. However, in the case of\r\nparsing and deserialization, this is not efficiently possible,\r\nwhich leads to a significant number of branch miss penal\u0002ties [2]. It is thus desirable to reduce the number of control\r\nflow branches in the parsing and deserialization methods.\r\nOne such possibility is data-parallelization.\r\nModern general purpose CPUs are super-scalar multi-core\r\nprocessors that allow not only parallelization at the task\r\nlevel but also at the data level—via single instruction multi\u0002ple data (SIMD) instructions and dedicated execution units.\r\nData parallelization is also referred to as vectorization where\r\na single instruction is performed simultaneously on multiple\r\noperands, referred to as a vector. Vectorization in general\r\nbenefits performance and energy efficiency [15]. In the past,\r\nSIMD extensions of x86 CPUs like SSE and 3DNow! mostly\r\ntargeted multimedia and scientific computing applications.\r\nSSE 4.2 [15] adds additional byte-comparing instructions for\r\nstring and text processing.\r\nProgrammers can use vectorization instructions manually\r\nvia intrinsics. Modern compilers such as GCC also try to\r\nautomatically vectorize source code. This is, however, re\u0002stricted to specific code patterns. To the best of our knowl\u0002edge, no compiler can (yet) automatically vectorize code us\u0002ing SSE 4.2 instructions. This is due to the fact that using\r\nthese instructions requires non-trivial changes to the design\r\nof algorithms.\r\n2This might require reading data from the preceeding chunk.\r\n1705",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/61a2c4e9-6664-4c46-a74a-444535f382cf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f4f663a222cea4c8a3a0aef7d5a48d68017adb2a949d1295a108809b3bc73939",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1003
      },
      {
        "segments": [
          {
            "segment_id": "1b5bb425-ed12-400a-8f0d-c8f14a80b0e5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "0 1 0 0\r\n0 0 0 0\r\n1 0 0 0\r\n\\\r\n0 0 0 0 c \\\r\nn \" | o\r\np\r\neran\r\nd1 0\r\n3\r\n\\n | 2 4\r\n3 operand2 0\r\n1 1 0 0\r\n2\r\nmask\r\nindex\r\nO\r\nR\r\nresult (a) EQUAL ANY\r\n1 1 1 1\r\n9\r\n1 0 1 1\r\n0\r\no\r\np\r\neran\r\nd1 0\r\n3\r\n7 E 5 7\r\n3 operand2 0\r\n0 1 0 0\r\n2\r\nmask (negated)\r\nindex\r\nGE\r\nLE AND\r\n(b) RANGES\r\nFigure 5: SSE 4.2 comparisons: (a) searching for\r\nspecial characters and (b) validating characters.\r\nCurrent x86 CPUs work on 128 bit SSE registers, i.e., 16\r\n8 bit characters per register. While the AVX instruction\r\nset increased SIMD register sizes to 256 bit, the SSE 4.2 in\u0002structions still work on 128 bit registers. It is of note that we\r\ndo not assume 16 byte aligned input for our SSE-optimized\r\nmethods. Even though aligned loads to SIMD registers had\r\nbeen significantly faster than unaligned loads in the past,\r\ncurrent generations of CPUs alleviate this penalty.\r\nSSE 4.2 includes instructions for the comparison of two\r\n16 byte operands of explicit or implicit lengths. We use the\r\nEQUAL ANY and RANGES comparison modes to speed up\r\nparsing and deserialization in Instant Loading: In EQUAL\r\nANY mode, each character in the second operand is checked\r\nwhether it is equal to any character in the first operand. In\r\nthe RANGES mode, each character in the second operand\r\nis checked whether it is in the ranges defined in the first\r\noperand. Each range is defined in pairs of two entries where\r\nthe first specifies the lower and the second the upper bound\r\nof the range. The result of intrinsics can either be a bitmask\r\nor an index that marks the first position of a hit. Results\r\ncan further be negated. Fig. 5 illustrates the two modes. For\r\npresentation purposes we narrowed the register size to 32 bit.\r\nTo improve parsing, we use EQUAL ANY to search for de\u0002limiters on a 16 byte at a time basis (cf., Fig. 5(a)). Branch\u0002ing is performed only if a special character is found. The\r\nfollowing pseudocode illustrates our method:\r\n1: procedure nextDelimiter(input,specialChars)\r\n2: while !endOfInput(input) do\r\n3: special = mm set epi8(specialChars)\r\n4: data = mm loadu si128(input)\r\n5: mode = SIDD CMP EQUAL ANY\r\n6: index = mm cmpistri(special,data,mode)\r\n7: if index < 16 then\r\n8: // handle special character\r\n9: input = input+16\r\nFor long fields, e.g., strings of variable length, finding the\r\nnext delimiter often requires to scan a lot more than 16 char\u0002acters. To improve parsing of these fields, we adapted the\r\nmethod shown above to compare 64 characters at a time:\r\nFirst, 64 byte (typically one cache line) are loaded into four\r\n128 bit SSE registers. For each of the registers a compar\u0002ison mask is generated using the _mm_cmpistrm intrinsic.\r\nThe four masks are interpreted as four 16 bit masks and are\r\nstored consecutively in one 64 bit integer where each bit indi\u0002cates if a special character is found at the position of the bit.\r\nIf the integer is 0, no special character was found. Otherwise,\r\nthe position of the first special byte is retrieved by count\u0002ing the number of trailing zeros. This operation is again\r\navailable as a CPU instruction and is thus highly efficient.\r\nTo improve deserialization methods, we use the RANGES\r\nmode for input validation (cf., Fig. 5(b)). We again illustrate\r\nour approach in form of pseudocode:\r\n1 Africa\r\n3 Asia\r\n4 Australia\r\n2 Antarctica\r\ninsert\r\nPartition\r\nPartition Buffer\r\n4 Australia\r\n3 Asia\r\n(a) insert-based\r\n1 Africa\r\n3 Asia\r\n4 Australia\r\n2 Antarctica\r\nPartition\r\nPartition Buffer\r\n3\r\n4 Australia\r\nAsia\r\nmemcpy memcpy\r\n(b) copy-based\r\nPartition\r\nchunks\r\n1 Africa\r\n2 Antarctica\r\nPartition Buffer\r\n3\r\n4 Australia\r\nAsia\r\nadd chunk reference\r\n(c) chunk-based\r\nFigure 6: Merging buffers with relation paritions.\r\n1: procedure deserializeIntegerSSE(input,length)\r\n2: if length < 4 then\r\n3: deserializeIntegerNoSSE(input,length)\r\n4: range = mm set epi8(0,...,0,’9’,’0’)\r\n5: data = mm loadu si128(input)\r\n6: mode = SIDD CMP RANGES| SIDD MASKED NEGATIVE POLARITY\r\n7: index = mm cmpestri(range,2,data,length,mode)\r\n8: if index != 16 then\r\n9: throw RuntimeException(\"invalid character\")\r\nExperiments have shown that for string lengths of less\r\nthan 4 byte, SSE optimized integer deserialization is slower\r\nthan a standard non-SSE variant with current x86 CPUs.\r\nFor integer deserialization we thus use a hybrid processing\r\nmodel where the SSE optimized variant is only used for\r\nstrings longer than 3 characters. Deserialization methods\r\nfor other data types were optimized analogously.\r\nThe evaluation in Sect. 5 shows that our vectorized meth\u0002ods reduce the number of branch misses significantly, im\u0002prove energy efficiency, and increase performance by about\r\n50% compared to non-vectorized methods.\r\n3.5 Partition Buffers\r\nCSV chunk-processing tasks store parsed and deserialized\r\ntuples as well as indexes on these tuples in partition buffers.\r\nThese buffers have the same physical layout as the relation\r\npartitions in order to avoid further transformations of data\r\nduring a merge step. In the following we discuss approaches\r\nto merge the tuples stored in a partition buffer with its cor\u0002responding relation partition in the storage backend (see\r\nFig. 6). Merging of indexes is discussed in the next sec\u0002tion. The insert- and copy-based approaches are viable for\r\ncontiguous memory-based as well as chunked storage back\u0002ends. The chunk-based approach requires a chunked storage\r\nbackend (see Sect. 2).\r\ninsert-based approach. The insert-based approach con\u0002stitutes the simplest approach. It iterates over the tuples in\r\nthe buffer and inserts the tuples one-by-one into the relation\r\npartition. This approach is obviously very simple to realize\r\nas insertion logic can be reused. However, its performance\r\nis bounded by the insertion speed of the storage backend.\r\ncopy-based approach. In contrast to the insert-based\r\napproach, the copy-based approach copies all tuples from the\r\nbuffer into the relation partition in one step. It is thereby\r\nfaster than the insert-based approach as it largely only de\u0002pends on the speed of the memcpy system call. We again\r\ntask-parallelized memcpying for large buffers to fully lever\u0002age the available memory bandwidth on modern hardware.\r\nNo additional transformations are necessary as the buffer\r\nalready uses the physical layout of the relation partition.\r\nchunk-based approach. For chunked storage backends\r\nthe memcpy system call can be avoided entirely. A merge\r\nstep then only consists of the insertion of a buffer reference\r\n1706",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/1b5bb425-ed12-400a-8f0d-c8f14a80b0e5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=263da9bef6092d0a48fbd3439dccecb6f0f178c08c2188d7c63f21b2ea2b3f6f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1044
      },
      {
        "segments": [
          {
            "segment_id": "1b5bb425-ed12-400a-8f0d-c8f14a80b0e5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "0 1 0 0\r\n0 0 0 0\r\n1 0 0 0\r\n\\\r\n0 0 0 0 c \\\r\nn \" | o\r\np\r\neran\r\nd1 0\r\n3\r\n\\n | 2 4\r\n3 operand2 0\r\n1 1 0 0\r\n2\r\nmask\r\nindex\r\nO\r\nR\r\nresult (a) EQUAL ANY\r\n1 1 1 1\r\n9\r\n1 0 1 1\r\n0\r\no\r\np\r\neran\r\nd1 0\r\n3\r\n7 E 5 7\r\n3 operand2 0\r\n0 1 0 0\r\n2\r\nmask (negated)\r\nindex\r\nGE\r\nLE AND\r\n(b) RANGES\r\nFigure 5: SSE 4.2 comparisons: (a) searching for\r\nspecial characters and (b) validating characters.\r\nCurrent x86 CPUs work on 128 bit SSE registers, i.e., 16\r\n8 bit characters per register. While the AVX instruction\r\nset increased SIMD register sizes to 256 bit, the SSE 4.2 in\u0002structions still work on 128 bit registers. It is of note that we\r\ndo not assume 16 byte aligned input for our SSE-optimized\r\nmethods. Even though aligned loads to SIMD registers had\r\nbeen significantly faster than unaligned loads in the past,\r\ncurrent generations of CPUs alleviate this penalty.\r\nSSE 4.2 includes instructions for the comparison of two\r\n16 byte operands of explicit or implicit lengths. We use the\r\nEQUAL ANY and RANGES comparison modes to speed up\r\nparsing and deserialization in Instant Loading: In EQUAL\r\nANY mode, each character in the second operand is checked\r\nwhether it is equal to any character in the first operand. In\r\nthe RANGES mode, each character in the second operand\r\nis checked whether it is in the ranges defined in the first\r\noperand. Each range is defined in pairs of two entries where\r\nthe first specifies the lower and the second the upper bound\r\nof the range. The result of intrinsics can either be a bitmask\r\nor an index that marks the first position of a hit. Results\r\ncan further be negated. Fig. 5 illustrates the two modes. For\r\npresentation purposes we narrowed the register size to 32 bit.\r\nTo improve parsing, we use EQUAL ANY to search for de\u0002limiters on a 16 byte at a time basis (cf., Fig. 5(a)). Branch\u0002ing is performed only if a special character is found. The\r\nfollowing pseudocode illustrates our method:\r\n1: procedure nextDelimiter(input,specialChars)\r\n2: while !endOfInput(input) do\r\n3: special = mm set epi8(specialChars)\r\n4: data = mm loadu si128(input)\r\n5: mode = SIDD CMP EQUAL ANY\r\n6: index = mm cmpistri(special,data,mode)\r\n7: if index < 16 then\r\n8: // handle special character\r\n9: input = input+16\r\nFor long fields, e.g., strings of variable length, finding the\r\nnext delimiter often requires to scan a lot more than 16 char\u0002acters. To improve parsing of these fields, we adapted the\r\nmethod shown above to compare 64 characters at a time:\r\nFirst, 64 byte (typically one cache line) are loaded into four\r\n128 bit SSE registers. For each of the registers a compar\u0002ison mask is generated using the _mm_cmpistrm intrinsic.\r\nThe four masks are interpreted as four 16 bit masks and are\r\nstored consecutively in one 64 bit integer where each bit indi\u0002cates if a special character is found at the position of the bit.\r\nIf the integer is 0, no special character was found. Otherwise,\r\nthe position of the first special byte is retrieved by count\u0002ing the number of trailing zeros. This operation is again\r\navailable as a CPU instruction and is thus highly efficient.\r\nTo improve deserialization methods, we use the RANGES\r\nmode for input validation (cf., Fig. 5(b)). We again illustrate\r\nour approach in form of pseudocode:\r\n1 Africa\r\n3 Asia\r\n4 Australia\r\n2 Antarctica\r\ninsert\r\nPartition\r\nPartition Buffer\r\n4 Australia\r\n3 Asia\r\n(a) insert-based\r\n1 Africa\r\n3 Asia\r\n4 Australia\r\n2 Antarctica\r\nPartition\r\nPartition Buffer\r\n3\r\n4 Australia\r\nAsia\r\nmemcpy memcpy\r\n(b) copy-based\r\nPartition\r\nchunks\r\n1 Africa\r\n2 Antarctica\r\nPartition Buffer\r\n3\r\n4 Australia\r\nAsia\r\nadd chunk reference\r\n(c) chunk-based\r\nFigure 6: Merging buffers with relation paritions.\r\n1: procedure deserializeIntegerSSE(input,length)\r\n2: if length < 4 then\r\n3: deserializeIntegerNoSSE(input,length)\r\n4: range = mm set epi8(0,...,0,’9’,’0’)\r\n5: data = mm loadu si128(input)\r\n6: mode = SIDD CMP RANGES| SIDD MASKED NEGATIVE POLARITY\r\n7: index = mm cmpestri(range,2,data,length,mode)\r\n8: if index != 16 then\r\n9: throw RuntimeException(\"invalid character\")\r\nExperiments have shown that for string lengths of less\r\nthan 4 byte, SSE optimized integer deserialization is slower\r\nthan a standard non-SSE variant with current x86 CPUs.\r\nFor integer deserialization we thus use a hybrid processing\r\nmodel where the SSE optimized variant is only used for\r\nstrings longer than 3 characters. Deserialization methods\r\nfor other data types were optimized analogously.\r\nThe evaluation in Sect. 5 shows that our vectorized meth\u0002ods reduce the number of branch misses significantly, im\u0002prove energy efficiency, and increase performance by about\r\n50% compared to non-vectorized methods.\r\n3.5 Partition Buffers\r\nCSV chunk-processing tasks store parsed and deserialized\r\ntuples as well as indexes on these tuples in partition buffers.\r\nThese buffers have the same physical layout as the relation\r\npartitions in order to avoid further transformations of data\r\nduring a merge step. In the following we discuss approaches\r\nto merge the tuples stored in a partition buffer with its cor\u0002responding relation partition in the storage backend (see\r\nFig. 6). Merging of indexes is discussed in the next sec\u0002tion. The insert- and copy-based approaches are viable for\r\ncontiguous memory-based as well as chunked storage back\u0002ends. The chunk-based approach requires a chunked storage\r\nbackend (see Sect. 2).\r\ninsert-based approach. The insert-based approach con\u0002stitutes the simplest approach. It iterates over the tuples in\r\nthe buffer and inserts the tuples one-by-one into the relation\r\npartition. This approach is obviously very simple to realize\r\nas insertion logic can be reused. However, its performance\r\nis bounded by the insertion speed of the storage backend.\r\ncopy-based approach. In contrast to the insert-based\r\napproach, the copy-based approach copies all tuples from the\r\nbuffer into the relation partition in one step. It is thereby\r\nfaster than the insert-based approach as it largely only de\u0002pends on the speed of the memcpy system call. We again\r\ntask-parallelized memcpying for large buffers to fully lever\u0002age the available memory bandwidth on modern hardware.\r\nNo additional transformations are necessary as the buffer\r\nalready uses the physical layout of the relation partition.\r\nchunk-based approach. For chunked storage backends\r\nthe memcpy system call can be avoided entirely. A merge\r\nstep then only consists of the insertion of a buffer reference\r\n1706",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/1b5bb425-ed12-400a-8f0d-c8f14a80b0e5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=263da9bef6092d0a48fbd3439dccecb6f0f178c08c2188d7c63f21b2ea2b3f6f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1044
      },
      {
        "segments": [
          {
            "segment_id": "9728256d-74a5-4983-b5b2-b5e00c327f8d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "into a list of chunk references in the backend. While merging\r\ntime is minimal, too small and too many chunks negatively\r\nimpact table scan and random access performance of the\r\nbackend due to caching effects. In general, it is advanta\u0002geous to have a small list of chunk references. Preferably,\r\nthe list should fit in the CPU caches, so that it can be ac\u0002cessed efficiently. For Instant Loading, we are faced with the\r\ntradeoff between using small CSV chunk sizes for a high de\u0002gree of task-parallelization (cf., Sect. 3.3) and creating large\r\nstorage backend chunks to keep the backend efficient.\r\nOne way to meet this challenge is to store the partition\r\nbuffer references of CSV chunk processing tasks in thread\u0002local storage. Partition buffers are then reused as threads\r\nare reused by the TBB library. Hence, the expected mean\r\nsize of relation partition chunks is the CSV input size divided\r\nby the number of hardware threads used for loading. Nev\u0002ertheless, this is no panacea. If partition buffers are reused,\r\nmerging of partition buffers with the relation can no longer\r\nbe interleaved with CSV chunk processing. Furthermore,\r\nthis approach requires CSV input to be of a respective size.\r\nFor chunked storage backends it can thus also make sense\r\nto use copy-based merging or a hybrid approach. We intend\r\nto investigate further merge algorithms for various types of\r\nchunked storage backends in future work.\r\nBuffer allocation. Allocation and reallocation of parti\u0002tion buffers on the heap is costly as, in general, it needs to\r\nbe synchronized. Using scalable allocators that provide per\u0002thread heaps is not an option as these are usually too small\r\nfor loading purposes where huge amounts of data are moved.\r\nWhile an initial allocation of a buffer is unavoidable, reallo\u0002cations can be saved by initially allocating enough memory\r\nfor the tuples in a CSV chunk. The difficulty lies in the\r\nestimation of the number of tuples in a CSV chunk of a cer\u0002tain size. This is mainly due to nullable attributes and at\u0002tributes of varying lengths. Our solution is to let CSV chunk\r\nprocessing tasks atomically update cardinality estimates for\r\nthe partition buffers that serve as allocation hints for future\r\ntasks. For our implementation, at a multiprogramming level\r\nof 8, this allocation strategy increases performance by about\r\n5% compared to dynamic allocation.\r\nFor hybrid OLTP&OLAP databases like HyPer, it fur\u0002ther makes sense to allocate partition buffers on huge virtual\r\nmemory pages. Huge pages have the advantage they have a\r\nseparate section in the memory management unit (MMU) on\r\nmost platforms. Hence, loading and mission-critical OLTP\r\ncompete less for the transaction lookaside buffer (TLB).\r\n3.6 Bulk Creation of Index Structures\r\nIndexes have a decisive impact on transaction and query\r\nexecution performance. However, there is a tradeoff be\u0002tween time spent on index creation and time saved dur\u0002ing query and transaction processing. Using standard ap\u0002proaches, creating indexes during bulk loading can signifi\u0002cantly slow down the loading throughput. Alternatives to\r\nthe creation of indexes at load time such as database crack\u0002ing [13] and adaptive indexing [14] propose to create indexes\r\nas a by-product of query processing and thereby allow faster\r\ndata loading and fast query performance over time. How\u0002ever, if data is bulk loaded to a mission-critical OLTP or\r\nOLAP system that needs execution time guarantees imme\u0002diately after loading, delayed index creation is not an option.\r\nThis is especially true for our proposed data staging process\u0002ing model where data is loaded, processed, and unloaded in\r\ncycles. Furthermore, to assure consistency, loading should\r\nat least check for primary key violations. We thus advo\u0002cate for the creation of primary indexes at load time. With\r\nInstant Loading, it is our goal to achieve this at wire speed.\r\nWe identified different options regarding how and when to\r\ncreate indexes during loading. The first option is to always\r\nhave a single index for the whole relation that is incremen\u0002tally updated by inserting keys of new tuples after they have\r\nbeen added to the relation. The second option is to com\u0002pletely recreate a new index from scratch. The first option\r\nis limited by the insertion speed of the index structure. The\r\nsecond option could benefit from index structures that allow\r\nthe efficient recreation of an index. However, depending on\r\nthe size of the relation, this might impose a huge overhead.\r\nWe thus propose a third way: each CSV chunk-processing\r\ntask maintains indexes in its partition buffers. These indexes\r\nare then merged with the indexes in the relation partition\r\nduring the merge step. We define indexes that allow our\r\napproach as merge-able index structures for bulk loading:\r\nDefinition (Merge-able index structures for bulk\r\nloading). Merge-able index structures for bulk loading are\r\nindex structures that allow the efficient and parallelized cre\u0002ation of the set of indexes I = {I1, . . . , In} over a set of keys\r\nK = {k1, . . . , km}, where K is partitioned into n nonempty\r\ndisjoint subsets K1, . . . , Kn and Ij is an index over Kj for\r\n1 ≤ j ≤ n. Further, there exists an efficient parallelized\r\nmerge function that, given I, yields a single unified index\r\nover K. The unified index creation time t is the aggregate\r\nof time needed to create I and time needed to merge I. For\r\nmerge-able index structures for bulk loading, t proportion\u0002ally decreases with an increasing number n of key partitions\r\nassuming n available hardware threads.\r\nIn the following we show that hash tables with chaining\r\nand the adaptive radix tree (ART) [20] are merge-able in\u0002dex structures for bulk loading. Our evaluation (see Sect. 5)\r\nfurther demonstrates that parallelized forms of these indexes\r\nachieve a near-linear speedup with the number of key parti\u0002tions and hardware threads used for bulk index creation.\r\n3.6.1 Hash table with chaining\r\nHash tables are a popular in-memory data structure and\r\nare often used for indexes in main memory databases. In\u0002dexes based on hash tables only allow point queries but\r\nare very fast due to their expected lookup time of O(1).\r\nHash tables inevitably face the problem of hash collisions.\r\nStrategies for conflict resolution include open addressing and\r\nchaining. Hash tables that use chaining for conflict resolu\u0002tion are particularly suitable as merge-able indexes for bulk\r\nloading. Our implementation of a merge-able hash table\r\nfor bulk loading uses a fixed-sized hash table, where entries\r\nwith the same hash value are chained in a linked list. For\r\na given partitioned key range, equally-sized hash tables us\u0002ing the same hash function are, in parallel, created for each\r\npartition. These hash tables are then repeatedly merged in\r\npairs of two by scanning one of the tables and concatenat\u0002ing each list entry for a specific hash value with the list for\r\nthat hash value in the other hash table. The scan operation\r\ncan thereby again be parallelized efficiently. It is of note\r\nthat a space-time tradeoff is immanent in hash table-based\r\nindex approaches. Our merge-able hash table with chaining\r\nallocates a fixed size hash table for each parallel task and is\r\nthus wasting space. In contrast to hash tables, the adaptive\r\nradix tree is highly space-efficient.\r\n1707",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/9728256d-74a5-4983-b5b2-b5e00c327f8d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f78d98e2c950f11f9e759475986fdfbc5281d6f67682da15411595eb9f02920",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1173
      },
      {
        "segments": [
          {
            "segment_id": "9728256d-74a5-4983-b5b2-b5e00c327f8d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "into a list of chunk references in the backend. While merging\r\ntime is minimal, too small and too many chunks negatively\r\nimpact table scan and random access performance of the\r\nbackend due to caching effects. In general, it is advanta\u0002geous to have a small list of chunk references. Preferably,\r\nthe list should fit in the CPU caches, so that it can be ac\u0002cessed efficiently. For Instant Loading, we are faced with the\r\ntradeoff between using small CSV chunk sizes for a high de\u0002gree of task-parallelization (cf., Sect. 3.3) and creating large\r\nstorage backend chunks to keep the backend efficient.\r\nOne way to meet this challenge is to store the partition\r\nbuffer references of CSV chunk processing tasks in thread\u0002local storage. Partition buffers are then reused as threads\r\nare reused by the TBB library. Hence, the expected mean\r\nsize of relation partition chunks is the CSV input size divided\r\nby the number of hardware threads used for loading. Nev\u0002ertheless, this is no panacea. If partition buffers are reused,\r\nmerging of partition buffers with the relation can no longer\r\nbe interleaved with CSV chunk processing. Furthermore,\r\nthis approach requires CSV input to be of a respective size.\r\nFor chunked storage backends it can thus also make sense\r\nto use copy-based merging or a hybrid approach. We intend\r\nto investigate further merge algorithms for various types of\r\nchunked storage backends in future work.\r\nBuffer allocation. Allocation and reallocation of parti\u0002tion buffers on the heap is costly as, in general, it needs to\r\nbe synchronized. Using scalable allocators that provide per\u0002thread heaps is not an option as these are usually too small\r\nfor loading purposes where huge amounts of data are moved.\r\nWhile an initial allocation of a buffer is unavoidable, reallo\u0002cations can be saved by initially allocating enough memory\r\nfor the tuples in a CSV chunk. The difficulty lies in the\r\nestimation of the number of tuples in a CSV chunk of a cer\u0002tain size. This is mainly due to nullable attributes and at\u0002tributes of varying lengths. Our solution is to let CSV chunk\r\nprocessing tasks atomically update cardinality estimates for\r\nthe partition buffers that serve as allocation hints for future\r\ntasks. For our implementation, at a multiprogramming level\r\nof 8, this allocation strategy increases performance by about\r\n5% compared to dynamic allocation.\r\nFor hybrid OLTP&OLAP databases like HyPer, it fur\u0002ther makes sense to allocate partition buffers on huge virtual\r\nmemory pages. Huge pages have the advantage they have a\r\nseparate section in the memory management unit (MMU) on\r\nmost platforms. Hence, loading and mission-critical OLTP\r\ncompete less for the transaction lookaside buffer (TLB).\r\n3.6 Bulk Creation of Index Structures\r\nIndexes have a decisive impact on transaction and query\r\nexecution performance. However, there is a tradeoff be\u0002tween time spent on index creation and time saved dur\u0002ing query and transaction processing. Using standard ap\u0002proaches, creating indexes during bulk loading can signifi\u0002cantly slow down the loading throughput. Alternatives to\r\nthe creation of indexes at load time such as database crack\u0002ing [13] and adaptive indexing [14] propose to create indexes\r\nas a by-product of query processing and thereby allow faster\r\ndata loading and fast query performance over time. How\u0002ever, if data is bulk loaded to a mission-critical OLTP or\r\nOLAP system that needs execution time guarantees imme\u0002diately after loading, delayed index creation is not an option.\r\nThis is especially true for our proposed data staging process\u0002ing model where data is loaded, processed, and unloaded in\r\ncycles. Furthermore, to assure consistency, loading should\r\nat least check for primary key violations. We thus advo\u0002cate for the creation of primary indexes at load time. With\r\nInstant Loading, it is our goal to achieve this at wire speed.\r\nWe identified different options regarding how and when to\r\ncreate indexes during loading. The first option is to always\r\nhave a single index for the whole relation that is incremen\u0002tally updated by inserting keys of new tuples after they have\r\nbeen added to the relation. The second option is to com\u0002pletely recreate a new index from scratch. The first option\r\nis limited by the insertion speed of the index structure. The\r\nsecond option could benefit from index structures that allow\r\nthe efficient recreation of an index. However, depending on\r\nthe size of the relation, this might impose a huge overhead.\r\nWe thus propose a third way: each CSV chunk-processing\r\ntask maintains indexes in its partition buffers. These indexes\r\nare then merged with the indexes in the relation partition\r\nduring the merge step. We define indexes that allow our\r\napproach as merge-able index structures for bulk loading:\r\nDefinition (Merge-able index structures for bulk\r\nloading). Merge-able index structures for bulk loading are\r\nindex structures that allow the efficient and parallelized cre\u0002ation of the set of indexes I = {I1, . . . , In} over a set of keys\r\nK = {k1, . . . , km}, where K is partitioned into n nonempty\r\ndisjoint subsets K1, . . . , Kn and Ij is an index over Kj for\r\n1 ≤ j ≤ n. Further, there exists an efficient parallelized\r\nmerge function that, given I, yields a single unified index\r\nover K. The unified index creation time t is the aggregate\r\nof time needed to create I and time needed to merge I. For\r\nmerge-able index structures for bulk loading, t proportion\u0002ally decreases with an increasing number n of key partitions\r\nassuming n available hardware threads.\r\nIn the following we show that hash tables with chaining\r\nand the adaptive radix tree (ART) [20] are merge-able in\u0002dex structures for bulk loading. Our evaluation (see Sect. 5)\r\nfurther demonstrates that parallelized forms of these indexes\r\nachieve a near-linear speedup with the number of key parti\u0002tions and hardware threads used for bulk index creation.\r\n3.6.1 Hash table with chaining\r\nHash tables are a popular in-memory data structure and\r\nare often used for indexes in main memory databases. In\u0002dexes based on hash tables only allow point queries but\r\nare very fast due to their expected lookup time of O(1).\r\nHash tables inevitably face the problem of hash collisions.\r\nStrategies for conflict resolution include open addressing and\r\nchaining. Hash tables that use chaining for conflict resolu\u0002tion are particularly suitable as merge-able indexes for bulk\r\nloading. Our implementation of a merge-able hash table\r\nfor bulk loading uses a fixed-sized hash table, where entries\r\nwith the same hash value are chained in a linked list. For\r\na given partitioned key range, equally-sized hash tables us\u0002ing the same hash function are, in parallel, created for each\r\npartition. These hash tables are then repeatedly merged in\r\npairs of two by scanning one of the tables and concatenat\u0002ing each list entry for a specific hash value with the list for\r\nthat hash value in the other hash table. The scan operation\r\ncan thereby again be parallelized efficiently. It is of note\r\nthat a space-time tradeoff is immanent in hash table-based\r\nindex approaches. Our merge-able hash table with chaining\r\nallocates a fixed size hash table for each parallel task and is\r\nthus wasting space. In contrast to hash tables, the adaptive\r\nradix tree is highly space-efficient.\r\n1707",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/9728256d-74a5-4983-b5b2-b5e00c327f8d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f78d98e2c950f11f9e759475986fdfbc5281d6f67682da15411595eb9f02920",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1173
      },
      {
        "segments": [
          {
            "segment_id": "f5b48748-a60c-424b-8864-b58e23575e2a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "ANT AND ANY ART\r\nA\r\nN R\r\nT D Y T (a) tree 1\r\nARE BED BEE\r\nA B\r\nR\r\nE\r\nE\r\nD E (b) tree 2\r\nE T\r\nR\r\nA\r\nE\r\nD E\r\ndigit 1\r\ndigit 2\r\ndigit 3\r\nBED BEE leaves\r\nB\r\nN\r\nANT AND ANY ART\r\nT D Y\r\nARE\r\nupdated reference\r\nnew node\r\n(c) merged tree\r\nFigure 7: Two adaptive radix trees (ART) (a) and\r\n(b) and the result of merging the two trees (c).\r\n3.6.2 Adaptive Radix Tree (ART)\r\nThe adaptive radix tree (ART) [20] is a high performance\r\nand space-efficient general purpose index structure for main\r\nmemory databases that is tuned for modern hardware. Com\u0002pared to hash tables, radix trees, also known as tries, di\u0002rectly use the digital representation of keys for comparison.\r\nThe idea of a radix tree is similar to that of a thumb index of\r\ndictionaries, which indexes its entries according to their first\r\ncharacter prefix. Radix trees use this technique recursively\r\nuntil a specific entry is found. An example of an ART index\r\nis shown in Fig. 7(a). ART is a byte-wise radix tree that\r\nuses the individual bytes of a key for indexing. As a result,\r\nall operations have a complexity of O(k), where k is the byte\r\nlength of the indexed keys. Compared to hash tables, which\r\nare not order-preserving, radix trees store keys in their lex\u0002icographical order. This allows not only exact lookups but\r\nalso range scans, prefix lookups, and top-k queries.\r\nWhile other radix tree implementations rely on a glob\u0002ally fixed fanout parameter and thus have to trade off tree\r\nheight against space efficiency, ART distinguishes itself from\r\nthese implementations by using adaptively sized nodes. In\r\nART, nodes are represented using four types of efficient and\r\ncompact data structures with different sizes of up to 256 en\u0002tries. The type of a node is chosen dynamically depending\r\non the number of child nodes, which optimizes space utiliza\u0002tion and access efficiency at the same time. The evaluation\r\nin [20] shows that ART is the fastest general purpose index\r\nstructure for main memory databases optimized for mod\u0002ern hardware. Its performance is only met by hash tables,\r\nwhich, however, only support exact key lookups.\r\nIn this work we show that ART further belongs to the class\r\nof merge-able index structures for bulk loading by specifying\r\nan efficient parallelized merge algorithm. Fig. 7 illustrates\r\nthe merging of two ART indexes. Radix trees in general are\r\nnaturally suited for efficient parallelized merging: starting\r\nwith the two root nodes, for each pair of nodes, children with\r\ncommon prefixes in the two trees are recursively merged in\r\nparallel. When all children with common prefixes have been\r\nmerged, children of the smaller node that have no match\r\nin the bigger node are inserted into the bigger node. This\r\nbigger node is then used in the merged tree. Ideally, merging\r\nis thus reducible to a single insertion for non-empty trees.\r\nIn the worst case, both trees contain only keys with common\r\nprefixes and nodes at maximum depth need to be merged.\r\nIn general, merging of two radix trees t1 and t2 needs O(d)\r\ncopy operations, where d is the minimum of diff (t1, t2) and\r\ndiff (t2, t1), where diff (x, y) is the number of inner nodes\r\nand leaves of y that are not present in x and are children of\r\na node that does not already count towards this number.\r\nOur parallelized merge algorithm looks as follows:\r\n1: procedure merge(t1,t2,depth)\r\n2: if isLeaf(t1) then insert(t2,t1.keyByte,t1,depth)\r\n3: return t2\r\n4: if isLeaf(t2) then insert(t1,t2.keyByte,t2,depth)\r\n5: return t1\r\n6: // ensure that t1 is the bigger node\r\n7: if t1.count > t2.count then swap(t1,t2)\r\n8: // descend trees in parallel for common key bytes\r\n9: parallel for each entry e in t2 do\r\n10: c = findChildPtr(t1,e.keyByte)\r\n11: if c then c = merge((c,e.child,depth+1))\r\n12: // sequentially insert t2’s unique entries in t1\r\n13: for each entry e in t2 do\r\n14: c = findChildPtr(t1,e.keyByte)\r\n15: if !c then insert(t1,e.keyByte,e.child,depth)\r\n16: return t1\r\nAs mentioned before, we insert entries of key bytes of\r\nthe smaller node that have no match in the bigger node se\u0002quentially and after all children with common prefixes have\r\nbeen merged in parallel. In ART, this separation into par\u0002allel and sequential phases is particularly due to the fact\r\nthat nodes can grow when inserting new entries. For the\r\nbiggest node type, which is essentially an array of size 256,\r\ninsertions can further be parallelized using lock-free atomic\r\noperations. This kind of insertion parallelization is also ap\u0002plicable to other radix trees that work with nodes of a fixed\r\nsize. It is indeed also feasible to implement a completely\r\nlock-free version of ART, which is, however, out of scope for\r\nthis work, as we focused on an efficient merge algorithm.\r\n4. INSTANT LOADING IN HYPER\r\n4.1 The HyPer Main Memory Database\r\nWe integrated our generic Instant Loading approach in\r\nHyPer [19], our high-performance relational main memory\r\ndatabase system. HyPer belongs to an emerging class of\r\nhybrid databases, which enable real-time business intelli\u0002gence by evaluating OLAP queries directly in the trans\u0002actional database. Using a novel snapshotting technique,\r\nHyPer achieves highest performance—compared to state of\r\nthe art in-memory databases—for both, OLTP and OLAP\r\nworkloads, operating simultaneously on the same database.\r\nOLAP is decoupled from mission-critical OLTP using a\r\nsnapshot mechanism with (almost) no synchronization over\u0002head. The mechanism is based on the POSIX system call\r\nfork(): OLAP queries are executed in a process that is\r\nforked from the OLTP process (see Fig. 8). This is very\r\nefficient as only the virtual page table of the OLTP process\r\nis copied. The operating system uses the processor’s mem\u0002ory management unit to implement efficient copy-on-update\r\nsemantics for snapshotted pages. Whenever the OLTP pro\u0002cess modifies a snapshotted page for the first time, the page\r\nis replicated in the forked process (see Fig. 8).\r\nTransactions are specified in SQL or in a PL/SQL style\r\nscripting language and are compiled into machine code us\u0002ing the LLVM compiler framework [22]. Together with the\r\nelimination of ballast caused by buffer management, locking,\r\nand latching, HyPer can process more than 100,000 TPC-C\r\n1708",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/f5b48748-a60c-424b-8864-b58e23575e2a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6c8f82ff4d0746f1630f23f267406febc0200945d67c85fbf1432e6b4f670026",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1017
      },
      {
        "segments": [
          {
            "segment_id": "f5b48748-a60c-424b-8864-b58e23575e2a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "ANT AND ANY ART\r\nA\r\nN R\r\nT D Y T (a) tree 1\r\nARE BED BEE\r\nA B\r\nR\r\nE\r\nE\r\nD E (b) tree 2\r\nE T\r\nR\r\nA\r\nE\r\nD E\r\ndigit 1\r\ndigit 2\r\ndigit 3\r\nBED BEE leaves\r\nB\r\nN\r\nANT AND ANY ART\r\nT D Y\r\nARE\r\nupdated reference\r\nnew node\r\n(c) merged tree\r\nFigure 7: Two adaptive radix trees (ART) (a) and\r\n(b) and the result of merging the two trees (c).\r\n3.6.2 Adaptive Radix Tree (ART)\r\nThe adaptive radix tree (ART) [20] is a high performance\r\nand space-efficient general purpose index structure for main\r\nmemory databases that is tuned for modern hardware. Com\u0002pared to hash tables, radix trees, also known as tries, di\u0002rectly use the digital representation of keys for comparison.\r\nThe idea of a radix tree is similar to that of a thumb index of\r\ndictionaries, which indexes its entries according to their first\r\ncharacter prefix. Radix trees use this technique recursively\r\nuntil a specific entry is found. An example of an ART index\r\nis shown in Fig. 7(a). ART is a byte-wise radix tree that\r\nuses the individual bytes of a key for indexing. As a result,\r\nall operations have a complexity of O(k), where k is the byte\r\nlength of the indexed keys. Compared to hash tables, which\r\nare not order-preserving, radix trees store keys in their lex\u0002icographical order. This allows not only exact lookups but\r\nalso range scans, prefix lookups, and top-k queries.\r\nWhile other radix tree implementations rely on a glob\u0002ally fixed fanout parameter and thus have to trade off tree\r\nheight against space efficiency, ART distinguishes itself from\r\nthese implementations by using adaptively sized nodes. In\r\nART, nodes are represented using four types of efficient and\r\ncompact data structures with different sizes of up to 256 en\u0002tries. The type of a node is chosen dynamically depending\r\non the number of child nodes, which optimizes space utiliza\u0002tion and access efficiency at the same time. The evaluation\r\nin [20] shows that ART is the fastest general purpose index\r\nstructure for main memory databases optimized for mod\u0002ern hardware. Its performance is only met by hash tables,\r\nwhich, however, only support exact key lookups.\r\nIn this work we show that ART further belongs to the class\r\nof merge-able index structures for bulk loading by specifying\r\nan efficient parallelized merge algorithm. Fig. 7 illustrates\r\nthe merging of two ART indexes. Radix trees in general are\r\nnaturally suited for efficient parallelized merging: starting\r\nwith the two root nodes, for each pair of nodes, children with\r\ncommon prefixes in the two trees are recursively merged in\r\nparallel. When all children with common prefixes have been\r\nmerged, children of the smaller node that have no match\r\nin the bigger node are inserted into the bigger node. This\r\nbigger node is then used in the merged tree. Ideally, merging\r\nis thus reducible to a single insertion for non-empty trees.\r\nIn the worst case, both trees contain only keys with common\r\nprefixes and nodes at maximum depth need to be merged.\r\nIn general, merging of two radix trees t1 and t2 needs O(d)\r\ncopy operations, where d is the minimum of diff (t1, t2) and\r\ndiff (t2, t1), where diff (x, y) is the number of inner nodes\r\nand leaves of y that are not present in x and are children of\r\na node that does not already count towards this number.\r\nOur parallelized merge algorithm looks as follows:\r\n1: procedure merge(t1,t2,depth)\r\n2: if isLeaf(t1) then insert(t2,t1.keyByte,t1,depth)\r\n3: return t2\r\n4: if isLeaf(t2) then insert(t1,t2.keyByte,t2,depth)\r\n5: return t1\r\n6: // ensure that t1 is the bigger node\r\n7: if t1.count > t2.count then swap(t1,t2)\r\n8: // descend trees in parallel for common key bytes\r\n9: parallel for each entry e in t2 do\r\n10: c = findChildPtr(t1,e.keyByte)\r\n11: if c then c = merge((c,e.child,depth+1))\r\n12: // sequentially insert t2’s unique entries in t1\r\n13: for each entry e in t2 do\r\n14: c = findChildPtr(t1,e.keyByte)\r\n15: if !c then insert(t1,e.keyByte,e.child,depth)\r\n16: return t1\r\nAs mentioned before, we insert entries of key bytes of\r\nthe smaller node that have no match in the bigger node se\u0002quentially and after all children with common prefixes have\r\nbeen merged in parallel. In ART, this separation into par\u0002allel and sequential phases is particularly due to the fact\r\nthat nodes can grow when inserting new entries. For the\r\nbiggest node type, which is essentially an array of size 256,\r\ninsertions can further be parallelized using lock-free atomic\r\noperations. This kind of insertion parallelization is also ap\u0002plicable to other radix trees that work with nodes of a fixed\r\nsize. It is indeed also feasible to implement a completely\r\nlock-free version of ART, which is, however, out of scope for\r\nthis work, as we focused on an efficient merge algorithm.\r\n4. INSTANT LOADING IN HYPER\r\n4.1 The HyPer Main Memory Database\r\nWe integrated our generic Instant Loading approach in\r\nHyPer [19], our high-performance relational main memory\r\ndatabase system. HyPer belongs to an emerging class of\r\nhybrid databases, which enable real-time business intelli\u0002gence by evaluating OLAP queries directly in the trans\u0002actional database. Using a novel snapshotting technique,\r\nHyPer achieves highest performance—compared to state of\r\nthe art in-memory databases—for both, OLTP and OLAP\r\nworkloads, operating simultaneously on the same database.\r\nOLAP is decoupled from mission-critical OLTP using a\r\nsnapshot mechanism with (almost) no synchronization over\u0002head. The mechanism is based on the POSIX system call\r\nfork(): OLAP queries are executed in a process that is\r\nforked from the OLTP process (see Fig. 8). This is very\r\nefficient as only the virtual page table of the OLTP process\r\nis copied. The operating system uses the processor’s mem\u0002ory management unit to implement efficient copy-on-update\r\nsemantics for snapshotted pages. Whenever the OLTP pro\u0002cess modifies a snapshotted page for the first time, the page\r\nis replicated in the forked process (see Fig. 8).\r\nTransactions are specified in SQL or in a PL/SQL style\r\nscripting language and are compiled into machine code us\u0002ing the LLVM compiler framework [22]. Together with the\r\nelimination of ballast caused by buffer management, locking,\r\nand latching, HyPer can process more than 100,000 TPC-C\r\n1708",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/f5b48748-a60c-424b-8864-b58e23575e2a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6c8f82ff4d0746f1630f23f267406febc0200945d67c85fbf1432e6b4f670026",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1017
      },
      {
        "segments": [
          {
            "segment_id": "2c4f336b-c1c8-435f-b37f-c62793d5deba",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "updates\r\nfork()\r\nOLTP data\r\nOLAP snapshot\r\nqueries\r\ncopy-on-update\r\nA\r\nB\r\nA'\r\nB\r\nreplicated page\r\n(due to update of A)\r\nupdate A to A' Figure 8: HyPer’s snapshotting mechanism.\r\ntransactions per second in a single thread on modern hard\u0002ware [19]. Similar to the design pioneered by H-Store [18]\r\nand VoltDB, HyPer also implements a partitioned execution\r\nmodel: The database is partitioned in a way that most trans\u0002actions only need to access a single partition. Transactions\r\nthat each exclusively work on a different partition can thus\r\nbe processed in parallel without synchronization. Synchro\u0002nization is only necessary for partition-crossing transactions.\r\nLike transactions, SQL queries are compiled into LLVM\r\ncode [22]. The data-centric compiler aims at good code and\r\ndata locality and a predictable branch layout. LLVM code\r\nis then compiled into efficient machine code using LLVM’s\r\njust-in-time compiler. Together with its advanced query op\u0002timizer, HyPer achieves superior query response times [19]\r\ncomparable to those of MonetDB [4] or Vectorwise.\r\n4.2 Instant Loading in HyPer\r\nInstant Loading in HyPer allows (lwu)* workflows but can\r\nindeed also be used for other use cases that require the load\u0002ing of CSV data. This includes initial loads and incremental\r\nloads for continuous data integration.\r\nThe interface of Instant Loading in HyPer is designed in\r\nthe style of the PostgreSQL COPY operator. Instant Loading\r\ntakes CSV input, the schema it adheres to, and the CSV\r\nspecial characters as input. Except for \"\\r\\n\", which we\r\nallow to be used as a record delimiter, we assume that spe\u0002cial characters are single ASCII characters. For each rela\u0002tion that is created or altered, we generate LLVM glue code\r\nfunctions for the processing of CSV chunks and for partition\r\nbuffer merging (cf., the two steps in Fig. 4). Code genera\u0002tion and compilation of these functions at runtime has the\r\nadvantage that the resulting code has good locality and pre\u0002dictable branching as the relation layout, e.g., the number\r\nof attributes and the attribute types, are known. Searching\r\nfor delimiters and the deserialization methods are imple\u0002mented as generic C++ functions that are not tailored to\r\nthe design of HyPer. Just like the LLVM functions HyPer\r\ncompiles for transactions and queries [22], the Instant Load\u0002ing LLVM glue code calls these statically compiled C++\r\nfunctions. Such LLVM glue code functions can further be\r\ncreated for other CSV-like formats using the C++ functions\r\nsimilar to a library. Code generation of the LLVM functions\r\nfor CSV data is implemented for the four storage backend\r\ntypes in HyPer (cf. Sect. 2).\r\nOffline loading. In offline loading mode, loading has ex\u0002clusive access to the relation, i.e., there are no concurrent\r\ntransactions and queries; and loading is not logged. Pro\u0002cessing of CSV chunks and merge steps are interleaved as\r\nmuch as possible to reduce overall loading time. If an er\u0002ror occurs during the loading process, an exception is raised\r\nbut the database might be left in a state where it is only\r\npartially loaded. For use cases such as (lwu)* workflows, in\u0002situ querying, and initial loading this is usually acceptable\r\nas the database can be recreated from scratch.\r\nOnline transactional loading. Online transactional\r\nloading supports loading with ACID semantics where only\r\nthe merge steps need to be encapsulated in a single merge\r\ntransaction. Processing of CSV chunks can happen in par\u0002allel to transaction processing. There is a tradeoff between\r\noverall loading time and the duration of the merge transac\u0002tion: To achieve online loading optimized for a short loading\r\ntime, chunk processing is interleaved with merge steps. The\r\nduration of the merge transaction starts with the first and\r\nends with last merge step. No other transactions can be pro\u0002cessed in that time. To achieve a short merge transaction\r\nduration, first all chunks are processed and then all merge\r\nsteps are processed at once.\r\n5. EVALUATION\r\nThe evaluation of Instant Loading in HyPer was con\u0002ducted on a commodity workstation with an Intel Core i7-\r\n3770 CPU and 32 GB dual-channel DDR3-1600 DRAM. The\r\nCPU is based on the Ivy Bridge microarchitecture and sup\u0002ports the SSE 4.2 string and text instructions, has 4 cores\r\n(8 hardware threads), a 3.4 GHz clock rate, and a 8 MB last\u0002level shared L3 cache. As operating system we used Linux\r\n3.5 in 64 bit mode. Sources were compiled using GCC 4.7\r\nwith -O3 -march=native optimizations. For lack of a high\u0002speed network-attached storage or distributed file system in\r\nour lab, we used the in-memory file system ramfs as the CSV\r\nsource to emulate a wire speed of multiple Gbit/s. Prior to\r\neach measurement we flushed the file system caches.\r\n5.1 Parsing and Deserialization\r\nWe first evaluated our task- and data-parallelized parsing\r\nand deserialization methods in isolation from the rest of the\r\nloading process. CSV data was read from ramfs, parsed,\r\ndeserialized, and stored in heap-allocated result buffers. We\r\nimplemented a variant that is SSE 4.2 optimized (SSE) as\r\ndescribed in Sect. 3.4 and one that is not (non-SSE). As a\r\ncontestant for these methods we used a parsing and dese\u0002rialization implementation based on the Boost Spirit C++\r\nlibrary v2.5.2. In particular, we used Boost Spirit.Qi, which\r\nallows the generation of a recursive descent parser for a given\r\ngrammar. We also experimented with an implementation\r\nbased on Boost.Tokenizer and Boost.Lexical Cast but its\r\nperformance trailed that of the Boost Spirit.Qi variant. Just\r\nlike our SSE and non-SSE variants, we task-parallelized our\r\nBoost implementation as described in Sect. 3.3.\r\nAs input for the experiment we chose TPC-H CSV data\r\ngenerated with a scale-factor of 10 (∼10 GB). While the SSE\r\nand non-SSE variants only require schema information at\r\nrun-time, the Spirit.Qi parser generator is a set of templated\r\nC++ functions that require schema information at compile\u0002time. For the Boost Spirit.Qi variant we thus hardcoded the\r\nTPC-H schema information into the source code.\r\nFig. 9 shows that SSE and non-SSE perform better than\r\nBoost Spirit.Qi at all multiprogramming levels. SSE outper\u0002forms non-SSE and shows a higher speedup: SSE achieves a\r\nparsing and deserialization throughput of over 1.6 GB/s with\r\na multiprogramming level of 8 compared to about 1.0 GB/s\r\nwith non-SSE, an improvement of 60%. The superior per\u0002formance of SSE can be explained by (i) the exploitation of\r\nvector execution engines in addition to scalar execution units\r\nacross all cores and (ii) by the reduced number of branch\r\n1709",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/2c4f336b-c1c8-435f-b37f-c62793d5deba.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eaf3f6e09752a62759c7a7b156be41cc9753865e99fc84d621a7e937ba6b185a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1035
      },
      {
        "segments": [
          {
            "segment_id": "2c4f336b-c1c8-435f-b37f-c62793d5deba",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "updates\r\nfork()\r\nOLTP data\r\nOLAP snapshot\r\nqueries\r\ncopy-on-update\r\nA\r\nB\r\nA'\r\nB\r\nreplicated page\r\n(due to update of A)\r\nupdate A to A' Figure 8: HyPer’s snapshotting mechanism.\r\ntransactions per second in a single thread on modern hard\u0002ware [19]. Similar to the design pioneered by H-Store [18]\r\nand VoltDB, HyPer also implements a partitioned execution\r\nmodel: The database is partitioned in a way that most trans\u0002actions only need to access a single partition. Transactions\r\nthat each exclusively work on a different partition can thus\r\nbe processed in parallel without synchronization. Synchro\u0002nization is only necessary for partition-crossing transactions.\r\nLike transactions, SQL queries are compiled into LLVM\r\ncode [22]. The data-centric compiler aims at good code and\r\ndata locality and a predictable branch layout. LLVM code\r\nis then compiled into efficient machine code using LLVM’s\r\njust-in-time compiler. Together with its advanced query op\u0002timizer, HyPer achieves superior query response times [19]\r\ncomparable to those of MonetDB [4] or Vectorwise.\r\n4.2 Instant Loading in HyPer\r\nInstant Loading in HyPer allows (lwu)* workflows but can\r\nindeed also be used for other use cases that require the load\u0002ing of CSV data. This includes initial loads and incremental\r\nloads for continuous data integration.\r\nThe interface of Instant Loading in HyPer is designed in\r\nthe style of the PostgreSQL COPY operator. Instant Loading\r\ntakes CSV input, the schema it adheres to, and the CSV\r\nspecial characters as input. Except for \"\\r\\n\", which we\r\nallow to be used as a record delimiter, we assume that spe\u0002cial characters are single ASCII characters. For each rela\u0002tion that is created or altered, we generate LLVM glue code\r\nfunctions for the processing of CSV chunks and for partition\r\nbuffer merging (cf., the two steps in Fig. 4). Code genera\u0002tion and compilation of these functions at runtime has the\r\nadvantage that the resulting code has good locality and pre\u0002dictable branching as the relation layout, e.g., the number\r\nof attributes and the attribute types, are known. Searching\r\nfor delimiters and the deserialization methods are imple\u0002mented as generic C++ functions that are not tailored to\r\nthe design of HyPer. Just like the LLVM functions HyPer\r\ncompiles for transactions and queries [22], the Instant Load\u0002ing LLVM glue code calls these statically compiled C++\r\nfunctions. Such LLVM glue code functions can further be\r\ncreated for other CSV-like formats using the C++ functions\r\nsimilar to a library. Code generation of the LLVM functions\r\nfor CSV data is implemented for the four storage backend\r\ntypes in HyPer (cf. Sect. 2).\r\nOffline loading. In offline loading mode, loading has ex\u0002clusive access to the relation, i.e., there are no concurrent\r\ntransactions and queries; and loading is not logged. Pro\u0002cessing of CSV chunks and merge steps are interleaved as\r\nmuch as possible to reduce overall loading time. If an er\u0002ror occurs during the loading process, an exception is raised\r\nbut the database might be left in a state where it is only\r\npartially loaded. For use cases such as (lwu)* workflows, in\u0002situ querying, and initial loading this is usually acceptable\r\nas the database can be recreated from scratch.\r\nOnline transactional loading. Online transactional\r\nloading supports loading with ACID semantics where only\r\nthe merge steps need to be encapsulated in a single merge\r\ntransaction. Processing of CSV chunks can happen in par\u0002allel to transaction processing. There is a tradeoff between\r\noverall loading time and the duration of the merge transac\u0002tion: To achieve online loading optimized for a short loading\r\ntime, chunk processing is interleaved with merge steps. The\r\nduration of the merge transaction starts with the first and\r\nends with last merge step. No other transactions can be pro\u0002cessed in that time. To achieve a short merge transaction\r\nduration, first all chunks are processed and then all merge\r\nsteps are processed at once.\r\n5. EVALUATION\r\nThe evaluation of Instant Loading in HyPer was con\u0002ducted on a commodity workstation with an Intel Core i7-\r\n3770 CPU and 32 GB dual-channel DDR3-1600 DRAM. The\r\nCPU is based on the Ivy Bridge microarchitecture and sup\u0002ports the SSE 4.2 string and text instructions, has 4 cores\r\n(8 hardware threads), a 3.4 GHz clock rate, and a 8 MB last\u0002level shared L3 cache. As operating system we used Linux\r\n3.5 in 64 bit mode. Sources were compiled using GCC 4.7\r\nwith -O3 -march=native optimizations. For lack of a high\u0002speed network-attached storage or distributed file system in\r\nour lab, we used the in-memory file system ramfs as the CSV\r\nsource to emulate a wire speed of multiple Gbit/s. Prior to\r\neach measurement we flushed the file system caches.\r\n5.1 Parsing and Deserialization\r\nWe first evaluated our task- and data-parallelized parsing\r\nand deserialization methods in isolation from the rest of the\r\nloading process. CSV data was read from ramfs, parsed,\r\ndeserialized, and stored in heap-allocated result buffers. We\r\nimplemented a variant that is SSE 4.2 optimized (SSE) as\r\ndescribed in Sect. 3.4 and one that is not (non-SSE). As a\r\ncontestant for these methods we used a parsing and dese\u0002rialization implementation based on the Boost Spirit C++\r\nlibrary v2.5.2. In particular, we used Boost Spirit.Qi, which\r\nallows the generation of a recursive descent parser for a given\r\ngrammar. We also experimented with an implementation\r\nbased on Boost.Tokenizer and Boost.Lexical Cast but its\r\nperformance trailed that of the Boost Spirit.Qi variant. Just\r\nlike our SSE and non-SSE variants, we task-parallelized our\r\nBoost implementation as described in Sect. 3.3.\r\nAs input for the experiment we chose TPC-H CSV data\r\ngenerated with a scale-factor of 10 (∼10 GB). While the SSE\r\nand non-SSE variants only require schema information at\r\nrun-time, the Spirit.Qi parser generator is a set of templated\r\nC++ functions that require schema information at compile\u0002time. For the Boost Spirit.Qi variant we thus hardcoded the\r\nTPC-H schema information into the source code.\r\nFig. 9 shows that SSE and non-SSE perform better than\r\nBoost Spirit.Qi at all multiprogramming levels. SSE outper\u0002forms non-SSE and shows a higher speedup: SSE achieves a\r\nparsing and deserialization throughput of over 1.6 GB/s with\r\na multiprogramming level of 8 compared to about 1.0 GB/s\r\nwith non-SSE, an improvement of 60%. The superior per\u0002formance of SSE can be explained by (i) the exploitation of\r\nvector execution engines in addition to scalar execution units\r\nacross all cores and (ii) by the reduced number of branch\r\n1709",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/2c4f336b-c1c8-435f-b37f-c62793d5deba.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eaf3f6e09752a62759c7a7b156be41cc9753865e99fc84d621a7e937ba6b185a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1035
      },
      {
        "segments": [
          {
            "segment_id": "2e0f644c-236e-43bb-b5e6-cdf960019fd9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "1 2 3 4 5 6 7 8\r\n0\r\n0.5\r\n1\r\n1.5\r\n0.25\r\n0.75\r\n1.25\r\n1.75\r\nHyper-Threading\r\nmultiprogramming level [threads]\r\nthroughput [GB/s]\r\nSSE\r\nNon-SSE\r\nBoost Spirit.Qi\r\nFigure 9: Speedup of parsing and\r\ndeserialization methods with heap\u0002allocated result buffers.\r\n1 2 3 4 5 6 7 8\r\n0 M\r\n50 M\r\n100 M\r\n150 M\r\n25 M\r\n75 M\r\n125 M\r\nHyper-Threading\r\nmultiprogramming level [threads]\r\ncreation [keys/s]\r\nART ordered dense HT\r\nART unordered dense\r\nART sparse\r\nFigure 10: Speedup of merge-able HT\r\nand ART parallelized index building\r\nand merging for 10 M 32 bit keys.\r\nHDD SSD DDR3-1600\r\n0 %\r\n50 %\r\n100 %\r\n25 %\r\n75 %\r\ncompute\r\nbound\r\nwire speed saturation\r\nw/ 8 hardware threads\r\nw/o I/O prefetching\r\nw/ I/O prefetching\r\nFigure 11: Wire speed saturation\r\nof Instant Loading (cf., Fig. 1) and\r\nthe impact of I/O prefetching.\r\ninsert copy chunk\r\ncolumn-store 7841 ms 6939 ms 6092 ms\r\nrow-store 6609 ms 6608 ms 6049 ms\r\nTable 1: Loading of TPC-H CSV data (scale-factor\r\n10) to a column- and row-store using insert-, copy-,\r\nand chunk-based partition buffer merging.\r\nmisses compared to non-SSE. Performance counters show\r\nthat the number of branch misses is reduced from 194/kB\r\nCSV with non-SSE to just 89/kB CSV with SSE, a decrease\r\nof over 50%. Using all execution units of the CPU cores\r\nalso allows SSE to profit more from Hyper-Threading. This\r\ncomes at no additional cost and improves energy efficiency:\r\nMeasuring the Running Average Power Limit energy sensors\r\navailable in recent Intel CPUs reveals that SSE used 388 J\r\ncompared to 503 J (+23%) with non-SSE and 625 J (+38%)\r\nwith Boost Spirit.Qi.\r\n5.2 Partition Buffers\r\nWe evaluated Instant Loading for the column- and row\u0002store storage backend implementations in HyPer (cf., Sect. 2)\r\nand the three partition buffer merging approaches we pro\u0002posed in Sect. 3.5. For the insert- and copy-based merg\u0002ing approaches we used storage backends based on contigu\u0002ous memory, for the chunk-based approach we used chun\u0002ked storage backends. Table 1 shows the benchmark results\r\nwhen loading a TPC-H CSV data set with a scale-factor of\r\n10. For the column-store backends, copy was around 12%\r\nfaster than insert. The chunk-based approach improved per\u0002formance by another 12%. For the row-store backend, in\u0002sert and copy performed similarly; chunk-based merging was\r\n8.5% faster.\r\n5.3 Bulk Index Creation\r\nWe evaluated the parallelized creation of hash tables with\r\nchaining (HT) and adaptive radix trees (ART) on key range\r\npartitions and the parallelized merging of these indexes to\r\ncreate a unified index for the total key range.\r\nFig. 10 shows the speedup of index creation for a key range\r\nof 10M 32 bit keys. For ordered dense keys, i.e., ordered keys\r\nranging from 1 to 10M, ART allows a faster creation of the\r\nindex than the HT for all multiprogramming levels. Merg\u0002ing of ART indexes is, in the case of an ordered dense key\r\nrange, highly efficient and often only requires a few pointers\r\nto be copied such that the creation time of the unified in\u0002dex largely only depends on the insertion speed of the ART\r\nindexes that are created in parallel. The lower speedup of\r\nART (×2.2) compared to HT (×2.6) with a multiprogram\u0002ming level of 4 is due to caching effects. The performance\r\nof ART heavily depends on the size of the effectively usable\r\nCPU cache per index [20]. In absolute numbers, however,\r\nART achieves an index creation speed of 130M keys per sec\u0002ond compared to 27M keys per second with HT. While the\r\nperformance of HT does not depend on the distribution of\r\nkeys, an ordered dense key range is the best case for ART.\r\nFor unordered dense, i.e., randomly permuted dense keys,\r\nand sparse keys, i.e., randomly generated keys for which each\r\nbit is 1 or 0 with equal probability, the performance of ART\r\ndrops. The index creation speed is still slightly better than\r\nwith HT. For unordered key ranges merging is more costly\r\nthan for ordered key ranges because mostly leaf nodes need\r\nto be merged. For a multiprogramming level of 4, merging\r\naccounted for 1% of loading time for ordered dense, 16% for\r\nunordered dense, and 33% for sparse keys.\r\n5.4 Offline Loading\r\nTo evaluate the end-to-end application performance of\r\noffline loading we benchmarked a workload that consisted of\r\n(i) bulk loading TPC-H CSV data with a scale-factor of 10\r\n(∼10 GB) from ramfs and (ii) then executing the 22 TPC-H\r\nqueries in parallel query streams. We used an unpartitioned\r\nTPC-H database, i.e., only one merge task runs in parallel,\r\nand configure HyPer to use a column-store backend based\r\non contiguous memory. Partition buffers were merged using\r\nthe copy-based approach. We compared Instant Loading\r\nin HyPer to a Hadoop v1.1.1 Hive v0.10 [28] cluster con\u0002sisting of 4 nodes of the kind described at the beginning\r\nof Sect. 5 (1 GbE interconnect), SQLite v3.7.15 compiled\r\nfrom source, MySQL v5.5.29, MonetDB [4] v11.13.7 com\u0002piled from source, and Vectorwise v2.5.2.\r\nFig. 12 shows our benchmark results. Instant Loading\r\nachieves a superior combined bulk loading and query pro\u0002cessing performance compared to the contestants. Load\u0002ing took 6.9 s (HyPer), unloading the database as a LZ4-\r\ncompressed binary to ramfs after loading took an additional\r\n4.3 s (HyPer /w unload). The compressed binary has a size\r\nof 4.7 GB (50% the size of the CSV files) and can be loaded\r\nagain in 2.6 s (3× faster than loading the CSV files). In\r\nboth cases, the queries were evaluated in just under 12 s.\r\nOur unloading and binary loading approaches in HyPer are\r\nagain highly parallelized. We further evaluated the I/O sat\u0002uration when loading from local I/O devices. Fig. 11 shows\r\nthat Instant Loading fully saturates the wire speed of a tra\u0002ditional HDD (160 MB/s) and a SSD (500 MB/s). When the\r\n1710",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/2e0f644c-236e-43bb-b5e6-cdf960019fd9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f5161f6410be1f2bc0e5447b9ecb81a9880cc66c33e4c105a98f37be2fcd9294",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 951
      },
      {
        "segments": [
          {
            "segment_id": "2e0f644c-236e-43bb-b5e6-cdf960019fd9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "1 2 3 4 5 6 7 8\r\n0\r\n0.5\r\n1\r\n1.5\r\n0.25\r\n0.75\r\n1.25\r\n1.75\r\nHyper-Threading\r\nmultiprogramming level [threads]\r\nthroughput [GB/s]\r\nSSE\r\nNon-SSE\r\nBoost Spirit.Qi\r\nFigure 9: Speedup of parsing and\r\ndeserialization methods with heap\u0002allocated result buffers.\r\n1 2 3 4 5 6 7 8\r\n0 M\r\n50 M\r\n100 M\r\n150 M\r\n25 M\r\n75 M\r\n125 M\r\nHyper-Threading\r\nmultiprogramming level [threads]\r\ncreation [keys/s]\r\nART ordered dense HT\r\nART unordered dense\r\nART sparse\r\nFigure 10: Speedup of merge-able HT\r\nand ART parallelized index building\r\nand merging for 10 M 32 bit keys.\r\nHDD SSD DDR3-1600\r\n0 %\r\n50 %\r\n100 %\r\n25 %\r\n75 %\r\ncompute\r\nbound\r\nwire speed saturation\r\nw/ 8 hardware threads\r\nw/o I/O prefetching\r\nw/ I/O prefetching\r\nFigure 11: Wire speed saturation\r\nof Instant Loading (cf., Fig. 1) and\r\nthe impact of I/O prefetching.\r\ninsert copy chunk\r\ncolumn-store 7841 ms 6939 ms 6092 ms\r\nrow-store 6609 ms 6608 ms 6049 ms\r\nTable 1: Loading of TPC-H CSV data (scale-factor\r\n10) to a column- and row-store using insert-, copy-,\r\nand chunk-based partition buffer merging.\r\nmisses compared to non-SSE. Performance counters show\r\nthat the number of branch misses is reduced from 194/kB\r\nCSV with non-SSE to just 89/kB CSV with SSE, a decrease\r\nof over 50%. Using all execution units of the CPU cores\r\nalso allows SSE to profit more from Hyper-Threading. This\r\ncomes at no additional cost and improves energy efficiency:\r\nMeasuring the Running Average Power Limit energy sensors\r\navailable in recent Intel CPUs reveals that SSE used 388 J\r\ncompared to 503 J (+23%) with non-SSE and 625 J (+38%)\r\nwith Boost Spirit.Qi.\r\n5.2 Partition Buffers\r\nWe evaluated Instant Loading for the column- and row\u0002store storage backend implementations in HyPer (cf., Sect. 2)\r\nand the three partition buffer merging approaches we pro\u0002posed in Sect. 3.5. For the insert- and copy-based merg\u0002ing approaches we used storage backends based on contigu\u0002ous memory, for the chunk-based approach we used chun\u0002ked storage backends. Table 1 shows the benchmark results\r\nwhen loading a TPC-H CSV data set with a scale-factor of\r\n10. For the column-store backends, copy was around 12%\r\nfaster than insert. The chunk-based approach improved per\u0002formance by another 12%. For the row-store backend, in\u0002sert and copy performed similarly; chunk-based merging was\r\n8.5% faster.\r\n5.3 Bulk Index Creation\r\nWe evaluated the parallelized creation of hash tables with\r\nchaining (HT) and adaptive radix trees (ART) on key range\r\npartitions and the parallelized merging of these indexes to\r\ncreate a unified index for the total key range.\r\nFig. 10 shows the speedup of index creation for a key range\r\nof 10M 32 bit keys. For ordered dense keys, i.e., ordered keys\r\nranging from 1 to 10M, ART allows a faster creation of the\r\nindex than the HT for all multiprogramming levels. Merg\u0002ing of ART indexes is, in the case of an ordered dense key\r\nrange, highly efficient and often only requires a few pointers\r\nto be copied such that the creation time of the unified in\u0002dex largely only depends on the insertion speed of the ART\r\nindexes that are created in parallel. The lower speedup of\r\nART (×2.2) compared to HT (×2.6) with a multiprogram\u0002ming level of 4 is due to caching effects. The performance\r\nof ART heavily depends on the size of the effectively usable\r\nCPU cache per index [20]. In absolute numbers, however,\r\nART achieves an index creation speed of 130M keys per sec\u0002ond compared to 27M keys per second with HT. While the\r\nperformance of HT does not depend on the distribution of\r\nkeys, an ordered dense key range is the best case for ART.\r\nFor unordered dense, i.e., randomly permuted dense keys,\r\nand sparse keys, i.e., randomly generated keys for which each\r\nbit is 1 or 0 with equal probability, the performance of ART\r\ndrops. The index creation speed is still slightly better than\r\nwith HT. For unordered key ranges merging is more costly\r\nthan for ordered key ranges because mostly leaf nodes need\r\nto be merged. For a multiprogramming level of 4, merging\r\naccounted for 1% of loading time for ordered dense, 16% for\r\nunordered dense, and 33% for sparse keys.\r\n5.4 Offline Loading\r\nTo evaluate the end-to-end application performance of\r\noffline loading we benchmarked a workload that consisted of\r\n(i) bulk loading TPC-H CSV data with a scale-factor of 10\r\n(∼10 GB) from ramfs and (ii) then executing the 22 TPC-H\r\nqueries in parallel query streams. We used an unpartitioned\r\nTPC-H database, i.e., only one merge task runs in parallel,\r\nand configure HyPer to use a column-store backend based\r\non contiguous memory. Partition buffers were merged using\r\nthe copy-based approach. We compared Instant Loading\r\nin HyPer to a Hadoop v1.1.1 Hive v0.10 [28] cluster con\u0002sisting of 4 nodes of the kind described at the beginning\r\nof Sect. 5 (1 GbE interconnect), SQLite v3.7.15 compiled\r\nfrom source, MySQL v5.5.29, MonetDB [4] v11.13.7 com\u0002piled from source, and Vectorwise v2.5.2.\r\nFig. 12 shows our benchmark results. Instant Loading\r\nachieves a superior combined bulk loading and query pro\u0002cessing performance compared to the contestants. Load\u0002ing took 6.9 s (HyPer), unloading the database as a LZ4-\r\ncompressed binary to ramfs after loading took an additional\r\n4.3 s (HyPer /w unload). The compressed binary has a size\r\nof 4.7 GB (50% the size of the CSV files) and can be loaded\r\nagain in 2.6 s (3× faster than loading the CSV files). In\r\nboth cases, the queries were evaluated in just under 12 s.\r\nOur unloading and binary loading approaches in HyPer are\r\nagain highly parallelized. We further evaluated the I/O sat\u0002uration when loading from local I/O devices. Fig. 11 shows\r\nthat Instant Loading fully saturates the wire speed of a tra\u0002ditional HDD (160 MB/s) and a SSD (500 MB/s). When the\r\n1710",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/2e0f644c-236e-43bb-b5e6-cdf960019fd9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f5161f6410be1f2bc0e5447b9ecb81a9880cc66c33e4c105a98f37be2fcd9294",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 951
      },
      {
        "segments": [
          {
            "segment_id": "89aba9b9-6afa-404e-a87b-92547d466a51",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "Hadoop Hive\r\n4 nodes\r\nHadoop Hive\r\n4 nodes, RCFiles\r\nSQLite\r\nin-memory\r\nMySQL\r\nmemory engine\r\nMySQL\r\nCSV engine\r\nMonetDB Vectorwise\r\nw/o Q5\r\nHyPer HyPer\r\nw/ unload\r\n0\r\n50\r\n100\r\n150\r\n50 minutes 48 minutes > 1 hour > 1 hour > 1 hour\r\n133.7 s\r\n110.0 s 109.9 s\r\n101.6 s\r\n18.8 s\r\n6.9 s\r\n23.1 s\r\n11.2 s\r\n0 s\r\n173.5 s 302.0 s\r\n119.9 s\r\n0 s\r\nexecution time [s]\r\nLoading TPC-H CSV (scale-factor 10)\r\nProcessing the 22 TPC-H queries\r\nFigure 12: Offline CSV bulk loading and query processing performance in HyPer with Instant Loading, other\r\nmain memory databases, and a Hadoop Hive cluster with 4 nodes.\r\nmemory is used as the source and the sink, only 10% of the\r\navailable wire speed are saturated (CPU bound). Fig. 11\r\nfurther shows that advising the kernel to prefetch data from\r\nthe local I/O device (using madvise) is necessary to achieve\r\na near-100% saturation of local devices.\r\nHive is a data warehouse solution based on Hadoop. For\r\nour benchmark, we used 4 Hadoop nodes. Hadoop’s dis\u0002tributed file system (HDFS) and Hive were configured to\r\nstore data in ramfs. Other configuration settings were un\u0002touched, including the default replication count of 3 for\r\nHDFS. This means that each node in the setup had a replica\r\nof the CSV files. We did not include the HDFS loading time\r\n(125.8 s) in our results as we assume that data is ideally\r\nalready stored there. To evaluate the query performance,\r\nwe used an official implementation of the TPC-H queries in\r\nHiveQL3, Hive’s SQL-like query language. Even though no\r\nexplicit loading is required and 4 nodes instead of a sin\u0002gle one are used, Hive needed 50 minutes to process the\r\n22 queries. We also evaluated Hive with record columnar\r\nfiles (RCFiles). Loading the CSV files into RCFiles using\r\nthe BinaryColumnarSerDe, a transformation pass that de\u0002serializes strings to binary data type representations, took\r\n173.5 s. Query processing on these RCFiles was, however,\r\nonly 5 minutes faster than working on the raw CSV files.\r\nSQLite was started as an in-memory database using the\r\nspecial filename :memory:. For bulk loading, we locked the\r\ntables in exclusive mode and used the .import command.\r\nQuery performance of SQLite is, however, not satisfactory.\r\nProcessing of the 22 TPC-H queries took over 1 hour.\r\nFor MySQL we ran two benchmarks: one with MySQL’s\r\nmemory engine using the LOAD DATA INFILE command for\r\nbulk loading and one with MySQL’s CSV engine that al\u0002lows query processing directly on external CSV files. Bulk\r\nloading using the memory engine took just under 2 minutes.\r\nNevertheless, for both, the memory and CSV engine, pro\u0002cessing of the 22 TPC-H queries took over 1 hour again.\r\nWe compiled MonetDB with MonetDB5, MonetDB/SQL,\r\nand extra optimizations enabled. For bulk loading we used\r\nthe COPY INTO command with the LOCKED qualifier that tells\r\nMonetDB to skip logging operations. As advised in the doc\u0002umentation, primary key constraints were added to the ta\u0002bles after loading. We created the MonetDB database inside\r\nramfs so that BAT files written by MonetDB were again\r\nstored in memory. To the best of our knowledge MonetDB\r\nhas no option to solely bulk load data to memory without\r\nwriting the binary representation to BAT files. Bulk loading\r\nin MonetDB is thus best compared to Instant Loading with\r\nbinary unloading (HyPer w/ unload). While loading time is\r\n3\r\nhttp://issues.apache.org/jira/browse/HIVE-600\r\n0 1 2 3 4 5 6\r\n1\r\n1.2\r\n1.4\r\n1.6\r\nCSV chunk size [MB]\r\n[GB/s]\r\nFigure 13: Throughput as a function of chunk size.\r\nscale-factor loading throughput query time\r\n10 (∼10 GB) 1.14 GB/s (∼9 Gbit/s) 16.6 s\r\n30 (∼30 GB) 1.29 GB/s (∼10 Gbit/s) 57.9 s\r\n100 (∼100 GB) 1.36 GB/s (∼11 Gbit/s) 302.1 s\r\nTable 2: Scaleup of Instant Loading of TPC-H data\r\nsets on a server with 256 GB main memory.\r\ncomparable to the MySQL memory engine, queries are pro\u0002cessed much faster. The combined workload took 133.7 s to\r\ncomplete.\r\nFor Vectorwise, we bulk loaded the files using the vwload\r\nutility with rollback on failure turned off. Loading time is\r\ncomparable to MonetDB while queries are processed slightly\r\nfaster. TPC-H query 5 could not be processed without the\r\nprior generation of statistics using optimizedb. We did not\r\ninclude the creation of statistics in our benchmark results\r\nas it took several minutes in our experiments.\r\nWe would have liked to further compare Instant Loading\r\nto MonetDB’s CSV vault [16] but couldn’t get it running in\r\nthe current version of MonetDB. We would have also liked to\r\nevaluate the NoDB implementation PostgresRaw [3] in the\r\ncontext of high-performance I/O devices and main memory\r\ndatabases, but its implementation is not (yet) available.\r\nOptimal chunk size. Fig. 13 shows Instant Loading\r\nthroughput of a TPC-H data set as a function of chunk\r\nsize. Highest throughputs were measured between 256 kB\r\nand 1 MB, which equals a range of 0.25–1.0 times the L3\r\ncache size divided by the number of hardware threads used.\r\nScaleup of Instant Loading. We evaluated the scaleup\r\nof Instant Loading on a server machine with an 8 core In\u0002tel Xeon X7560 CPU and 256 GB of DDR3-1066 DRAM\r\nand bulk loaded TPC-H CSV data with scale-factors of 10\r\n(∼10 GB), 30 (∼30 GB), and 100 (∼100 GB). We then again\r\nexecuted the 22 TPC-H queries in parallel query streams. As\r\nshown in Table 2, Instant Loading achieves a linear scaleup.\r\nperf analysis of Instant Loading. A perf analysis of\r\nInstant Loading of a TPC-H scale-factor 10 lineitem CSV\r\nfile shows that 37% of CPU cycles are used to find delim\u0002iters, 11.2% to deserialize numerics, 9.1% to deserialize\r\ndates, 6.5% to deserialize integers, 5.5% in the LLVM glue\r\n1711",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/89aba9b9-6afa-404e-a87b-92547d466a51.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d4d8249d9f85f4daf748a6bab65ba61a80a72f15e7bd65fabd6acebe79a74a6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 938
      },
      {
        "segments": [
          {
            "segment_id": "89aba9b9-6afa-404e-a87b-92547d466a51",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "Hadoop Hive\r\n4 nodes\r\nHadoop Hive\r\n4 nodes, RCFiles\r\nSQLite\r\nin-memory\r\nMySQL\r\nmemory engine\r\nMySQL\r\nCSV engine\r\nMonetDB Vectorwise\r\nw/o Q5\r\nHyPer HyPer\r\nw/ unload\r\n0\r\n50\r\n100\r\n150\r\n50 minutes 48 minutes > 1 hour > 1 hour > 1 hour\r\n133.7 s\r\n110.0 s 109.9 s\r\n101.6 s\r\n18.8 s\r\n6.9 s\r\n23.1 s\r\n11.2 s\r\n0 s\r\n173.5 s 302.0 s\r\n119.9 s\r\n0 s\r\nexecution time [s]\r\nLoading TPC-H CSV (scale-factor 10)\r\nProcessing the 22 TPC-H queries\r\nFigure 12: Offline CSV bulk loading and query processing performance in HyPer with Instant Loading, other\r\nmain memory databases, and a Hadoop Hive cluster with 4 nodes.\r\nmemory is used as the source and the sink, only 10% of the\r\navailable wire speed are saturated (CPU bound). Fig. 11\r\nfurther shows that advising the kernel to prefetch data from\r\nthe local I/O device (using madvise) is necessary to achieve\r\na near-100% saturation of local devices.\r\nHive is a data warehouse solution based on Hadoop. For\r\nour benchmark, we used 4 Hadoop nodes. Hadoop’s dis\u0002tributed file system (HDFS) and Hive were configured to\r\nstore data in ramfs. Other configuration settings were un\u0002touched, including the default replication count of 3 for\r\nHDFS. This means that each node in the setup had a replica\r\nof the CSV files. We did not include the HDFS loading time\r\n(125.8 s) in our results as we assume that data is ideally\r\nalready stored there. To evaluate the query performance,\r\nwe used an official implementation of the TPC-H queries in\r\nHiveQL3, Hive’s SQL-like query language. Even though no\r\nexplicit loading is required and 4 nodes instead of a sin\u0002gle one are used, Hive needed 50 minutes to process the\r\n22 queries. We also evaluated Hive with record columnar\r\nfiles (RCFiles). Loading the CSV files into RCFiles using\r\nthe BinaryColumnarSerDe, a transformation pass that de\u0002serializes strings to binary data type representations, took\r\n173.5 s. Query processing on these RCFiles was, however,\r\nonly 5 minutes faster than working on the raw CSV files.\r\nSQLite was started as an in-memory database using the\r\nspecial filename :memory:. For bulk loading, we locked the\r\ntables in exclusive mode and used the .import command.\r\nQuery performance of SQLite is, however, not satisfactory.\r\nProcessing of the 22 TPC-H queries took over 1 hour.\r\nFor MySQL we ran two benchmarks: one with MySQL’s\r\nmemory engine using the LOAD DATA INFILE command for\r\nbulk loading and one with MySQL’s CSV engine that al\u0002lows query processing directly on external CSV files. Bulk\r\nloading using the memory engine took just under 2 minutes.\r\nNevertheless, for both, the memory and CSV engine, pro\u0002cessing of the 22 TPC-H queries took over 1 hour again.\r\nWe compiled MonetDB with MonetDB5, MonetDB/SQL,\r\nand extra optimizations enabled. For bulk loading we used\r\nthe COPY INTO command with the LOCKED qualifier that tells\r\nMonetDB to skip logging operations. As advised in the doc\u0002umentation, primary key constraints were added to the ta\u0002bles after loading. We created the MonetDB database inside\r\nramfs so that BAT files written by MonetDB were again\r\nstored in memory. To the best of our knowledge MonetDB\r\nhas no option to solely bulk load data to memory without\r\nwriting the binary representation to BAT files. Bulk loading\r\nin MonetDB is thus best compared to Instant Loading with\r\nbinary unloading (HyPer w/ unload). While loading time is\r\n3\r\nhttp://issues.apache.org/jira/browse/HIVE-600\r\n0 1 2 3 4 5 6\r\n1\r\n1.2\r\n1.4\r\n1.6\r\nCSV chunk size [MB]\r\n[GB/s]\r\nFigure 13: Throughput as a function of chunk size.\r\nscale-factor loading throughput query time\r\n10 (∼10 GB) 1.14 GB/s (∼9 Gbit/s) 16.6 s\r\n30 (∼30 GB) 1.29 GB/s (∼10 Gbit/s) 57.9 s\r\n100 (∼100 GB) 1.36 GB/s (∼11 Gbit/s) 302.1 s\r\nTable 2: Scaleup of Instant Loading of TPC-H data\r\nsets on a server with 256 GB main memory.\r\ncomparable to the MySQL memory engine, queries are pro\u0002cessed much faster. The combined workload took 133.7 s to\r\ncomplete.\r\nFor Vectorwise, we bulk loaded the files using the vwload\r\nutility with rollback on failure turned off. Loading time is\r\ncomparable to MonetDB while queries are processed slightly\r\nfaster. TPC-H query 5 could not be processed without the\r\nprior generation of statistics using optimizedb. We did not\r\ninclude the creation of statistics in our benchmark results\r\nas it took several minutes in our experiments.\r\nWe would have liked to further compare Instant Loading\r\nto MonetDB’s CSV vault [16] but couldn’t get it running in\r\nthe current version of MonetDB. We would have also liked to\r\nevaluate the NoDB implementation PostgresRaw [3] in the\r\ncontext of high-performance I/O devices and main memory\r\ndatabases, but its implementation is not (yet) available.\r\nOptimal chunk size. Fig. 13 shows Instant Loading\r\nthroughput of a TPC-H data set as a function of chunk\r\nsize. Highest throughputs were measured between 256 kB\r\nand 1 MB, which equals a range of 0.25–1.0 times the L3\r\ncache size divided by the number of hardware threads used.\r\nScaleup of Instant Loading. We evaluated the scaleup\r\nof Instant Loading on a server machine with an 8 core In\u0002tel Xeon X7560 CPU and 256 GB of DDR3-1066 DRAM\r\nand bulk loaded TPC-H CSV data with scale-factors of 10\r\n(∼10 GB), 30 (∼30 GB), and 100 (∼100 GB). We then again\r\nexecuted the 22 TPC-H queries in parallel query streams. As\r\nshown in Table 2, Instant Loading achieves a linear scaleup.\r\nperf analysis of Instant Loading. A perf analysis of\r\nInstant Loading of a TPC-H scale-factor 10 lineitem CSV\r\nfile shows that 37% of CPU cycles are used to find delim\u0002iters, 11.2% to deserialize numerics, 9.1% to deserialize\r\ndates, 6.5% to deserialize integers, 5.5% in the LLVM glue\r\n1711",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/89aba9b9-6afa-404e-a87b-92547d466a51.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d4d8249d9f85f4daf748a6bab65ba61a80a72f15e7bd65fabd6acebe79a74a6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 938
      },
      {
        "segments": [
          {
            "segment_id": "e580d85c-4ee9-4b82-8112-121fe6f33637",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "0 1 2 3 4 5 6 7 8 9\r\n100,000\r\n200,000\r\n300,000\r\n400,000\r\n500,000\r\n600,000\r\nmerge\r\nbeginning of loading\r\nexecution time [s]\r\nthroughput [TX/s]\r\nMT OLTP + chunk-parallel IL MT OLTP + ST IL\r\nST OLTP + chunk-parallel IL ST OLTP + ST IL\r\nFigure 14: Chunk-parallel and single-threaded (ST)\r\nonline CSV Instant Loading (IL) of 1M item and 4M\r\nstock entries and single-threaded (ST) and multi\u0002threaded (MT) TPC-C transaction processing.\r\ncode that processes CSV chunks, and 5% in the LLVM glue\r\ncode that merges partition buffers. The remaining cycles are\r\nmostly spent inside the kernel. In more detail, the costs of\r\ndeserialization methods and the method to find delimiters\r\nare dominated by the instructions that load data to the SSE\r\nregisters and the SSE comparison instructions.\r\n5.5 Online Transactional Loading\r\nFinally, we evaluated Instant Loading in the context of on\u0002line transactional loading with ACID semantics. In partic\u0002ular, we benchmarked the partitioned execution of TPC-C\r\ntransactions in a TPC-C database partitioned by warehouse\r\nwith 4 warehouses. In parallel to transaction processing, we\r\nbulk loaded a new product catalog with 1M new items into\r\nthe item table. In addition to the 1M items, for each ware\u0002house, 1M stock entries were inserted into the stock table.\r\nThe storage backend was a chunked row-store and we used\r\nchunk-based partition buffer merging. Fig. 14 shows the\r\nTPC-C throughput with online bulk loading of the afore\u0002mentioned data set (∼1.3 GB), which was stored as CSV\r\nfiles in ramfs. In our benchmark, loading started after 1 sec\u0002ond. We measured transaction throughput in four scenarios:\r\nsingle- (ST) and multi-threaded (MT) transaction process\u0002ing combined with single-threaded and CSV chunk-parallel\r\nInstant Loading. In case of ST transaction processing, a\r\nthroughput of 200,000 transactions per second was sustained\r\nwith ST Instant Loading; with chunk-parallel Instant Load\u0002ing throughput shortly dropped to 100,000 transactions per\r\nsecond. Loading took around 3.5 s with ST Instant Load\u0002ing and 1.2 s with chunk-parallel Instant Loading. Merge\r\ntransactions took 250 ms. In case of MT transaction process\u0002ing, transaction processing and Instant Loading compete for\r\nhardware resources and throughput decreased considerably\r\nfrom 600,000 to 250,000 transactions per second. With ST\r\nInstant Loading, the additional load on the system is lower\r\nand transaction throughput barely decreases. With chunk\u0002parallel Instant Loading, loading took 4.6 s; with ST Instant\r\nLoading 7.0 s. Merge transactions took 250 ms again.\r\nTo the best of our knowledge, none of our contestants sup\u0002ports online transactional loading yet. We still compared\r\nour approach to the MySQL memory engine, which, how\u0002ever, has no support for transactions. We thus executed the\r\nTPC-C transactions sequentially. MySQL achieved a trans\u0002action throughput of 36 transactions per second. Loading\r\ntook 19.70 s; no transactions were processed during loading.\r\n6. RELATED WORK\r\nDue to Amdahl’s law, emerging multi-core CPUs can only\r\nbe efficiently utilized by highly parallelized applications [17].\r\nInstant Loading highly parallelizes CSV bulk loading and re\u0002duces the proportion of sequential code to a minimum.\r\nSIMD instructions have been used to accelerate a variety\r\nof database operators [31, 29]. Vectorized processing and the\r\nreduction of branching often enabled superlinear speedups.\r\nCompilers such as GCC and the LLVM JIT compiler [22] try\r\nto use SIMD instructions automatically. However, often sub\u0002tle tricks, which can hardly be reproduced by compilers, are\r\nrequired to leverage SIMD instructions. To the best of our\r\nknowledge no compiler can yet automatically apply SSE 4.2\r\nstring and text instructions. To achieve highest speedups,\r\nalgorithms need to be redesigned from scratch.\r\nAlready in 2005, Gray et al. [10] called for a synthesis of\r\nfile systems and databases. Back then, scientists complained\r\nthat loading structured text data to a database doesn’t seem\r\nworth it and that once it is loaded, it can no longer be manip\u0002ulated using standard application programs. Recent works\r\naddressed these objections [12, 3, 16]. NoDB [3] describes\r\nsystems that “do not require data loading while still main\u0002taining the whole feature set of a modern database system”.\r\nNoDB directly works on files and populates positional maps,\r\ni.e., index structures on files, and caches as a by-product of\r\nquery processing. Even though the NoDB reference imple\u0002mentation PostgresRaw has shown that queries can be pro\u0002cessed without loading and query processing profits from\r\nthe positional maps and caches, major issues are not solved.\r\nThese, in our opinion, mainly include the efficient support of\r\ntransactions, the scalability and efficiency of query process\u0002ing, and the adaptability of the paradigm for main memory\r\ndatabases. Instant Loading is a different and novel approach\r\nthat does not face these issues: Instead of eliminating data\r\nloading and adding the overhead of an additional layer of\r\nindirection, our approach focusses on making loading and\r\nunloading as unobtrusive as possible.\r\nExtensions of MapReduce, e.g., Hive [28], added support\r\nfor declarative query languages to the paradigm. To im\u0002prove query performance, some approaches, e.g., HAIL [7],\r\npropose using binary representations of text files for query\r\nprocessing. The conversion of text data into these binary\r\nrepresentations is very similar to bulk loading in traditional\r\ndatabases. HadoopDB [1] is designed as a hybrid of tra\u0002ditional databases and Hadoop-based approaches. It inter\u0002connects relational single-node databases using a communi\u0002cation layer based on Hadoop. Loading of the single-node\r\ndatabases has been identified as one of the obstacles of the\r\napproach. With Instant Loading, this obstacle can be re\u0002moved. Polybase [6], a feature of the Microsoft SQL Server\r\nPDW, translates some SQL operators on HDFS-resident\r\ndata into MapReduce jobs. The decision of when to push\r\noperators from the database to Hadoop largely depends on\r\nthe text file loading performance of the database.\r\nBulk loading of index structures has, e.g., been discussed\r\nfor B-trees [8, 9]. Database cracking [13] and adaptive in\u0002dexing [14] propose an iterative creation of indexes as a by\u0002product of query processing. These works argue that a high\r\ncost has to be paid up-front if indexes are created at load\u0002time. While this is certainly true for disk-based systems, we\r\nhave shown that for main memory databases at least the cre\u0002ation of primary indexes—which enable the validation of pri\u0002mary key constraints—as a side-effect of loading is feasible.\r\n1712",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/e580d85c-4ee9-4b82-8112-121fe6f33637.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=376653171e0ad3d04f5b355b023401f5552471c9a157e582accc50915c2955ce",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1003
      },
      {
        "segments": [
          {
            "segment_id": "e580d85c-4ee9-4b82-8112-121fe6f33637",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "0 1 2 3 4 5 6 7 8 9\r\n100,000\r\n200,000\r\n300,000\r\n400,000\r\n500,000\r\n600,000\r\nmerge\r\nbeginning of loading\r\nexecution time [s]\r\nthroughput [TX/s]\r\nMT OLTP + chunk-parallel IL MT OLTP + ST IL\r\nST OLTP + chunk-parallel IL ST OLTP + ST IL\r\nFigure 14: Chunk-parallel and single-threaded (ST)\r\nonline CSV Instant Loading (IL) of 1M item and 4M\r\nstock entries and single-threaded (ST) and multi\u0002threaded (MT) TPC-C transaction processing.\r\ncode that processes CSV chunks, and 5% in the LLVM glue\r\ncode that merges partition buffers. The remaining cycles are\r\nmostly spent inside the kernel. In more detail, the costs of\r\ndeserialization methods and the method to find delimiters\r\nare dominated by the instructions that load data to the SSE\r\nregisters and the SSE comparison instructions.\r\n5.5 Online Transactional Loading\r\nFinally, we evaluated Instant Loading in the context of on\u0002line transactional loading with ACID semantics. In partic\u0002ular, we benchmarked the partitioned execution of TPC-C\r\ntransactions in a TPC-C database partitioned by warehouse\r\nwith 4 warehouses. In parallel to transaction processing, we\r\nbulk loaded a new product catalog with 1M new items into\r\nthe item table. In addition to the 1M items, for each ware\u0002house, 1M stock entries were inserted into the stock table.\r\nThe storage backend was a chunked row-store and we used\r\nchunk-based partition buffer merging. Fig. 14 shows the\r\nTPC-C throughput with online bulk loading of the afore\u0002mentioned data set (∼1.3 GB), which was stored as CSV\r\nfiles in ramfs. In our benchmark, loading started after 1 sec\u0002ond. We measured transaction throughput in four scenarios:\r\nsingle- (ST) and multi-threaded (MT) transaction process\u0002ing combined with single-threaded and CSV chunk-parallel\r\nInstant Loading. In case of ST transaction processing, a\r\nthroughput of 200,000 transactions per second was sustained\r\nwith ST Instant Loading; with chunk-parallel Instant Load\u0002ing throughput shortly dropped to 100,000 transactions per\r\nsecond. Loading took around 3.5 s with ST Instant Load\u0002ing and 1.2 s with chunk-parallel Instant Loading. Merge\r\ntransactions took 250 ms. In case of MT transaction process\u0002ing, transaction processing and Instant Loading compete for\r\nhardware resources and throughput decreased considerably\r\nfrom 600,000 to 250,000 transactions per second. With ST\r\nInstant Loading, the additional load on the system is lower\r\nand transaction throughput barely decreases. With chunk\u0002parallel Instant Loading, loading took 4.6 s; with ST Instant\r\nLoading 7.0 s. Merge transactions took 250 ms again.\r\nTo the best of our knowledge, none of our contestants sup\u0002ports online transactional loading yet. We still compared\r\nour approach to the MySQL memory engine, which, how\u0002ever, has no support for transactions. We thus executed the\r\nTPC-C transactions sequentially. MySQL achieved a trans\u0002action throughput of 36 transactions per second. Loading\r\ntook 19.70 s; no transactions were processed during loading.\r\n6. RELATED WORK\r\nDue to Amdahl’s law, emerging multi-core CPUs can only\r\nbe efficiently utilized by highly parallelized applications [17].\r\nInstant Loading highly parallelizes CSV bulk loading and re\u0002duces the proportion of sequential code to a minimum.\r\nSIMD instructions have been used to accelerate a variety\r\nof database operators [31, 29]. Vectorized processing and the\r\nreduction of branching often enabled superlinear speedups.\r\nCompilers such as GCC and the LLVM JIT compiler [22] try\r\nto use SIMD instructions automatically. However, often sub\u0002tle tricks, which can hardly be reproduced by compilers, are\r\nrequired to leverage SIMD instructions. To the best of our\r\nknowledge no compiler can yet automatically apply SSE 4.2\r\nstring and text instructions. To achieve highest speedups,\r\nalgorithms need to be redesigned from scratch.\r\nAlready in 2005, Gray et al. [10] called for a synthesis of\r\nfile systems and databases. Back then, scientists complained\r\nthat loading structured text data to a database doesn’t seem\r\nworth it and that once it is loaded, it can no longer be manip\u0002ulated using standard application programs. Recent works\r\naddressed these objections [12, 3, 16]. NoDB [3] describes\r\nsystems that “do not require data loading while still main\u0002taining the whole feature set of a modern database system”.\r\nNoDB directly works on files and populates positional maps,\r\ni.e., index structures on files, and caches as a by-product of\r\nquery processing. Even though the NoDB reference imple\u0002mentation PostgresRaw has shown that queries can be pro\u0002cessed without loading and query processing profits from\r\nthe positional maps and caches, major issues are not solved.\r\nThese, in our opinion, mainly include the efficient support of\r\ntransactions, the scalability and efficiency of query process\u0002ing, and the adaptability of the paradigm for main memory\r\ndatabases. Instant Loading is a different and novel approach\r\nthat does not face these issues: Instead of eliminating data\r\nloading and adding the overhead of an additional layer of\r\nindirection, our approach focusses on making loading and\r\nunloading as unobtrusive as possible.\r\nExtensions of MapReduce, e.g., Hive [28], added support\r\nfor declarative query languages to the paradigm. To im\u0002prove query performance, some approaches, e.g., HAIL [7],\r\npropose using binary representations of text files for query\r\nprocessing. The conversion of text data into these binary\r\nrepresentations is very similar to bulk loading in traditional\r\ndatabases. HadoopDB [1] is designed as a hybrid of tra\u0002ditional databases and Hadoop-based approaches. It inter\u0002connects relational single-node databases using a communi\u0002cation layer based on Hadoop. Loading of the single-node\r\ndatabases has been identified as one of the obstacles of the\r\napproach. With Instant Loading, this obstacle can be re\u0002moved. Polybase [6], a feature of the Microsoft SQL Server\r\nPDW, translates some SQL operators on HDFS-resident\r\ndata into MapReduce jobs. The decision of when to push\r\noperators from the database to Hadoop largely depends on\r\nthe text file loading performance of the database.\r\nBulk loading of index structures has, e.g., been discussed\r\nfor B-trees [8, 9]. Database cracking [13] and adaptive in\u0002dexing [14] propose an iterative creation of indexes as a by\u0002product of query processing. These works argue that a high\r\ncost has to be paid up-front if indexes are created at load\u0002time. While this is certainly true for disk-based systems, we\r\nhave shown that for main memory databases at least the cre\u0002ation of primary indexes—which enable the validation of pri\u0002mary key constraints—as a side-effect of loading is feasible.\r\n1712",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/e580d85c-4ee9-4b82-8112-121fe6f33637.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=376653171e0ad3d04f5b355b023401f5552471c9a157e582accc50915c2955ce",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1003
      },
      {
        "segments": [
          {
            "segment_id": "e5dbfef2-ba49-4275-8a9e-77bda838cf60",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "7. OUTLOOK AND CONCLUSION\r\nEver increasing main memory capacities have fostered the\r\ndevelopment of in-memory database systems and very fast\r\nnetwork infrastructures with wire speeds of tens of Gbit/s\r\nare becoming economical. Current bulk loading approaches\r\nfor main memory databases, however, fail to leverage these\r\nwire speeds when loading structured text data. In this work\r\nwe presented Instant Loading, a novel CSV loading approach\r\nthat allows scalable bulk loading at wire speed. Task- and\r\ndata-parallelization of every phase of loading allows us to\r\nfully leverage the performance of modern multi-core CPUs.\r\nWe integrated the generic Instant Loading approach in our\r\nHyPer system and evaluated its end-to-end application per\u0002formance. The performance results have shown that Instant\r\nLoading can indeed leverage the wire speed of emerging 10\r\nGbE connectors. This paves the way for new (load-work\u0002unload)* usage scenarios where the main memory database\r\nsystem serves as a flexible and high-performance compute\r\nengine for big data processing—instead of using resource\u0002heavy MapReduce-style infrastructures.\r\nIn the future we intend to support other structured text\r\nformats and include more data preprocessing steps such as\r\ncompression, clustering, and synopsis generation. E.g., small\r\nmaterialized aggregates [21] can efficiently be computed at\r\nload time. Another idea is to port our scalable loading ap\u0002proach to coprocessor hardware and general-purpose GPUs.\r\n8. ACKNOWLEDGEMENTS\r\nTobias M¨uhlbauer is a recipient of the Google Europe Fel\u0002lowship in Structured Data Analysis, and this research is\r\nsupported in part by this Google Fellowship. Wolf R¨odiger is\r\na recipient of the Oracle External Research Fellowship. This\r\nwork is further sponsored by the German Federal Ministry of\r\nEducation and Research (BMBF) grant HDBC 01IS12026.\r\n9. REFERENCES\r\n[1] A. Abouzeid, K. Bajda-Pawlikowski, D. J. Abadi,\r\nA. Rasin, and A. Silberschatz. HadoopDB: An\r\nArchitectural Hybrid of MapReduce and DBMS\r\nTechnologies for Analytical Workloads. PVLDB,\r\n2(1):922–933, 2009.\r\n[2] A. Ailamaki, D. J. DeWitt, M. D. Hill, and D. A.\r\nWood. DBMSs on a modern processor: Where does\r\ntime go? VLDB, pages 266–277, 1999.\r\n[3] I. Alagiannis, R. Borovica, M. Branco, S. Idreos, and\r\nA. Ailamaki. NoDB: Efficient Query Execution on\r\nRaw Data Files. In SIGMOD, pages 241–252, 2012.\r\n[4] P. Boncz, M. Zukowski, and N. Nes. MonetDB/X100:\r\nHyper-pipelining query execution. In CIDR, pages\r\n225–237, 2005.\r\n[5] J. Dean. MapReduce: simplified data processing on\r\nlarge clusters. CACM, 51(1):107–113, 2008.\r\n[6] D. J. DeWitt, A. Halverson, R. Nehme, S. Shankar,\r\nJ. Aguilar-Saborit, et al. Split Query Processing in\r\nPolybase. In SIGMOD, pages 1255–1266, 2013.\r\n[7] J. Dittrich, J.-A. Quian´e-Ruiz, S. Richter, S. Schuh,\r\nA. Jindal, and J. Schad. Only Aggressive Elephants\r\nare Fast Elephants. PVLDB, 5(11):1591–1602, 2012.\r\n[8] G. Graefe. B-tree indexes for high update rates.\r\nSIGMOD Rec., 35(1):39–44, 2006.\r\n[9] G. Graefe and H. Kuno. Fast Loads and Queries. In\r\nTLDKS II, number 6380 in LNCS, pages 31–72, 2010.\r\n[10] J. Gray, D. Liu, M. Nieto-Santisteban, A. Szalay,\r\nD. DeWitt, and G. Heber. Scientific Data\r\nManagement in the Coming Decade. SIGMOD Rec.,\r\n34(4):34–41, 2005.\r\n[11] Hive user group presentation from Netflix.\r\nhttp://slideshare.net/slideshow/embed_code/3483386.\r\n[12] S. Idreos, I. Alagiannis, R. Johnson, and A. Ailamaki.\r\nHere are my Data Files. Here are my Queries. Where\r\nare my Results? In CIDR, pages 57–68, 2011.\r\n[13] S. Idreos, M. L. Kersten, and S. Manegold. Database\r\nCracking. In CIDR, pages 68–78, 2007.\r\n[14] S. Idreos, S. Manegold, H. Kuno, and G. Graefe.\r\nMerging what’s cracked, cracking what’s merged:\r\nadaptive indexing in main-memory column-stores.\r\nPVLDB, 4(9):586–597, 2011.\r\n[15] Extending the worlds most popular processor\r\narchitecture. Intel Whitepaper, 2006.\r\n[16] M. Ivanova, M. Kersten, and S. Manegold. Data\r\nVaults: A Symbiosis between Database Technology\r\nand Scientific File Repositories. In SSDM, volume\r\n7338 of LNCS, pages 485–494, 2012.\r\n[17] R. Johnson and I. Pandis. The bionic DBMS is\r\ncoming, but what will it look like? In CIDR, 2013.\r\n[18] R. Kallman, H. Kimura, J. Natkins, A. Pavlo,\r\nA. Rasin, et al. H-store: a high-performance,\r\ndistributed main memory transaction processing\r\nsystem. PVLDB, 1(2):1496–1499, 2008.\r\n[19] A. Kemper and T. Neumann. HyPer: A hybrid\r\nOLTP&OLAP main memory database system based\r\non virtual memory snapshots. In ICDE, pages\r\n195–206, 2011.\r\n[20] V. Leis, A. Kemper, and T. Neumann. The Adaptive\r\nRadix Tree: ARTful Indexing for Main-Memory\r\nDatabases. In ICDE, pages 38–49, 2013.\r\n[21] G. Moerkotte. Small Materialized Aggregates: A Light\r\nWeight Index Structure for Data Warehousing. VLDB,\r\npages 476–487, 1998.\r\n[22] T. Neumann. Efficiently compiling efficient query\r\nplans for modern hardware. PVLDB, 4(9):539–550,\r\n2011.\r\n[23] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J.\r\nDeWitt, et al. A Comparison of Approaches to\r\nLarge-Scale Data Analysis. In SIGMOD, pages\r\n165–178, 2009.\r\n[24] J. Reinders. Intel threading building blocks: outfitting\r\nC++ for multi-core processor parallelism. 2007.\r\n[25] E. Sedlar. Oracle Labs. Personal comm. May 29, 2013.\r\n[26] A. Szalay. JHU. Personal comm. May 16, 2013.\r\n[27] A. Szalay, A. R. Thakar, and J. Gray. The sqlLoader\r\nData-Loading Pipeline. JCSE, 10:38–48, 2008.\r\n[28] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka,\r\net al. Hive: A warehousing solution over a map-reduce\r\nframework. PVLDB, 2(2):1626–1629, 2009.\r\n[29] T. Willhalm, N. Popovici, Y. Boshmaf, H. Plattner,\r\nA. Zeier, et al. SIMD-Scan: Ultra Fast in-Memory\r\nTable Scan using on-Chip Vector Processing Units.\r\nPVLDB, 2(1):385–394, 2009.\r\n[30] Y. Shafranovich. IETF RFC 4180, 2005.\r\n[31] J. Zhou and K. A. Ross. Implementing database\r\noperations using SIMD instructions. In SIGMOD,\r\npages 145–156, 2002.\r\n1713",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/e5dbfef2-ba49-4275-8a9e-77bda838cf60.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9c39140240536abb5ebe822e239aac003298055dcd0b685451f69d1c6f4180ee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 875
      },
      {
        "segments": [
          {
            "segment_id": "e5dbfef2-ba49-4275-8a9e-77bda838cf60",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "7. OUTLOOK AND CONCLUSION\r\nEver increasing main memory capacities have fostered the\r\ndevelopment of in-memory database systems and very fast\r\nnetwork infrastructures with wire speeds of tens of Gbit/s\r\nare becoming economical. Current bulk loading approaches\r\nfor main memory databases, however, fail to leverage these\r\nwire speeds when loading structured text data. In this work\r\nwe presented Instant Loading, a novel CSV loading approach\r\nthat allows scalable bulk loading at wire speed. Task- and\r\ndata-parallelization of every phase of loading allows us to\r\nfully leverage the performance of modern multi-core CPUs.\r\nWe integrated the generic Instant Loading approach in our\r\nHyPer system and evaluated its end-to-end application per\u0002formance. The performance results have shown that Instant\r\nLoading can indeed leverage the wire speed of emerging 10\r\nGbE connectors. This paves the way for new (load-work\u0002unload)* usage scenarios where the main memory database\r\nsystem serves as a flexible and high-performance compute\r\nengine for big data processing—instead of using resource\u0002heavy MapReduce-style infrastructures.\r\nIn the future we intend to support other structured text\r\nformats and include more data preprocessing steps such as\r\ncompression, clustering, and synopsis generation. E.g., small\r\nmaterialized aggregates [21] can efficiently be computed at\r\nload time. Another idea is to port our scalable loading ap\u0002proach to coprocessor hardware and general-purpose GPUs.\r\n8. ACKNOWLEDGEMENTS\r\nTobias M¨uhlbauer is a recipient of the Google Europe Fel\u0002lowship in Structured Data Analysis, and this research is\r\nsupported in part by this Google Fellowship. Wolf R¨odiger is\r\na recipient of the Oracle External Research Fellowship. This\r\nwork is further sponsored by the German Federal Ministry of\r\nEducation and Research (BMBF) grant HDBC 01IS12026.\r\n9. REFERENCES\r\n[1] A. Abouzeid, K. Bajda-Pawlikowski, D. J. Abadi,\r\nA. Rasin, and A. Silberschatz. HadoopDB: An\r\nArchitectural Hybrid of MapReduce and DBMS\r\nTechnologies for Analytical Workloads. PVLDB,\r\n2(1):922–933, 2009.\r\n[2] A. Ailamaki, D. J. DeWitt, M. D. Hill, and D. A.\r\nWood. DBMSs on a modern processor: Where does\r\ntime go? VLDB, pages 266–277, 1999.\r\n[3] I. Alagiannis, R. Borovica, M. Branco, S. Idreos, and\r\nA. Ailamaki. NoDB: Efficient Query Execution on\r\nRaw Data Files. In SIGMOD, pages 241–252, 2012.\r\n[4] P. Boncz, M. Zukowski, and N. Nes. MonetDB/X100:\r\nHyper-pipelining query execution. In CIDR, pages\r\n225–237, 2005.\r\n[5] J. Dean. MapReduce: simplified data processing on\r\nlarge clusters. CACM, 51(1):107–113, 2008.\r\n[6] D. J. DeWitt, A. Halverson, R. Nehme, S. Shankar,\r\nJ. Aguilar-Saborit, et al. Split Query Processing in\r\nPolybase. In SIGMOD, pages 1255–1266, 2013.\r\n[7] J. Dittrich, J.-A. Quian´e-Ruiz, S. Richter, S. Schuh,\r\nA. Jindal, and J. Schad. Only Aggressive Elephants\r\nare Fast Elephants. PVLDB, 5(11):1591–1602, 2012.\r\n[8] G. Graefe. B-tree indexes for high update rates.\r\nSIGMOD Rec., 35(1):39–44, 2006.\r\n[9] G. Graefe and H. Kuno. Fast Loads and Queries. In\r\nTLDKS II, number 6380 in LNCS, pages 31–72, 2010.\r\n[10] J. Gray, D. Liu, M. Nieto-Santisteban, A. Szalay,\r\nD. DeWitt, and G. Heber. Scientific Data\r\nManagement in the Coming Decade. SIGMOD Rec.,\r\n34(4):34–41, 2005.\r\n[11] Hive user group presentation from Netflix.\r\nhttp://slideshare.net/slideshow/embed_code/3483386.\r\n[12] S. Idreos, I. Alagiannis, R. Johnson, and A. Ailamaki.\r\nHere are my Data Files. Here are my Queries. Where\r\nare my Results? In CIDR, pages 57–68, 2011.\r\n[13] S. Idreos, M. L. Kersten, and S. Manegold. Database\r\nCracking. In CIDR, pages 68–78, 2007.\r\n[14] S. Idreos, S. Manegold, H. Kuno, and G. Graefe.\r\nMerging what’s cracked, cracking what’s merged:\r\nadaptive indexing in main-memory column-stores.\r\nPVLDB, 4(9):586–597, 2011.\r\n[15] Extending the worlds most popular processor\r\narchitecture. Intel Whitepaper, 2006.\r\n[16] M. Ivanova, M. Kersten, and S. Manegold. Data\r\nVaults: A Symbiosis between Database Technology\r\nand Scientific File Repositories. In SSDM, volume\r\n7338 of LNCS, pages 485–494, 2012.\r\n[17] R. Johnson and I. Pandis. The bionic DBMS is\r\ncoming, but what will it look like? In CIDR, 2013.\r\n[18] R. Kallman, H. Kimura, J. Natkins, A. Pavlo,\r\nA. Rasin, et al. H-store: a high-performance,\r\ndistributed main memory transaction processing\r\nsystem. PVLDB, 1(2):1496–1499, 2008.\r\n[19] A. Kemper and T. Neumann. HyPer: A hybrid\r\nOLTP&OLAP main memory database system based\r\non virtual memory snapshots. In ICDE, pages\r\n195–206, 2011.\r\n[20] V. Leis, A. Kemper, and T. Neumann. The Adaptive\r\nRadix Tree: ARTful Indexing for Main-Memory\r\nDatabases. In ICDE, pages 38–49, 2013.\r\n[21] G. Moerkotte. Small Materialized Aggregates: A Light\r\nWeight Index Structure for Data Warehousing. VLDB,\r\npages 476–487, 1998.\r\n[22] T. Neumann. Efficiently compiling efficient query\r\nplans for modern hardware. PVLDB, 4(9):539–550,\r\n2011.\r\n[23] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J.\r\nDeWitt, et al. A Comparison of Approaches to\r\nLarge-Scale Data Analysis. In SIGMOD, pages\r\n165–178, 2009.\r\n[24] J. Reinders. Intel threading building blocks: outfitting\r\nC++ for multi-core processor parallelism. 2007.\r\n[25] E. Sedlar. Oracle Labs. Personal comm. May 29, 2013.\r\n[26] A. Szalay. JHU. Personal comm. May 16, 2013.\r\n[27] A. Szalay, A. R. Thakar, and J. Gray. The sqlLoader\r\nData-Loading Pipeline. JCSE, 10:38–48, 2008.\r\n[28] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka,\r\net al. Hive: A warehousing solution over a map-reduce\r\nframework. PVLDB, 2(2):1626–1629, 2009.\r\n[29] T. Willhalm, N. Popovici, Y. Boshmaf, H. Plattner,\r\nA. Zeier, et al. SIMD-Scan: Ultra Fast in-Memory\r\nTable Scan using on-Chip Vector Processing Units.\r\nPVLDB, 2(1):385–394, 2009.\r\n[30] Y. Shafranovich. IETF RFC 4180, 2005.\r\n[31] J. Zhou and K. A. Ross. Implementing database\r\noperations using SIMD instructions. In SIGMOD,\r\npages 145–156, 2002.\r\n1713",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/dac75bef-a10f-4340-ae25-759b50d5202b/images/e5dbfef2-ba49-4275-8a9e-77bda838cf60.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T024223Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9c39140240536abb5ebe822e239aac003298055dcd0b685451f69d1c6f4180ee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 875
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\n  \"title\": \"\"\n}\n```\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "```\nauthor:  (missing information)\n```\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "```json\n{\n  \"date_published\": \"\"\n}\n```\n"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "```\nlocation:  (no location is provided in the context)\n```\n"
        }
      ]
    }
  }
}