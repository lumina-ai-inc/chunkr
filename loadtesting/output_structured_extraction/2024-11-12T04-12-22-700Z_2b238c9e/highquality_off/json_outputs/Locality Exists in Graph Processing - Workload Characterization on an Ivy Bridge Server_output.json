{
  "file_name": "Locality Exists in Graph Processing - Workload Characterization on an Ivy Bridge Server.pdf",
  "task_id": "0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "c6205c69-937c-4106-a2cf-5fd95e6d5c98",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Locality Exists in Graph Processing:\r\nWorkload Characterization on an Ivy Bridge Server\r\nScott Beamer Krste Asanovic David Patterson ´\r\nElectrical Engineering & Computer Sciences Department\r\nUniversity of California\r\nBerkeley, California\r\n{sbeamer,krste,pattrsn}@eecs.berkeley.edu\r\nAbstract—Graph processing is an increasingly important ap\u0002plication domain and is typically communication-bound. In this\r\nwork, we analyze the performance characteristics of three high\u0002performance graph algorithm codebases using hardware per\u0002formance counters on a conventional dual-socket server. Unlike\r\nmany other communication-bound workloads, graph algorithms\r\nstruggle to fully utilize the platform’s memory bandwidth and\r\nso increasing memory bandwidth utilization could be just as\r\neffective as decreasing communication. Based on our observations\r\nof simultaneous low compute and bandwidth utilization, we find\r\nthere is substantial room for a different processor architecture to\r\nimprove performance without requiring a new memory system.\r\nI. INTRODUCTION\r\nGraph processing is experiencing a surge of interest, as\r\napplications in social networks and their analysis have grown\r\nin importance [25], [31], [45]. Additionally, graph algorithms\r\nhave found new applications in recognition [28], [46] and the\r\nsciences [39].\r\nGraph algorithms are notoriously difficult to execute ef\u0002ficiently, and so there has been considerable recent effort in\r\nimproving the performance of processing large graphs for these\r\nimportant applications. Their inefficiency is due to the large\r\nvolume and irregular pattern of communication between com\u0002putations at each vertex or edge. When executed on a shared\u0002memory multiprocessor, this large volume of communication\r\nis translated into loads and stores in the memory hierarchy.\r\nWhen executed in parallel on a large-scale distributed cluster,\r\nthis communication is translated into messages across the\r\ninter-processor network. Because message-passing is far less\r\nefficient than accessing memory in contemporary systems,\r\ndistributed clusters are a poor match to graph processing. For\r\nexample, on Graph500, a world ranking of the fastest super\u0002computers for graph algorithms, the efficiency of each core in\r\na cluster is on average one to two orders-of-magnitude lower\r\nthan cores in shared-memory nodes [21]. This communication\u0002bound behavior has led to surprising results, where a single\r\nMac Mini operating on a large graph stored in an SSD is able\r\nto outperform a medium-sized cluster [26].\r\nDue to the inefficiency of message-passing communication,\r\nthe only reason to use a cluster for graph processing is if the\r\ndata is too large to fit on a single node [30]. However, many\r\ninteresting graph problems are not large enough to justify a\r\ncluster. For example, the entire Facebook friend graph requires\r\nonly 1.5 TB uncompressed [3], which can reside in a single\r\nhigh-end server node’s memory today.\r\nIn this paper, we focus on the performance of a shared\u0002memory multiprocessor node executing optimized graph algo\u0002rithms. We analyze the performance of three high-performance\r\ngraph processing codebases each using a different parallel\r\nruntime, and we measure results for these graph libraries using\r\nfive different graph kernels and a variety of input graphs. We\r\nuse microbenchmarks and hardware performance counters to\r\nanalyze the bottlenecks these optimized codes experience when\r\nexecuted on a modern Intel Ivy Bridge server. We derive the\r\nfollowing insights from our analysis:\r\n• Memory bandwidth is not fully utilized - Surprisingly,\r\nthe other bottlenecks described below prevent the off\u0002chip memory system from achieving full utilization on\r\nwell-tuned parallel graph codes. In other words, there\r\nis the potential for significant performance improve\u0002ment on graph codes with current off-chip memory\r\nsystems.\r\n• Graph codes exhibit substantial locality - Optimized\r\ngraph codes experience a moderately high last-level\r\ncache (LLC) hit rate.\r\n• Reorder buffer size limits achievable memory through\u0002put - The relatively high LLC hit rate implies many\r\ninstructions are executed for each LLC miss. These\r\ninstructions fill the reorder buffer in the core, prevent\u0002ing future loads that will miss in the LLC from issuing\r\nearly, resulting in unused memory bandwidth.\r\n• Multithreading has limited potential for graph pro\u0002cessing - In the context of a large superscalar out\u0002of-order multicore, we see only modest room for\r\nperformance improvement on graph codes from mul\u0002tithreading and that is likely achievable with only a\r\nmodest number (2) of threads per core.\r\nWe also confirm conventional wisdom that the most effi\u0002cient algorithms are often the hardest to parallelize, and that\r\nthese algorithms have their scaling hampered by load imbal\u0002ance, synchronization overheads, and non-uniform memory\r\naccess (NUMA) penalties. Additionally, we find that different\r\ninput graph sizes and topologies can lead to very different\r\nconclusions for algorithms and architectures, so it is important\r\nto consider a range of input graphs in any analysis.\r\nBased on our empirical results, we make recommendations\r\nfor future work in both hardware and software to improve\r\ngraph algorithm performance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/c6205c69-937c-4106-a2cf-5fd95e6d5c98.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=96dd9a0859891a01e281a282950c0e4299f53bc1c7ddee5dc30a5b762a004ade",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 745
      },
      {
        "segments": [
          {
            "segment_id": "c6205c69-937c-4106-a2cf-5fd95e6d5c98",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Locality Exists in Graph Processing:\r\nWorkload Characterization on an Ivy Bridge Server\r\nScott Beamer Krste Asanovic David Patterson ´\r\nElectrical Engineering & Computer Sciences Department\r\nUniversity of California\r\nBerkeley, California\r\n{sbeamer,krste,pattrsn}@eecs.berkeley.edu\r\nAbstract—Graph processing is an increasingly important ap\u0002plication domain and is typically communication-bound. In this\r\nwork, we analyze the performance characteristics of three high\u0002performance graph algorithm codebases using hardware per\u0002formance counters on a conventional dual-socket server. Unlike\r\nmany other communication-bound workloads, graph algorithms\r\nstruggle to fully utilize the platform’s memory bandwidth and\r\nso increasing memory bandwidth utilization could be just as\r\neffective as decreasing communication. Based on our observations\r\nof simultaneous low compute and bandwidth utilization, we find\r\nthere is substantial room for a different processor architecture to\r\nimprove performance without requiring a new memory system.\r\nI. INTRODUCTION\r\nGraph processing is experiencing a surge of interest, as\r\napplications in social networks and their analysis have grown\r\nin importance [25], [31], [45]. Additionally, graph algorithms\r\nhave found new applications in recognition [28], [46] and the\r\nsciences [39].\r\nGraph algorithms are notoriously difficult to execute ef\u0002ficiently, and so there has been considerable recent effort in\r\nimproving the performance of processing large graphs for these\r\nimportant applications. Their inefficiency is due to the large\r\nvolume and irregular pattern of communication between com\u0002putations at each vertex or edge. When executed on a shared\u0002memory multiprocessor, this large volume of communication\r\nis translated into loads and stores in the memory hierarchy.\r\nWhen executed in parallel on a large-scale distributed cluster,\r\nthis communication is translated into messages across the\r\ninter-processor network. Because message-passing is far less\r\nefficient than accessing memory in contemporary systems,\r\ndistributed clusters are a poor match to graph processing. For\r\nexample, on Graph500, a world ranking of the fastest super\u0002computers for graph algorithms, the efficiency of each core in\r\na cluster is on average one to two orders-of-magnitude lower\r\nthan cores in shared-memory nodes [21]. This communication\u0002bound behavior has led to surprising results, where a single\r\nMac Mini operating on a large graph stored in an SSD is able\r\nto outperform a medium-sized cluster [26].\r\nDue to the inefficiency of message-passing communication,\r\nthe only reason to use a cluster for graph processing is if the\r\ndata is too large to fit on a single node [30]. However, many\r\ninteresting graph problems are not large enough to justify a\r\ncluster. For example, the entire Facebook friend graph requires\r\nonly 1.5 TB uncompressed [3], which can reside in a single\r\nhigh-end server node’s memory today.\r\nIn this paper, we focus on the performance of a shared\u0002memory multiprocessor node executing optimized graph algo\u0002rithms. We analyze the performance of three high-performance\r\ngraph processing codebases each using a different parallel\r\nruntime, and we measure results for these graph libraries using\r\nfive different graph kernels and a variety of input graphs. We\r\nuse microbenchmarks and hardware performance counters to\r\nanalyze the bottlenecks these optimized codes experience when\r\nexecuted on a modern Intel Ivy Bridge server. We derive the\r\nfollowing insights from our analysis:\r\n• Memory bandwidth is not fully utilized - Surprisingly,\r\nthe other bottlenecks described below prevent the off\u0002chip memory system from achieving full utilization on\r\nwell-tuned parallel graph codes. In other words, there\r\nis the potential for significant performance improve\u0002ment on graph codes with current off-chip memory\r\nsystems.\r\n• Graph codes exhibit substantial locality - Optimized\r\ngraph codes experience a moderately high last-level\r\ncache (LLC) hit rate.\r\n• Reorder buffer size limits achievable memory through\u0002put - The relatively high LLC hit rate implies many\r\ninstructions are executed for each LLC miss. These\r\ninstructions fill the reorder buffer in the core, prevent\u0002ing future loads that will miss in the LLC from issuing\r\nearly, resulting in unused memory bandwidth.\r\n• Multithreading has limited potential for graph pro\u0002cessing - In the context of a large superscalar out\u0002of-order multicore, we see only modest room for\r\nperformance improvement on graph codes from mul\u0002tithreading and that is likely achievable with only a\r\nmodest number (2) of threads per core.\r\nWe also confirm conventional wisdom that the most effi\u0002cient algorithms are often the hardest to parallelize, and that\r\nthese algorithms have their scaling hampered by load imbal\u0002ance, synchronization overheads, and non-uniform memory\r\naccess (NUMA) penalties. Additionally, we find that different\r\ninput graph sizes and topologies can lead to very different\r\nconclusions for algorithms and architectures, so it is important\r\nto consider a range of input graphs in any analysis.\r\nBased on our empirical results, we make recommendations\r\nfor future work in both hardware and software to improve\r\ngraph algorithm performance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/c6205c69-937c-4106-a2cf-5fd95e6d5c98.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=96dd9a0859891a01e281a282950c0e4299f53bc1c7ddee5dc30a5b762a004ade",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 745
      },
      {
        "segments": [
          {
            "segment_id": "4d96bfc7-34c0-45d2-baa8-7b3986a8012f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "II. GRAPH BACKGROUND\r\nGraph applications are characterized not only by the algo\u0002rithms used, but also by the structure of the graphs that make\r\nup their workload. A graph’s diameter is the largest number\r\nof vertices that must be traversed in order to travel from one\r\nvertex to another when paths that backtrack, detour, or loop\r\nare excluded from consideration. The degree of a node in a\r\ngraph is the number of connections it has to other nodes, and\r\nthe degree distribution is the probability distribution of these\r\ndegrees over the whole graph.\r\nCommonly used graphs can be divided into two broad\r\ncategories named for their most emblematic members: meshes\r\nand social networks [7]. Meshes tend to be derived from\r\nphysically spatial sources, such as road maps or the finite\u0002element mesh of a simulated car body, so they can be relatively\r\nreadily partitioned along the few original spatial dimensions.\r\nDue to their physical origin, they usually have a high diameter\r\nand a degree distribution that is both bounded and low.\r\nConversely, social networks come from non-spatial sources,\r\nand consequently are difficult to partition using any reasonable\r\nnumber of dimensions. Additionally, social networks have a\r\nlow diameter (“small-world”) and a power-law degree distri\u0002bution (“scale-free”). In a small-world graph, most nodes are\r\nnot neighbors of one another, but most nodes can be reached\r\nfrom every other by a small number of hops [42]. A scale\u0002free graph has a degree distribution that follows a power law,\r\nat least asymptotically [5]. The fraction of nodes in a scale\u0002free graph having k connections to other nodes is P(k) ∼ k\r\n−γ\r\n,\r\nwhere γ is a parameter typically in the range 2 < γ < 3.\r\nMeshes are perhaps the most common mental model for\r\ngraphs, since they are typically used in textbook figures.\r\nUnfortunately, they do not capture the challenges posed by\r\nthe small-world and scale-free properties of social network\r\ntopologies. The small-world property makes them difficult to\r\npartition (few cut edges relative to enclosed edges), while the\r\nscale-free property makes it difficult to load balance a parallel\r\nexecution since there can be many orders of magnitude dif\u0002ference between the work for different vertices. Although the\r\nhighest degree vertices are rare, their incident edges constitute\r\na large fraction of the graph.\r\nIII. METHODOLOGY\r\nTo provide a representative graph workload, we chose five\r\npopular graph kernels and exercised them with five different\r\ninput graphs using three high-performance graph codebases\r\nrunning on a modern high-end server. Unless otherwise stated,\r\nwe measure the full workload of all combinations of code\u0002bases, kernels, and graphs (75 data points) for each system\r\nconfiguration.\r\nA. Graph Kernels and Input Graphs\r\nWe selected five graph kernels based on their popularity in\r\napplications as well as in other research papers:\r\n1) Breadth-First Search (BFS) is actually only a traver\u0002sal order, but it is so fundamental to graph algorithms\r\nthat we include it in our suite. We convert BFS into a\r\nkernel by tracking the parent vertex in the traversal.\r\n2) Single-Source Shortest Paths (SSSP) computes the\r\ndistance to all reachable vertices from a start vertex.\r\n3) PageRank (PR) is way of determining influence\r\nwithin a graph, and was initially used to sort search\r\nresults [38].\r\n4) Connected Components (CC) attaches the same\r\nlabel to all vertices in the same connected component.\r\n5) Betweenness Centrality (BC) is commonly used\r\nin social network analysis to measure the influence\r\na vertex has on a graph. A vertex’s betweenness\u0002centrality score is related to the fraction of shortest\r\npaths between all vertices that pass through the ver\u0002tex. To keep runtimes tractable, our BC benchmark\r\nstarts from only a few root vertices instead of every\r\nvertex.\r\nWe selected the input graphs used in our evaluation to be\r\ntopologically diverse and Table I lists them. Twitter, road, and\r\nweb are all from real-world data, while kron and uniform are\r\nsynthetic. Twitter, web, and kron all have the “social network”\r\ntopology, as they have low effective diameters and a power-law\r\ndegree distribution. Road is an example of a “mesh” topology,\r\nwith its high diameter, low average degree, and low maximum\r\ndegree. Even though our graph suite includes some of the\r\nlargest publicly available real-world graphs, they do not fully\r\nuse the memory capacity of our system. As is done in the\r\nGraph500 benchmark, we generate arbitrarily large synthetic\r\ngraphs to fill our memory capacity. Our parameters for kron\r\nare chosen to match those of Graph500 [21]. Uniform is low\r\ndiameter, like a social network, but its degree distribution is\r\nnormal rather than a power law. Hence, in our uniform graph\r\neach vertex tends to be accessed roughly the same number\r\nof times, unlike social networks where a few vertices are\r\naccessed disproportionately often. Uniform represents the most\r\nadversarial graph, as by design it has no locality, however, it\r\nis also the most unrealistic and serves to act as lower bound\r\non performance.\r\nB. Graph Processing Frameworks\r\nFor this study, we use three of the fastest available graph\r\ncodebases, which each use a different parallel runtime.\r\nGalois [36] uses its own custom parallel runtime specifi\u0002cally designed to handle irregular fine-grained task parallelism.\r\nAlgorithms implemented in Galois are free to use autonomous\r\nscheduling (no synchronization barriers), which should re\u0002duce the synchronization otherwise needed for high-diameter\r\ngraphs. Additionally, Galois’ scheduler takes into consideration\r\nthe plaform’s core and socket topology.\r\nLigra [40] uses the Cilk [8] parallel runtime and is built on\r\nthe abstractions of edge maps and vertex maps. When applying\r\nthese map functions, Ligra uses heuristics to determine in\r\nwhich direction to apply them (push or pull) and what data\r\nstructures to use (sparse or dense). These optimizations make\r\nLigra especially well suited for low-diameter graphs.\r\nGAP Benchmark Suite (GAPBS) [6], [19] is a collec\u0002tion of high-performance implementations written directly in\r\nOpenMP with C++11. GAPBS is not a framework, so it does\r\nnot force common abstractions onto all implementations, but\r\ninstead frees each to do whatever is appropriate for a given\r\nalgorithm.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/4d96bfc7-34c0-45d2-baa8-7b3986a8012f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=38a6edd3df8b08f97c9f6082ba51891f8d2ccd9d782aee4e1aabfb5da64a0bf6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 988
      },
      {
        "segments": [
          {
            "segment_id": "4d96bfc7-34c0-45d2-baa8-7b3986a8012f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "II. GRAPH BACKGROUND\r\nGraph applications are characterized not only by the algo\u0002rithms used, but also by the structure of the graphs that make\r\nup their workload. A graph’s diameter is the largest number\r\nof vertices that must be traversed in order to travel from one\r\nvertex to another when paths that backtrack, detour, or loop\r\nare excluded from consideration. The degree of a node in a\r\ngraph is the number of connections it has to other nodes, and\r\nthe degree distribution is the probability distribution of these\r\ndegrees over the whole graph.\r\nCommonly used graphs can be divided into two broad\r\ncategories named for their most emblematic members: meshes\r\nand social networks [7]. Meshes tend to be derived from\r\nphysically spatial sources, such as road maps or the finite\u0002element mesh of a simulated car body, so they can be relatively\r\nreadily partitioned along the few original spatial dimensions.\r\nDue to their physical origin, they usually have a high diameter\r\nand a degree distribution that is both bounded and low.\r\nConversely, social networks come from non-spatial sources,\r\nand consequently are difficult to partition using any reasonable\r\nnumber of dimensions. Additionally, social networks have a\r\nlow diameter (“small-world”) and a power-law degree distri\u0002bution (“scale-free”). In a small-world graph, most nodes are\r\nnot neighbors of one another, but most nodes can be reached\r\nfrom every other by a small number of hops [42]. A scale\u0002free graph has a degree distribution that follows a power law,\r\nat least asymptotically [5]. The fraction of nodes in a scale\u0002free graph having k connections to other nodes is P(k) ∼ k\r\n−γ\r\n,\r\nwhere γ is a parameter typically in the range 2 < γ < 3.\r\nMeshes are perhaps the most common mental model for\r\ngraphs, since they are typically used in textbook figures.\r\nUnfortunately, they do not capture the challenges posed by\r\nthe small-world and scale-free properties of social network\r\ntopologies. The small-world property makes them difficult to\r\npartition (few cut edges relative to enclosed edges), while the\r\nscale-free property makes it difficult to load balance a parallel\r\nexecution since there can be many orders of magnitude dif\u0002ference between the work for different vertices. Although the\r\nhighest degree vertices are rare, their incident edges constitute\r\na large fraction of the graph.\r\nIII. METHODOLOGY\r\nTo provide a representative graph workload, we chose five\r\npopular graph kernels and exercised them with five different\r\ninput graphs using three high-performance graph codebases\r\nrunning on a modern high-end server. Unless otherwise stated,\r\nwe measure the full workload of all combinations of code\u0002bases, kernels, and graphs (75 data points) for each system\r\nconfiguration.\r\nA. Graph Kernels and Input Graphs\r\nWe selected five graph kernels based on their popularity in\r\napplications as well as in other research papers:\r\n1) Breadth-First Search (BFS) is actually only a traver\u0002sal order, but it is so fundamental to graph algorithms\r\nthat we include it in our suite. We convert BFS into a\r\nkernel by tracking the parent vertex in the traversal.\r\n2) Single-Source Shortest Paths (SSSP) computes the\r\ndistance to all reachable vertices from a start vertex.\r\n3) PageRank (PR) is way of determining influence\r\nwithin a graph, and was initially used to sort search\r\nresults [38].\r\n4) Connected Components (CC) attaches the same\r\nlabel to all vertices in the same connected component.\r\n5) Betweenness Centrality (BC) is commonly used\r\nin social network analysis to measure the influence\r\na vertex has on a graph. A vertex’s betweenness\u0002centrality score is related to the fraction of shortest\r\npaths between all vertices that pass through the ver\u0002tex. To keep runtimes tractable, our BC benchmark\r\nstarts from only a few root vertices instead of every\r\nvertex.\r\nWe selected the input graphs used in our evaluation to be\r\ntopologically diverse and Table I lists them. Twitter, road, and\r\nweb are all from real-world data, while kron and uniform are\r\nsynthetic. Twitter, web, and kron all have the “social network”\r\ntopology, as they have low effective diameters and a power-law\r\ndegree distribution. Road is an example of a “mesh” topology,\r\nwith its high diameter, low average degree, and low maximum\r\ndegree. Even though our graph suite includes some of the\r\nlargest publicly available real-world graphs, they do not fully\r\nuse the memory capacity of our system. As is done in the\r\nGraph500 benchmark, we generate arbitrarily large synthetic\r\ngraphs to fill our memory capacity. Our parameters for kron\r\nare chosen to match those of Graph500 [21]. Uniform is low\r\ndiameter, like a social network, but its degree distribution is\r\nnormal rather than a power law. Hence, in our uniform graph\r\neach vertex tends to be accessed roughly the same number\r\nof times, unlike social networks where a few vertices are\r\naccessed disproportionately often. Uniform represents the most\r\nadversarial graph, as by design it has no locality, however, it\r\nis also the most unrealistic and serves to act as lower bound\r\non performance.\r\nB. Graph Processing Frameworks\r\nFor this study, we use three of the fastest available graph\r\ncodebases, which each use a different parallel runtime.\r\nGalois [36] uses its own custom parallel runtime specifi\u0002cally designed to handle irregular fine-grained task parallelism.\r\nAlgorithms implemented in Galois are free to use autonomous\r\nscheduling (no synchronization barriers), which should re\u0002duce the synchronization otherwise needed for high-diameter\r\ngraphs. Additionally, Galois’ scheduler takes into consideration\r\nthe plaform’s core and socket topology.\r\nLigra [40] uses the Cilk [8] parallel runtime and is built on\r\nthe abstractions of edge maps and vertex maps. When applying\r\nthese map functions, Ligra uses heuristics to determine in\r\nwhich direction to apply them (push or pull) and what data\r\nstructures to use (sparse or dense). These optimizations make\r\nLigra especially well suited for low-diameter graphs.\r\nGAP Benchmark Suite (GAPBS) [6], [19] is a collec\u0002tion of high-performance implementations written directly in\r\nOpenMP with C++11. GAPBS is not a framework, so it does\r\nnot force common abstractions onto all implementations, but\r\ninstead frees each to do whatever is appropriate for a given\r\nalgorithm.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/4d96bfc7-34c0-45d2-baa8-7b3986a8012f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=38a6edd3df8b08f97c9f6082ba51891f8d2ccd9d782aee4e1aabfb5da64a0bf6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 988
      },
      {
        "segments": [
          {
            "segment_id": "823a4213-d9ac-4fbc-b4d2-01b46a377f31",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Short Name Description # Vertices (M) # Edges (M) Degree Degree Distribution Approximate Diameter References\r\nroad Roads of USA 23.9 58.3 2.4 bounded 6,304 [15]\r\ntwitter Twitter Follow Links 61.6 1,468.4 23.8 power 14 [25]\r\nweb Web Crawl of .sk Domain 50.6 1,949.4 38.5 power 135 [14]\r\nkron Kronecker Synthetic Graph 128.0 2,048.0 16.0 power 6 [21], [27]\r\nuniform Uniform Random Graph 128.0 2,048.0 16.0 normal 7 [18]\r\nTABLE I. GRAPHS USED FOR EVALUATION\r\nAll three codebases are competitive, and depending on the\r\ninput graph or kernel, a different codebase is the fastest. For\r\ndescriptions of the implementations and their parallelization\r\nstrategies, we refer the reader to the original publications.\r\nC. Hardware Platform\r\nTo perform our measurements, we use a current dual\u0002socket Intel Ivy Bridge server (IVB) with E5-2667 v2 pro\u0002cessors, similar to what one would find in a datacenter. Each\r\nsocket contains eight 3.3 GHz two-way multithreaded cores\r\nand 25 MB of last-level cache (LLC). The server has 128 GB\r\nof DDR3-1600 DRAM provided by 16 DIMMS. To access\r\nhardware performance counters, we use Intel PCM [24] and\r\nPAPI [32]. We compile all code with gcc-4.8, except Ligra that\r\nuses Cilk Plus gcc-4.8. To ensure consistency across runs, we\r\ndisable Turbo Boost (dynamic voltage and frequency scaling).\r\nWhen reporting memory traffic from the performance coun\u0002ters, we focus on memory requests caused by LLC misses as\r\nthese are the most problematic for performance. We do not\r\ninclude prefetch traffic measurements because they obscure\r\nthe results, but benefits of successful prefetching appear in\u0002directly as fewer cache misses. During our study, we observed\r\nIVB intelligently prefetching aggressively when the memory\r\nbandwidth utilization would otherwise be low, but ceasing\r\nprefetching when the application is using a large fraction of the\r\nmemory bandwidth (the hardware prefetcher does not prevent\r\nfull memory bandwidth utilization).\r\nIV. MEMORY BANDWIDTH POTENTIAL\r\nAny LLC miss will cause even a large out-of-order proces\u0002sor to stall for a significant number of cycles. Ideally, while\r\nwaiting for the first cache miss to resolve, at least some useful\r\nwork could be done, including initiating loads early that will\r\ncause future cache misses. Unfortunately, a load must satisfy\r\nthe following four requirements before it can be issued:\r\n1) Processor fetches load instruction - Control flow\r\nreaches the load instruction (possibly speculatively).\r\n2) Space in instruction window - The Reorder Buffer\r\n(ROB) is not full and has room for the load.\r\n3) Register operands are available - The load address\r\ncan be generated.\r\n4) Memory bandwidth is available - At the core level\r\nthere is a miss-status holding register (MSHR) avail\u0002able and there is not excessive contention within the\r\non-chip interconnect or at the memory controller.\r\nIf any of the above requirements is not met, the load will be\r\nunable to issue. In particular, memory bandwidth cannot be a\r\nbottleneck unless the first three requirements are satisfied, thus\r\nthe other factors can prevent memory bandwidth from being\r\nfully utilized.\r\nWe use a parallel pointer-chase as a synthetic microbench\u0002mark to measure the achievable memory bandwidth on IVB\r\nunder various conditions. A parallel pointer-chase exposes the\r\nneeded parameters but is otherwise quite simple [1], [35]. With\r\na single pointer-chase, there is no memory-level parallelism\r\n(MLP) and the memory latency is exposed since requests must\r\nbe completed serially. To generate more MLP, we simply add\r\nmore parallel pointer chases to the same thread.\r\nTo force loads to access the memory system, we set\r\npointers to point randomly within an array sized large enough\r\nsuch that LLC hit rates are less than 1.5% (typically ≥ 2\r\nGB). We report bandwidths in terms of memory references\r\nper second as measured by performance counters. We also\r\nreport achieved bandwidths in terms of effective MLP, which\r\nis the average number of memory requests in flight according\r\nto Little’s Law (memory bandwidth × memory latency). It\r\nis worth distinguishing this from application MLP, which\r\nis how much memory-request parallelism is allowed by the\r\napplication’s data dependencies, which will be always greater\r\nthan or equal to the achieved effective MLP.\r\nOur simple microbenchmark is designed to trivially satisfy\r\nthe first two requirements above, allowing us to focus on\r\nand measure the last two. Branch mispredictions should be\r\nrare since the loop repeats many times, so fetching the load\r\ninstructions should not be hindered. The microbenchmark is\r\na tight loop, so there should be a relatively high density of\r\nloads thus reducing the impact of instruction window size (168\r\nfor Ivy Bridge). By changing the number of parallel pointer\u0002chases, we can artificially control the maximum application\r\nMLP possible, which allows us to moderate the operand\r\navailability requirement. We can then observe what bandwidths\r\nare possible and even what the bandwidth limits are.\r\nFigure 1 shows the microbenchmark results. The local\r\nmemory latency is 86 ns (MLP=1). Local bandwidth for a\r\nsingle thread appears to saturate when MLP≥10, implying the\r\ncore supports 10 outstanding misses, and this is confirmed by\r\npublished sources on the Ivy Bridge microarchitecture. Using a\r\nsecond thread on the same core does not change the maximum\r\nbandwidth regardless of how the outstanding memory requests\r\nare spread across the two threads.\r\nTo see the impacts of Non-Uniform Memory Access\r\n(NUMA) on our dual-socket system, instead of allocating the\r\nmemory being used by our microbenchmark on the same\r\nsocket (local), we allocate on the other socket (remote) or\r\ninterleaved across both sockets (interleaved). NUMA may\r\nintroduce bandwidth restrictions, but for a single core in isola\u0002tion, the primary consequence is twice the latency (≈184 ns).\r\nWhen accessing remote memory, the maximum bandwidth is\r\nhalved due to the same number of outstanding data requests\r\nexperiencing twice the latency.\r\nAfter exploring how application MLP changes bandwidth",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/823a4213-d9ac-4fbc-b4d2-01b46a377f31.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7bb9f4f4dd84ab870abebdf9b831fcb09776cdbbe55076835f63c66c96aade72",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 939
      },
      {
        "segments": [
          {
            "segment_id": "823a4213-d9ac-4fbc-b4d2-01b46a377f31",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Short Name Description # Vertices (M) # Edges (M) Degree Degree Distribution Approximate Diameter References\r\nroad Roads of USA 23.9 58.3 2.4 bounded 6,304 [15]\r\ntwitter Twitter Follow Links 61.6 1,468.4 23.8 power 14 [25]\r\nweb Web Crawl of .sk Domain 50.6 1,949.4 38.5 power 135 [14]\r\nkron Kronecker Synthetic Graph 128.0 2,048.0 16.0 power 6 [21], [27]\r\nuniform Uniform Random Graph 128.0 2,048.0 16.0 normal 7 [18]\r\nTABLE I. GRAPHS USED FOR EVALUATION\r\nAll three codebases are competitive, and depending on the\r\ninput graph or kernel, a different codebase is the fastest. For\r\ndescriptions of the implementations and their parallelization\r\nstrategies, we refer the reader to the original publications.\r\nC. Hardware Platform\r\nTo perform our measurements, we use a current dual\u0002socket Intel Ivy Bridge server (IVB) with E5-2667 v2 pro\u0002cessors, similar to what one would find in a datacenter. Each\r\nsocket contains eight 3.3 GHz two-way multithreaded cores\r\nand 25 MB of last-level cache (LLC). The server has 128 GB\r\nof DDR3-1600 DRAM provided by 16 DIMMS. To access\r\nhardware performance counters, we use Intel PCM [24] and\r\nPAPI [32]. We compile all code with gcc-4.8, except Ligra that\r\nuses Cilk Plus gcc-4.8. To ensure consistency across runs, we\r\ndisable Turbo Boost (dynamic voltage and frequency scaling).\r\nWhen reporting memory traffic from the performance coun\u0002ters, we focus on memory requests caused by LLC misses as\r\nthese are the most problematic for performance. We do not\r\ninclude prefetch traffic measurements because they obscure\r\nthe results, but benefits of successful prefetching appear in\u0002directly as fewer cache misses. During our study, we observed\r\nIVB intelligently prefetching aggressively when the memory\r\nbandwidth utilization would otherwise be low, but ceasing\r\nprefetching when the application is using a large fraction of the\r\nmemory bandwidth (the hardware prefetcher does not prevent\r\nfull memory bandwidth utilization).\r\nIV. MEMORY BANDWIDTH POTENTIAL\r\nAny LLC miss will cause even a large out-of-order proces\u0002sor to stall for a significant number of cycles. Ideally, while\r\nwaiting for the first cache miss to resolve, at least some useful\r\nwork could be done, including initiating loads early that will\r\ncause future cache misses. Unfortunately, a load must satisfy\r\nthe following four requirements before it can be issued:\r\n1) Processor fetches load instruction - Control flow\r\nreaches the load instruction (possibly speculatively).\r\n2) Space in instruction window - The Reorder Buffer\r\n(ROB) is not full and has room for the load.\r\n3) Register operands are available - The load address\r\ncan be generated.\r\n4) Memory bandwidth is available - At the core level\r\nthere is a miss-status holding register (MSHR) avail\u0002able and there is not excessive contention within the\r\non-chip interconnect or at the memory controller.\r\nIf any of the above requirements is not met, the load will be\r\nunable to issue. In particular, memory bandwidth cannot be a\r\nbottleneck unless the first three requirements are satisfied, thus\r\nthe other factors can prevent memory bandwidth from being\r\nfully utilized.\r\nWe use a parallel pointer-chase as a synthetic microbench\u0002mark to measure the achievable memory bandwidth on IVB\r\nunder various conditions. A parallel pointer-chase exposes the\r\nneeded parameters but is otherwise quite simple [1], [35]. With\r\na single pointer-chase, there is no memory-level parallelism\r\n(MLP) and the memory latency is exposed since requests must\r\nbe completed serially. To generate more MLP, we simply add\r\nmore parallel pointer chases to the same thread.\r\nTo force loads to access the memory system, we set\r\npointers to point randomly within an array sized large enough\r\nsuch that LLC hit rates are less than 1.5% (typically ≥ 2\r\nGB). We report bandwidths in terms of memory references\r\nper second as measured by performance counters. We also\r\nreport achieved bandwidths in terms of effective MLP, which\r\nis the average number of memory requests in flight according\r\nto Little’s Law (memory bandwidth × memory latency). It\r\nis worth distinguishing this from application MLP, which\r\nis how much memory-request parallelism is allowed by the\r\napplication’s data dependencies, which will be always greater\r\nthan or equal to the achieved effective MLP.\r\nOur simple microbenchmark is designed to trivially satisfy\r\nthe first two requirements above, allowing us to focus on\r\nand measure the last two. Branch mispredictions should be\r\nrare since the loop repeats many times, so fetching the load\r\ninstructions should not be hindered. The microbenchmark is\r\na tight loop, so there should be a relatively high density of\r\nloads thus reducing the impact of instruction window size (168\r\nfor Ivy Bridge). By changing the number of parallel pointer\u0002chases, we can artificially control the maximum application\r\nMLP possible, which allows us to moderate the operand\r\navailability requirement. We can then observe what bandwidths\r\nare possible and even what the bandwidth limits are.\r\nFigure 1 shows the microbenchmark results. The local\r\nmemory latency is 86 ns (MLP=1). Local bandwidth for a\r\nsingle thread appears to saturate when MLP≥10, implying the\r\ncore supports 10 outstanding misses, and this is confirmed by\r\npublished sources on the Ivy Bridge microarchitecture. Using a\r\nsecond thread on the same core does not change the maximum\r\nbandwidth regardless of how the outstanding memory requests\r\nare spread across the two threads.\r\nTo see the impacts of Non-Uniform Memory Access\r\n(NUMA) on our dual-socket system, instead of allocating the\r\nmemory being used by our microbenchmark on the same\r\nsocket (local), we allocate on the other socket (remote) or\r\ninterleaved across both sockets (interleaved). NUMA may\r\nintroduce bandwidth restrictions, but for a single core in isola\u0002tion, the primary consequence is twice the latency (≈184 ns).\r\nWhen accessing remote memory, the maximum bandwidth is\r\nhalved due to the same number of outstanding data requests\r\nexperiencing twice the latency.\r\nAfter exploring how application MLP changes bandwidth",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/823a4213-d9ac-4fbc-b4d2-01b46a377f31.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7bb9f4f4dd84ab870abebdf9b831fcb09776cdbbe55076835f63c66c96aade72",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 939
      },
      {
        "segments": [
          {
            "segment_id": "610781d1-591f-4769-ab3d-b64e244a657e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "2 4 6 8 10 12\r\nApplication MLP / Thread\r\n0\r\n20\r\n40\r\n60\r\n80\r\n100\r\nMemory Requests (M) / Second\r\n1T Local\r\n2T Local\r\n1T Interleave\r\n2T Interleave\r\n1T Remote\r\n2T Remote\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nFig. 1. Memory bandwidth achieved by parallel pointer chase microbench\u0002mark (random) in units of memory requests per second (left axis) or equivalent\r\neffective MLP (right axis) versus the number of parallel chases (application\r\nMLP). Single core using 1 or 2 threads and differing memory allocation\r\nlocations (local, remote, and interleave).\r\n0 50 100 150 200 250\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nApplication MLP\r\n12\r\n8\r\n4\r\n2\r\n1\r\nmodel\r\nFig. 2. Memory bandwidth achieved by parallel pointer chase microbench\u0002mark with varying number of nops inserted (varies IPM). Using a single thread\r\nwith differing numbers of parallel chases (application MLP).\r\n(requirement 3) and how many outstanding misses the hard\u0002ware supports (requirement 4), we now return to the impact\r\nof the instruction window size (requirement 2). Using inline\r\nassembly, we add nops to our pointer-chase loop, thus moving\r\nthe loads farther apart in the instruction stream. To examine\r\nthe net result, we use the metric instructions per miss (IPM),\r\nwhich is the inverse of the common misses per kilo-instruction\r\nmetric (MPKI = 1000/IPM).\r\nAs shown in Figure 2, window size is an important con\u0002straint on our platform, as bandwidth is inversely related to\r\nIPM, which confirms our intuition that memory requests must\r\nfit in the window in order to be issued. Assuming the loads are\r\nevenly spaced, we obtain a simple model for an upper-bound\r\n(with w as the window size):\r\nMLPmodel = min(MLPmax, w/IPM + 1)\r\nFor our IVB core, MLPmax = 10 and w = 168. The\r\n2 4 6 8 10 12\r\nApplication MLP / Thread\r\n0\r\n20\r\n40\r\n60\r\n80\r\n100\r\nMemory Requests (M) / Second\r\nSmall-2MB\r\nSmall-1GB\r\nLarge-2MB\r\nLarge-1GB\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nFig. 3. Impact of 2 MB and 1 GB page sizes on memory bandwidth achieved\r\nby single-thread parallel pointer chase for array sizes of small (1 GB) and\r\nlarge (16 GB).\r\ncurved region adds one because if the window can hold n\r\nIPM-sized intervals, it can hold n + 1 endpoints. Our model\r\nis pessimistic as it assumes cache misses are evenly spaced.\r\nIf there is substantial variation in the miss interval (jitter),\r\nit is possible to exceed the model bound, but we find this\r\nsimple model instructive for the rest of the study as we observe\r\nbandwidth is inversely related to IPM.\r\nMemory bandwidth can also be constrained by frequent\r\nTLB misses. The four requirements above are necessary for\r\na load to issue, but once issued, missing in the TLB incurs\r\na latency penalty for its refill, which in turn will decrease\r\nbandwidth for the same number of outstanding memory re\u0002quests. IVB’s Linux distribution supports Transparent Huge\r\nPages (THP), which eagerly combines consecutive 4 KB pages\r\ninto 2 MB pages when possible. IVB also supports 1 GB pages,\r\nbut these must be set aside by Linux in advance and require\r\nsubstantial application code modifications. Larger pages not\r\nonly reduce the chance of a TLB miss, but they also reduce\r\nthe time per refill by needing fewer hops to walk the page\r\ntable and by reducing the size of the page-table working set\r\n(better cache locality).\r\nFigure 3 varies the page size (2 MB or 1 GB) and the\r\narray size (1 GB or 16 GB) for our pointer-chase synthetic\r\nmicrobenchmark. With 2 MB pages from THP, most loads for\r\nboth array sizes will result in a cache miss and a TLB miss\r\n(IVB has only 32 2 MB TLB entries), but the maximum band\u0002width obtained with the larger array is substantially reduced\r\ndue to increases in TLB refill time (confirmed by performance\r\ncounters). Using 1 GB pages restores the bandwidth even\r\nthough there will still be frequent TLB misses (only 4 1 GB\r\nTLB entries) because the refill time is reduced enough to not\r\nbe problematic. With 1 GB pages, the page table will only need\r\n16 entries and will likely remain in the L1 cache. Our random\r\nmicrobenchmark exemplifies the worst case for the TLB, so\r\nany form of locality will reduce the performance penalties from\r\nTLB misses.\r\nWe further parallelize our microbenchmark and run it\r\nacross all of the cores, and the maximum bandwidths we\r\nachieve are visible in Figure 10. The data in this section shows\r\nthe maximum achievable memory bandwidth for a core, socket,\r\nor entire system given the amount of application MLP, IPM,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/610781d1-591f-4769-ab3d-b64e244a657e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d5230588ab0206577a0f416870ed9665746b1e24a33b462e18dffb6c9ade2daa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 767
      },
      {
        "segments": [
          {
            "segment_id": "610781d1-591f-4769-ab3d-b64e244a657e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "2 4 6 8 10 12\r\nApplication MLP / Thread\r\n0\r\n20\r\n40\r\n60\r\n80\r\n100\r\nMemory Requests (M) / Second\r\n1T Local\r\n2T Local\r\n1T Interleave\r\n2T Interleave\r\n1T Remote\r\n2T Remote\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nFig. 1. Memory bandwidth achieved by parallel pointer chase microbench\u0002mark (random) in units of memory requests per second (left axis) or equivalent\r\neffective MLP (right axis) versus the number of parallel chases (application\r\nMLP). Single core using 1 or 2 threads and differing memory allocation\r\nlocations (local, remote, and interleave).\r\n0 50 100 150 200 250\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nApplication MLP\r\n12\r\n8\r\n4\r\n2\r\n1\r\nmodel\r\nFig. 2. Memory bandwidth achieved by parallel pointer chase microbench\u0002mark with varying number of nops inserted (varies IPM). Using a single thread\r\nwith differing numbers of parallel chases (application MLP).\r\n(requirement 3) and how many outstanding misses the hard\u0002ware supports (requirement 4), we now return to the impact\r\nof the instruction window size (requirement 2). Using inline\r\nassembly, we add nops to our pointer-chase loop, thus moving\r\nthe loads farther apart in the instruction stream. To examine\r\nthe net result, we use the metric instructions per miss (IPM),\r\nwhich is the inverse of the common misses per kilo-instruction\r\nmetric (MPKI = 1000/IPM).\r\nAs shown in Figure 2, window size is an important con\u0002straint on our platform, as bandwidth is inversely related to\r\nIPM, which confirms our intuition that memory requests must\r\nfit in the window in order to be issued. Assuming the loads are\r\nevenly spaced, we obtain a simple model for an upper-bound\r\n(with w as the window size):\r\nMLPmodel = min(MLPmax, w/IPM + 1)\r\nFor our IVB core, MLPmax = 10 and w = 168. The\r\n2 4 6 8 10 12\r\nApplication MLP / Thread\r\n0\r\n20\r\n40\r\n60\r\n80\r\n100\r\nMemory Requests (M) / Second\r\nSmall-2MB\r\nSmall-1GB\r\nLarge-2MB\r\nLarge-1GB\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nFig. 3. Impact of 2 MB and 1 GB page sizes on memory bandwidth achieved\r\nby single-thread parallel pointer chase for array sizes of small (1 GB) and\r\nlarge (16 GB).\r\ncurved region adds one because if the window can hold n\r\nIPM-sized intervals, it can hold n + 1 endpoints. Our model\r\nis pessimistic as it assumes cache misses are evenly spaced.\r\nIf there is substantial variation in the miss interval (jitter),\r\nit is possible to exceed the model bound, but we find this\r\nsimple model instructive for the rest of the study as we observe\r\nbandwidth is inversely related to IPM.\r\nMemory bandwidth can also be constrained by frequent\r\nTLB misses. The four requirements above are necessary for\r\na load to issue, but once issued, missing in the TLB incurs\r\na latency penalty for its refill, which in turn will decrease\r\nbandwidth for the same number of outstanding memory re\u0002quests. IVB’s Linux distribution supports Transparent Huge\r\nPages (THP), which eagerly combines consecutive 4 KB pages\r\ninto 2 MB pages when possible. IVB also supports 1 GB pages,\r\nbut these must be set aside by Linux in advance and require\r\nsubstantial application code modifications. Larger pages not\r\nonly reduce the chance of a TLB miss, but they also reduce\r\nthe time per refill by needing fewer hops to walk the page\r\ntable and by reducing the size of the page-table working set\r\n(better cache locality).\r\nFigure 3 varies the page size (2 MB or 1 GB) and the\r\narray size (1 GB or 16 GB) for our pointer-chase synthetic\r\nmicrobenchmark. With 2 MB pages from THP, most loads for\r\nboth array sizes will result in a cache miss and a TLB miss\r\n(IVB has only 32 2 MB TLB entries), but the maximum band\u0002width obtained with the larger array is substantially reduced\r\ndue to increases in TLB refill time (confirmed by performance\r\ncounters). Using 1 GB pages restores the bandwidth even\r\nthough there will still be frequent TLB misses (only 4 1 GB\r\nTLB entries) because the refill time is reduced enough to not\r\nbe problematic. With 1 GB pages, the page table will only need\r\n16 entries and will likely remain in the L1 cache. Our random\r\nmicrobenchmark exemplifies the worst case for the TLB, so\r\nany form of locality will reduce the performance penalties from\r\nTLB misses.\r\nWe further parallelize our microbenchmark and run it\r\nacross all of the cores, and the maximum bandwidths we\r\nachieve are visible in Figure 10. The data in this section shows\r\nthe maximum achievable memory bandwidth for a core, socket,\r\nor entire system given the amount of application MLP, IPM,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/610781d1-591f-4769-ab3d-b64e244a657e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d5230588ab0206577a0f416870ed9665746b1e24a33b462e18dffb6c9ade2daa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 767
      },
      {
        "segments": [
          {
            "segment_id": "68a4c817-b8a6-44c7-988f-7557fad128df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "0 2 4 6 8 10\r\nEffective MLP\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\nIPC\r\nCodebase\r\nGalois\r\nGAPBS\r\nLigra\r\n0 2 4 6 8 10\r\nEffective MLP\r\nKernel\r\nBC\r\nBFS\r\nCC\r\nPR\r\nSSSP\r\n0 2 4 6 8 10\r\nEffective MLP\r\nGraph\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 4. Single-thread performance in terms of instructions per cycle (IPC) of full workload colored by: codebase (left), kernel (middle), and input graph (right).\r\nmemory location (NUMA), and page size. In the following\r\nsection, we measure the memory bandwidths achieved by the\r\nhigh-performance graph codebases.\r\nV. SINGLE-CORE ANALYSIS\r\nIn this section, we begin to characterize our workload using\r\nonly a single thread on a single core in order to remove any\r\nparallel execution effects (multithreading, poor parallel scaling,\r\nload imbalance, NUMA penalties). Despite being amongst\r\nthe highest-performance implementations, all three codebases\r\noften execute instructions at a surprisingly low IPC (Figure 4),\r\nand this disappointing performance observed is not specific to\r\nany graph algorithm or codebase. The input graph does have\r\na large impact as we will discuss later in this section.\r\nFigure 4 shows that there is an unsurprising tradeoff\r\nbetween computation and communication, as no executions\r\nsustain a high IPC and a high memory bandwidth. A processor\r\ncan only execute instructions at a high rate if it rarely waits\r\non memory, and hence consumes little memory bandwidth.\r\nConversely, for a processor to use a great deal of memory\r\nbandwidth, it must have many memory requests outstanding,\r\ncausing it to be commonly waiting on memory and will thus\r\nexecute instructions slowly. Although some executions do use\r\nan appreciable amount of compute (upper left of Figure 4) or\r\nuse an appreciable fraction of the memory bandwidth (lower\r\nright), most do not. Many executions are actually in the worst\r\nlower-left quadrant, where they use little memory bandwidth,\r\nbut their compute throughput is also low, presumably due to\r\nmemory latency.\r\nIn general across our codebases, kernels, and inputs graphs,\r\na single core struggles to use all of the raw bandwidth\r\navailable (10 outstanding misses). With the same communi\u0002cation volume, utilizing more bandwidth should lead to higher\r\nperformance. Using the four requirements from Section IV, we\r\ninvestigate what is limiting the core’s bandwidth utilization for\r\nwhat should be a memory-bound graph processing workload.\r\nTo have many loads outstanding, the processor must first\r\nfetch those load instructions, and this typically requires cor\u0002rectly predicting the control flow. Although frequent branch\r\nmispredictions will be harmful to performance in theory, if the\r\nprocessor is already waiting on memory (achieving moderate\r\nmemory bandwidth utilization), performance is insensitive to\r\nthe branch misprediction rate (Figure 5), implying many of\r\nthese branches are miss independent. When the processor is\r\n0.00 0.05 0.10 0.15\r\nBranch Misprediction Rate\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\nIPC\r\nMLP > 3\r\nMLP < 3\r\nFig. 5. Single-thread performance of full workload relative to branch\r\nmisprediction rate colored by memory bandwidth utilization.\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nmodel\r\nFig. 6. Single-thread achieved memory bandwidth of full workload relative\r\nto instructions per miss (IPM). Note: Some points from road & web not visible\r\ndue to IPM>1000 but model continues to serve as an upper bound.)\r\nnot memory-bound, frequent branch mispredictions will hurt\r\nperformance, but a low misprediction rate is no guarantee for\r\ngood performance, implying there are remaining unaccounted\r\nbottlenecks.\r\nOnce the processor fetches the future outstanding loads,\r\nthose loads need to be able to fit into the instruction window,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/68a4c817-b8a6-44c7-988f-7557fad128df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b302a37a12371201a58a83024ffe5ea2573a901bed5f936e5a142bd3f44ca947",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 588
      },
      {
        "segments": [
          {
            "segment_id": "68a4c817-b8a6-44c7-988f-7557fad128df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "0 2 4 6 8 10\r\nEffective MLP\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\nIPC\r\nCodebase\r\nGalois\r\nGAPBS\r\nLigra\r\n0 2 4 6 8 10\r\nEffective MLP\r\nKernel\r\nBC\r\nBFS\r\nCC\r\nPR\r\nSSSP\r\n0 2 4 6 8 10\r\nEffective MLP\r\nGraph\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 4. Single-thread performance in terms of instructions per cycle (IPC) of full workload colored by: codebase (left), kernel (middle), and input graph (right).\r\nmemory location (NUMA), and page size. In the following\r\nsection, we measure the memory bandwidths achieved by the\r\nhigh-performance graph codebases.\r\nV. SINGLE-CORE ANALYSIS\r\nIn this section, we begin to characterize our workload using\r\nonly a single thread on a single core in order to remove any\r\nparallel execution effects (multithreading, poor parallel scaling,\r\nload imbalance, NUMA penalties). Despite being amongst\r\nthe highest-performance implementations, all three codebases\r\noften execute instructions at a surprisingly low IPC (Figure 4),\r\nand this disappointing performance observed is not specific to\r\nany graph algorithm or codebase. The input graph does have\r\na large impact as we will discuss later in this section.\r\nFigure 4 shows that there is an unsurprising tradeoff\r\nbetween computation and communication, as no executions\r\nsustain a high IPC and a high memory bandwidth. A processor\r\ncan only execute instructions at a high rate if it rarely waits\r\non memory, and hence consumes little memory bandwidth.\r\nConversely, for a processor to use a great deal of memory\r\nbandwidth, it must have many memory requests outstanding,\r\ncausing it to be commonly waiting on memory and will thus\r\nexecute instructions slowly. Although some executions do use\r\nan appreciable amount of compute (upper left of Figure 4) or\r\nuse an appreciable fraction of the memory bandwidth (lower\r\nright), most do not. Many executions are actually in the worst\r\nlower-left quadrant, where they use little memory bandwidth,\r\nbut their compute throughput is also low, presumably due to\r\nmemory latency.\r\nIn general across our codebases, kernels, and inputs graphs,\r\na single core struggles to use all of the raw bandwidth\r\navailable (10 outstanding misses). With the same communi\u0002cation volume, utilizing more bandwidth should lead to higher\r\nperformance. Using the four requirements from Section IV, we\r\ninvestigate what is limiting the core’s bandwidth utilization for\r\nwhat should be a memory-bound graph processing workload.\r\nTo have many loads outstanding, the processor must first\r\nfetch those load instructions, and this typically requires cor\u0002rectly predicting the control flow. Although frequent branch\r\nmispredictions will be harmful to performance in theory, if the\r\nprocessor is already waiting on memory (achieving moderate\r\nmemory bandwidth utilization), performance is insensitive to\r\nthe branch misprediction rate (Figure 5), implying many of\r\nthese branches are miss independent. When the processor is\r\n0.00 0.05 0.10 0.15\r\nBranch Misprediction Rate\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\nIPC\r\nMLP > 3\r\nMLP < 3\r\nFig. 5. Single-thread performance of full workload relative to branch\r\nmisprediction rate colored by memory bandwidth utilization.\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nmodel\r\nFig. 6. Single-thread achieved memory bandwidth of full workload relative\r\nto instructions per miss (IPM). Note: Some points from road & web not visible\r\ndue to IPM>1000 but model continues to serve as an upper bound.)\r\nnot memory-bound, frequent branch mispredictions will hurt\r\nperformance, but a low misprediction rate is no guarantee for\r\ngood performance, implying there are remaining unaccounted\r\nbottlenecks.\r\nOnce the processor fetches the future outstanding loads,\r\nthose loads need to be able to fit into the instruction window,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/68a4c817-b8a6-44c7-988f-7557fad128df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b302a37a12371201a58a83024ffe5ea2573a901bed5f936e5a142bd3f44ca947",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 588
      },
      {
        "segments": [
          {
            "segment_id": "8aa6edbe-7b0d-4cd8-8f44-8f35da7ecd5c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "0 20 40 60 80 100 120\r\nMisses per Kilo Instruction (MPKI)\r\n0\r\n5\r\n10\r\n15\r\n20\r\n25\r\nCount\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 7. Single-thread MPKI (in terms of LLC misses) of full workload.\r\nand the model from Section IV serves as an upper bound for\r\nour workload (Figure 6). Although the model is technically\r\na pessimistic upper bound since it assumes outstanding loads\r\nare evenly spaced apart, in practice this seems to be a suitable\r\napproximation. In spite of the core being capable of handling\r\n10 outstanding misses, an IPM of greater than 18.7 will not\r\nallow all these loads to fit in the window according to our\r\nmodel. Most of the executions have an IPM greater than this\r\ncutoff, and thus have their effective bandwidth limited by the\r\ninstruction window size. The caches achieve a modest hit rate\r\n(Figure 7), which raises the IPM by absorbing much of the\r\nmemory traffic.\r\nAs mentioned above, the properties of the graph can have\r\na substantial impact on the cache performance, which in turn\r\nwill affect not only the amount of memory traffic, but also\r\nhow fast it can be transferred. For example, in Figure 6 the\r\ngraph road has a high IPM because it is much smaller than\r\nthe other graphs. The topology can also have an impact, as the\r\ngraphs kron and uniform are about the same size and diameter,\r\nand yet uniform typically uses more bandwidth because it\r\nhas a lower IPM caused by more cache misses. The graph\r\nkron experiences fewer cache misses because it is scale-free,\r\nas a few high degree vertices will be accessed frequently\r\n(great temporal locality). Finally, the graph web has a higher\r\ndegree, which allows for longer contiguous reads (better spatial\r\nlocality) causing more cache hits and thus a higher IPM.\r\nAlthough there is not typically substantial benefit from\r\nusing 1 GB pages, using 4 KB pages does have quite a perfor\u0002mance penalty. Fortunately, THP is on by default and requires\r\nno application modifications. We vary the operating system\r\npage size for the GAPBS codebase in Figure 8. Relative\r\nto the baseline using THP (2 MB pages), using 1 GB pages\r\nimproves performance by more than 10% in only 4/25 cases\r\nbut disabling THP, which forces all pages to be 4 KB, decreases\r\nperformance by at least 10% in 19/25 cases. To use 1 GB\r\npages, we modify GAPBS to allocate 1 GB pages for the graph,\r\nthe output array, or both (typically the best) and pick whichever\r\none is fastest. The general insensitivity to the 1 GB page size\r\nfor our graph workload is another indication of locality.\r\nWe compare data dependencies versus branch mispredic\u0002tions to explain performance slowdown, and while difficult to\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\n1GB-best\r\n2MB-THP\r\n4KB-only\r\nmodel\r\nFig. 8. Single-thread achieved memory bandwidth of GAPBS for all\r\nkernels and graphs varying the operating system page size. 2 MB-THP uses\r\nTransparent Hugepages (THP) and lets the operating system choose to promote\r\n4 KB to 2 MB pages (happens frequently). 1 GB-best is the fastest execution\r\nusing manually allocated 1 GB pages for the output array, the graph, or both.\r\ndisentangle, the evidence points much more strongly to the\r\nformer than to the latter. With a combination of knowledge\r\nof IVB’s architecture and confirmation from performance\r\ncounters, we eliminate other possible performance limiters.\r\nDue to sophisticated hashing of memory addresses, there is not\r\nsignificant bank contention in the LLC or at the memory con\u0002trollers. The load buffer can hold 64 entries, so it rarely limits\r\noutstanding loads before the ROB (168 entries) or the MSHRs\r\n(10 per core). Mis-speculated loads are already counted by\r\nthe performance counters we utilize. The graph workloads\r\nwe measure have clearly dominant application phases (no\r\nsubstantial temporal variation).\r\nNone of executions of actual graph processing workloads\r\nare able to achieve a memory bandwidth corresponding to the\r\n10 outstanding misses our synthetic microbenchmarks demon\u0002strate the cores are capable of sustaining, and most are not\r\neven close. For a single thread, the biggest bandwidth limiter\r\nis fitting loads into the instruction window, which prevents\r\noff-chip memory bandwidth from becoming a bottleneck.\r\nVI. PARALLEL PERFORMANCE\r\nWith an understanding of the limits and capabilities of a\r\nsingle thread, we move on to the whole system. Running the\r\ncodebases at full capacity delivers speedups for all executions,\r\nand with 32 threads on 16 cores we achieve a speedup greater\r\nthan 8× (relative to single-thread) in 49 of 75 cases and a\r\nmedian speedup of 9.3× (Figure 9). Unfortunately, some of the\r\nexecutions (typically road and web) increase their bandwidth\r\nconsumption by more than they improve runtime, implying\r\ntheir parallel executions have more memory traffic than their\r\nsingle-threaded counterparts.\r\nThe compute and throughput utilization for the parallel\r\nexecutions (Figure 10) is strikingly similar to utilizations\r\nfor a single core (Figure 4). Although web and sometimes\r\nroad appear to break the trend by simultaneously using more\r\ncompute throughput and memory bandwidth, they do move\r\nextra data. The similarities between parallel utilization and\r\nserial utilization suggest that the bottlenecks of the core persist",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/8aa6edbe-7b0d-4cd8-8f44-8f35da7ecd5c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5b019c234593489d4405adb24a1bca6cfc709fdc2089ebf0b22a402af641f2ce",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 859
      },
      {
        "segments": [
          {
            "segment_id": "8aa6edbe-7b0d-4cd8-8f44-8f35da7ecd5c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "0 20 40 60 80 100 120\r\nMisses per Kilo Instruction (MPKI)\r\n0\r\n5\r\n10\r\n15\r\n20\r\n25\r\nCount\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 7. Single-thread MPKI (in terms of LLC misses) of full workload.\r\nand the model from Section IV serves as an upper bound for\r\nour workload (Figure 6). Although the model is technically\r\na pessimistic upper bound since it assumes outstanding loads\r\nare evenly spaced apart, in practice this seems to be a suitable\r\napproximation. In spite of the core being capable of handling\r\n10 outstanding misses, an IPM of greater than 18.7 will not\r\nallow all these loads to fit in the window according to our\r\nmodel. Most of the executions have an IPM greater than this\r\ncutoff, and thus have their effective bandwidth limited by the\r\ninstruction window size. The caches achieve a modest hit rate\r\n(Figure 7), which raises the IPM by absorbing much of the\r\nmemory traffic.\r\nAs mentioned above, the properties of the graph can have\r\na substantial impact on the cache performance, which in turn\r\nwill affect not only the amount of memory traffic, but also\r\nhow fast it can be transferred. For example, in Figure 6 the\r\ngraph road has a high IPM because it is much smaller than\r\nthe other graphs. The topology can also have an impact, as the\r\ngraphs kron and uniform are about the same size and diameter,\r\nand yet uniform typically uses more bandwidth because it\r\nhas a lower IPM caused by more cache misses. The graph\r\nkron experiences fewer cache misses because it is scale-free,\r\nas a few high degree vertices will be accessed frequently\r\n(great temporal locality). Finally, the graph web has a higher\r\ndegree, which allows for longer contiguous reads (better spatial\r\nlocality) causing more cache hits and thus a higher IPM.\r\nAlthough there is not typically substantial benefit from\r\nusing 1 GB pages, using 4 KB pages does have quite a perfor\u0002mance penalty. Fortunately, THP is on by default and requires\r\nno application modifications. We vary the operating system\r\npage size for the GAPBS codebase in Figure 8. Relative\r\nto the baseline using THP (2 MB pages), using 1 GB pages\r\nimproves performance by more than 10% in only 4/25 cases\r\nbut disabling THP, which forces all pages to be 4 KB, decreases\r\nperformance by at least 10% in 19/25 cases. To use 1 GB\r\npages, we modify GAPBS to allocate 1 GB pages for the graph,\r\nthe output array, or both (typically the best) and pick whichever\r\none is fastest. The general insensitivity to the 1 GB page size\r\nfor our graph workload is another indication of locality.\r\nWe compare data dependencies versus branch mispredic\u0002tions to explain performance slowdown, and while difficult to\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\n1GB-best\r\n2MB-THP\r\n4KB-only\r\nmodel\r\nFig. 8. Single-thread achieved memory bandwidth of GAPBS for all\r\nkernels and graphs varying the operating system page size. 2 MB-THP uses\r\nTransparent Hugepages (THP) and lets the operating system choose to promote\r\n4 KB to 2 MB pages (happens frequently). 1 GB-best is the fastest execution\r\nusing manually allocated 1 GB pages for the output array, the graph, or both.\r\ndisentangle, the evidence points much more strongly to the\r\nformer than to the latter. With a combination of knowledge\r\nof IVB’s architecture and confirmation from performance\r\ncounters, we eliminate other possible performance limiters.\r\nDue to sophisticated hashing of memory addresses, there is not\r\nsignificant bank contention in the LLC or at the memory con\u0002trollers. The load buffer can hold 64 entries, so it rarely limits\r\noutstanding loads before the ROB (168 entries) or the MSHRs\r\n(10 per core). Mis-speculated loads are already counted by\r\nthe performance counters we utilize. The graph workloads\r\nwe measure have clearly dominant application phases (no\r\nsubstantial temporal variation).\r\nNone of executions of actual graph processing workloads\r\nare able to achieve a memory bandwidth corresponding to the\r\n10 outstanding misses our synthetic microbenchmarks demon\u0002strate the cores are capable of sustaining, and most are not\r\neven close. For a single thread, the biggest bandwidth limiter\r\nis fitting loads into the instruction window, which prevents\r\noff-chip memory bandwidth from becoming a bottleneck.\r\nVI. PARALLEL PERFORMANCE\r\nWith an understanding of the limits and capabilities of a\r\nsingle thread, we move on to the whole system. Running the\r\ncodebases at full capacity delivers speedups for all executions,\r\nand with 32 threads on 16 cores we achieve a speedup greater\r\nthan 8× (relative to single-thread) in 49 of 75 cases and a\r\nmedian speedup of 9.3× (Figure 9). Unfortunately, some of the\r\nexecutions (typically road and web) increase their bandwidth\r\nconsumption by more than they improve runtime, implying\r\ntheir parallel executions have more memory traffic than their\r\nsingle-threaded counterparts.\r\nThe compute and throughput utilization for the parallel\r\nexecutions (Figure 10) is strikingly similar to utilizations\r\nfor a single core (Figure 4). Although web and sometimes\r\nroad appear to break the trend by simultaneously using more\r\ncompute throughput and memory bandwidth, they do move\r\nextra data. The similarities between parallel utilization and\r\nserial utilization suggest that the bottlenecks of the core persist",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/8aa6edbe-7b0d-4cd8-8f44-8f35da7ecd5c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5b019c234593489d4405adb24a1bca6cfc709fdc2089ebf0b22a402af641f2ce",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 859
      },
      {
        "segments": [
          {
            "segment_id": "9fc963fc-796b-420f-80fd-356117aaf4e1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "0 5 10 15 20 25\r\nMemory Bandwidth Increase (x)\r\n0\r\n5\r\n10\r\n15\r\n20\r\n25\r\nRuntime Speedup (x)\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 9. Improvements in runtime and memory bandwidth utilization of full\r\nworkload for full system (32 threads on 16 cores) relative to 1 thread. Points\r\nalong unit slope transfer the same amount of data, so points below the unit\r\nslope (often road and web) transfer extra data.\r\n0 20 40 60 80 100\r\nEffective MLP\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\nIPC / core\r\nsocket\r\ninterleave\r\nsystem\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 10. Full system (32 threads on 16 cores) performance of full workload.\r\nVertical lines correspond to maximum achieved bandwidths from Section IV\r\nfor a single socket (socket), both sockets with memory interleaved (interleave),\r\nand both sockets with locally allocated memory (system).\r\nand hurt utilization at the system scale. Due to the generally\r\nlinear relation between performance and memory bandwidth,\r\nfully utilizing the off-chip memory system could improve\r\nperformance by 1.3–47× (median 2.4×).\r\nThere may be graph algorithm implementations with better\r\nparallel scaling properties, but these advanced algorithms are\r\nused because they deliver better absolute performance. Parallel\r\nscaling can be hampered by software issues (poor scalability,\r\nload imbalance, synchronization overheads, and redundant\r\ncommunication), but in the remainder of this work we will\r\nconsider hardware imposed complications for parallelization:\r\nNUMA and multithreading.\r\nVII. NUMA PENALTY\r\nWith multi-socket systems, non-uniform memory access\r\n(NUMA) penalties are a common challenge. From the results\r\nof Section IV, it would appear that NUMA should halve\r\nperformance, but our results indicate the penalty for NUMA\r\nmay be substantially less severe in practice.\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective Remote MLP\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nmodel\r\nFig. 11. Single-thread achieved memory bandwidth of full workload\r\nexecuting out of remote memory. Calculating effective MLP with remote\r\nmemory latency (instead of local memory latency) returns a result similar\r\nto local memory results (Figure 6).\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0.8\r\n1.0\r\n1.2\r\n1.4\r\n1.6\r\n1.8\r\n2.0\r\nTime Slowdown\r\nRemote\r\nInterleave\r\nFig. 12. Single-socket (8 cores) slowdown relative to local memory of full\r\nworkload executing out of remote memory or interleaved memory.\r\nFor a single thread using only remote memory, performance\r\nis halved as it transfers the same amount of data with the\r\nsame number of outstanding memory requests but at twice\r\nthe latency for effectively half the bandwidth. Calculating the\r\neffective MLP with the remote memory latency instead of\r\nthe local memory latency shows the workload still obeys the\r\nsimple bandwidth model (Figure 11).\r\nWith more cores, this NUMA penalty is reduced (Fig\u0002ure 12), and for executions that use less memory bandwidth\r\n(higher IPM), the NUMA penalty is reduced further. A core\r\nusing only remote memory is clearly an adversarial worst case.\r\nFor a full system workload without locality, half of the traffic\r\nshould still go to local memory. Consequently, the interleaved\r\npattern in Figure 12 is more realistic and it has one third\r\nthe performance loss of remote (median 1.16× slowdown vs.\r\n1.48× slowdown).\r\nWe confirm that NUMA has a moderate performance\r\npenalty. Unfortunately, many graphs of interest are low diam\u0002eter and hard to partition effectively [20] so it is challenging\r\nto avoid inter-socket communication. Therefore, efforts to",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/9fc963fc-796b-420f-80fd-356117aaf4e1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2c3d924d455352fc21ceb98a044e21ba25701c433c3d94b77f80ffb7f09d13a5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 556
      },
      {
        "segments": [
          {
            "segment_id": "9fc963fc-796b-420f-80fd-356117aaf4e1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "0 5 10 15 20 25\r\nMemory Bandwidth Increase (x)\r\n0\r\n5\r\n10\r\n15\r\n20\r\n25\r\nRuntime Speedup (x)\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 9. Improvements in runtime and memory bandwidth utilization of full\r\nworkload for full system (32 threads on 16 cores) relative to 1 thread. Points\r\nalong unit slope transfer the same amount of data, so points below the unit\r\nslope (often road and web) transfer extra data.\r\n0 20 40 60 80 100\r\nEffective MLP\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\nIPC / core\r\nsocket\r\ninterleave\r\nsystem\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 10. Full system (32 threads on 16 cores) performance of full workload.\r\nVertical lines correspond to maximum achieved bandwidths from Section IV\r\nfor a single socket (socket), both sockets with memory interleaved (interleave),\r\nand both sockets with locally allocated memory (system).\r\nand hurt utilization at the system scale. Due to the generally\r\nlinear relation between performance and memory bandwidth,\r\nfully utilizing the off-chip memory system could improve\r\nperformance by 1.3–47× (median 2.4×).\r\nThere may be graph algorithm implementations with better\r\nparallel scaling properties, but these advanced algorithms are\r\nused because they deliver better absolute performance. Parallel\r\nscaling can be hampered by software issues (poor scalability,\r\nload imbalance, synchronization overheads, and redundant\r\ncommunication), but in the remainder of this work we will\r\nconsider hardware imposed complications for parallelization:\r\nNUMA and multithreading.\r\nVII. NUMA PENALTY\r\nWith multi-socket systems, non-uniform memory access\r\n(NUMA) penalties are a common challenge. From the results\r\nof Section IV, it would appear that NUMA should halve\r\nperformance, but our results indicate the penalty for NUMA\r\nmay be substantially less severe in practice.\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective Remote MLP\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nmodel\r\nFig. 11. Single-thread achieved memory bandwidth of full workload\r\nexecuting out of remote memory. Calculating effective MLP with remote\r\nmemory latency (instead of local memory latency) returns a result similar\r\nto local memory results (Figure 6).\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0.8\r\n1.0\r\n1.2\r\n1.4\r\n1.6\r\n1.8\r\n2.0\r\nTime Slowdown\r\nRemote\r\nInterleave\r\nFig. 12. Single-socket (8 cores) slowdown relative to local memory of full\r\nworkload executing out of remote memory or interleaved memory.\r\nFor a single thread using only remote memory, performance\r\nis halved as it transfers the same amount of data with the\r\nsame number of outstanding memory requests but at twice\r\nthe latency for effectively half the bandwidth. Calculating the\r\neffective MLP with the remote memory latency instead of\r\nthe local memory latency shows the workload still obeys the\r\nsimple bandwidth model (Figure 11).\r\nWith more cores, this NUMA penalty is reduced (Fig\u0002ure 12), and for executions that use less memory bandwidth\r\n(higher IPM), the NUMA penalty is reduced further. A core\r\nusing only remote memory is clearly an adversarial worst case.\r\nFor a full system workload without locality, half of the traffic\r\nshould still go to local memory. Consequently, the interleaved\r\npattern in Figure 12 is more realistic and it has one third\r\nthe performance loss of remote (median 1.16× slowdown vs.\r\n1.48× slowdown).\r\nWe confirm that NUMA has a moderate performance\r\npenalty. Unfortunately, many graphs of interest are low diam\u0002eter and hard to partition effectively [20] so it is challenging\r\nto avoid inter-socket communication. Therefore, efforts to",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/9fc963fc-796b-420f-80fd-356117aaf4e1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2c3d924d455352fc21ceb98a044e21ba25701c433c3d94b77f80ffb7f09d13a5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 556
      },
      {
        "segments": [
          {
            "segment_id": "c0588072-f5c5-4c50-8263-3a8ad6f21e0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "0\r\n5\r\n10\r\n15\r\nCount\r\n1 core\r\n0\r\n5\r\n10\r\n15\r\nCount\r\n8 cores\r\n0.5 1.0 1.5 2.0\r\nSpeedup (2 threads/core / 1 thread/core)\r\n0\r\n5\r\n10\r\n15\r\nCount\r\n16 cores\r\nFig. 13. Distribution of speedups of using two threads per core relative to\r\none thread per core of full workload for one core, one socket (8 cores), and\r\nwhole system (2 sockets)). Dotted line is median.\r\nmove computation (rather than data) have fared the best when\r\noptimizing graph processing for NUMA [1], [11].\r\nVIII. LIMITED ROOM FOR SMT\r\nMultithreading, and in this work’s context of a su\u0002perscalar out-of-order processor, simultaneous multithread\u0002ing (SMT) [17], aims to increase utilization. The additional\r\nsoftware-exposed parallelism threads provide can be used to\r\nmitigate unresolved data dependences by increasing applica\u0002tion MLP as well as reducing the demand placed on branch\r\nprediction since each thread will have fewer instructions in\r\nflight. Using IVB, we measure the performance gains of using\r\na second thread per core, which evaluates how well SMT re\u0002duces the performance loss from unresolved data dependencies\r\nand branch predictions without incurring new overheads.\r\nAcross all scales (single core, single socket, or single\r\nsystem), the second thread is usually beneficial, but only\r\nto a modest degree (Figure 13) as most speedups are less\r\nthan 1.5×. Even so, these modest speedups from SMT are\r\nnot inconsequential, as SMT economically improves system\r\nperformance.\r\nMultithreading also has the potential to introduce new per\u0002formance challenges. More threads increase parallelism, which\r\nin turn can worsen the damage caused by load imbalances and\r\nsynchronization overheads. Worse yet, more threads can end\r\nup competing for capacity in the cache resulting in increased\r\nmemory traffic. Analogous to the results for multicore (Sec\u0002tion VI), the road and web graphs in Figure 14 are examples\r\nof this competition as the improvement in bandwidth is greater\r\nthan the improvement in runtime.\r\nFor a single thread, we find the biggest performance limiter\r\nto be fitting loads into the instruction window, and SMT is\r\nno different as the addition of a second thread to the same\r\ncore still mostly obeys our simple model (Figure 15). If the\r\nworkload of the two threads is heterogenous it is possible for\r\nan SMT core to exceed our simple model. One thread could\r\ngenerate most of the cache misses sustaining a high effective\r\nMLP while the other thread (unencumbered by cache misses)\r\ncould execute instructions quickly to increase IPM. In practice,\r\n0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2\r\nMemory Bandwidth Increase (x)\r\n0.8\r\n1.0\r\n1.2\r\n1.4\r\n1.6\r\n1.8\r\n2.0\r\n2.2\r\nRuntime Speedup (x)\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 14. Improvements in runtime and memory bandwidth utilization of full\r\nworkload for one core using two threads relative to one thread.\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\n1 Thread\r\n2 Threads\r\nmodel\r\nFig. 15. Achieved memory bandwidth of full workload relative to instructions\r\nper miss (IPM) with one or two threads on one core.\r\nthe variation between threads is modest and thus most points\r\nare not far above our model.\r\nMultithreading can improve performance, but in the context\r\nof this study (graph processing workload on a superscalar out\u0002of-order multi-socket system), it has limited potential. The\r\nmodest improvements two-way multithreading provides in this\r\nstudy cast doubts on how much more performance is to be\r\ngained by additional threads.\r\nIX. RELATED WORK\r\nOur study touches on many aspects of computer archi\u0002tecture, so we focus this section specifically on prior work\r\nrelevant to graph algorithms. Compared to prior work on the\r\narchitectural requirements for graph algorithms, our study has\r\na much larger and more diverse graph workload. We study 5\r\nkernels from 3 codebases with 5 input graphs (some which are\r\nreal and not synthetic).\r\nA survey [29] of both hardware and software concerns\r\nfor parallel graph processing lists ‘poor locality’ as one of its\r\nchief concerns. Although it is cognizant of the greater cost\r\nof heavily multithreaded systems, it argues they are better for",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/c0588072-f5c5-4c50-8263-3a8ad6f21e0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4295e03749560f4bfb06973541726d2243ce6a6d79b91caeea1499f57753f821",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 664
      },
      {
        "segments": [
          {
            "segment_id": "c0588072-f5c5-4c50-8263-3a8ad6f21e0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "0\r\n5\r\n10\r\n15\r\nCount\r\n1 core\r\n0\r\n5\r\n10\r\n15\r\nCount\r\n8 cores\r\n0.5 1.0 1.5 2.0\r\nSpeedup (2 threads/core / 1 thread/core)\r\n0\r\n5\r\n10\r\n15\r\nCount\r\n16 cores\r\nFig. 13. Distribution of speedups of using two threads per core relative to\r\none thread per core of full workload for one core, one socket (8 cores), and\r\nwhole system (2 sockets)). Dotted line is median.\r\nmove computation (rather than data) have fared the best when\r\noptimizing graph processing for NUMA [1], [11].\r\nVIII. LIMITED ROOM FOR SMT\r\nMultithreading, and in this work’s context of a su\u0002perscalar out-of-order processor, simultaneous multithread\u0002ing (SMT) [17], aims to increase utilization. The additional\r\nsoftware-exposed parallelism threads provide can be used to\r\nmitigate unresolved data dependences by increasing applica\u0002tion MLP as well as reducing the demand placed on branch\r\nprediction since each thread will have fewer instructions in\r\nflight. Using IVB, we measure the performance gains of using\r\na second thread per core, which evaluates how well SMT re\u0002duces the performance loss from unresolved data dependencies\r\nand branch predictions without incurring new overheads.\r\nAcross all scales (single core, single socket, or single\r\nsystem), the second thread is usually beneficial, but only\r\nto a modest degree (Figure 13) as most speedups are less\r\nthan 1.5×. Even so, these modest speedups from SMT are\r\nnot inconsequential, as SMT economically improves system\r\nperformance.\r\nMultithreading also has the potential to introduce new per\u0002formance challenges. More threads increase parallelism, which\r\nin turn can worsen the damage caused by load imbalances and\r\nsynchronization overheads. Worse yet, more threads can end\r\nup competing for capacity in the cache resulting in increased\r\nmemory traffic. Analogous to the results for multicore (Sec\u0002tion VI), the road and web graphs in Figure 14 are examples\r\nof this competition as the improvement in bandwidth is greater\r\nthan the improvement in runtime.\r\nFor a single thread, we find the biggest performance limiter\r\nto be fitting loads into the instruction window, and SMT is\r\nno different as the addition of a second thread to the same\r\ncore still mostly obeys our simple model (Figure 15). If the\r\nworkload of the two threads is heterogenous it is possible for\r\nan SMT core to exceed our simple model. One thread could\r\ngenerate most of the cache misses sustaining a high effective\r\nMLP while the other thread (unencumbered by cache misses)\r\ncould execute instructions quickly to increase IPM. In practice,\r\n0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2\r\nMemory Bandwidth Increase (x)\r\n0.8\r\n1.0\r\n1.2\r\n1.4\r\n1.6\r\n1.8\r\n2.0\r\n2.2\r\nRuntime Speedup (x)\r\nkron\r\nroad\r\nweb\r\ntwitter\r\nuniform\r\nFig. 14. Improvements in runtime and memory bandwidth utilization of full\r\nworkload for one core using two threads relative to one thread.\r\n0 50 100 150 200 250 300 350\r\nInstructions per Miss (IPM)\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nEffective MLP\r\n1 Thread\r\n2 Threads\r\nmodel\r\nFig. 15. Achieved memory bandwidth of full workload relative to instructions\r\nper miss (IPM) with one or two threads on one core.\r\nthe variation between threads is modest and thus most points\r\nare not far above our model.\r\nMultithreading can improve performance, but in the context\r\nof this study (graph processing workload on a superscalar out\u0002of-order multi-socket system), it has limited potential. The\r\nmodest improvements two-way multithreading provides in this\r\nstudy cast doubts on how much more performance is to be\r\ngained by additional threads.\r\nIX. RELATED WORK\r\nOur study touches on many aspects of computer archi\u0002tecture, so we focus this section specifically on prior work\r\nrelevant to graph algorithms. Compared to prior work on the\r\narchitectural requirements for graph algorithms, our study has\r\na much larger and more diverse graph workload. We study 5\r\nkernels from 3 codebases with 5 input graphs (some which are\r\nreal and not synthetic).\r\nA survey [29] of both hardware and software concerns\r\nfor parallel graph processing lists ‘poor locality’ as one of its\r\nchief concerns. Although it is cognizant of the greater cost\r\nof heavily multithreaded systems, it argues they are better for",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/c0588072-f5c5-4c50-8263-3a8ad6f21e0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4295e03749560f4bfb06973541726d2243ce6a6d79b91caeea1499f57753f821",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 664
      },
      {
        "segments": [
          {
            "segment_id": "5b5ba224-066a-425a-974b-804a80d43ce1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "graph algorithms due to their memory latency tolerance and\r\nsupport for fine-grained dynamic threading. Bader et al. [4]\r\nalso endorse heavily threaded systems because of concerns of\r\nmemory accesses being mostly non-contiguous (low locality).\r\nCong et al. [12] compare a Sun Niagara 2 to a IBM Power\r\n7 when executing graph algorithms to understand architec\u0002tural implications. They find memory latency (not memory\r\nbandwidth) to be a bottleneck for both platforms, and neither\r\nplatform has enough threads to fully hide it.\r\nTo better understand graph algorithm architectural require\u0002ments, prior work has explicitly examined the locality behavior\r\nof graph algorithms. Cong et al. [13] study several Mini\u0002mum Spanning Tree algorithms with a reuse distance metric\r\n(temporal locality). They find graph algorithms do have less\r\n(but not no) locality, but observe some algorithms with less\r\nlocality sometimes perform better, and hypothesize this is due\r\nto not accounting for spatial locality. Analytical models for\r\nBFS can accurately predict the reuse distance of BFS on certain\r\nrandom graphs [47]. Murphy et al. [33] examine serial traces\r\nfrom a variety of benchmark suites including graph algorithms.\r\nDespite locality metrics based on an extremely small cache for\r\nthe time of publication, they observe that integer applications\r\ntend to have less locality than floating-point applications, but\r\nare still far better than random.\r\nEfforts to improve performance by explicit NUMA opti\u0002mizations typically require complicated manual modifications\r\nand are not generally applicable to all graph algorithms.\r\nAgarwal et al. [1] improve BFS performance using custom\r\ninter-socket queues. With a high-end quad-socket server, they\r\nare able to outperform a Cray XMT. Satish et al. [11] minimize\r\ninter-socket communication for BFS, and provide a detailed\r\nperformance model for their implementation.\r\nAlthough hardware prefetchers may struggle to predict\r\nnon-streaming memory accesses, explicit software prefetching\r\nhas been investigated as a means to improve graph algorithm\r\nperformance [1], [12], [23]. Not unlike explicit NUMA op\u0002timizations, for graph algorithms, using software prefetching\r\nrequires human intervention. Software prefetching can be dif\u0002ficult to implement effectively for all graph algorithms because\r\nit is often hard to generate the addresses desired sufficiently\r\nbefore they are needed.\r\nGreen et al. investigate improving graph algorithm perfor\u0002mance by reducing branch mispredictions using conditional\r\nmoves [22]. They conclude that branch mispredictions are\r\nresponsible for a 30%–50% performance loss, but in our results\r\n(Section V) we do not observe such a large penalty when\r\nconsidering the limitations imposed by data dependences and\r\nfitting loads into the instruction window.\r\nRunahead execution is a technique to improve processor\r\nperformance in the presence of cache misses [16], and in\r\nthe case of an out-of-order core, runahead execution attempts\r\nto economically obtain the benefits of a larger instruction\r\nwindow [34].\r\nThe Cray XMT, and its predecessor the MTA-2 [2], are\r\nsystems explicitly designed to handle irregular problems in\u0002cluding graph algorithms [41]. Designed for workloads without\r\nlocality, they feature many hardware threads and no data\r\ncaches.\r\nThere has been substantial effort characterizing graph pro\u0002cessing workloads on GPUs. Since GPUs are optimized for\r\nregular data parallelism, Burtscher et al. propose metrics to\r\nquantify control-flow irregularity and memory-access irregu\u0002larity and they perform performance counter measurements on\r\nreal hardware [9]. For some graph algorithms, they observe\r\nthe performance characteristics depend substantially on the\r\ninputs. A continuation of that research uses a software sim\u0002ulator to change GPU architectural parameters and observes\r\nperformance is more sensitive to L2 cache parameters than\r\nto DRAM parameters, which suggests there is exploitable\r\nlocality [37]. Xu et al. also use a simulator and identify\r\nsynchronization with the CPU (kernel invocations and data\r\ntransfers) as well as GPU memory latency to be the biggest\r\nperformance bottlenecks [44]. Che et al. profile the Pannotia\r\nsuite of graph algorithms and observe substantial diversity\r\nacross algorithms and inputs [10]. Wu et al. investigate the\r\nmost important primitives needed for higher-level program\u0002ming models for graph algorithms [43]. Contrasting these GPU\r\nworks from our work, in addition to the difference in hardware\r\nplatform (CPU versus GPU), we use much larger input graphs\r\nenabled by executing on real hardware (no downsizing to\r\nreduce simulation time) and by using server-sized memory (not\r\nconstrained by GPU memory capacity).\r\nX. CONCLUSION\r\nOur diverse workload (varied implementations, algorithms,\r\nand input graphs) demonstrates there is no single representative\r\nbenchmark and we find the input graph to have the largest\r\nimpact on the performance characteristics.\r\nMost of our workload fails to fully utilize IVB’s off\u0002chip memory bandwidth due to having an insufficient num\u0002ber of outstanding memory requests. The biggest bandwidth\r\nbottleneck is the instruction window, because it cannot hold\r\na sufficient number of instructions to incorporate the needed\r\nnumber of rare cache-missing instructions. A high LLC hit rate\r\nmakes these cache misses rare, and we find this challenges\r\nthe misconception that graph algorithms have little locality.\r\nTLB misses are only measurably detrimental when at least a\r\nmoderate amount of memory bandwidth is utilized, and we find\r\ntransparent huge pages to be effective at ameliorating much of\r\nthe performance loss due to TLB misses. Branch mispredic\u0002tions and unresolved data dependences can also hinder memory\r\nbandwidth utilization, but they are secondary to the interaction\r\nbetween the cache hit rate and the instruction window size.\r\nBandwidth is also moderately hindered by NUMA effects,\r\nso software techniques to increase intra-socket locality or\r\nhardware techniques to decrease inter-socket latency will be\r\nbeneficial.\r\nThe parallel scaling of our workload indicates that perfor\u0002mance typically scales linearly with memory bandwidth con\u0002sumption. Since our workload fails to fully utilize IVB’s mem\u0002ory bandwidth, an improved processor architecture could use\r\nthe same memory system but improve performance by utilizing\r\nmore memory bandwidth. For our workload on IVB, SMT\r\nis typically beneficial, and when it improves performance, it\r\ndoes so by using more memory bandwidth. Unfortunately, in\r\nthe context of an out-of-order core, SMT helps only modestly,\r\nand additional techniques will be needed to utilize the rest of\r\nthe unused memory bandwidth.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/5b5ba224-066a-425a-974b-804a80d43ce1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7282de1e8ed97b3a83a95b06949de15f33ca444ec8ab711a6acec27eb506ef0a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 968
      },
      {
        "segments": [
          {
            "segment_id": "5b5ba224-066a-425a-974b-804a80d43ce1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "graph algorithms due to their memory latency tolerance and\r\nsupport for fine-grained dynamic threading. Bader et al. [4]\r\nalso endorse heavily threaded systems because of concerns of\r\nmemory accesses being mostly non-contiguous (low locality).\r\nCong et al. [12] compare a Sun Niagara 2 to a IBM Power\r\n7 when executing graph algorithms to understand architec\u0002tural implications. They find memory latency (not memory\r\nbandwidth) to be a bottleneck for both platforms, and neither\r\nplatform has enough threads to fully hide it.\r\nTo better understand graph algorithm architectural require\u0002ments, prior work has explicitly examined the locality behavior\r\nof graph algorithms. Cong et al. [13] study several Mini\u0002mum Spanning Tree algorithms with a reuse distance metric\r\n(temporal locality). They find graph algorithms do have less\r\n(but not no) locality, but observe some algorithms with less\r\nlocality sometimes perform better, and hypothesize this is due\r\nto not accounting for spatial locality. Analytical models for\r\nBFS can accurately predict the reuse distance of BFS on certain\r\nrandom graphs [47]. Murphy et al. [33] examine serial traces\r\nfrom a variety of benchmark suites including graph algorithms.\r\nDespite locality metrics based on an extremely small cache for\r\nthe time of publication, they observe that integer applications\r\ntend to have less locality than floating-point applications, but\r\nare still far better than random.\r\nEfforts to improve performance by explicit NUMA opti\u0002mizations typically require complicated manual modifications\r\nand are not generally applicable to all graph algorithms.\r\nAgarwal et al. [1] improve BFS performance using custom\r\ninter-socket queues. With a high-end quad-socket server, they\r\nare able to outperform a Cray XMT. Satish et al. [11] minimize\r\ninter-socket communication for BFS, and provide a detailed\r\nperformance model for their implementation.\r\nAlthough hardware prefetchers may struggle to predict\r\nnon-streaming memory accesses, explicit software prefetching\r\nhas been investigated as a means to improve graph algorithm\r\nperformance [1], [12], [23]. Not unlike explicit NUMA op\u0002timizations, for graph algorithms, using software prefetching\r\nrequires human intervention. Software prefetching can be dif\u0002ficult to implement effectively for all graph algorithms because\r\nit is often hard to generate the addresses desired sufficiently\r\nbefore they are needed.\r\nGreen et al. investigate improving graph algorithm perfor\u0002mance by reducing branch mispredictions using conditional\r\nmoves [22]. They conclude that branch mispredictions are\r\nresponsible for a 30%–50% performance loss, but in our results\r\n(Section V) we do not observe such a large penalty when\r\nconsidering the limitations imposed by data dependences and\r\nfitting loads into the instruction window.\r\nRunahead execution is a technique to improve processor\r\nperformance in the presence of cache misses [16], and in\r\nthe case of an out-of-order core, runahead execution attempts\r\nto economically obtain the benefits of a larger instruction\r\nwindow [34].\r\nThe Cray XMT, and its predecessor the MTA-2 [2], are\r\nsystems explicitly designed to handle irregular problems in\u0002cluding graph algorithms [41]. Designed for workloads without\r\nlocality, they feature many hardware threads and no data\r\ncaches.\r\nThere has been substantial effort characterizing graph pro\u0002cessing workloads on GPUs. Since GPUs are optimized for\r\nregular data parallelism, Burtscher et al. propose metrics to\r\nquantify control-flow irregularity and memory-access irregu\u0002larity and they perform performance counter measurements on\r\nreal hardware [9]. For some graph algorithms, they observe\r\nthe performance characteristics depend substantially on the\r\ninputs. A continuation of that research uses a software sim\u0002ulator to change GPU architectural parameters and observes\r\nperformance is more sensitive to L2 cache parameters than\r\nto DRAM parameters, which suggests there is exploitable\r\nlocality [37]. Xu et al. also use a simulator and identify\r\nsynchronization with the CPU (kernel invocations and data\r\ntransfers) as well as GPU memory latency to be the biggest\r\nperformance bottlenecks [44]. Che et al. profile the Pannotia\r\nsuite of graph algorithms and observe substantial diversity\r\nacross algorithms and inputs [10]. Wu et al. investigate the\r\nmost important primitives needed for higher-level program\u0002ming models for graph algorithms [43]. Contrasting these GPU\r\nworks from our work, in addition to the difference in hardware\r\nplatform (CPU versus GPU), we use much larger input graphs\r\nenabled by executing on real hardware (no downsizing to\r\nreduce simulation time) and by using server-sized memory (not\r\nconstrained by GPU memory capacity).\r\nX. CONCLUSION\r\nOur diverse workload (varied implementations, algorithms,\r\nand input graphs) demonstrates there is no single representative\r\nbenchmark and we find the input graph to have the largest\r\nimpact on the performance characteristics.\r\nMost of our workload fails to fully utilize IVB’s off\u0002chip memory bandwidth due to having an insufficient num\u0002ber of outstanding memory requests. The biggest bandwidth\r\nbottleneck is the instruction window, because it cannot hold\r\na sufficient number of instructions to incorporate the needed\r\nnumber of rare cache-missing instructions. A high LLC hit rate\r\nmakes these cache misses rare, and we find this challenges\r\nthe misconception that graph algorithms have little locality.\r\nTLB misses are only measurably detrimental when at least a\r\nmoderate amount of memory bandwidth is utilized, and we find\r\ntransparent huge pages to be effective at ameliorating much of\r\nthe performance loss due to TLB misses. Branch mispredic\u0002tions and unresolved data dependences can also hinder memory\r\nbandwidth utilization, but they are secondary to the interaction\r\nbetween the cache hit rate and the instruction window size.\r\nBandwidth is also moderately hindered by NUMA effects,\r\nso software techniques to increase intra-socket locality or\r\nhardware techniques to decrease inter-socket latency will be\r\nbeneficial.\r\nThe parallel scaling of our workload indicates that perfor\u0002mance typically scales linearly with memory bandwidth con\u0002sumption. Since our workload fails to fully utilize IVB’s mem\u0002ory bandwidth, an improved processor architecture could use\r\nthe same memory system but improve performance by utilizing\r\nmore memory bandwidth. For our workload on IVB, SMT\r\nis typically beneficial, and when it improves performance, it\r\ndoes so by using more memory bandwidth. Unfortunately, in\r\nthe context of an out-of-order core, SMT helps only modestly,\r\nand additional techniques will be needed to utilize the rest of\r\nthe unused memory bandwidth.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/5b5ba224-066a-425a-974b-804a80d43ce1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7282de1e8ed97b3a83a95b06949de15f33ca444ec8ab711a6acec27eb506ef0a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 968
      },
      {
        "segments": [
          {
            "segment_id": "348edd94-61e9-4ea4-8b63-ba64fd93c85f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "Overall, we see no perfect solution to the performance\r\nchallenges presented by graph algorithms. Many techniques\r\ncan improve performance, but all of them will have quickly\r\ndiminishing returns, so greatly improving performance will\r\nrequire a multifaceted approach.\r\nACKNOWLEDGEMENTS\r\nWe thank the reviewers and David Ediger for their helpful\r\nfeedback. Research partially funded by DARPA Award Num\u0002ber HR0011-12-2-0016, the Center for Future Architecture\r\nResearch, a member of STARnet, a Semiconductor Research\r\nCorporation program sponsored by MARCO and DARPA, and\r\nASPIRE Lab industrial sponsors and affiliates Intel, Google,\r\nHuawei, Nokia, NVIDIA, Oracle, and Samsung. Any opinions,\r\nfindings, conclusions, or recommendations in this paper are\r\nsolely those of the authors and does not necessarily reflect the\r\nposition or the policy of the sponsors.\r\nREFERENCES\r\n[1] V. Agarwal et al., “Scalable graph exploration on multicore processors,”\r\nSupercomputing (SC), 2010.\r\n[2] W. Anderson et al., “Early experience with scientific programs on the\r\nCray MTA–2,” in Supercomputing (SC), 2003.\r\n[3] L. Backstrom et al., “Four degrees of separation,” ACM Web Science\r\nConference, pp. 45–54, 2012.\r\n[4] D. A. Bader, G. Cong, and J. Feo, “On the architectural requirements\r\nfor efficient execution of graph algorithms,” International Conference\r\non Parallel Processing, Jul 2005.\r\n[5] A.-L. Barabasi and R. Albert, “Emergence of scaling in random ´\r\nnetworks,” Science, vol. 286, pp. 509–512, Oct 1999.\r\n[6] S. Beamer, K. Asanovic, and D. A. Patterson, “The GAP benchmark ´\r\nsuite,” arXiv:1508.03619, August 2015.\r\n[7] S. Beamer, K. Asanovic, and D. A. Patterson, “Direction-optimizing ´\r\nbreadth-first search,” Proceedings of the International Conference for\r\nHigh Performance Computing, Networking, Storage and Analysis (SC),\r\n2012.\r\n[8] R. D. Blumofe et al., “Cilk: An efficient multithreaded runtime system,”\r\nJournal of parallel and distributed computing (JPDC), vol. 37, no. 1,\r\npp. 55–69, 1996.\r\n[9] M. Burtscher, R. Nasre, and K. Pingali, “A quantitative study of irregular\r\nprograms on GPUs,” IISWC, pp. 141–151, 2012.\r\n[10] S. Che et al., “Pannotia: Understanding irregular GPGPU graph appli\u0002cations,” in IISWC, 2013.\r\n[11] J. Chhugani et al., “Fast and efficient graph traversal algorithm for\r\nCPUs: Maximizing single-node efficiency,” IPDPS, 2012.\r\n[12] G. Cong and K. Makarychev, “Optimizing large-scale graph analysis\r\non multithreaded, multicore platforms,” IPDPS, Feb 2011.\r\n[13] G. Cong and S. Sbaraglia, “A study on the locality behavior of minimum\r\nspanning tree algorithms,” in High Performance Computing (HiPC).\r\nSpringer, 2006, pp. 583–594.\r\n[14] T. Davis and Y. Hu, “The University of Florida sparse matrix collec\u0002tion,” ACM Transactions on Mathematical Software, vol. 38, pp. 1:1 –\r\n1:25, 2011.\r\n[15] “9th DIMACS implementation challenge - shortest paths.”\r\nhttp://www.dis.uniroma1.it/challenge9/, 2006.\r\n[16] J. Dundas and T. Mudge, “Improving data cache performance by pre\u0002executing instructions under a cache miss,” in International Conference\r\non Supercomputing (ICS), 1997.\r\n[17] S. J. Eggers et al., “Simultaneous multithreading: A platform for next\u0002generation processors,” IEEE Micro, vol. 17, no. 5, pp. 12–19, 1997.\r\n[18] P. Erdos and A. R ˝ eyni, “On random graphs. I,” ´ Publicationes Mathe\u0002maticae, vol. 6, pp. 290–297, 1959.\r\n[19] “GAP benchmark suite reference code v0.6.”\r\nhttps://github.com/sbeamer/gapbs.\r\n[20] J. E. Gonzalez et al., “Powergraph: Distributed graph-parallel computa\u0002tion on natural graphs,” Symposium on Operating Systems Design and\r\nImplementation (OSDI), pp. 17–30, 2012.\r\n[21] “Graph500 benchmark.” www.graph500.org.\r\n[22] O. Green, M. Dukhan, and R. Vuduc, “Branch-avoiding graph algo\u0002rithms,” Symposium on Parallelism in Algorithms and Architectures\r\n(SPAA), 2015.\r\n[23] S. Hong, T. Oguntebi, and K. Olukotun, “Efficient parallel graph\r\nexploration on multi-core CPU and GPU,” Parallel Architectures and\r\nCompilation Techniques (PACT), 2011.\r\n[24] “Intel performance counter monitor.”\r\nwww.intel.com/software/pcm.\r\n[25] H. Kwak et al., “What is Twitter, a social network or a news media?”\r\nInternational World Wide Web Conference (WWW), 2010.\r\n[26] A. Kyrola, G. Blelloch, and C. Guestrin, “GraphChi: Large-scale graph\r\ncomputation on just a PC,” Symposium on Operating Systems Design\r\nand Implementation (OSDI), pp. 1–17, Oct 2012.\r\n[27] J. Leskovec et al., “Realistic, mathematically tractable graph generation\r\nand evolution, using Kronecker multiplication,” European Conference\r\non Principles and Practice of Knowledge Discovery in Databases, 2005.\r\n[28] Y. Low et al., “GraphLab: A new framework for parallel machine\r\nlearning,” Uncertainty in Artificial Intelligence, 2010.\r\n[29] A. Lumsdaine et al., “Challenges in parallel graph processing,” Parallel\r\nProcessing Letters, vol. 17, no. 01, pp. 5–20, 2007.\r\n[30] F. McSherry, M. Isard, and D. G. Murray, “Scalability! but at what\r\nCOST?” Workshop on Hot Topics in Operating Systems (HotOS), 2015.\r\n[31] A. Mislove et al., “Measurement and analysis of online social net\u0002works,” ACM SIGCOMM Conference on Internet Measurement (IMC),\r\n2007.\r\n[32] P. J. Mucci et al., “PAPI: A portable interface to hardware performance\r\ncounters,” in Department of Defense HPCMP Users Group Conference,\r\n1999.\r\n[33] R. C. Murphy and P. M. Kogge, “On the memory access patterns of\r\nsupercomputer applications: Benchmark selection and its implications,”\r\nIEEE Transactions on Computers, vol. 56, no. 7, pp. 937–945, 2007.\r\n[34] O. Mutlu et al., “Runahead execution: An alternative to very large\r\ninstruction windows for out-of-order processors,” in International Sym\u0002posium on High-Performance Computer Architecture (HPCA), 2003.\r\n[35] J. Nelson et al., “Crunching large graphs with commodity processors,”\r\nUSENIX conference on Hot topic in parallelism (HotPAR), 2011.\r\n[36] D. Nguyen, A. Lenharth, and K. Pingali, “A lightweight infrastructure\r\nfor graph analytics,” Symposium on Operating Systems Principles\r\n(SOSP), 2013.\r\n[37] M. A. O’Neil and M. Burtscher, “Microarchitectural performance\r\ncharacterization of irregular GPU kernels,” IISWC, 2014.\r\n[38] L. Page et al., “The pagerank citation ranking: Bringing order to the\r\nweb.” Stanford InfoLab, Technical Report 1999-66, November 1999.\r\n[39] J. B. Pereira-Leal, A. J. Enright, and C. A. Ouzounis, “Detection of\r\nfunctional modules from protein interaction networks,” PROTEINS:\r\nStructure, Function, and Bioinformatics, vol. 54, no. 1, pp. 49–57, 2004.\r\n[40] J. Shun and G. E. Blelloch, “Ligra: a lightweight graph processing\r\nframework for shared memory,” Symposium on Principles and Practice\r\nof Parallel Programming (PPoPP), 2013.\r\n[41] K. D. Underwood et al., “Analyzing the scalability of graph algorithms\r\non Eldorado,” in IPDPS, 2007, pp. 1–8.\r\n[42] D. Watts and S. Strogatz, “Collective dynamics of ‘small-world’ net\u0002works,” Nature, vol. 393, pp. 440–442, June 1998.\r\n[43] Y. Wu et al., “Performance characterization for high-level programming\r\nmodels for GPU graph analytics,” in IISWC, 2015.\r\n[44] Q. Xu, H. Jeon, and M. Annavaram, “Graph processing on GPUs:\r\nWhere are the bottlenecks?” IISWC, 2014.\r\n[45] J. Yang and J. Leskovec, “Defining and evaluating network communities\r\nbased on ground-truth,” CoRR, vol. abs/1205.6233, 2012.\r\n[46] K. You et al., “Scalable HMM-based inference engine in large vocabu\u0002lary continuous speech recognition,” IEEE Signal Processing Magazine,\r\n2010.\r\n[47] L. Yuan et al., “Modeling the locality in graph traversals,” in Interna\u0002tional Conference on Parallel Processing (ICPP), 2012.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/348edd94-61e9-4ea4-8b63-ba64fd93c85f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e992fe3b54b0cbf17795c108383b67d26d47ef4eb4607b791b470da5709c9a1a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1072
      },
      {
        "segments": [
          {
            "segment_id": "348edd94-61e9-4ea4-8b63-ba64fd93c85f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "Overall, we see no perfect solution to the performance\r\nchallenges presented by graph algorithms. Many techniques\r\ncan improve performance, but all of them will have quickly\r\ndiminishing returns, so greatly improving performance will\r\nrequire a multifaceted approach.\r\nACKNOWLEDGEMENTS\r\nWe thank the reviewers and David Ediger for their helpful\r\nfeedback. Research partially funded by DARPA Award Num\u0002ber HR0011-12-2-0016, the Center for Future Architecture\r\nResearch, a member of STARnet, a Semiconductor Research\r\nCorporation program sponsored by MARCO and DARPA, and\r\nASPIRE Lab industrial sponsors and affiliates Intel, Google,\r\nHuawei, Nokia, NVIDIA, Oracle, and Samsung. Any opinions,\r\nfindings, conclusions, or recommendations in this paper are\r\nsolely those of the authors and does not necessarily reflect the\r\nposition or the policy of the sponsors.\r\nREFERENCES\r\n[1] V. Agarwal et al., “Scalable graph exploration on multicore processors,”\r\nSupercomputing (SC), 2010.\r\n[2] W. Anderson et al., “Early experience with scientific programs on the\r\nCray MTA–2,” in Supercomputing (SC), 2003.\r\n[3] L. Backstrom et al., “Four degrees of separation,” ACM Web Science\r\nConference, pp. 45–54, 2012.\r\n[4] D. A. Bader, G. Cong, and J. Feo, “On the architectural requirements\r\nfor efficient execution of graph algorithms,” International Conference\r\non Parallel Processing, Jul 2005.\r\n[5] A.-L. Barabasi and R. Albert, “Emergence of scaling in random ´\r\nnetworks,” Science, vol. 286, pp. 509–512, Oct 1999.\r\n[6] S. Beamer, K. Asanovic, and D. A. Patterson, “The GAP benchmark ´\r\nsuite,” arXiv:1508.03619, August 2015.\r\n[7] S. Beamer, K. Asanovic, and D. A. Patterson, “Direction-optimizing ´\r\nbreadth-first search,” Proceedings of the International Conference for\r\nHigh Performance Computing, Networking, Storage and Analysis (SC),\r\n2012.\r\n[8] R. D. Blumofe et al., “Cilk: An efficient multithreaded runtime system,”\r\nJournal of parallel and distributed computing (JPDC), vol. 37, no. 1,\r\npp. 55–69, 1996.\r\n[9] M. Burtscher, R. Nasre, and K. Pingali, “A quantitative study of irregular\r\nprograms on GPUs,” IISWC, pp. 141–151, 2012.\r\n[10] S. Che et al., “Pannotia: Understanding irregular GPGPU graph appli\u0002cations,” in IISWC, 2013.\r\n[11] J. Chhugani et al., “Fast and efficient graph traversal algorithm for\r\nCPUs: Maximizing single-node efficiency,” IPDPS, 2012.\r\n[12] G. Cong and K. Makarychev, “Optimizing large-scale graph analysis\r\non multithreaded, multicore platforms,” IPDPS, Feb 2011.\r\n[13] G. Cong and S. Sbaraglia, “A study on the locality behavior of minimum\r\nspanning tree algorithms,” in High Performance Computing (HiPC).\r\nSpringer, 2006, pp. 583–594.\r\n[14] T. Davis and Y. Hu, “The University of Florida sparse matrix collec\u0002tion,” ACM Transactions on Mathematical Software, vol. 38, pp. 1:1 –\r\n1:25, 2011.\r\n[15] “9th DIMACS implementation challenge - shortest paths.”\r\nhttp://www.dis.uniroma1.it/challenge9/, 2006.\r\n[16] J. Dundas and T. Mudge, “Improving data cache performance by pre\u0002executing instructions under a cache miss,” in International Conference\r\non Supercomputing (ICS), 1997.\r\n[17] S. J. Eggers et al., “Simultaneous multithreading: A platform for next\u0002generation processors,” IEEE Micro, vol. 17, no. 5, pp. 12–19, 1997.\r\n[18] P. Erdos and A. R ˝ eyni, “On random graphs. I,” ´ Publicationes Mathe\u0002maticae, vol. 6, pp. 290–297, 1959.\r\n[19] “GAP benchmark suite reference code v0.6.”\r\nhttps://github.com/sbeamer/gapbs.\r\n[20] J. E. Gonzalez et al., “Powergraph: Distributed graph-parallel computa\u0002tion on natural graphs,” Symposium on Operating Systems Design and\r\nImplementation (OSDI), pp. 17–30, 2012.\r\n[21] “Graph500 benchmark.” www.graph500.org.\r\n[22] O. Green, M. Dukhan, and R. Vuduc, “Branch-avoiding graph algo\u0002rithms,” Symposium on Parallelism in Algorithms and Architectures\r\n(SPAA), 2015.\r\n[23] S. Hong, T. Oguntebi, and K. Olukotun, “Efficient parallel graph\r\nexploration on multi-core CPU and GPU,” Parallel Architectures and\r\nCompilation Techniques (PACT), 2011.\r\n[24] “Intel performance counter monitor.”\r\nwww.intel.com/software/pcm.\r\n[25] H. Kwak et al., “What is Twitter, a social network or a news media?”\r\nInternational World Wide Web Conference (WWW), 2010.\r\n[26] A. Kyrola, G. Blelloch, and C. Guestrin, “GraphChi: Large-scale graph\r\ncomputation on just a PC,” Symposium on Operating Systems Design\r\nand Implementation (OSDI), pp. 1–17, Oct 2012.\r\n[27] J. Leskovec et al., “Realistic, mathematically tractable graph generation\r\nand evolution, using Kronecker multiplication,” European Conference\r\non Principles and Practice of Knowledge Discovery in Databases, 2005.\r\n[28] Y. Low et al., “GraphLab: A new framework for parallel machine\r\nlearning,” Uncertainty in Artificial Intelligence, 2010.\r\n[29] A. Lumsdaine et al., “Challenges in parallel graph processing,” Parallel\r\nProcessing Letters, vol. 17, no. 01, pp. 5–20, 2007.\r\n[30] F. McSherry, M. Isard, and D. G. Murray, “Scalability! but at what\r\nCOST?” Workshop on Hot Topics in Operating Systems (HotOS), 2015.\r\n[31] A. Mislove et al., “Measurement and analysis of online social net\u0002works,” ACM SIGCOMM Conference on Internet Measurement (IMC),\r\n2007.\r\n[32] P. J. Mucci et al., “PAPI: A portable interface to hardware performance\r\ncounters,” in Department of Defense HPCMP Users Group Conference,\r\n1999.\r\n[33] R. C. Murphy and P. M. Kogge, “On the memory access patterns of\r\nsupercomputer applications: Benchmark selection and its implications,”\r\nIEEE Transactions on Computers, vol. 56, no. 7, pp. 937–945, 2007.\r\n[34] O. Mutlu et al., “Runahead execution: An alternative to very large\r\ninstruction windows for out-of-order processors,” in International Sym\u0002posium on High-Performance Computer Architecture (HPCA), 2003.\r\n[35] J. Nelson et al., “Crunching large graphs with commodity processors,”\r\nUSENIX conference on Hot topic in parallelism (HotPAR), 2011.\r\n[36] D. Nguyen, A. Lenharth, and K. Pingali, “A lightweight infrastructure\r\nfor graph analytics,” Symposium on Operating Systems Principles\r\n(SOSP), 2013.\r\n[37] M. A. O’Neil and M. Burtscher, “Microarchitectural performance\r\ncharacterization of irregular GPU kernels,” IISWC, 2014.\r\n[38] L. Page et al., “The pagerank citation ranking: Bringing order to the\r\nweb.” Stanford InfoLab, Technical Report 1999-66, November 1999.\r\n[39] J. B. Pereira-Leal, A. J. Enright, and C. A. Ouzounis, “Detection of\r\nfunctional modules from protein interaction networks,” PROTEINS:\r\nStructure, Function, and Bioinformatics, vol. 54, no. 1, pp. 49–57, 2004.\r\n[40] J. Shun and G. E. Blelloch, “Ligra: a lightweight graph processing\r\nframework for shared memory,” Symposium on Principles and Practice\r\nof Parallel Programming (PPoPP), 2013.\r\n[41] K. D. Underwood et al., “Analyzing the scalability of graph algorithms\r\non Eldorado,” in IPDPS, 2007, pp. 1–8.\r\n[42] D. Watts and S. Strogatz, “Collective dynamics of ‘small-world’ net\u0002works,” Nature, vol. 393, pp. 440–442, June 1998.\r\n[43] Y. Wu et al., “Performance characterization for high-level programming\r\nmodels for GPU graph analytics,” in IISWC, 2015.\r\n[44] Q. Xu, H. Jeon, and M. Annavaram, “Graph processing on GPUs:\r\nWhere are the bottlenecks?” IISWC, 2014.\r\n[45] J. Yang and J. Leskovec, “Defining and evaluating network communities\r\nbased on ground-truth,” CoRR, vol. abs/1205.6233, 2012.\r\n[46] K. You et al., “Scalable HMM-based inference engine in large vocabu\u0002lary continuous speech recognition,” IEEE Signal Processing Magazine,\r\n2010.\r\n[47] L. Yuan et al., “Modeling the locality in graph traversals,” in Interna\u0002tional Conference on Parallel Processing (ICPP), 2012.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0486a3c3-e2fb-4c2d-89b4-9c2936bfa9b1/images/348edd94-61e9-4ea4-8b63-ba64fd93c85f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041245Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e992fe3b54b0cbf17795c108383b67d26d47ef4eb4607b791b470da5709c9a1a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1072
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\"title\": \"Locality Exists in Graph Processing: Workload Characterization on an Ivy Bridge Server\"}\n```"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "```string\nScott Beamer, Krste Asanovic, David Patterson\n```"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "2015\n"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "University of California, Berkeley, California"
        }
      ]
    }
  }
}