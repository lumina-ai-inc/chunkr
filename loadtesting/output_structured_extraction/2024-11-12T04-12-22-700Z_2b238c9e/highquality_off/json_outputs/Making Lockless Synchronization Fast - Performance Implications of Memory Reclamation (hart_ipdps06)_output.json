{
  "file_name": "Making Lockless Synchronization Fast - Performance Implications of Memory Reclamation (hart_ipdps06).pdf",
  "task_id": "660586f2-48cb-4876-82b6-ee43d867ddd3",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "1107b2d7-8774-4b81-9fee-f097f92999ac",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Making Lockless Synchronization Fast:\r\nPerformance Implications of Memory Reclamation\r\nThomas E. Hart1∗, Paul E. McKenney2, and Angela Demke Brown1\r\n1University of Toronto 2IBM Beaverton\r\nDept. of Computer Science Linux Technology Center\r\nToronto, ON M5S 2E4 CAN Beaverton, OR 97006 USA\r\n{tomhart, demke}@cs.toronto.edu paulmck@us.ibm.com\r\nAbstract\r\nAchieving high performance for concurrent applications\r\non modern multiprocessorsremains challenging. Many pro\u0002grammers avoid locking to improve performance, while oth\u0002ers replace locks with non-blocking synchronization to pro\u0002tect against deadlock, priority inversion, and convoying. In\r\nboth cases, dynamic data structures that avoid locking, re\u0002quire a memory reclamation scheme that reclaims nodes\r\nonce they are no longer in use.\r\nThe performance of existing memory reclamation\r\nschemes has not been thoroughly evaluated. We conduct\r\nthe first fair and comprehensive comparison of three recent\r\nschemes—quiescent-state-based reclamation, epoch-based\r\nreclamation, and hazard-pointer-based reclamation—using\r\na flexible microbenchmark. Our results show that there is\r\nno globally optimal scheme. When evaluating lockless syn\u0002chronization, programmers and algorithm designers should\r\nthus carefully consider the data structure, the workload,\r\nand the execution environment, each of which can dramati\u0002cally affect memory reclamation performance.\r\n1 Introduction\r\nAs multiprocessors become mainstream, multithreaded\r\napplications will become more common, increasing the\r\nneed for efficient coordination of concurrent accesses to\r\nshared data structures. Traditional locking requires expen\u0002sive atomic operations, such as compare-and-swap (CAS),\r\neven when locks are uncontended. For example, acquiring\r\nand releasing an uncontended spinlock requires over 400\r\ncycles on an IBM!R POWERTM\r\nCPU. Therefore, many re\u0002searchers recommend avoiding locking [2, 7, 22]. Some\r\nsystems, such as Linux\r\nTM\r\n, use concurrently-readable syn\u0002chronization, which uses locks for updates but not for reads.\r\n∗Supported by an NSERC Canada Graduate Scholarship.\r\nFigure 1. Read/reclaim race.\r\nLocking is also susceptible to priority inversion, convoying,\r\ndeadlock, and blocking due to thread failure [3, 10], leading\r\nresearchers to pursue non-blocking (or lock-free) synchro\u0002nization [6, 12, 13, 14, 16, 29]. In some cases, lock-free\r\napproaches can bring performance benefits [25]. For clar\u0002ity, we describe all strategies that avoid locks as lockless.\r\nA major challenge for lockless synchronization is han\u0002dling the read/reclaim races that arise in dynamic data\r\nstructures. Figure 1 illustrates the problem: thread T1 re\u0002moves node N from a list while thread T2 is referencing it.\r\nN’s memory must be reclaimed to allow reuse, lest memory\r\nexhaustion block all threads, but such reuse is unsafe while\r\nT2 continues referencing N. For languages like C, where\r\nmemory must be explicitly reclaimed (e.g. via free()),\r\nprogrammers must combine a memory reclamation scheme\r\nwith their lockless data structures to resolve these races.\r\n1\r\nSeveral such reclamation schemes have been proposed.\r\nProgrammers need to understand the semantics and the\r\nperformance implications of each scheme, since the over\u0002head of inefficient reclamation can be worse than that of\r\nlocking. For example, reference counting [5, 29] has high\r\noverhead in the base case and scales poorly with data\u0002structure size. This is unacceptable when performance is\r\nthe motivation for lockless synchronization. Unfortunately,\r\nthere is no single optimal scheme and existing work is rela\u0002tively silent on factors affecting reclamation performance.\r\n1Reclamation is subsumed into automatic garbage collectors in envi\u0002ronments that provide them, such as Java\r\nTM\r\n.\r\n1-4244-0054-6/06/$20.00 ©2006 IEEE",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/1107b2d7-8774-4b81-9fee-f097f92999ac.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=62838c929d29955943fe1f2b686f73b0b797124c8cd0b9baa5885311018d8e7f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "c1e5deb2-29e6-4722-8d79-1045cb0fd275",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 2. Illustration of QSBR. Black boxes\r\nrepresent quiescent states.\r\nWe address this deficit by comparing three recent recla\u0002mation schemes, showing the respective strengths and\r\nweaknesses of each. In Sections 2 and 3, we review these\r\nschemes and describe factors affecting their performance.\r\nSection 4 explains our experimental setup. Our analysis, in\r\nSection 5, reveals substantial performance differences be\u0002tween these schemes, the greatest source of which is per\u0002operation atomic instructions. In Section 6, we discuss the\r\nrelevance of our work to designers and implementers. We\r\nshow that lockless algorithms and reclamation schemes are\r\nmostly independent, by combining a blocking reclamation\r\nscheme and a non-blocking algorithm, then comparing this\r\ncombination to a fully non-blocking equivalent. We also\r\npresent a new reclamation scheme that combines aspects of\r\ntwo other schemes to give good performance and ease of\r\nuse. We close with a discussion of related work in Section 7\r\nand summarize our conclusions in Section 8.\r\n2 Memory Reclamation Schemes\r\nThis section briefly reviews the reclamation schemes we\r\nconsider: quiescent-state-based reclamation (QSBR) [22,\r\n2], epoch-based reclamation (EBR) [6], hazard-pointer\u0002based reclamation (HPBR) [23, 24], and reference count\u0002ing [29, 26]. We provide an overview of each scheme to\r\nhelp the reader understand our work; further details are\r\navailable in the papers cited.\r\n2.1 Blocking Schemes\r\nWe discuss two blocking schemes, QSBR and EBR,\r\nwhich use the concept of a grace period. A grace period\r\nis a time interval [a,b] such that, after time b, all nodes\r\nremoved before time a may safely be reclaimed. These\r\nmethods force threads to wait for other threads to complete\r\ntheir lockless operations in order for a grace period to oc\u0002cur. Failed or delayed threads can therefore prevent memory\r\nfrom being reclaimed. Eventually, memory allocation will\r\nfail, causing threads to block.\r\nFigure 3. QSBR is inherently blocking.\r\nFigure 4. Illustration of EBR.\r\n2.1.1 Quiescent-State-Based Reclamation (QSBR)\r\nQSBR uses quiescent states to detect grace periods. A\r\nquiescent state for thread T is a state in which T holds\r\nno references to shared nodes; hence, a grace period for\r\nQSBR is any interval of time during which all threads pass\r\nthrough at least one quiescent state. The choice of quies\u0002cent states is application-dependent. Many operating sys\u0002tem kernels contain natural quiescent states, such as volun\u0002tary context switch, and use QSBR to implement read-copy\r\nupdate (RCU) [22, 20, 7].\r\nFigure 2 illustrates the relationship between quiescent\r\nstates and grace periods in QSBR. Thread T1 goes through\r\nquiescent states at times t1 and t5, T2 at times t2 and t4,\r\nand T3 at time t3. Hence, a grace period is any time interval\r\ncontaining either [t1,t3] or [t3,t5].\r\nQSBR must detect grace periods so that removed nodes\r\nmay be reclaimed; however, detecting minimal grace peri\u0002ods is unnecessary. In Figure 2, for example, any interval\r\ncontaining [t1,t3] or [t3,t5] is a quiescent state; implemen\u0002tations which check for grace periods only when threads en\u0002ter quiescent states would detect [t1,t5], since T1’s two qui\u0002escent states form the only pair from a single thread which\r\nenclose a grace period.\r\nFigure 3 shows why QSBR is blocking. Here, T 2 goes\r\nthrough no quiescent states (for example, due to blocking\r\non I/O). Threads T 1 and T 3 are then prevented from re\u0002claiming memory,since no grace periods exist. The ensuing\r\nmemory exhaustion will eventually block all threads.\r\n2.1.2 Epoch-Based Reclamation (EBR)\r\nFraser’s EBR [6] follows QSBR in using grace periods, but\r\nuses epochs in place of QSBR’s quiescent states. Each\r\nthread executes in one of three logical epochs, and may lag",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/c1e5deb2-29e6-4722-8d79-1045cb0fd275.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3de1bec2fdc74b986368b3f50ea5eee253f55398967464baa3b2a8438f1fc89d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 582
      },
      {
        "segments": [
          {
            "segment_id": "c1e5deb2-29e6-4722-8d79-1045cb0fd275",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 2. Illustration of QSBR. Black boxes\r\nrepresent quiescent states.\r\nWe address this deficit by comparing three recent recla\u0002mation schemes, showing the respective strengths and\r\nweaknesses of each. In Sections 2 and 3, we review these\r\nschemes and describe factors affecting their performance.\r\nSection 4 explains our experimental setup. Our analysis, in\r\nSection 5, reveals substantial performance differences be\u0002tween these schemes, the greatest source of which is per\u0002operation atomic instructions. In Section 6, we discuss the\r\nrelevance of our work to designers and implementers. We\r\nshow that lockless algorithms and reclamation schemes are\r\nmostly independent, by combining a blocking reclamation\r\nscheme and a non-blocking algorithm, then comparing this\r\ncombination to a fully non-blocking equivalent. We also\r\npresent a new reclamation scheme that combines aspects of\r\ntwo other schemes to give good performance and ease of\r\nuse. We close with a discussion of related work in Section 7\r\nand summarize our conclusions in Section 8.\r\n2 Memory Reclamation Schemes\r\nThis section briefly reviews the reclamation schemes we\r\nconsider: quiescent-state-based reclamation (QSBR) [22,\r\n2], epoch-based reclamation (EBR) [6], hazard-pointer\u0002based reclamation (HPBR) [23, 24], and reference count\u0002ing [29, 26]. We provide an overview of each scheme to\r\nhelp the reader understand our work; further details are\r\navailable in the papers cited.\r\n2.1 Blocking Schemes\r\nWe discuss two blocking schemes, QSBR and EBR,\r\nwhich use the concept of a grace period. A grace period\r\nis a time interval [a,b] such that, after time b, all nodes\r\nremoved before time a may safely be reclaimed. These\r\nmethods force threads to wait for other threads to complete\r\ntheir lockless operations in order for a grace period to oc\u0002cur. Failed or delayed threads can therefore prevent memory\r\nfrom being reclaimed. Eventually, memory allocation will\r\nfail, causing threads to block.\r\nFigure 3. QSBR is inherently blocking.\r\nFigure 4. Illustration of EBR.\r\n2.1.1 Quiescent-State-Based Reclamation (QSBR)\r\nQSBR uses quiescent states to detect grace periods. A\r\nquiescent state for thread T is a state in which T holds\r\nno references to shared nodes; hence, a grace period for\r\nQSBR is any interval of time during which all threads pass\r\nthrough at least one quiescent state. The choice of quies\u0002cent states is application-dependent. Many operating sys\u0002tem kernels contain natural quiescent states, such as volun\u0002tary context switch, and use QSBR to implement read-copy\r\nupdate (RCU) [22, 20, 7].\r\nFigure 2 illustrates the relationship between quiescent\r\nstates and grace periods in QSBR. Thread T1 goes through\r\nquiescent states at times t1 and t5, T2 at times t2 and t4,\r\nand T3 at time t3. Hence, a grace period is any time interval\r\ncontaining either [t1,t3] or [t3,t5].\r\nQSBR must detect grace periods so that removed nodes\r\nmay be reclaimed; however, detecting minimal grace peri\u0002ods is unnecessary. In Figure 2, for example, any interval\r\ncontaining [t1,t3] or [t3,t5] is a quiescent state; implemen\u0002tations which check for grace periods only when threads en\u0002ter quiescent states would detect [t1,t5], since T1’s two qui\u0002escent states form the only pair from a single thread which\r\nenclose a grace period.\r\nFigure 3 shows why QSBR is blocking. Here, T 2 goes\r\nthrough no quiescent states (for example, due to blocking\r\non I/O). Threads T 1 and T 3 are then prevented from re\u0002claiming memory,since no grace periods exist. The ensuing\r\nmemory exhaustion will eventually block all threads.\r\n2.1.2 Epoch-Based Reclamation (EBR)\r\nFraser’s EBR [6] follows QSBR in using grace periods, but\r\nuses epochs in place of QSBR’s quiescent states. Each\r\nthread executes in one of three logical epochs, and may lag",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/c1e5deb2-29e6-4722-8d79-1045cb0fd275.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3de1bec2fdc74b986368b3f50ea5eee253f55398967464baa3b2a8438f1fc89d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 582
      },
      {
        "segments": [
          {
            "segment_id": "739b1499-b40c-46f2-b003-9ee9e793e807",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "1 int search (struct list *l, long key)\r\n2 {\r\n3 node_t *cur;\r\n4 critical_enter();\r\n5 for (cur = l->list_head;\r\n6 cur != NULL; cur = cur->next) {\r\n7 if (cur->key >= key) {\r\n8 critical_exit();\r\n9 return (cur->key == key);\r\n10 }\r\n11 }\r\n12 critical_exit();\r\n13 return (0);\r\n14 }\r\nFigure 5. EBR concurrently-readable search.\r\nat most one epoch behind the global epoch. Each thread\r\natomically sets a per-thread flag upon entry into a critical\r\nregion, indicating that the thread intends to access shared\r\ndata without locks. Upon exit, the thread atomically clears\r\nits flag. No thread is allowed to access an EBR-protected\r\nobject outside of a critical region.\r\nFigure 4 shows how EBR tracks epochs, allowing safe\r\nmemory reclamation. Upon entering a critical region, a\r\nthread updates its local epoch to match the global epoch.\r\nHence, if the global epoch is e, threads in critical regions\r\ncan be in either epoch e or e-1, but not e+1 (all mod 3). Any\r\nnode a thread removes during a given epoch may be safely\r\nreclaimed the next time the thread re-enters that epoch.\r\nThus, the time period [t1,t2] in the figure is a grace period.\r\nA thread will attempt to update the global epoch only if it\r\nhas not changed for some pre-determined number of critical\r\nregion entries.\r\nAs with QSBR, reclamation can be stalled by threads\r\nwhich fail in critical regions, but threads not in critical re\u0002gions cannot stall EBR. EBR’s bookkeeping is invisible to\r\nthe applications programmer, making it easy for a program\u0002mer to use; however, Section 5 shows that this property im\u0002poses significant overhead on EBR.\r\nFigure 5 shows an example of a search of a linked list\r\nwhich allows lockless reads but uses locking for updates.\r\nQSBR omits lines 4 and 12, which handle EBR’s epoch\r\nbookkeeping, but is otherwise identical; QSBR’s quiescent\r\nstates are flagged explicitly at a higher level (see Figure 8).\r\n2.2 Non-blocking schemes\r\nThis section presents the non-blocking reclamation\r\nschemes we evaluate: hazard-pointer-based reclamation\r\n(HPBR) and lock-free reference counting (LFRC).\r\n2.2.1 Hazard-Pointer-Based Reclamation (HPBR)\r\nMichael’s HPBR [23] provides an existence locking mech\u0002anism for dynamically-allocated nodes. Each thread per\u0002forming lockless operations has K hazard pointers which\r\nit uses to protect nodes from reclamation by other threads;\r\nFigure 6. Illustration of HPBR.\r\n1 int search (struct list *l, long key)\r\n2 {\r\n3 node_t **prev, *cur, *next;\r\n4 /* Index of our first hazard pointer. */\r\n5 int base = getTID()*K;\r\n6 /* Offset into our hazard pointer segment. */\r\n7 int off = 0;\r\n8 try_again:\r\n9 prev = &l->list_head;\r\n10 for (cur = *prev; cur != NULL; cur = next & ˜1) {\r\n11 /* Protect cur with a hazard pointer. */\r\n12 HP[base+off] = cur;\r\n13 memory_fence();\r\n14 if (*prev != cur)\r\n15 goto try_again;\r\n16 next = cur->next;\r\n17 if (cur->key >= key)\r\n18 return (cur->key == key);\r\n19 prev = &cur->next;\r\n20 off = (off+1) % K;\r\n21 }\r\n22 return (0);\r\n23 }\r\nFigure 7. HPBR concurrently-readable\r\nsearch.\r\nhence, we have H=NK hazard pointers in total. K is data\u0002structure-dependent, and often small. Queues and linked\r\nlists need K=2 hazard pointers, while stacks require only\r\nK=1; however, we know of no upper bound on K for gen\u0002eral tree- or graph-traversal algorithms.\r\nAfter removing a node, a thread places that node in a\r\nprivate list. When the list grows to a predefined size R,\r\nthe thread reclaims each node lacking a corresponding haz\u0002ard pointer. Increasing R amortizes reclamation overhead\r\nacross more nodes, but increases memory usage.\r\nAn algorithm using HPBR must identify all hazardous\r\nreferences — referencesto shared nodesthat may have been\r\nremoved by other threads or that are vulnerable to the ABA\r\nproblem [24]. Such references require hazard pointers. The\r\nalgorithm sets a hazard pointer, then checks for node re\u0002moval; if the node has not been removed, then it may safely\r\nbe referenced. As long as the hazard pointer references the\r\nnode, HPBR’s reclamation routine refrains from reclaiming\r\nit. Figure 6 illustratesthe use of HPBR. Node N has been re\u0002moved from the linked list, but cannot be reclaimed because\r\nT2’s hazard pointer HP[2] references it.\r\nFigure 7, showing code adapted from Michael [24],\r\ndemonstrates HPBR with a search algorithm corresponding\r\nto Figure 5. At most two nodes must be protected: the cur\u0002rent node and its predecessor (K=2). The code removing",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/739b1499-b40c-46f2-b003-9ee9e793e807.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=547cbd7ee6577f2aa6aa0ed1419fc5a3f5033daa8e78024328489c263d6a095a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 722
      },
      {
        "segments": [
          {
            "segment_id": "739b1499-b40c-46f2-b003-9ee9e793e807",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "1 int search (struct list *l, long key)\r\n2 {\r\n3 node_t *cur;\r\n4 critical_enter();\r\n5 for (cur = l->list_head;\r\n6 cur != NULL; cur = cur->next) {\r\n7 if (cur->key >= key) {\r\n8 critical_exit();\r\n9 return (cur->key == key);\r\n10 }\r\n11 }\r\n12 critical_exit();\r\n13 return (0);\r\n14 }\r\nFigure 5. EBR concurrently-readable search.\r\nat most one epoch behind the global epoch. Each thread\r\natomically sets a per-thread flag upon entry into a critical\r\nregion, indicating that the thread intends to access shared\r\ndata without locks. Upon exit, the thread atomically clears\r\nits flag. No thread is allowed to access an EBR-protected\r\nobject outside of a critical region.\r\nFigure 4 shows how EBR tracks epochs, allowing safe\r\nmemory reclamation. Upon entering a critical region, a\r\nthread updates its local epoch to match the global epoch.\r\nHence, if the global epoch is e, threads in critical regions\r\ncan be in either epoch e or e-1, but not e+1 (all mod 3). Any\r\nnode a thread removes during a given epoch may be safely\r\nreclaimed the next time the thread re-enters that epoch.\r\nThus, the time period [t1,t2] in the figure is a grace period.\r\nA thread will attempt to update the global epoch only if it\r\nhas not changed for some pre-determined number of critical\r\nregion entries.\r\nAs with QSBR, reclamation can be stalled by threads\r\nwhich fail in critical regions, but threads not in critical re\u0002gions cannot stall EBR. EBR’s bookkeeping is invisible to\r\nthe applications programmer, making it easy for a program\u0002mer to use; however, Section 5 shows that this property im\u0002poses significant overhead on EBR.\r\nFigure 5 shows an example of a search of a linked list\r\nwhich allows lockless reads but uses locking for updates.\r\nQSBR omits lines 4 and 12, which handle EBR’s epoch\r\nbookkeeping, but is otherwise identical; QSBR’s quiescent\r\nstates are flagged explicitly at a higher level (see Figure 8).\r\n2.2 Non-blocking schemes\r\nThis section presents the non-blocking reclamation\r\nschemes we evaluate: hazard-pointer-based reclamation\r\n(HPBR) and lock-free reference counting (LFRC).\r\n2.2.1 Hazard-Pointer-Based Reclamation (HPBR)\r\nMichael’s HPBR [23] provides an existence locking mech\u0002anism for dynamically-allocated nodes. Each thread per\u0002forming lockless operations has K hazard pointers which\r\nit uses to protect nodes from reclamation by other threads;\r\nFigure 6. Illustration of HPBR.\r\n1 int search (struct list *l, long key)\r\n2 {\r\n3 node_t **prev, *cur, *next;\r\n4 /* Index of our first hazard pointer. */\r\n5 int base = getTID()*K;\r\n6 /* Offset into our hazard pointer segment. */\r\n7 int off = 0;\r\n8 try_again:\r\n9 prev = &l->list_head;\r\n10 for (cur = *prev; cur != NULL; cur = next & ˜1) {\r\n11 /* Protect cur with a hazard pointer. */\r\n12 HP[base+off] = cur;\r\n13 memory_fence();\r\n14 if (*prev != cur)\r\n15 goto try_again;\r\n16 next = cur->next;\r\n17 if (cur->key >= key)\r\n18 return (cur->key == key);\r\n19 prev = &cur->next;\r\n20 off = (off+1) % K;\r\n21 }\r\n22 return (0);\r\n23 }\r\nFigure 7. HPBR concurrently-readable\r\nsearch.\r\nhence, we have H=NK hazard pointers in total. K is data\u0002structure-dependent, and often small. Queues and linked\r\nlists need K=2 hazard pointers, while stacks require only\r\nK=1; however, we know of no upper bound on K for gen\u0002eral tree- or graph-traversal algorithms.\r\nAfter removing a node, a thread places that node in a\r\nprivate list. When the list grows to a predefined size R,\r\nthe thread reclaims each node lacking a corresponding haz\u0002ard pointer. Increasing R amortizes reclamation overhead\r\nacross more nodes, but increases memory usage.\r\nAn algorithm using HPBR must identify all hazardous\r\nreferences — referencesto shared nodesthat may have been\r\nremoved by other threads or that are vulnerable to the ABA\r\nproblem [24]. Such references require hazard pointers. The\r\nalgorithm sets a hazard pointer, then checks for node re\u0002moval; if the node has not been removed, then it may safely\r\nbe referenced. As long as the hazard pointer references the\r\nnode, HPBR’s reclamation routine refrains from reclaiming\r\nit. Figure 6 illustratesthe use of HPBR. Node N has been re\u0002moved from the linked list, but cannot be reclaimed because\r\nT2’s hazard pointer HP[2] references it.\r\nFigure 7, showing code adapted from Michael [24],\r\ndemonstrates HPBR with a search algorithm corresponding\r\nto Figure 5. At most two nodes must be protected: the cur\u0002rent node and its predecessor (K=2). The code removing",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/739b1499-b40c-46f2-b003-9ee9e793e807.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=547cbd7ee6577f2aa6aa0ed1419fc5a3f5033daa8e78024328489c263d6a095a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 722
      },
      {
        "segments": [
          {
            "segment_id": "e812a17f-b27e-4eaa-b112-1bdaaa49f749",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "nodes, which is not shown here, uses the low-order bit of\r\nthe next pointer as a flag. This guarantees that the valida\u0002tion step on line 14 will fail and retry in case of concurrent\r\nremoval. Full details are given by Michael [24].\r\nHerlihy et al. [15] presented a very similar scheme called\r\nPass the Buck (PTB). Since HPBR and PTB have similar\r\nper-operation costs, we believe that our HPBR results apply\r\nto PTB as well.\r\n2.2.2 Lock-Free Reference Counting (LFRC)\r\nLock-free reference counting (LFRC) is a well-known non\u0002blocking garbage-collection technique. Threads track the\r\nnumber of references to nodes, reclaiming any node whose\r\ncount is zero. Valois’s LFRC scheme [29] (corrected\r\nby Michael and Scott [26]) uses CAS and fetch-and-add\r\n(FAA), and requires nodes to retain their type after recla\u0002mation. Sundell’s scheme [28], based on Valois’s, is wait\u0002free. The scheme of Detlefs et al. [5] allows nodes’ types\r\nto change upon reclamation, but requires double compare\u0002and-swap (DCAS), which no current CPU supports.\r\nMichael [24] showed that LFRC introduces overhead\r\nwhich often makes lockless algorithms perform worse than\r\nlock-based versions. We include some experiments with\r\nValois’ scheme to reproduce Michael’s findings.\r\n3 Reclamation Performance Factors\r\nWe categorize factors which can affect reclamation\r\nscheme performance; we vary these factors in Section 5.\r\n3.1 Memory Consistency\r\nCurrent literature on lock-free algorithms generally as\u0002sumes a sequentially-consistent [18] memory model, which\r\nprohibits instruction reordering and globally orders mem\u0002ory references. For performance reasons, however, mod\u0002ern CPUs provide weaker memory consistency models in\r\nwhich ordering can be enforced only when needed via spe\u0002cial fence instructions. Although fences are often omitted\r\nfrom pseudocode, they are expensive on most modern CPUs\r\nand must be included in realistic performance analyses.\r\nHPBR, EBR, and LFRC require per-operation fences.\r\nHPBR, as shown in Figure 7, requires a fence between\r\nhazard-pointer setting and validation, thus one fence per\r\nvisited node. LFRC also requires per-node fences, in ad\u0002dition to atomic instructions needed to maintain reference\r\ncounts. EBR requires two fences per operation: one when\r\nsetting a flag when entering a critical region, and one when\r\nclearing it upon exit. Since QSBR has no per-operation\r\nfences, its per-operation overhead can be very low.\r\n3.2 Data Structures and Workload\r\nData structures differ in both the operationsthey provide,\r\nand in their common workloads. Queues are write-only,\r\nbut linked lists and hash tables are often read-mostly [19].\r\nBlocking schemes may perform poorly with update-heavy\r\nstructures, since the risk of memory exhaustion is higher.\r\nConversely, non-blocking schemes may perform poorly\r\nwith operations such as list or tree traversal which visit\r\nmany nodes, since they require per-node fences.\r\n3.3 Threads and Scheduling\r\nWe expect contention due to concurrent threads to be\r\na minor source of reclamation overhead; however, for the\r\nnon-blocking schemes, it could be unbounded in degener\u0002ate cases: readersforced to repeatedly restart their traversals\r\nmust repeatedly execute fence instructions for every node.\r\nThread preemption, especially when there are more\r\nthreads than processors, can adversely affecting blocking\r\nschemes. Descheduled threads can delay reclamation, po\u0002tentially exhausting memory, particularly in update-heavy\r\nworkloads. Longer scheduling quanta may increase the risk\r\nof this exhaustion.\r\n3.4 Memory Constraints\r\nAlthough lock-free memory allocators exist [25], many\r\nallocators use locking. Blocking methods will see greater\r\nlock contention because they must access a lock-protected\r\nglobal pool more frequently. Furthermore, if a thread is pre\u0002empted while holding such a lock, other threads will block\r\non memory allocation. The size of the global pool is finite,\r\nand governsthe likelihood of a blocking scheme exhausting\r\nall memory. Only HPBR [23] provides a provable bound on\r\nthe amount of unreclaimed memory; it should thus be less\r\nsensitive to these constraints.\r\n4 Experimental Setup\r\nWe evaluated the memory reclamation strategies with re\u0002spect to the factors outlined in Section 3 using commodity\r\nSMP systems with IBM POWER CPUs. This section pro\u0002vides details on these aspects of our experiments.\r\n4.1 Data Structures Used\r\nWe tested the reclamation schemes on linked lists and\r\nqueues. We used Michael’s ordered lock-free linked\r\nlist [24], which forbids duplicate keys, and coded our\r\nconcurrently-readable lists similarly. Because linked lists\r\npermit arbitrary lengths and read-to-write ratios, we used",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/e812a17f-b27e-4eaa-b112-1bdaaa49f749.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=44d852f05ebe57d6473941df930ec84b150cca74fbfdad2eab69cd736e67a983",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 676
      },
      {
        "segments": [
          {
            "segment_id": "e812a17f-b27e-4eaa-b112-1bdaaa49f749",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "nodes, which is not shown here, uses the low-order bit of\r\nthe next pointer as a flag. This guarantees that the valida\u0002tion step on line 14 will fail and retry in case of concurrent\r\nremoval. Full details are given by Michael [24].\r\nHerlihy et al. [15] presented a very similar scheme called\r\nPass the Buck (PTB). Since HPBR and PTB have similar\r\nper-operation costs, we believe that our HPBR results apply\r\nto PTB as well.\r\n2.2.2 Lock-Free Reference Counting (LFRC)\r\nLock-free reference counting (LFRC) is a well-known non\u0002blocking garbage-collection technique. Threads track the\r\nnumber of references to nodes, reclaiming any node whose\r\ncount is zero. Valois’s LFRC scheme [29] (corrected\r\nby Michael and Scott [26]) uses CAS and fetch-and-add\r\n(FAA), and requires nodes to retain their type after recla\u0002mation. Sundell’s scheme [28], based on Valois’s, is wait\u0002free. The scheme of Detlefs et al. [5] allows nodes’ types\r\nto change upon reclamation, but requires double compare\u0002and-swap (DCAS), which no current CPU supports.\r\nMichael [24] showed that LFRC introduces overhead\r\nwhich often makes lockless algorithms perform worse than\r\nlock-based versions. We include some experiments with\r\nValois’ scheme to reproduce Michael’s findings.\r\n3 Reclamation Performance Factors\r\nWe categorize factors which can affect reclamation\r\nscheme performance; we vary these factors in Section 5.\r\n3.1 Memory Consistency\r\nCurrent literature on lock-free algorithms generally as\u0002sumes a sequentially-consistent [18] memory model, which\r\nprohibits instruction reordering and globally orders mem\u0002ory references. For performance reasons, however, mod\u0002ern CPUs provide weaker memory consistency models in\r\nwhich ordering can be enforced only when needed via spe\u0002cial fence instructions. Although fences are often omitted\r\nfrom pseudocode, they are expensive on most modern CPUs\r\nand must be included in realistic performance analyses.\r\nHPBR, EBR, and LFRC require per-operation fences.\r\nHPBR, as shown in Figure 7, requires a fence between\r\nhazard-pointer setting and validation, thus one fence per\r\nvisited node. LFRC also requires per-node fences, in ad\u0002dition to atomic instructions needed to maintain reference\r\ncounts. EBR requires two fences per operation: one when\r\nsetting a flag when entering a critical region, and one when\r\nclearing it upon exit. Since QSBR has no per-operation\r\nfences, its per-operation overhead can be very low.\r\n3.2 Data Structures and Workload\r\nData structures differ in both the operationsthey provide,\r\nand in their common workloads. Queues are write-only,\r\nbut linked lists and hash tables are often read-mostly [19].\r\nBlocking schemes may perform poorly with update-heavy\r\nstructures, since the risk of memory exhaustion is higher.\r\nConversely, non-blocking schemes may perform poorly\r\nwith operations such as list or tree traversal which visit\r\nmany nodes, since they require per-node fences.\r\n3.3 Threads and Scheduling\r\nWe expect contention due to concurrent threads to be\r\na minor source of reclamation overhead; however, for the\r\nnon-blocking schemes, it could be unbounded in degener\u0002ate cases: readersforced to repeatedly restart their traversals\r\nmust repeatedly execute fence instructions for every node.\r\nThread preemption, especially when there are more\r\nthreads than processors, can adversely affecting blocking\r\nschemes. Descheduled threads can delay reclamation, po\u0002tentially exhausting memory, particularly in update-heavy\r\nworkloads. Longer scheduling quanta may increase the risk\r\nof this exhaustion.\r\n3.4 Memory Constraints\r\nAlthough lock-free memory allocators exist [25], many\r\nallocators use locking. Blocking methods will see greater\r\nlock contention because they must access a lock-protected\r\nglobal pool more frequently. Furthermore, if a thread is pre\u0002empted while holding such a lock, other threads will block\r\non memory allocation. The size of the global pool is finite,\r\nand governsthe likelihood of a blocking scheme exhausting\r\nall memory. Only HPBR [23] provides a provable bound on\r\nthe amount of unreclaimed memory; it should thus be less\r\nsensitive to these constraints.\r\n4 Experimental Setup\r\nWe evaluated the memory reclamation strategies with re\u0002spect to the factors outlined in Section 3 using commodity\r\nSMP systems with IBM POWER CPUs. This section pro\u0002vides details on these aspects of our experiments.\r\n4.1 Data Structures Used\r\nWe tested the reclamation schemes on linked lists and\r\nqueues. We used Michael’s ordered lock-free linked\r\nlist [24], which forbids duplicate keys, and coded our\r\nconcurrently-readable lists similarly. Because linked lists\r\npermit arbitrary lengths and read-to-write ratios, we used",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/e812a17f-b27e-4eaa-b112-1bdaaa49f749.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=44d852f05ebe57d6473941df930ec84b150cca74fbfdad2eab69cd736e67a983",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 676
      },
      {
        "segments": [
          {
            "segment_id": "fc2e0cca-0ae6-40eb-bbec-15301962ba6d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "1 while (parent’s timer has not expired) {\r\n2 for i from 1 to 100 do {\r\n3 key = random key;\r\n4 op = random operation;\r\n5 d = data structure;\r\n6 op(d, key);\r\n7 }\r\n8 if (using QSBR)\r\n9 quiescent_state();\r\n10 }\r\nFigure 8. Per-thread test pseudocode.\r\nthem heavily in our experiments. Our lock-free queue fol\u0002lows Michael and Scott’s design [24]. Queues allow evalu\u0002ating QSBR on a write-only data structure, which no prior\r\nstudies have done.\r\n4.2 Test Program\r\nIn our tests, a parent thread creates N child threads,starts\r\na timer, and stops the threads upon timer expiry. Child\r\nthreads count the number of operations they perform, and\r\nthe parent then calculates the average execution time per\r\noperation by dividing the duration of the test by the total\r\nnumber of operations. The CPU time is the execution time\r\ndivided by the minimum of the number of threads and the\r\nnumber of processors. CPU time compensates for increas\u0002ing numbers of CPUs, allowing us to focus on synchroniza\u0002tion overhead. Our tests report the average of five trials.\r\nEach thread runs repeatedly through the test loop shown\r\nin Figure 8 until the timer expires. QSBR tests place a\r\nquiescent state at the end of the loop. The probabilities\r\nof inserting and removing nodes are equal, keeping data\u0002structure size roughly constant throughout a given run.\r\nWe vary the number of threads and nodes. For linked\r\nlists, we also vary the ratio of reads to updates. As shown\r\nin Figure 8, each thread performs 100 operations per qui\u0002escent state; hence, grace-period-related overhead is amor\u0002tized across 100 operations. For EBR, each op in Figure 8\r\nis a critical region; a thread attempts to update the global\r\nepoch whenever it has entered a critical region 100 times\r\nsince the last update, again amortizing grace-period-related\r\noverhead across 100 operations. For HPBR, we amortized\r\nreclamation overhead over R = 2H + 100 node removals.\r\nFor consistency, QSBR and EBR both used the fuzzy bar\u0002rier [11] algorithm from Fraser’s EBR implementation [6].\r\nThe code for our experiments is available at\r\nhttp://www.cs.toronto.edu/˜tomhart/\r\nperflab/ipdps06.tgz.\r\n4.3 Operating Environment\r\nWe performed our experiments on the two machines\r\nshown in Table 1. The last line of this table gives the com\u0002bined costs of locking and then unlocking a spinlock.\r\nTable 1. Characteristics of Machines\r\nXServe IBM POWER\r\nCPUs 2x 2.0GHz PowerPC G5 8x 1.45 GHz POWER4+\r\nKernel Linux 2.6.8-1.ydl.7g5-smp Linux 2.6.13 (kernel.org)\r\nFence 78ns (156 cycles) 76ns (110 cycles)\r\nCAS 52ns (104 cycles) 59ns (86 cycles)\r\nLock 231ns (462 cycles) 243ns (352 cycles)\r\nOur experiment implements threads using processes.\r\nOur memory allocator is similar to that of Bonwick [4].\r\nEach thread has two freelists of up to 100 elements each,\r\nand can acquire more memory from a global non-blocking\r\nstack of freelists. This non-blocking allocator allowed us to\r\nstudy reclamation performance without considering patho\u0002logical locking conditions discussed in Section 3.\r\nWe implemented CAS using POWER’s LL/SC instruc\u0002tions (larx and stcx), and fences using the eieio in\u0002struction. Our spinlocks were implemented using CAS and\r\nfences. Our algorithms used exponential backoff [1] upon\r\nencountering conflicts.\r\n4.4 Limitations of Experiment\r\nMicrobenchmarks are never perfect [17], however, they\r\nallow us to study reclamation performance by varying each\r\nof the factors outlined in Section 3 independently. Our re\u0002sults show that these factors significantly affect reclamation\r\nperformance. In macrobenchmark experiments, it is more\r\ndifficult to gain insight into the causes of performance dif\u0002ferences, and to test the schemes comprehensively.\r\nSome applications may not have natural quiescentstates;\r\nfurthermore, detecting quiescent states in other applications\r\nmay be more expensive than it is in our experiments. Our\r\nQSBR implementation, for example, is faster than that used\r\nin the Linux kernel, due to the latter’s need to support dy\u0002namic insertion and removal of CPUs, interrupt handlers,\r\nand real-time workloads.\r\nOur HPBR experiments statically allocate hazard point\u0002ers. Although this is sufficient for our experiments,some al\u0002gorithms, to the best of our knowledge, require unbounded\r\nnumbers of hazard pointers.\r\nDespite these limitations, we believe that our experi\u0002ments thoroughly evaluate these schemes, and show when\r\neach scheme is and is not efficient.\r\n5 Performance Analysis\r\nWe first investigate the base costs for the reclamation\r\nschemes: single-threaded execution on small data struc\u0002tures. We then show how workload, list traversal length,\r\nnumber of threads, and preemption affect performance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/fc2e0cca-0ae6-40eb-bbec-15301962ba6d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=799476a7e8f3eee18d7268cbcc3eb5436d682fd6ad47744faa1278ea1850e9e5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 710
      },
      {
        "segments": [
          {
            "segment_id": "fc2e0cca-0ae6-40eb-bbec-15301962ba6d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "1 while (parent’s timer has not expired) {\r\n2 for i from 1 to 100 do {\r\n3 key = random key;\r\n4 op = random operation;\r\n5 d = data structure;\r\n6 op(d, key);\r\n7 }\r\n8 if (using QSBR)\r\n9 quiescent_state();\r\n10 }\r\nFigure 8. Per-thread test pseudocode.\r\nthem heavily in our experiments. Our lock-free queue fol\u0002lows Michael and Scott’s design [24]. Queues allow evalu\u0002ating QSBR on a write-only data structure, which no prior\r\nstudies have done.\r\n4.2 Test Program\r\nIn our tests, a parent thread creates N child threads,starts\r\na timer, and stops the threads upon timer expiry. Child\r\nthreads count the number of operations they perform, and\r\nthe parent then calculates the average execution time per\r\noperation by dividing the duration of the test by the total\r\nnumber of operations. The CPU time is the execution time\r\ndivided by the minimum of the number of threads and the\r\nnumber of processors. CPU time compensates for increas\u0002ing numbers of CPUs, allowing us to focus on synchroniza\u0002tion overhead. Our tests report the average of five trials.\r\nEach thread runs repeatedly through the test loop shown\r\nin Figure 8 until the timer expires. QSBR tests place a\r\nquiescent state at the end of the loop. The probabilities\r\nof inserting and removing nodes are equal, keeping data\u0002structure size roughly constant throughout a given run.\r\nWe vary the number of threads and nodes. For linked\r\nlists, we also vary the ratio of reads to updates. As shown\r\nin Figure 8, each thread performs 100 operations per qui\u0002escent state; hence, grace-period-related overhead is amor\u0002tized across 100 operations. For EBR, each op in Figure 8\r\nis a critical region; a thread attempts to update the global\r\nepoch whenever it has entered a critical region 100 times\r\nsince the last update, again amortizing grace-period-related\r\noverhead across 100 operations. For HPBR, we amortized\r\nreclamation overhead over R = 2H + 100 node removals.\r\nFor consistency, QSBR and EBR both used the fuzzy bar\u0002rier [11] algorithm from Fraser’s EBR implementation [6].\r\nThe code for our experiments is available at\r\nhttp://www.cs.toronto.edu/˜tomhart/\r\nperflab/ipdps06.tgz.\r\n4.3 Operating Environment\r\nWe performed our experiments on the two machines\r\nshown in Table 1. The last line of this table gives the com\u0002bined costs of locking and then unlocking a spinlock.\r\nTable 1. Characteristics of Machines\r\nXServe IBM POWER\r\nCPUs 2x 2.0GHz PowerPC G5 8x 1.45 GHz POWER4+\r\nKernel Linux 2.6.8-1.ydl.7g5-smp Linux 2.6.13 (kernel.org)\r\nFence 78ns (156 cycles) 76ns (110 cycles)\r\nCAS 52ns (104 cycles) 59ns (86 cycles)\r\nLock 231ns (462 cycles) 243ns (352 cycles)\r\nOur experiment implements threads using processes.\r\nOur memory allocator is similar to that of Bonwick [4].\r\nEach thread has two freelists of up to 100 elements each,\r\nand can acquire more memory from a global non-blocking\r\nstack of freelists. This non-blocking allocator allowed us to\r\nstudy reclamation performance without considering patho\u0002logical locking conditions discussed in Section 3.\r\nWe implemented CAS using POWER’s LL/SC instruc\u0002tions (larx and stcx), and fences using the eieio in\u0002struction. Our spinlocks were implemented using CAS and\r\nfences. Our algorithms used exponential backoff [1] upon\r\nencountering conflicts.\r\n4.4 Limitations of Experiment\r\nMicrobenchmarks are never perfect [17], however, they\r\nallow us to study reclamation performance by varying each\r\nof the factors outlined in Section 3 independently. Our re\u0002sults show that these factors significantly affect reclamation\r\nperformance. In macrobenchmark experiments, it is more\r\ndifficult to gain insight into the causes of performance dif\u0002ferences, and to test the schemes comprehensively.\r\nSome applications may not have natural quiescentstates;\r\nfurthermore, detecting quiescent states in other applications\r\nmay be more expensive than it is in our experiments. Our\r\nQSBR implementation, for example, is faster than that used\r\nin the Linux kernel, due to the latter’s need to support dy\u0002namic insertion and removal of CPUs, interrupt handlers,\r\nand real-time workloads.\r\nOur HPBR experiments statically allocate hazard point\u0002ers. Although this is sufficient for our experiments,some al\u0002gorithms, to the best of our knowledge, require unbounded\r\nnumbers of hazard pointers.\r\nDespite these limitations, we believe that our experi\u0002ments thoroughly evaluate these schemes, and show when\r\neach scheme is and is not efficient.\r\n5 Performance Analysis\r\nWe first investigate the base costs for the reclamation\r\nschemes: single-threaded execution on small data struc\u0002tures. We then show how workload, list traversal length,\r\nnumber of threads, and preemption affect performance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/fc2e0cca-0ae6-40eb-bbec-15301962ba6d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=799476a7e8f3eee18d7268cbcc3eb5436d682fd6ad47744faa1278ea1850e9e5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 710
      },
      {
        "segments": [
          {
            "segment_id": "f9e6b506-92ae-4e28-a5c8-2a4cd9ee1107",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "QSBR\r\nHPBR\r\nEBR\r\nLFRC\r\nList Reads\r\nQSBR\r\nHPBR\r\nEBR\r\nList Writes\r\nQSBR\r\nHPBR\r\nEBR\r\nQueue\r\n0\r\n200\r\n400\r\n600\r\n800\r\n1000\r\nAvg Exec Time (ns)\r\nFigure 9. Base costs — single-threaded data\r\nfrom 8-CPU machine.\r\n5.1 Base Costs\r\nFigure 9 shows the single-threaded base costs of these\r\nschemes on non-blocking queues and single-element linked\r\nlists with no preemption or contention. We ran LFRC only\r\non read-only workloads; these were sufficient for us to cor\u0002roborate Michael’s [24] result that LFRC performs poorly.\r\nIn these base cases, the dominant influence on perfor\u0002mance is per-operation atomic instructions: compare-and\u0002swap (CAS), fetch-and-add (FAA), and fences make LFRC\r\nmuch more expensive than the other schemes. Since EBR\r\nrequires two fences per operation, and HPBR requires one\r\nfor most operations considered here, EBR is usually the\r\nnext most expensive. QSBR, needing no per-operation\r\natomic instructions, is the cheapest scheme in the base case.\r\nWorkload affects the performance of these schemes. Un\u0002der an update-intensive workload, a significant number of\r\noperations will involve removing nodes; for each attempt\r\nto reclaimed a removed node, HPBR must search the array\r\nof hazard pointers. This overhead can become significant\r\nfor update-intensive workloads, as can be seen in Figure 9.\r\nWe note that in our experiments, the performance of QSBR,\r\nEBR, and HPBR all increased linearly between read-only\r\nand update-only workloads.\r\n5.2 Scalability with Traversal Length\r\nFigure 10 shows the effect of list length on a single\u0002threaded read-only workload. We observed similar results\r\nin write-only workloads. As expected, per-element fence\r\ninstructions degrade HPBR’s performance on long chains\r\nof elements; QSBR and EBR do much better.\r\nFigure 11 shows the same scenario, but also includes\r\nLFRC. At best, LFRC takes more than twice as long as the\r\nnext slowest scheme, and the performance gap rapidly in\u0002creases with list length due to the multiple per-node atomic\r\ninstructions. Because LFRC is always the worst scheme in\r\nterms of performance, we do not consider it further.\r\n 0\r\n 500\r\n 1000\r\n 1500\r\n 2000\r\n 2500\r\n 3000\r\n 3500\r\n 4000\r\n 4500\r\n 5000\r\n 0 20 40 60 80 100\r\nAvg CPU Time (ns)\r\nNumber of Elements\r\nHPBR\r\nQSBR\r\nEBR\r\nFigure 10. Effect of traversal length — read\u0002only lock-free list, one thread, 8 CPUs.\r\n 0\r\n 2000\r\n 4000\r\n 6000\r\n 8000\r\n 10000\r\n 12000\r\n 14000\r\n 16000\r\n 18000\r\n 20000\r\n 0 20 40 60 80 100\r\nAvg CPU Time (ns)\r\nNumber of Elements\r\nHPBR\r\nQSBR\r\nEBR\r\nLFRC\r\nFigure 11. Effect of traversal length, including\r\nLFRC — read-only lock-free list, one thread,\r\n8 CPUs.\r\n5.3 Scalability with Threads\r\nConcurrent performance is an obvious concern for mem\u0002ory reclamation schemes. We study the effect of threads\r\nsharing the data structure when there is no CPU contention,\r\nand when threads must also compete for the CPU.\r\n5.3.1 No Preemption\r\nTo reduce the effects of CPU contention (thread preemp\u0002tion, migration, etc.), we use a maximum of seven threads,\r\nensuring that one CPU is available for other processes, fol\u0002lowing Fraser [6].\r\nFigures 12 and 13 show the performance of the reclama\u0002tion schemes with a read-only workload on a linked list, and\r\nwith a write-only workload on a queue. All three schemes\r\nscale almost linearly in the read-only case. In both cases,\r\nthe schemes’ relative performance seems to be unaffected\r\nby the number of threads.\r\n5.3.2 With Preemption\r\nTo evaluate the performance of the reclamation schemes un\u0002der preemption, we ran our tests on our 2-CPU machine,\r\nvarying the number of threads from 1 to 32.\r\nFigure 14 shows the performance of the schemes on a\r\none-element lock-free linked list with a read-only workload.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/f9e6b506-92ae-4e28-a5c8-2a4cd9ee1107.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4111f8e266d4850e16f21d792a667634b2855ddbc1d1feb962220a975ce5f70a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 577
      },
      {
        "segments": [
          {
            "segment_id": "f9e6b506-92ae-4e28-a5c8-2a4cd9ee1107",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "QSBR\r\nHPBR\r\nEBR\r\nLFRC\r\nList Reads\r\nQSBR\r\nHPBR\r\nEBR\r\nList Writes\r\nQSBR\r\nHPBR\r\nEBR\r\nQueue\r\n0\r\n200\r\n400\r\n600\r\n800\r\n1000\r\nAvg Exec Time (ns)\r\nFigure 9. Base costs — single-threaded data\r\nfrom 8-CPU machine.\r\n5.1 Base Costs\r\nFigure 9 shows the single-threaded base costs of these\r\nschemes on non-blocking queues and single-element linked\r\nlists with no preemption or contention. We ran LFRC only\r\non read-only workloads; these were sufficient for us to cor\u0002roborate Michael’s [24] result that LFRC performs poorly.\r\nIn these base cases, the dominant influence on perfor\u0002mance is per-operation atomic instructions: compare-and\u0002swap (CAS), fetch-and-add (FAA), and fences make LFRC\r\nmuch more expensive than the other schemes. Since EBR\r\nrequires two fences per operation, and HPBR requires one\r\nfor most operations considered here, EBR is usually the\r\nnext most expensive. QSBR, needing no per-operation\r\natomic instructions, is the cheapest scheme in the base case.\r\nWorkload affects the performance of these schemes. Un\u0002der an update-intensive workload, a significant number of\r\noperations will involve removing nodes; for each attempt\r\nto reclaimed a removed node, HPBR must search the array\r\nof hazard pointers. This overhead can become significant\r\nfor update-intensive workloads, as can be seen in Figure 9.\r\nWe note that in our experiments, the performance of QSBR,\r\nEBR, and HPBR all increased linearly between read-only\r\nand update-only workloads.\r\n5.2 Scalability with Traversal Length\r\nFigure 10 shows the effect of list length on a single\u0002threaded read-only workload. We observed similar results\r\nin write-only workloads. As expected, per-element fence\r\ninstructions degrade HPBR’s performance on long chains\r\nof elements; QSBR and EBR do much better.\r\nFigure 11 shows the same scenario, but also includes\r\nLFRC. At best, LFRC takes more than twice as long as the\r\nnext slowest scheme, and the performance gap rapidly in\u0002creases with list length due to the multiple per-node atomic\r\ninstructions. Because LFRC is always the worst scheme in\r\nterms of performance, we do not consider it further.\r\n 0\r\n 500\r\n 1000\r\n 1500\r\n 2000\r\n 2500\r\n 3000\r\n 3500\r\n 4000\r\n 4500\r\n 5000\r\n 0 20 40 60 80 100\r\nAvg CPU Time (ns)\r\nNumber of Elements\r\nHPBR\r\nQSBR\r\nEBR\r\nFigure 10. Effect of traversal length — read\u0002only lock-free list, one thread, 8 CPUs.\r\n 0\r\n 2000\r\n 4000\r\n 6000\r\n 8000\r\n 10000\r\n 12000\r\n 14000\r\n 16000\r\n 18000\r\n 20000\r\n 0 20 40 60 80 100\r\nAvg CPU Time (ns)\r\nNumber of Elements\r\nHPBR\r\nQSBR\r\nEBR\r\nLFRC\r\nFigure 11. Effect of traversal length, including\r\nLFRC — read-only lock-free list, one thread,\r\n8 CPUs.\r\n5.3 Scalability with Threads\r\nConcurrent performance is an obvious concern for mem\u0002ory reclamation schemes. We study the effect of threads\r\nsharing the data structure when there is no CPU contention,\r\nand when threads must also compete for the CPU.\r\n5.3.1 No Preemption\r\nTo reduce the effects of CPU contention (thread preemp\u0002tion, migration, etc.), we use a maximum of seven threads,\r\nensuring that one CPU is available for other processes, fol\u0002lowing Fraser [6].\r\nFigures 12 and 13 show the performance of the reclama\u0002tion schemes with a read-only workload on a linked list, and\r\nwith a write-only workload on a queue. All three schemes\r\nscale almost linearly in the read-only case. In both cases,\r\nthe schemes’ relative performance seems to be unaffected\r\nby the number of threads.\r\n5.3.2 With Preemption\r\nTo evaluate the performance of the reclamation schemes un\u0002der preemption, we ran our tests on our 2-CPU machine,\r\nvarying the number of threads from 1 to 32.\r\nFigure 14 shows the performance of the schemes on a\r\none-element lock-free linked list with a read-only workload.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/f9e6b506-92ae-4e28-a5c8-2a4cd9ee1107.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4111f8e266d4850e16f21d792a667634b2855ddbc1d1feb962220a975ce5f70a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 577
      },
      {
        "segments": [
          {
            "segment_id": "8f7c1c62-788c-444f-b638-494362ae606a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": " 0\r\n 50\r\n 100\r\n 150\r\n 200\r\n 250\r\n 300\r\n 350\r\n 400\r\n 450\r\n 1 2 3 4 5 6 7\r\nAvg CPU Time (ns)\r\nNumber of Threads\r\nHPBR\r\nQSBR\r\nEBR\r\nFigure 12. Effect of adding threads — read\u0002only lock-free list, one element, 8 CPUs.\r\n 0\r\n 500\r\n 1000\r\n 1500\r\n 2000\r\n 2500\r\n 3000\r\n 3500\r\n 4000\r\n 1 2 3 4 5 6 7\r\nAvg CPU Time (ns)\r\nNumber of Threads\r\nHPBR\r\nQSBR\r\nEBR\r\nFigure 13. Effect of adding threads — lock\u0002free queue, 8 CPUs.\r\nThis case eliminates reclamation overhead, focusing solely\r\non read-side and fuzzy barrier overhead. In this case, the\r\nalgorithms all scale well, with QSBR remaining cheapest.\r\nFor the write-heavy workloads shown in Figure 15, HPBR\r\nperforms best due to its non-blocking design.\r\nThe blocking schemes perform well on this write-heavy\r\nworkload only because threads yield the processor upon al\u0002location failure. Figure 16 shows the same test as Figure 15,\r\nbut with busy-waiting upon allocation failure. Here, HPBR\r\nperforms well, but EBR and QSBR quickly exhaust the pool\r\nof free memory. Each thread spins waiting for more mem\u0002ory to become free, thereby preventing grace periods from\r\ncompleting in a timely manner and hence delaying memory\r\n 0\r\n 50\r\n 100\r\n 150\r\n 200\r\n 250\r\n 300\r\n 5 10 15 20 25 30\r\nAvg Execution Time (ns)\r\nNumber of Threads\r\nHPBR\r\nQSBR\r\nEBR\r\nFigure 14. Effect of preemption — read-only\r\nlock-free list, 2 CPUs.\r\n 0\r\n 100\r\n 200\r\n 300\r\n 400\r\n 500\r\n 600\r\n 700\r\n 800\r\n 5 10 15 20 25 30\r\nAvg Execution Time (ns)\r\nNumber of Threads\r\nHPBR\r\nQSBR\r\nEBR\r\nFigure 15. Effect of preemption — lock-free\r\nqueue, 2 CPUs.\r\n 0\r\n 10000\r\n 20000\r\n 30000\r\n 40000\r\n 50000\r\n 60000\r\n 70000\r\n 80000\r\n 90000\r\n 0 5 10 15 20 25 30 35\r\nAvg Execution Time (ns)\r\nNumber of Threads\r\nHPBR\r\nQSBR\r\nEBR\r\nFigure 16. Effect of busy waiting when out of\r\nmemory — lock-free queue, 2 CPUs.\r\nreclamation.\r\nAlthough this busy waiting would be a poor design\r\nchoice in a real application, this test demonstrates that pre\u0002emption and write-heavy workloads can cause QSBR and\r\nEBR to exhaust all memory. Similarly, Sarma and McKen\u0002ney [27] have shown how QSBR-based components of the\r\nLinux kernel are vulnerable to denial-of-service attacks. Al\u0002though this can be avoided with engineering effort – and has\r\nbeen, in Linux – it is in these situations that HPBR’s fault\u0002tolerance becomes valuable.\r\n5.4 Summary\r\nWe note several trends of interest. First, in the base case,\r\natomic instructions such as fences are the dominant cost.\r\nSecond, when large numbers of elements must be traversed,\r\nHPBR and reference counting suffer from significant over\u0002head due to extra atomic instructions. QSBR and EBR per\u0002form poorly when grace periods are stalled and the work\u0002load is update-intensive.\r\n6 Consequences of Analysis\r\nWe describe the consequences of our analysis for com\u0002paring algorithms, designing new reclamation schemes, and\r\nchoosing reclamation schemes for applications.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/8f7c1c62-788c-444f-b638-494362ae606a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a95535449607f9b76c4da83147c7b7985a9187e222380ee2b9e91f5e089e403f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 462
      },
      {
        "segments": [
          {
            "segment_id": "118264ab-5bcf-462e-8544-e33e054706c7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": " 0\r\n 200\r\n 400\r\n 600\r\n 800\r\n 1000\r\n 0 0.2 0.4 0.6 0.8 1\r\nAvg CPU Time (ns)\r\nUpdate Fraction\r\nLF-HPBR\r\nLF-QSBR\r\nCR-QSBR\r\nFigure 17. Lock-free versus concurrently\u0002readable algorithms — ten-element lists, one\r\nthread, 8 CPUs.\r\n 0\r\n 200\r\n 400\r\n 600\r\n 800\r\n 1000\r\n 0 0.2 0.4 0.6 0.8 1\r\nAvg CPU Time (ns)\r\nUpdate Fraction\r\nLF-QSBR\r\nCR-QSBR\r\nFigure 18. Lock-free versus concurrently\u0002readable algorithms — hash tables with load\r\nfactor 1, four threads, 8 CPUs.\r\n6.1 Fair Evaluation of Algorithms\r\nReclamation schemes have profound performance ef\u0002fects that must be accounted for when experimentally eval\u0002uating new lockless algorithms.\r\nFor example, one of our early experiments compared a\r\nconcurrently-readable linked list with QSBR, as is used in\r\nthe Linux kernel, with a lock-free HPBR equivalent. Our\r\nintuition was that lock-free algorithms might pay a perfor\u0002mance penalty for their fault-tolerance properties, as they\r\ndo in read-mostly situations. The LF-HPBR and CR-QSBR\r\ntraces in Figure 17 might lead to the erroneous conclusion\r\nthat the concurrently-readable algorithm is always faster. A\r\nbetter analysis takes the LF-QSBR trace into account, not\u0002ing that as the update fraction increases, lock-free perfor\u0002mance improves, since its updates require fewer atomic in\u0002structions than does locking. This example shows that one\r\ncan accurately compare two lockless algorithms only when\r\neach is using the same reclamation scheme.\r\nLF-QSBR’s higher per-node overhead makes it more\r\nattractive when there are fewer nodes. Figure 18 shows\r\nthe performance of hash tables consisting of arrays of LF\u0002QSBR or CR-QSBR single-element lists being concurrently\r\naccessed by four threads. For clarity, we omit HPBR from\r\nthis graph — our intent is to compare the lock-free and\r\n 0\r\n 50\r\n 100\r\n 150\r\n 200\r\n 250\r\n 300\r\n 350\r\n 400\r\n 1 2 3 4 5 6 7\r\nAvg CPU Time (ns)\r\nNumber of Threads\r\nHPBR\r\nQSBR\r\nEBR\r\nNEBR\r\nFigure 19. Performance of NEBR — lock-free\r\nlist, 8 CPUs, read-only workload, variable\r\nnumber of threads.\r\nconcurrently-readable algorithms using a common reclama\u0002tion scheme. Here, the lock-free algorithm out-performs\r\nlocking for update fractions above about 15%. Lock-free\r\nlists and hash might therefore be practical for update-heavy\r\nsituations in environments providing QSBR, such as OS\r\nkernels like Linux.\r\nNew reclamation schemes should also be evaluated by\r\nvarying each of the factorsthat can affect their performance.\r\nFor example, Gidenstam et al. [8] recently proposed a new\r\nnon-blocking reclamation scheme that combines reference\r\ncounting with HPBR, and can be proven to have several\r\nattractive properties. However, like HPBR and reference\r\ncounting, it requires expensive per-node atomic operations.\r\nThe evaluation of thisscheme consisted only of experiments\r\non double-ended queues, thus failing to evaluate scalability\r\nwith data-structure size, an HPBR weakness. This failing\r\nshows the value of our analysis: it is necessary to vary the\r\nexperimental parameters we considered to gain a full under\u0002standing of a given scheme’s performance.\r\n6.2 Improving Reclamation Performance\r\nImproved reclamation schemes can be designed based\r\non an understanding of the factors that affect performance.\r\nFor example, we observe that a key difference between\r\nQSBR and EBR is the per-operation overhead of EBR’s\r\ntwo fences. This observation allows us to make a modest\r\nimprovement to EBR called new epoch-based-reclamation\r\n(NEBR).\r\nNEBR requires compromising EBR’s application\u0002independence. Instead of setting and clearing the flag at\r\nthe start and end of every lockless operation, we set it at the\r\napplication level before entering any code that might con\u0002tain NEBR critical regions. Since our flag is set and cleared\r\nat the application level, we can amortize the overhead of\r\nthe corresponding fence instructions over a larger number\r\nof operations. We reran the experiment shown in Figure 12,\r\nbut including NEBR, and found that NEBR scaled linearly\r\nand performed slightly better than did HPBR. Furthermore,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/118264ab-5bcf-462e-8544-e33e054706c7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ee1c4b2974ff132f2467263d3052b1b5ee26aae0fd8d7444bd72429ed9dcda68",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 598
      },
      {
        "segments": [
          {
            "segment_id": "118264ab-5bcf-462e-8544-e33e054706c7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": " 0\r\n 200\r\n 400\r\n 600\r\n 800\r\n 1000\r\n 0 0.2 0.4 0.6 0.8 1\r\nAvg CPU Time (ns)\r\nUpdate Fraction\r\nLF-HPBR\r\nLF-QSBR\r\nCR-QSBR\r\nFigure 17. Lock-free versus concurrently\u0002readable algorithms — ten-element lists, one\r\nthread, 8 CPUs.\r\n 0\r\n 200\r\n 400\r\n 600\r\n 800\r\n 1000\r\n 0 0.2 0.4 0.6 0.8 1\r\nAvg CPU Time (ns)\r\nUpdate Fraction\r\nLF-QSBR\r\nCR-QSBR\r\nFigure 18. Lock-free versus concurrently\u0002readable algorithms — hash tables with load\r\nfactor 1, four threads, 8 CPUs.\r\n6.1 Fair Evaluation of Algorithms\r\nReclamation schemes have profound performance ef\u0002fects that must be accounted for when experimentally eval\u0002uating new lockless algorithms.\r\nFor example, one of our early experiments compared a\r\nconcurrently-readable linked list with QSBR, as is used in\r\nthe Linux kernel, with a lock-free HPBR equivalent. Our\r\nintuition was that lock-free algorithms might pay a perfor\u0002mance penalty for their fault-tolerance properties, as they\r\ndo in read-mostly situations. The LF-HPBR and CR-QSBR\r\ntraces in Figure 17 might lead to the erroneous conclusion\r\nthat the concurrently-readable algorithm is always faster. A\r\nbetter analysis takes the LF-QSBR trace into account, not\u0002ing that as the update fraction increases, lock-free perfor\u0002mance improves, since its updates require fewer atomic in\u0002structions than does locking. This example shows that one\r\ncan accurately compare two lockless algorithms only when\r\neach is using the same reclamation scheme.\r\nLF-QSBR’s higher per-node overhead makes it more\r\nattractive when there are fewer nodes. Figure 18 shows\r\nthe performance of hash tables consisting of arrays of LF\u0002QSBR or CR-QSBR single-element lists being concurrently\r\naccessed by four threads. For clarity, we omit HPBR from\r\nthis graph — our intent is to compare the lock-free and\r\n 0\r\n 50\r\n 100\r\n 150\r\n 200\r\n 250\r\n 300\r\n 350\r\n 400\r\n 1 2 3 4 5 6 7\r\nAvg CPU Time (ns)\r\nNumber of Threads\r\nHPBR\r\nQSBR\r\nEBR\r\nNEBR\r\nFigure 19. Performance of NEBR — lock-free\r\nlist, 8 CPUs, read-only workload, variable\r\nnumber of threads.\r\nconcurrently-readable algorithms using a common reclama\u0002tion scheme. Here, the lock-free algorithm out-performs\r\nlocking for update fractions above about 15%. Lock-free\r\nlists and hash might therefore be practical for update-heavy\r\nsituations in environments providing QSBR, such as OS\r\nkernels like Linux.\r\nNew reclamation schemes should also be evaluated by\r\nvarying each of the factorsthat can affect their performance.\r\nFor example, Gidenstam et al. [8] recently proposed a new\r\nnon-blocking reclamation scheme that combines reference\r\ncounting with HPBR, and can be proven to have several\r\nattractive properties. However, like HPBR and reference\r\ncounting, it requires expensive per-node atomic operations.\r\nThe evaluation of thisscheme consisted only of experiments\r\non double-ended queues, thus failing to evaluate scalability\r\nwith data-structure size, an HPBR weakness. This failing\r\nshows the value of our analysis: it is necessary to vary the\r\nexperimental parameters we considered to gain a full under\u0002standing of a given scheme’s performance.\r\n6.2 Improving Reclamation Performance\r\nImproved reclamation schemes can be designed based\r\non an understanding of the factors that affect performance.\r\nFor example, we observe that a key difference between\r\nQSBR and EBR is the per-operation overhead of EBR’s\r\ntwo fences. This observation allows us to make a modest\r\nimprovement to EBR called new epoch-based-reclamation\r\n(NEBR).\r\nNEBR requires compromising EBR’s application\u0002independence. Instead of setting and clearing the flag at\r\nthe start and end of every lockless operation, we set it at the\r\napplication level before entering any code that might con\u0002tain NEBR critical regions. Since our flag is set and cleared\r\nat the application level, we can amortize the overhead of\r\nthe corresponding fence instructions over a larger number\r\nof operations. We reran the experiment shown in Figure 12,\r\nbut including NEBR, and found that NEBR scaled linearly\r\nand performed slightly better than did HPBR. Furthermore,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/118264ab-5bcf-462e-8544-e33e054706c7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ee1c4b2974ff132f2467263d3052b1b5ee26aae0fd8d7444bd72429ed9dcda68",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 598
      },
      {
        "segments": [
          {
            "segment_id": "9ae1a9b4-fa42-4901-859c-884b2c2a0b8e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "NEBR does not need the expensive per-node atomic opera\u0002tions that ruin HPBR’s performance for long traversals.\r\nNEBR is attractive because it is almost as fast as QSBR,\r\nbut does not require quiescent states. Interestingly, recent\r\nrealtime variants of the Linux-kernel RCU also dispense\r\nwith quiescent states [21]. Ongoing work is expected to\r\nsubstantially reduce realtime RCU read-side overhead.\r\nIt is interesting to consider what sort of weakened non\u0002blocking property, if any, could be defined such that one\r\ncould create a corresponding reclamation scheme without\r\nrequiring expensive per-node atomic operations.\r\n6.3 Blocking Memory Reclamation for\r\nNon-Blocking Data Structures\r\nWe have shown that non-blocking data structures often\r\nperform better when using blocking reclamation schemes.\r\nOne might question why one would want to use a non\u0002blocking data structure in this case, since a halted would\r\ncause an infinite memory leak, thus destroying the non\u0002blocking data structure’s fault-tolerance guarantees.\r\nHowever, non-blocking data structures are often used for\r\nreasons other than fault-tolerance; for example, Qprof [3]\r\nand Cache Kernel [10] both use such structures because\r\nthey can be accessed from signal handlers without risk of\r\nself-deadlock. Blocking memory reclamation does not re\u0002move this benefit. In fact, Cache Kernel uses a blocking im\u0002plementation of Type-Stable Memory to guard against read\u0002reclamation races; its implementation [9] has similarities\r\nto QSBR. Non-blocking algorithms with blocking reclama\u0002tion schemes similarly continue to benefit from resistance\r\nto preemption-induced convoying and priority inversion.\r\nWe view combining a non-blocking algorithm with a\r\nblocking reclamation scheme as part of a trend towards\r\nweakened non-blocking properties [16, 3], designed to pre\u0002serve selected advantages of non-blocking synchronization\r\nwhile improving performance. In this case, threads have all\r\nthe advantages of non-blocking synchronization, unless the\r\nsystem runs out of memory.\r\nConversely, one may also use non-blocking reclamation\r\nwith blocking algorithms to reduce the amount of memory\r\nawaiting reclamation in face of preempted or failed threads.\r\n7 Related Work\r\nRelevant work on reclamation scheme design was dis\u0002cussed in Section 2. Previous work on the performance of\r\nthese schemes, however, is limited.\r\nMichael [24] criticized QSBR for its unbounded mem\u0002ory use, but did not compare the performance of QSBR to\r\nthat of HPBR, or determine when this limitation affects a\r\nprogram.\r\nAuslander implemented a lock-free hash table with\r\nQSBR in K42 [19]. No performance evaluation, ei\u0002ther between different reclamation methods or between\r\nconcurrently-readable and lock-free hash tables, was pro\u0002vided. We are unaware of any work combining QSBR with\r\nupdate-intensive non-blocking algorithms such as queues.\r\nFraser [6] noted, but did not thoroughly evaluate,\r\nHPBR’s fence overhead, and used his EBR instead. Our\r\nwork extends Fraser’s, showing that EBR itself has high\r\noverhead, often exceeding that of HPBR.\r\n8 Conclusions\r\nWe have performed the first fair comparison of blocking\r\nand non-blocking reclamation across a range of workloads,\r\nshowing that reclamation has a huge effect on lockless al\u0002gorithm performance. Choosing the right scheme for the\r\nenvironment in which a concurrent algorithm is expected to\r\nrun is essential to having the algorithm perform well.\r\nOur results show that QSBR is usually the best\u0002performing reclamation scheme; however, the performance\r\nof both QSBR and EBR can suffer due to memory exhaus\u0002tion in the face of thread preemption or failure. HPBR and\r\nEBR have higher base costs than QSBR due to their re\u0002quired fences. For EBR, the worst-case overhead of fences\r\nis constant, while for HPBR it is unbounded. HPBR and\r\nLFRC scale poorly when many nodes must be traversed.\r\nOur analysis helped us to identify the main source of\r\noverhead in EBR and decrease it, resulting in NEBR. Fur\u0002thermore, understanding the impact of reclamation schemes\r\non algorithm performance enablesfair comparison of differ\u0002ent algorithms — in our case, lock-free and concurrently\u0002readable lists and hash tables.\r\nWe reiterate that blocking reclamation can be useful with\r\nnon-blocking algorithms: in the absence of thread fail\u0002ure, non-blocking algorithms still benefit from deadlock\u0002freedom, signal handler safety, and avoidance of priority\r\ninversion. Nevertheless, the question of what sort of weak\u0002ened non-blocking property could be satisfied by a reclama\u0002tion scheme without the per-node overhead of current non\u0002blocking reclamation scheme designs remains open.\r\n9 Acknowledgments\r\nWe owe thanks to Maged Michael and Keir Fraser for\r\nhelpful comments on their respective work, to Faith Fich\r\nand Cristiana Amza for much helpful advice, and to Dan\r\nFrye for his support of this effort. We are indebted to Martin\r\nBligh, Andy Whitcroft, and the ABAT team for the use of\r\nthe 8-CPU machine used in our experiments.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/9ae1a9b4-fa42-4901-859c-884b2c2a0b8e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a41caf5b6ef2f2e1e42f1e6a7f9701e5130375cb68ab387d0478b5377373b857",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 728
      },
      {
        "segments": [
          {
            "segment_id": "9ae1a9b4-fa42-4901-859c-884b2c2a0b8e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "NEBR does not need the expensive per-node atomic opera\u0002tions that ruin HPBR’s performance for long traversals.\r\nNEBR is attractive because it is almost as fast as QSBR,\r\nbut does not require quiescent states. Interestingly, recent\r\nrealtime variants of the Linux-kernel RCU also dispense\r\nwith quiescent states [21]. Ongoing work is expected to\r\nsubstantially reduce realtime RCU read-side overhead.\r\nIt is interesting to consider what sort of weakened non\u0002blocking property, if any, could be defined such that one\r\ncould create a corresponding reclamation scheme without\r\nrequiring expensive per-node atomic operations.\r\n6.3 Blocking Memory Reclamation for\r\nNon-Blocking Data Structures\r\nWe have shown that non-blocking data structures often\r\nperform better when using blocking reclamation schemes.\r\nOne might question why one would want to use a non\u0002blocking data structure in this case, since a halted would\r\ncause an infinite memory leak, thus destroying the non\u0002blocking data structure’s fault-tolerance guarantees.\r\nHowever, non-blocking data structures are often used for\r\nreasons other than fault-tolerance; for example, Qprof [3]\r\nand Cache Kernel [10] both use such structures because\r\nthey can be accessed from signal handlers without risk of\r\nself-deadlock. Blocking memory reclamation does not re\u0002move this benefit. In fact, Cache Kernel uses a blocking im\u0002plementation of Type-Stable Memory to guard against read\u0002reclamation races; its implementation [9] has similarities\r\nto QSBR. Non-blocking algorithms with blocking reclama\u0002tion schemes similarly continue to benefit from resistance\r\nto preemption-induced convoying and priority inversion.\r\nWe view combining a non-blocking algorithm with a\r\nblocking reclamation scheme as part of a trend towards\r\nweakened non-blocking properties [16, 3], designed to pre\u0002serve selected advantages of non-blocking synchronization\r\nwhile improving performance. In this case, threads have all\r\nthe advantages of non-blocking synchronization, unless the\r\nsystem runs out of memory.\r\nConversely, one may also use non-blocking reclamation\r\nwith blocking algorithms to reduce the amount of memory\r\nawaiting reclamation in face of preempted or failed threads.\r\n7 Related Work\r\nRelevant work on reclamation scheme design was dis\u0002cussed in Section 2. Previous work on the performance of\r\nthese schemes, however, is limited.\r\nMichael [24] criticized QSBR for its unbounded mem\u0002ory use, but did not compare the performance of QSBR to\r\nthat of HPBR, or determine when this limitation affects a\r\nprogram.\r\nAuslander implemented a lock-free hash table with\r\nQSBR in K42 [19]. No performance evaluation, ei\u0002ther between different reclamation methods or between\r\nconcurrently-readable and lock-free hash tables, was pro\u0002vided. We are unaware of any work combining QSBR with\r\nupdate-intensive non-blocking algorithms such as queues.\r\nFraser [6] noted, but did not thoroughly evaluate,\r\nHPBR’s fence overhead, and used his EBR instead. Our\r\nwork extends Fraser’s, showing that EBR itself has high\r\noverhead, often exceeding that of HPBR.\r\n8 Conclusions\r\nWe have performed the first fair comparison of blocking\r\nand non-blocking reclamation across a range of workloads,\r\nshowing that reclamation has a huge effect on lockless al\u0002gorithm performance. Choosing the right scheme for the\r\nenvironment in which a concurrent algorithm is expected to\r\nrun is essential to having the algorithm perform well.\r\nOur results show that QSBR is usually the best\u0002performing reclamation scheme; however, the performance\r\nof both QSBR and EBR can suffer due to memory exhaus\u0002tion in the face of thread preemption or failure. HPBR and\r\nEBR have higher base costs than QSBR due to their re\u0002quired fences. For EBR, the worst-case overhead of fences\r\nis constant, while for HPBR it is unbounded. HPBR and\r\nLFRC scale poorly when many nodes must be traversed.\r\nOur analysis helped us to identify the main source of\r\noverhead in EBR and decrease it, resulting in NEBR. Fur\u0002thermore, understanding the impact of reclamation schemes\r\non algorithm performance enablesfair comparison of differ\u0002ent algorithms — in our case, lock-free and concurrently\u0002readable lists and hash tables.\r\nWe reiterate that blocking reclamation can be useful with\r\nnon-blocking algorithms: in the absence of thread fail\u0002ure, non-blocking algorithms still benefit from deadlock\u0002freedom, signal handler safety, and avoidance of priority\r\ninversion. Nevertheless, the question of what sort of weak\u0002ened non-blocking property could be satisfied by a reclama\u0002tion scheme without the per-node overhead of current non\u0002blocking reclamation scheme designs remains open.\r\n9 Acknowledgments\r\nWe owe thanks to Maged Michael and Keir Fraser for\r\nhelpful comments on their respective work, to Faith Fich\r\nand Cristiana Amza for much helpful advice, and to Dan\r\nFrye for his support of this effort. We are indebted to Martin\r\nBligh, Andy Whitcroft, and the ABAT team for the use of\r\nthe 8-CPU machine used in our experiments.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/9ae1a9b4-fa42-4901-859c-884b2c2a0b8e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a41caf5b6ef2f2e1e42f1e6a7f9701e5130375cb68ab387d0478b5377373b857",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 728
      },
      {
        "segments": [
          {
            "segment_id": "ec035c04-ec0d-44a8-9e88-890b94ac4ccb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "10 Legal Statement\r\nIBM and POWER are trademarks or registered trade\u0002marks of International Business Machines Corporation in\r\nthe United States and/or other countries.\r\nJava and all Java-based trademarks are trademarks of\r\nSun Microsystems, Inc. in the United States, other coun\u0002tries, or both.\r\nLinux is a trademark of Linus Torvalds in the United\r\nStates, other countries, or both.\r\nOther company, product, and service names may be\r\ntrademarks or service marks of others.\r\nReferences\r\n[1] T. E. Anderson. The performance of spin lock alternatives\r\nfor shared-money multiprocessors. IEEE Trans. Parallel and\r\nDistributed Syst., 1(1):6–16, 1990.\r\n[2] A. Arcangeli, M. Cao, P. E. McKenney, and D. Sarma. Us\u0002ing read-copy update techniques for System V IPC in the\r\nLinux 2.5 kernel. In Proc. USENIX Annual Technical Conf.\r\n(FREENIX Track), pages 297–310. USENIX Association,\r\nJune 2003.\r\n[3] H.-J. Boehm. An almost non-blocking stack. In Proc. 23rd\r\nACM Symp. on Principles of Distributed Computing, pages\r\n40–49, 2004.\r\n[4] J. Bonwick and J. Adams. Magazines and Vmem: Extend\u0002ing the slab allocator to many CPUs and arbitrary resources.\r\nIn USENIX Technical Conf., General Track, pages 15–33,\r\n2001.\r\n[5] D. L. Detlefs, P. A. Martin, M. Moir, and G. L. Steele,\r\nJr. Lock-free reference counting. Distributed Computing,\r\n15(4):255–271, 2002.\r\n[6] K. Fraser. Practical Lock-Freedom. PhD thesis, University\r\nof Cambridge Computer Laboratory, 2004.\r\n[7] B. Gamsa, O. Krieger, J. Appavoo, and M. Stumm. Tornado:\r\nMaximizing locality and concurrency in a shared memory\r\nmultiprocessor operating system. In Proc. 3rd Symp. on Op\u0002erating Syst. Design and Impl., pages 87–100, 1999.\r\n[8] A. Gidenstam, M. Papatriantafilou, H. Sundell, and P. Tsi\u0002gas. Efficient and reliable lock-free memory reclamation\r\nbased on reference counting. In Proc. 8th I-SPAN, 2005.\r\n[9] M. Greenwald. Non-blocking Synchronization and System\r\nDesign. PhD thesis, Stanford University, 1999.\r\n[10] M. Greenwald and D. Cheriton. The synergy between non\u0002blocking synchronization and operating system structure. In\r\nProc. 2nd USENIX Symp. on Operating Syst. Design and\r\nImpl., pages 123–136. ACM Press, 1996.\r\n[11] R. Gupta. The fuzzy barrier: a mechanism for high speed\r\nsynchronization of processors. In Proc. 3rd Intl. Conf. on\r\nArch. Support for Prog. Lang. and Operating Syst. (ASP\u0002LOS), pages 54–63, New York, NY, USA, 1989. ACM Press.\r\n[12] T. L. Harris. A pragmatic implementation of non-blocking\r\nlinked-lists. In Proc. 15th Intl. Conf. on Distributed Com\u0002puting, pages 300–314. Springer-Verlag, 2001.\r\n[13] M. Herlihy. Wait-free synchronization. ACM Trans. Prog.\r\nLang. Syst., 13(1):124–149, 1991.\r\n[14] M. Herlihy. A methodology for implementing highly\r\nconcurrent data objects. ACM Trans. Prog. Lang. Syst.,\r\n15(5):745–770, Nov. 1993.\r\n[15] M. Herlihy, V. Luchangco, and M. Moir. The repeat offender\r\nproblem: A mechanism for supporting dynamic-sized, lock\u0002free data structures. In Proc. 16th Intl. Symp. on Distributed\r\nComputing, Oct. 2002.\r\n[16] M. Herlihy, V. Luchangco, and M. Moir. Obstruction-free\r\nsynchronization: Double-ended queues as an example. In\r\nProc. 23rd Intl. Conf. on Distributed Computing Syst., pages\r\n522–529. IEEE Computer Society, 2003.\r\n[17] S. Kumar, D. Jiang, R. Chandra, and J. P. Singh. Evalu\u0002ating synchronization on shared address space multiproces\u0002sors: methodology and performance. In Proc. SIGMETRICS\r\n1999, pages 23–34, New York, NY, USA, 1999. ACM Press.\r\n[18] L. Lamport. How to make a multiprocessor computer that\r\ncorrectly executes multiprocess programs. IEEE Trans.\r\nComput., 28(9):690–691, Sept. 1979.\r\n[19] P. E. McKenney. Exploiting Deferred Destruction: An Anal\u0002ysis of Read-Copy-Update Techniques in Operating System\r\nKernels. PhD thesis, OGI School of Science and Engineer\u0002ing at Oregon Health and Sciences University, 2004.\r\n[20] P. E. McKenney, J. Appavoo, A. Kleen, O. Krieger, R. Rus\u0002sell, D. Sarma, and M. Soni. Read-copy update. In Ottawa\r\nLinux Symp., July 2001.\r\n[21] P. E. McKenney and D. Sarma. Towards hard realtime\r\nresponse from the Linux kernel on SMP hardware. In\r\nlinux.conf.au, Canberra, AU, April 2005.\r\n[22] P. E. McKenney and J. D. Slingwine. Read-copy update:\r\nUsing execution history to solve concurrency problems. In\r\nParallel and Distributed Computing and Syst., pages 509–\r\n518, Las Vegas, NV, Oct. 1998.\r\n[23] M. M. Michael. Safe memory reclamation for dynamic lock\u0002free objects using atomic reads and writes. In Proc. 21st\r\nACM Symp. on Principles of Distributed Computing, pages\r\n21–30, July 2002.\r\n[24] M. M. Michael. Hazard pointers: Safe memory reclama\u0002tion for lock-free objects. IEEE Trans. on Parallel and Dis\u0002tributed Syst., 15(6):491–504, June 2004.\r\n[25] M. M. Michael. Scalable lock-free dynamic memory allo\u0002cation. In Proc. ACM Conf. on Programming Language De\u0002sign and Implementation, pages 35–46, June 2004.\r\n[26] M. M. Michael and M. L. Scott. Correction of a memory\r\nmanagement method for lock-free data structures. Technical\r\nReport TR599, Computer Science Department, University\r\nof Rochester, Dec. 1995.\r\n[27] D. Sarma and P. E. McKenney. Issues with selected scala\u0002bility features of the 2.6 kernel. In Ottawa Linux Symp., July\r\n2004.\r\n[28] H. Sundell. Wait-free reference counting and memory man\u0002agement. In Proc. 19th Intl. Parallel and Distributed Pro\u0002cessing Symp., Apr. 2005.\r\n[29] J. D. Valois. Lock-free linked lists using compare-and-swap.\r\nIn Proc. 14th ACM Symp. on Principles of Distributed Com\u0002puting, pages 214–222, Aug. 1995.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/ec035c04-ec0d-44a8-9e88-890b94ac4ccb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=62bbfec5e1b169cf531944dad4340a02693cc5dbd5006c9f0f7beabbabf76fd7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 803
      },
      {
        "segments": [
          {
            "segment_id": "ec035c04-ec0d-44a8-9e88-890b94ac4ccb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "10 Legal Statement\r\nIBM and POWER are trademarks or registered trade\u0002marks of International Business Machines Corporation in\r\nthe United States and/or other countries.\r\nJava and all Java-based trademarks are trademarks of\r\nSun Microsystems, Inc. in the United States, other coun\u0002tries, or both.\r\nLinux is a trademark of Linus Torvalds in the United\r\nStates, other countries, or both.\r\nOther company, product, and service names may be\r\ntrademarks or service marks of others.\r\nReferences\r\n[1] T. E. Anderson. The performance of spin lock alternatives\r\nfor shared-money multiprocessors. IEEE Trans. Parallel and\r\nDistributed Syst., 1(1):6–16, 1990.\r\n[2] A. Arcangeli, M. Cao, P. E. McKenney, and D. Sarma. Us\u0002ing read-copy update techniques for System V IPC in the\r\nLinux 2.5 kernel. In Proc. USENIX Annual Technical Conf.\r\n(FREENIX Track), pages 297–310. USENIX Association,\r\nJune 2003.\r\n[3] H.-J. Boehm. An almost non-blocking stack. In Proc. 23rd\r\nACM Symp. on Principles of Distributed Computing, pages\r\n40–49, 2004.\r\n[4] J. Bonwick and J. Adams. Magazines and Vmem: Extend\u0002ing the slab allocator to many CPUs and arbitrary resources.\r\nIn USENIX Technical Conf., General Track, pages 15–33,\r\n2001.\r\n[5] D. L. Detlefs, P. A. Martin, M. Moir, and G. L. Steele,\r\nJr. Lock-free reference counting. Distributed Computing,\r\n15(4):255–271, 2002.\r\n[6] K. Fraser. Practical Lock-Freedom. PhD thesis, University\r\nof Cambridge Computer Laboratory, 2004.\r\n[7] B. Gamsa, O. Krieger, J. Appavoo, and M. Stumm. Tornado:\r\nMaximizing locality and concurrency in a shared memory\r\nmultiprocessor operating system. In Proc. 3rd Symp. on Op\u0002erating Syst. Design and Impl., pages 87–100, 1999.\r\n[8] A. Gidenstam, M. Papatriantafilou, H. Sundell, and P. Tsi\u0002gas. Efficient and reliable lock-free memory reclamation\r\nbased on reference counting. In Proc. 8th I-SPAN, 2005.\r\n[9] M. Greenwald. Non-blocking Synchronization and System\r\nDesign. PhD thesis, Stanford University, 1999.\r\n[10] M. Greenwald and D. Cheriton. The synergy between non\u0002blocking synchronization and operating system structure. In\r\nProc. 2nd USENIX Symp. on Operating Syst. Design and\r\nImpl., pages 123–136. ACM Press, 1996.\r\n[11] R. Gupta. The fuzzy barrier: a mechanism for high speed\r\nsynchronization of processors. In Proc. 3rd Intl. Conf. on\r\nArch. Support for Prog. Lang. and Operating Syst. (ASP\u0002LOS), pages 54–63, New York, NY, USA, 1989. ACM Press.\r\n[12] T. L. Harris. A pragmatic implementation of non-blocking\r\nlinked-lists. In Proc. 15th Intl. Conf. on Distributed Com\u0002puting, pages 300–314. Springer-Verlag, 2001.\r\n[13] M. Herlihy. Wait-free synchronization. ACM Trans. Prog.\r\nLang. Syst., 13(1):124–149, 1991.\r\n[14] M. Herlihy. A methodology for implementing highly\r\nconcurrent data objects. ACM Trans. Prog. Lang. Syst.,\r\n15(5):745–770, Nov. 1993.\r\n[15] M. Herlihy, V. Luchangco, and M. Moir. The repeat offender\r\nproblem: A mechanism for supporting dynamic-sized, lock\u0002free data structures. In Proc. 16th Intl. Symp. on Distributed\r\nComputing, Oct. 2002.\r\n[16] M. Herlihy, V. Luchangco, and M. Moir. Obstruction-free\r\nsynchronization: Double-ended queues as an example. In\r\nProc. 23rd Intl. Conf. on Distributed Computing Syst., pages\r\n522–529. IEEE Computer Society, 2003.\r\n[17] S. Kumar, D. Jiang, R. Chandra, and J. P. Singh. Evalu\u0002ating synchronization on shared address space multiproces\u0002sors: methodology and performance. In Proc. SIGMETRICS\r\n1999, pages 23–34, New York, NY, USA, 1999. ACM Press.\r\n[18] L. Lamport. How to make a multiprocessor computer that\r\ncorrectly executes multiprocess programs. IEEE Trans.\r\nComput., 28(9):690–691, Sept. 1979.\r\n[19] P. E. McKenney. Exploiting Deferred Destruction: An Anal\u0002ysis of Read-Copy-Update Techniques in Operating System\r\nKernels. PhD thesis, OGI School of Science and Engineer\u0002ing at Oregon Health and Sciences University, 2004.\r\n[20] P. E. McKenney, J. Appavoo, A. Kleen, O. Krieger, R. Rus\u0002sell, D. Sarma, and M. Soni. Read-copy update. In Ottawa\r\nLinux Symp., July 2001.\r\n[21] P. E. McKenney and D. Sarma. Towards hard realtime\r\nresponse from the Linux kernel on SMP hardware. In\r\nlinux.conf.au, Canberra, AU, April 2005.\r\n[22] P. E. McKenney and J. D. Slingwine. Read-copy update:\r\nUsing execution history to solve concurrency problems. In\r\nParallel and Distributed Computing and Syst., pages 509–\r\n518, Las Vegas, NV, Oct. 1998.\r\n[23] M. M. Michael. Safe memory reclamation for dynamic lock\u0002free objects using atomic reads and writes. In Proc. 21st\r\nACM Symp. on Principles of Distributed Computing, pages\r\n21–30, July 2002.\r\n[24] M. M. Michael. Hazard pointers: Safe memory reclama\u0002tion for lock-free objects. IEEE Trans. on Parallel and Dis\u0002tributed Syst., 15(6):491–504, June 2004.\r\n[25] M. M. Michael. Scalable lock-free dynamic memory allo\u0002cation. In Proc. ACM Conf. on Programming Language De\u0002sign and Implementation, pages 35–46, June 2004.\r\n[26] M. M. Michael and M. L. Scott. Correction of a memory\r\nmanagement method for lock-free data structures. Technical\r\nReport TR599, Computer Science Department, University\r\nof Rochester, Dec. 1995.\r\n[27] D. Sarma and P. E. McKenney. Issues with selected scala\u0002bility features of the 2.6 kernel. In Ottawa Linux Symp., July\r\n2004.\r\n[28] H. Sundell. Wait-free reference counting and memory man\u0002agement. In Proc. 19th Intl. Parallel and Distributed Pro\u0002cessing Symp., Apr. 2005.\r\n[29] J. D. Valois. Lock-free linked lists using compare-and-swap.\r\nIn Proc. 14th ACM Symp. on Principles of Distributed Com\u0002puting, pages 214–222, Aug. 1995.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/660586f2-48cb-4876-82b6-ee43d867ddd3/images/ec035c04-ec0d-44a8-9e88-890b94ac4ccb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041912Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=62bbfec5e1b169cf531944dad4340a02693cc5dbd5006c9f0f7beabbabf76fd7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 803
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "Making Lockless Synchronization Fast: Performance Implications of Memory Reclamation\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "```json\n{\"author\": \"Thomas E. Hart, Paul E. McKenney, and Angela Demke Brown\"}\n```"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "```json\n{\"date_published\": \"2006\"}\n```"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "United States, other countries, United Kingdom, Oregon, Canada, New York, Las Vegas, Ottawa, Canberra\n"
        }
      ]
    }
  }
}