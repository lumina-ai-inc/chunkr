{
  "file_name": "Linux Block IO - Introducing Multi-queue SSD Access on Multicore Systems.pdf",
  "task_id": "2b161c03-d9f4-4898-ae29-34c0cd231f3f",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "39739b86-1be3-41c3-8008-7088746951c9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Linux Block IO: Introducing Multi-queue SSD Access on\r\nMulti-core Systems\r\nMatias Bjørling*† Jens Axboe† David Nellans† Philippe Bonnet*\r\n*\r\nIT University of Copenhagen\r\n{mabj,phbo}@itu.dk\r\n†Fusion-io\r\n{jaxboe,dnellans}@fusionio.com\r\nABSTRACT\r\nThe IO performance of storage devices has accelerated from\r\nhundreds of IOPS five years ago, to hundreds of thousands\r\nof IOPS today, and tens of millions of IOPS projected in five\r\nyears. This sharp evolution is primarily due to the introduc\u0002tion of NAND-flash devices and their data parallel design. In\r\nthis work, we demonstrate that the block layer within the\r\noperating system, originally designed to handle thousands\r\nof IOPS, has become a bottleneck to overall storage system\r\nperformance, specially on the high NUMA-factor processors\r\nsystems that are becoming commonplace. We describe the\r\ndesign of a next generation block layer that is capable of\r\nhandling tens of millions of IOPS on a multi-core system\r\nequipped with a single storage device. Our experiments\r\nshow that our design scales graciously with the number of\r\ncores, even on NUMA systems with multiple sockets.\r\nCategories and Subject Descriptors\r\nD.4.2 [Operating System]: Storage Management—Sec\u0002ondary storage; D.4.8 [Operating System]: Performance—\r\nmeasurements\r\nGeneral Terms\r\nDesign, Experimentation, Measurement, Performance.\r\nKeywords\r\nLinux, Block Layer, Solid State Drives, Non-volatile Mem\u0002ory, Latency, Throughput.\r\n1 Introduction\r\nAs long as secondary storage has been synonymous with\r\nhard disk drives (HDD), IO latency and throughput have\r\nbeen shaped by the physical characteristics of rotational de\u0002vices: Random accesses that require disk head movement\r\nare slow and sequential accesses that only require rotation\r\nof the disk platter are fast. Generations of IO intensive al\u0002gorithms and systems have been designed based on these\r\ntwo fundamental characteristics. Today, the advent of solid\r\nstate disks (SSD) based on non-volatile memories (NVM)\r\nPermission to make digital or hard copies of all or part of this work for\r\npersonal or classroom use is granted without fee provided that copies are\r\nnot made or distributed for profit or commercial advantage and that copies\r\nbear this notice and the full citation on the first page. To copy otherwise, to\r\nrepublish, to post on servers or to redistribute to lists, requires prior specific\r\npermission and/or a fee.\r\nSYSTOR ’13 June 30 - July 02 2013, Haifa, Israel\r\nCopyright 2013 ACM 978-1-4503-2116-7/13/06 ...$15.00.\r\n2010 2011 2012\r\n4K\r\nRead IOPS\r\n0\r\n200k\r\n400k\r\n600k\r\n800k\r\n1M\r\n785000\r\n608000\r\n498000\r\n90000 60000\r\nSSD 1 SSD 2 SSD 3 SSD 4 SSD 5\r\nFigure 1: IOPS for 4K random read for five SSD\r\ndevices.\r\n(e.g., flash or phase-change memory [11, 6]) is transforming\r\nthe performance characteristics of secondary storage. SSDs\r\noften exhibit little latency difference between sequential and\r\nrandom IOs [16]. IO latency for SSDs is in the order of tens\r\nof microseconds as opposed to tens of milliseconds for HDDs.\r\nLarge internal data parallelism in SSDs disks enables many\r\nconcurrent IO operations which, in turn, allows single de\u0002vices to achieve close to a million IOs per second (IOPS)\r\nfor random accesses, as opposed to just hundreds on tradi\u0002tional magnetic hard drives. In Figure 1, we illustrate the\r\nevolution of SSD performance over the last couple of years.\r\nA similar, albeit slower, performance transformation has\r\nalready been witnessed for network systems. Ethernet speed\r\nevolved steadily from 10 Mb/s in the early 1990s to 100 Gb/s\r\nin 2010. Such a regular evolution over a 20 years period has\r\nallowed for a smooth transition between lab prototypes and\r\nmainstream deployments over time. For storage, the rate of\r\nchange is much faster. We have seen a 10,000x improvement\r\nover just a few years. The throughput of modern storage de\u0002vices is now often limited by their hardware (i.e., SATA/SAS\r\nor PCI-E) and software interfaces [28, 26]. Such rapid leaps\r\nin hardware performance have exposed previously unnoticed\r\nbottlenecks at the software level, both in the operating sys\u0002tem and application layers. Today, with Linux, a single\r\nCPU core can sustain an IO submission rate of around 800\r\nthousand IOPS. Regardless of how many cores are used to\r\nsubmit IOs, the operating system block layer can not scale\r\nup to over one million IOPS. This may be fast enough for\r\ntoday’s SSDs - but not for tomorrow’s.\r\nWe can expect that (a) SSDs are going to get faster, by\r\nincreasing their internal parallelism1[9, 8] and (b) CPU\r\n1\r\nIf we look at the performance of NAND-flash chips, access\r\ntimes are getting slower, not faster, in timings [17]. Access\r\ntime, for individual flash chips, increases with shrinking fea\u0002ture size, and increasing number of dies per package. The\r\ndecrease in individual chip performance is compensated by\r\nimproved parallelism within and across chips.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/39739b86-1be3-41c3-8008-7088746951c9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a7a48951fef76b98c306698aa0cf768433195c37f8253d0fc0aebcda1290f037",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 751
      },
      {
        "segments": [
          {
            "segment_id": "39739b86-1be3-41c3-8008-7088746951c9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Linux Block IO: Introducing Multi-queue SSD Access on\r\nMulti-core Systems\r\nMatias Bjørling*† Jens Axboe† David Nellans† Philippe Bonnet*\r\n*\r\nIT University of Copenhagen\r\n{mabj,phbo}@itu.dk\r\n†Fusion-io\r\n{jaxboe,dnellans}@fusionio.com\r\nABSTRACT\r\nThe IO performance of storage devices has accelerated from\r\nhundreds of IOPS five years ago, to hundreds of thousands\r\nof IOPS today, and tens of millions of IOPS projected in five\r\nyears. This sharp evolution is primarily due to the introduc\u0002tion of NAND-flash devices and their data parallel design. In\r\nthis work, we demonstrate that the block layer within the\r\noperating system, originally designed to handle thousands\r\nof IOPS, has become a bottleneck to overall storage system\r\nperformance, specially on the high NUMA-factor processors\r\nsystems that are becoming commonplace. We describe the\r\ndesign of a next generation block layer that is capable of\r\nhandling tens of millions of IOPS on a multi-core system\r\nequipped with a single storage device. Our experiments\r\nshow that our design scales graciously with the number of\r\ncores, even on NUMA systems with multiple sockets.\r\nCategories and Subject Descriptors\r\nD.4.2 [Operating System]: Storage Management—Sec\u0002ondary storage; D.4.8 [Operating System]: Performance—\r\nmeasurements\r\nGeneral Terms\r\nDesign, Experimentation, Measurement, Performance.\r\nKeywords\r\nLinux, Block Layer, Solid State Drives, Non-volatile Mem\u0002ory, Latency, Throughput.\r\n1 Introduction\r\nAs long as secondary storage has been synonymous with\r\nhard disk drives (HDD), IO latency and throughput have\r\nbeen shaped by the physical characteristics of rotational de\u0002vices: Random accesses that require disk head movement\r\nare slow and sequential accesses that only require rotation\r\nof the disk platter are fast. Generations of IO intensive al\u0002gorithms and systems have been designed based on these\r\ntwo fundamental characteristics. Today, the advent of solid\r\nstate disks (SSD) based on non-volatile memories (NVM)\r\nPermission to make digital or hard copies of all or part of this work for\r\npersonal or classroom use is granted without fee provided that copies are\r\nnot made or distributed for profit or commercial advantage and that copies\r\nbear this notice and the full citation on the first page. To copy otherwise, to\r\nrepublish, to post on servers or to redistribute to lists, requires prior specific\r\npermission and/or a fee.\r\nSYSTOR ’13 June 30 - July 02 2013, Haifa, Israel\r\nCopyright 2013 ACM 978-1-4503-2116-7/13/06 ...$15.00.\r\n2010 2011 2012\r\n4K\r\nRead IOPS\r\n0\r\n200k\r\n400k\r\n600k\r\n800k\r\n1M\r\n785000\r\n608000\r\n498000\r\n90000 60000\r\nSSD 1 SSD 2 SSD 3 SSD 4 SSD 5\r\nFigure 1: IOPS for 4K random read for five SSD\r\ndevices.\r\n(e.g., flash or phase-change memory [11, 6]) is transforming\r\nthe performance characteristics of secondary storage. SSDs\r\noften exhibit little latency difference between sequential and\r\nrandom IOs [16]. IO latency for SSDs is in the order of tens\r\nof microseconds as opposed to tens of milliseconds for HDDs.\r\nLarge internal data parallelism in SSDs disks enables many\r\nconcurrent IO operations which, in turn, allows single de\u0002vices to achieve close to a million IOs per second (IOPS)\r\nfor random accesses, as opposed to just hundreds on tradi\u0002tional magnetic hard drives. In Figure 1, we illustrate the\r\nevolution of SSD performance over the last couple of years.\r\nA similar, albeit slower, performance transformation has\r\nalready been witnessed for network systems. Ethernet speed\r\nevolved steadily from 10 Mb/s in the early 1990s to 100 Gb/s\r\nin 2010. Such a regular evolution over a 20 years period has\r\nallowed for a smooth transition between lab prototypes and\r\nmainstream deployments over time. For storage, the rate of\r\nchange is much faster. We have seen a 10,000x improvement\r\nover just a few years. The throughput of modern storage de\u0002vices is now often limited by their hardware (i.e., SATA/SAS\r\nor PCI-E) and software interfaces [28, 26]. Such rapid leaps\r\nin hardware performance have exposed previously unnoticed\r\nbottlenecks at the software level, both in the operating sys\u0002tem and application layers. Today, with Linux, a single\r\nCPU core can sustain an IO submission rate of around 800\r\nthousand IOPS. Regardless of how many cores are used to\r\nsubmit IOs, the operating system block layer can not scale\r\nup to over one million IOPS. This may be fast enough for\r\ntoday’s SSDs - but not for tomorrow’s.\r\nWe can expect that (a) SSDs are going to get faster, by\r\nincreasing their internal parallelism1[9, 8] and (b) CPU\r\n1\r\nIf we look at the performance of NAND-flash chips, access\r\ntimes are getting slower, not faster, in timings [17]. Access\r\ntime, for individual flash chips, increases with shrinking fea\u0002ture size, and increasing number of dies per package. The\r\ndecrease in individual chip performance is compensated by\r\nimproved parallelism within and across chips.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/39739b86-1be3-41c3-8008-7088746951c9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a7a48951fef76b98c306698aa0cf768433195c37f8253d0fc0aebcda1290f037",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 751
      },
      {
        "segments": [
          {
            "segment_id": "98571f8e-c8b4-478d-8f37-bfc0fd883d84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "performance will improve largely due to the addition of more\r\ncores, whose performance may largely remain stable [24, 27].\r\nIf we consider a SSD that can provide 2 million IOPS, ap\u0002plications will no longer be able to fully utilize a single stor\u0002age device, regardless of the number of threads and CPUs\r\nit is parallelized across due to current limitations within the\r\noperating system.\r\nBecause of the performance bottleneck that exists today\r\nwithin the operating system, some applications and device\r\ndrivers are already choosing to bypass the Linux block layer\r\nin order to improve performance [8]. This choice increases\r\ncomplexity in both driver and hardware implementations.\r\nMore specifically, it increases duplicate code across error\u0002prone driver implementations, and removes generic features\r\nsuch as IO scheduling and quality of service traffic shaping\r\nthat are provided by a common OS storage layer.\r\nRather than discarding the block layer to keep up with im\u0002proving storage performance, we propose a new design that\r\nfixes the scaling issues of the existing block layer, while pre\u0002serving its best features. More specifically, our contributions\r\nare the following:\r\n1. We recognize that the Linux block layer has become a\r\nbottleneck (we detail our analysis in Section 2). The\r\ncurrent design employs a single coarse lock design for\r\nprotecting the request queue, which becomes the main\r\nbottleneck to overall storage performance as device\r\nperformance approaches 800 thousand IOPS. This sin\u0002gle lock design is especially painful on parallel CPUs,\r\nas all cores must agree on the state of the request queue\r\nlock, which quickly results in significant performance\r\ndegradation.\r\n2. We propose a new design for IO management within\r\nthe block layer. Our design relies on multiple IO sub\u0002mission/completion queues to minimize cache coher\u0002ence across CPU cores. The main idea of our design\r\nis to introduce two levels of queues within the block\r\nlayer: (i) software queues that manage the IOs sub\u0002mitted from a given CPU core (e.g., the block layer\r\nrunning on a CPU with 8 cores will be equipped with\r\n8 software queues), and (ii) hardware queues mapped\r\non the underlying SSD driver submission queue.\r\n3. We evaluate our multi-queue design based on a func\u0002tional implementation within the Linux kernel. We\r\nimplement a new no-op block driver that allows de\u0002velopers to investigate OS block layer improvements.\r\nWe then compare our new block layer to the existing\r\none on top of the noop driver (thus focusing purely\r\non the block layer performance). We show that a\r\ntwo-level locking design reduces the number of cache\r\nand pipeline flushes compared to a single level design,\r\nscales gracefully in high NUMA-factor architectures,\r\nand can scale up to 10 million IOPS to meet the de\u0002mand of future storage products.\r\nThe rest of the paper is organized as follows: In Section 2\r\nwe review the current implementation of the Linux block\r\nlayer and its performance limitations. In Section 3 we pro\u0002pose a new multi-queue design for the Linux block layer. In\r\nSection 4 we describe our experimental framework, and in\r\nSection 5, we discuss the performance impact of our multi\u0002queue design. We discuss related work in Section 6, before\r\ndrawing our conclusions in Section 7.\r\nFigure 2: Current single queue Linux block layer\r\ndesign.\r\n2 OS Block Layer\r\nSimply put, the operating system block layer is responsible\r\nfor shepherding IO requests from applications to storage de\u0002vices [2]. The block layer is a glue that, on the one hand,\r\nallows applications to access diverse storage devices in a uni\u0002form way, and on the other hand, provides storage devices\r\nand drivers with a single point of entry from all applica\u0002tions. It is a convenience library to hide the complexity\r\nand diversity of storage devices from the application while\r\nproviding common services that are valuable to applications.\r\nIn addition, the block layer implements IO-fairness, IO-error\r\nhandling, IO-statistics, and IO-scheduling that improve per\u0002formance and help protect end-users from poor or malicious\r\nimplementations of other applications or device drivers.\r\n2.1 Architecture\r\nFigure 2 illustrates the architecture of the current Linux\r\nblock layer. Applications submit IOs via a kernel system\r\ncall, that converts them into a data structure called a block\r\nIO. Each block IO contains information such as IO address,\r\nIO size, IO modality (read or write) or IO type (synchronous/\r\nasynchronous)2. It is then transferred to either libaio for\r\nasynchronous IOs or directly to the block layer for syn\u0002chronous IO that submit it to the block layer. Once an IO\r\nrequest is submitted, the corresponding block IO is buffered\r\nin the staging area, which is implemented as a queue, de\u0002noted the request queue.\r\nOnce a request is in the staging area, the block layer may\r\nperform IO scheduling and adjust accounting information\r\nbefore scheduling IO submissions to the appropriate storage\r\n2See include/linux/blk types.h in the Linux kernel (ker\u0002nel.org) for a complete description of the Block IO data\r\nstructure.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/98571f8e-c8b4-478d-8f37-bfc0fd883d84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0fb698587fb7b37e029ae4a7b4d0412e95daed5ed359aaed65d7d731deb11e1c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 798
      },
      {
        "segments": [
          {
            "segment_id": "98571f8e-c8b4-478d-8f37-bfc0fd883d84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "performance will improve largely due to the addition of more\r\ncores, whose performance may largely remain stable [24, 27].\r\nIf we consider a SSD that can provide 2 million IOPS, ap\u0002plications will no longer be able to fully utilize a single stor\u0002age device, regardless of the number of threads and CPUs\r\nit is parallelized across due to current limitations within the\r\noperating system.\r\nBecause of the performance bottleneck that exists today\r\nwithin the operating system, some applications and device\r\ndrivers are already choosing to bypass the Linux block layer\r\nin order to improve performance [8]. This choice increases\r\ncomplexity in both driver and hardware implementations.\r\nMore specifically, it increases duplicate code across error\u0002prone driver implementations, and removes generic features\r\nsuch as IO scheduling and quality of service traffic shaping\r\nthat are provided by a common OS storage layer.\r\nRather than discarding the block layer to keep up with im\u0002proving storage performance, we propose a new design that\r\nfixes the scaling issues of the existing block layer, while pre\u0002serving its best features. More specifically, our contributions\r\nare the following:\r\n1. We recognize that the Linux block layer has become a\r\nbottleneck (we detail our analysis in Section 2). The\r\ncurrent design employs a single coarse lock design for\r\nprotecting the request queue, which becomes the main\r\nbottleneck to overall storage performance as device\r\nperformance approaches 800 thousand IOPS. This sin\u0002gle lock design is especially painful on parallel CPUs,\r\nas all cores must agree on the state of the request queue\r\nlock, which quickly results in significant performance\r\ndegradation.\r\n2. We propose a new design for IO management within\r\nthe block layer. Our design relies on multiple IO sub\u0002mission/completion queues to minimize cache coher\u0002ence across CPU cores. The main idea of our design\r\nis to introduce two levels of queues within the block\r\nlayer: (i) software queues that manage the IOs sub\u0002mitted from a given CPU core (e.g., the block layer\r\nrunning on a CPU with 8 cores will be equipped with\r\n8 software queues), and (ii) hardware queues mapped\r\non the underlying SSD driver submission queue.\r\n3. We evaluate our multi-queue design based on a func\u0002tional implementation within the Linux kernel. We\r\nimplement a new no-op block driver that allows de\u0002velopers to investigate OS block layer improvements.\r\nWe then compare our new block layer to the existing\r\none on top of the noop driver (thus focusing purely\r\non the block layer performance). We show that a\r\ntwo-level locking design reduces the number of cache\r\nand pipeline flushes compared to a single level design,\r\nscales gracefully in high NUMA-factor architectures,\r\nand can scale up to 10 million IOPS to meet the de\u0002mand of future storage products.\r\nThe rest of the paper is organized as follows: In Section 2\r\nwe review the current implementation of the Linux block\r\nlayer and its performance limitations. In Section 3 we pro\u0002pose a new multi-queue design for the Linux block layer. In\r\nSection 4 we describe our experimental framework, and in\r\nSection 5, we discuss the performance impact of our multi\u0002queue design. We discuss related work in Section 6, before\r\ndrawing our conclusions in Section 7.\r\nFigure 2: Current single queue Linux block layer\r\ndesign.\r\n2 OS Block Layer\r\nSimply put, the operating system block layer is responsible\r\nfor shepherding IO requests from applications to storage de\u0002vices [2]. The block layer is a glue that, on the one hand,\r\nallows applications to access diverse storage devices in a uni\u0002form way, and on the other hand, provides storage devices\r\nand drivers with a single point of entry from all applica\u0002tions. It is a convenience library to hide the complexity\r\nand diversity of storage devices from the application while\r\nproviding common services that are valuable to applications.\r\nIn addition, the block layer implements IO-fairness, IO-error\r\nhandling, IO-statistics, and IO-scheduling that improve per\u0002formance and help protect end-users from poor or malicious\r\nimplementations of other applications or device drivers.\r\n2.1 Architecture\r\nFigure 2 illustrates the architecture of the current Linux\r\nblock layer. Applications submit IOs via a kernel system\r\ncall, that converts them into a data structure called a block\r\nIO. Each block IO contains information such as IO address,\r\nIO size, IO modality (read or write) or IO type (synchronous/\r\nasynchronous)2. It is then transferred to either libaio for\r\nasynchronous IOs or directly to the block layer for syn\u0002chronous IO that submit it to the block layer. Once an IO\r\nrequest is submitted, the corresponding block IO is buffered\r\nin the staging area, which is implemented as a queue, de\u0002noted the request queue.\r\nOnce a request is in the staging area, the block layer may\r\nperform IO scheduling and adjust accounting information\r\nbefore scheduling IO submissions to the appropriate storage\r\n2See include/linux/blk types.h in the Linux kernel (ker\u0002nel.org) for a complete description of the Block IO data\r\nstructure.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/98571f8e-c8b4-478d-8f37-bfc0fd883d84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0fb698587fb7b37e029ae4a7b4d0412e95daed5ed359aaed65d7d731deb11e1c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 798
      },
      {
        "segments": [
          {
            "segment_id": "222a416e-2133-44e0-8fec-3a062b2d04fc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 3: Simplified overview of bottlenecks in the\r\nblock layer on a system equipped with two cores and\r\na SSD.\r\ndevice driver. Note that the Linux block layer supports plug\u0002gable IO schedulers: noop (no scheduling), deadline-based\r\nscheduling [12], and CFQ [10] that can all operate on IO\r\nwithin this staging area. The block layer also provides a\r\nmechanism for dealing with IO completions: each time an\r\nIO completes within the device driver, this driver calls up\r\nthe stack to the generic completion function in the block\r\nlayer. In turn the block layer then calls up to an IO com\u0002pletion function in the libaio library, or returns from the\r\nsynchronous read or write system call, which provides the\r\nIO completion signal to the application.\r\nWith the current block layer, the staging area is repre\u0002sented by a request queue structure. One such queue is\r\ninstantiated per block device. Access is uniform across all\r\nblock devices and an application need not know what the\r\ncontrol flow pattern is within the block layer. A consequence\r\nof this single queue per device design however is that the\r\nblock layer cannot support IO scheduling across devices.\r\n2.2 Scalability\r\nWe analyzed the Linux kernel to evaluate the performance\r\nof the current block layer on high performance computing\r\nsystems equipped with high-factor NUMA multi-core pro\u0002cessors and high IOPS NAND-flash SSDs. We found that\r\nthe block layer had a considerable overhead for each IO;\r\nSpecifically, we identified three main problems, illustrated\r\nin Figure 3:\r\n1. Request Queue Locking: The block layer fundamentally\r\nsynchronizes shared accesses to an exclusive resource:\r\nthe IO request queue. (i) Whenever a block IO is in\u0002serted or removed from the request queue, this lock\r\nmust be acquired. (ii) Whenever the request queue is\r\nmanipulated via IO submission, this lock must be ac\u0002quired. (iii) As IOs are submitted, the block layer pro\u0002ceeds to optimizations such as plugging (letting IOs ac\u0002cumulate before issuing them to hardware to improve\r\ncache efficiency), (iv) IO reordering, and (v) fairness\r\nscheduling. Before any of these operations can pro\u0002ceed, the request queue lock must be acquired. This is\r\na major source of contention.\r\n2. Hardware Interrupts: The high number of IOPS causes\r\na proportionally high number of interrupts. Most of to\u0002day’s storage devices are designed such that one core\r\n(within CPU 0 on Figure 3) is responsible for han\u0002dling all hardware interrupts and forwarding them to\r\nother cores as soft interrupts regardless of the CPU\r\nIOPS\r\nNumber of Cores\r\n1 socket\r\n0\r\n250k\r\n500k\r\n750k\r\n1M\r\n1.25M\r\n1.5M\r\n1 2 3 4 5 6\r\n2 socket\r\n2 4 6 8 10 12\r\n4 socket\r\n0\r\n250k\r\n500k\r\n750k\r\n1M\r\n1.25M\r\n1.5M\r\n5 10 15 20 25 30 10 20 30 40 50 60 70 80\r\n8 socket\r\nFigure 4: IOPS throughput of Linux block layer as\r\na function of number of CPU’s issuing IO. Divided\r\ninto 1, 2, 4 and 8 socket systems. Note: Dotted line\r\nshow socket divisions.\r\nissuing and completing the IO. As a result, a single\r\ncore may spend considerable time in handling these\r\ninterrupts, context switching, and polluting L1 and L2\r\ncaches that applications could rely on for data local\u0002ity [31]. The other cores (within CPU N on Figure 3)\r\nthen also must take an IPI to perform the IO comple\u0002tion routine. As a result, in many cases two interrupts\r\nand context switches are required to complete just a\r\nsingle IO.\r\n3. Remote Memory Accesses: Request queue lock con\u0002tention is exacerbated when it forces remote mem\u0002ory accesses across CPU cores (or across sockets in\r\na NUMA architecture). Such remote memory accesses\r\nare needed whenever an IO completes on a different\r\ncore from the one on which it was issued. In such\r\ncases, acquiring a lock on the request queue to remove\r\nthe block IO from the request queue incurs a remote\r\nmemory access to the lock state stored in the cache of\r\nthe core where that lock was last acquired, the cache\r\nline is then marked shared on both cores. When up\u0002dated, the copy is explicitly invalidated from the re\u0002mote cache. If more than one core is actively issuing\r\nIO and thus competing for this lock, then the cache\r\nline associated with this lock is continuously bounced\r\nbetween those cores.\r\nFigure 4 shows 512 bytes IOs being submitted to the ker\u0002nel as fast as possible; IOPS throughput is depicted as a\r\nfunction of the number of CPU’s that are submitting and\r\ncompleting IOs to a single device simultaneously. We ob\u0002serve that when the number of processes is lower than the\r\nnumber cores on a single socket (i.e., 6), throughput im\u0002proves, or is at least maintained, as multiple CPU’s issue\r\nIOs. For 2, 4, and 8-socket architectures which have largely\r\nsupplanted single socket machines in the HPC space, when\r\nIOs are issued from a CPU that is located on a remote socket\r\n(and typically NUMA node), absolute performance drops\r\nsubstantially regardless the absolute number of sockets in\r\nthe system.\r\nRemote cacheline invalidation of the request queue lock is\r\nsignificantly more costly on complex four and eight socket\r\nsystems where the NUMA-factor is high and large cache di\u0002rectory structures are expensive to access. On four and eight",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/222a416e-2133-44e0-8fec-3a062b2d04fc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5d0323f4a05fd61e5b3628f61f0357a86be1a1cbee4936b27b4dfeded13d91e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 857
      },
      {
        "segments": [
          {
            "segment_id": "222a416e-2133-44e0-8fec-3a062b2d04fc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 3: Simplified overview of bottlenecks in the\r\nblock layer on a system equipped with two cores and\r\na SSD.\r\ndevice driver. Note that the Linux block layer supports plug\u0002gable IO schedulers: noop (no scheduling), deadline-based\r\nscheduling [12], and CFQ [10] that can all operate on IO\r\nwithin this staging area. The block layer also provides a\r\nmechanism for dealing with IO completions: each time an\r\nIO completes within the device driver, this driver calls up\r\nthe stack to the generic completion function in the block\r\nlayer. In turn the block layer then calls up to an IO com\u0002pletion function in the libaio library, or returns from the\r\nsynchronous read or write system call, which provides the\r\nIO completion signal to the application.\r\nWith the current block layer, the staging area is repre\u0002sented by a request queue structure. One such queue is\r\ninstantiated per block device. Access is uniform across all\r\nblock devices and an application need not know what the\r\ncontrol flow pattern is within the block layer. A consequence\r\nof this single queue per device design however is that the\r\nblock layer cannot support IO scheduling across devices.\r\n2.2 Scalability\r\nWe analyzed the Linux kernel to evaluate the performance\r\nof the current block layer on high performance computing\r\nsystems equipped with high-factor NUMA multi-core pro\u0002cessors and high IOPS NAND-flash SSDs. We found that\r\nthe block layer had a considerable overhead for each IO;\r\nSpecifically, we identified three main problems, illustrated\r\nin Figure 3:\r\n1. Request Queue Locking: The block layer fundamentally\r\nsynchronizes shared accesses to an exclusive resource:\r\nthe IO request queue. (i) Whenever a block IO is in\u0002serted or removed from the request queue, this lock\r\nmust be acquired. (ii) Whenever the request queue is\r\nmanipulated via IO submission, this lock must be ac\u0002quired. (iii) As IOs are submitted, the block layer pro\u0002ceeds to optimizations such as plugging (letting IOs ac\u0002cumulate before issuing them to hardware to improve\r\ncache efficiency), (iv) IO reordering, and (v) fairness\r\nscheduling. Before any of these operations can pro\u0002ceed, the request queue lock must be acquired. This is\r\na major source of contention.\r\n2. Hardware Interrupts: The high number of IOPS causes\r\na proportionally high number of interrupts. Most of to\u0002day’s storage devices are designed such that one core\r\n(within CPU 0 on Figure 3) is responsible for han\u0002dling all hardware interrupts and forwarding them to\r\nother cores as soft interrupts regardless of the CPU\r\nIOPS\r\nNumber of Cores\r\n1 socket\r\n0\r\n250k\r\n500k\r\n750k\r\n1M\r\n1.25M\r\n1.5M\r\n1 2 3 4 5 6\r\n2 socket\r\n2 4 6 8 10 12\r\n4 socket\r\n0\r\n250k\r\n500k\r\n750k\r\n1M\r\n1.25M\r\n1.5M\r\n5 10 15 20 25 30 10 20 30 40 50 60 70 80\r\n8 socket\r\nFigure 4: IOPS throughput of Linux block layer as\r\na function of number of CPU’s issuing IO. Divided\r\ninto 1, 2, 4 and 8 socket systems. Note: Dotted line\r\nshow socket divisions.\r\nissuing and completing the IO. As a result, a single\r\ncore may spend considerable time in handling these\r\ninterrupts, context switching, and polluting L1 and L2\r\ncaches that applications could rely on for data local\u0002ity [31]. The other cores (within CPU N on Figure 3)\r\nthen also must take an IPI to perform the IO comple\u0002tion routine. As a result, in many cases two interrupts\r\nand context switches are required to complete just a\r\nsingle IO.\r\n3. Remote Memory Accesses: Request queue lock con\u0002tention is exacerbated when it forces remote mem\u0002ory accesses across CPU cores (or across sockets in\r\na NUMA architecture). Such remote memory accesses\r\nare needed whenever an IO completes on a different\r\ncore from the one on which it was issued. In such\r\ncases, acquiring a lock on the request queue to remove\r\nthe block IO from the request queue incurs a remote\r\nmemory access to the lock state stored in the cache of\r\nthe core where that lock was last acquired, the cache\r\nline is then marked shared on both cores. When up\u0002dated, the copy is explicitly invalidated from the re\u0002mote cache. If more than one core is actively issuing\r\nIO and thus competing for this lock, then the cache\r\nline associated with this lock is continuously bounced\r\nbetween those cores.\r\nFigure 4 shows 512 bytes IOs being submitted to the ker\u0002nel as fast as possible; IOPS throughput is depicted as a\r\nfunction of the number of CPU’s that are submitting and\r\ncompleting IOs to a single device simultaneously. We ob\u0002serve that when the number of processes is lower than the\r\nnumber cores on a single socket (i.e., 6), throughput im\u0002proves, or is at least maintained, as multiple CPU’s issue\r\nIOs. For 2, 4, and 8-socket architectures which have largely\r\nsupplanted single socket machines in the HPC space, when\r\nIOs are issued from a CPU that is located on a remote socket\r\n(and typically NUMA node), absolute performance drops\r\nsubstantially regardless the absolute number of sockets in\r\nthe system.\r\nRemote cacheline invalidation of the request queue lock is\r\nsignificantly more costly on complex four and eight socket\r\nsystems where the NUMA-factor is high and large cache di\u0002rectory structures are expensive to access. On four and eight",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/222a416e-2133-44e0-8fec-3a062b2d04fc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5d0323f4a05fd61e5b3628f61f0357a86be1a1cbee4936b27b4dfeded13d91e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 857
      },
      {
        "segments": [
          {
            "segment_id": "be6984a6-09fe-4f0f-88cc-98c29554dc84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "socket architectures, the request queue lock contention is so\r\nhigh that multiple sockets issuing IOs reduces the through\u0002put of the Linux block layer to just about 125 thousand\r\nIOPS even though there have been high end solid state de\u0002vices on the market for several years able to achieve higher\r\nIOPS than this. The scalability of the Linux block layer is\r\nnot an issue that we might encounter in the future, it is a\r\nsignificant problem being faced by HPC in practice today.\r\n3 Multi-Queue Block Layer\r\nAs we have seen in Section 2.2, reducing lock contention\r\nand remote memory accesses are key challenges when re\u0002designing the block layer to scale on high NUMA-factor\r\narchitectures. Dealing efficiently with the high number of\r\nhardware interrupts is beyond the control of the block layer\r\n(more on this below) as the block layer cannot dictate how a\r\ndevice driver interacts with its hardware. In this Section, we\r\npropose a two-level multi-queue design for the Linux block\r\nlayer and discuss its key differences and advantages over the\r\ncurrent single queue block layer implementation. Before we\r\ndetail our design, we summarize the general block layer re\u0002quirements.\r\n3.1 Requirements\r\nBased on our analysis of the Linux block layer, we identify\r\nthree major requirements for a block layer:\r\n• Single Device Fairness\r\nMany application processes may use the same device.\r\nIt is important to enforce that a single process should\r\nnot be able to starve all others. This is a task for the\r\nblock layer. Traditionally, techniques such as CFQ or\r\ndeadline scheduling have been used to enforce fairness\r\nin the block layer. Without a centralized arbiter of de\u0002vice access, applications must either coordinate among\r\nthemselves for fairness or rely on the fairness policies\r\nimplemented in device drivers (which rarely exist).\r\n• Single and Multiple Device Accounting\r\nThe block layer should make it easy for system admin\u0002istrators to debug or simply monitor accesses to stor\u0002age devices. Having a uniform interface for system per\u0002formance monitoring and accounting enables applica\u0002tions and other operating system components to make\r\nintelligent decisions about application scheduling, load\r\nbalancing, and performance. If these were maintained\r\ndirectly by device drivers, it would be nearly impossi\u0002ble to enforce the convenience of consistency applica\u0002tion writers have become accustom to.\r\n• Single Device IO Staging Area\r\nTo improve performance and enforce fairness, the block\r\nlayer must be able to perform some form of IO schedul\u0002ing. To do this, the block layer requires a staging area,\r\nwhere IOs may be buffered before they are sent down\r\ninto the device driver. Using a staging area, the block\r\nlayer can reorder IOs, typically to promote sequential\r\naccesses over random ones, or it can group IOs, to sub\u0002mit larger IOs to the underlying device. In addition,\r\nthe staging area allows the block layer to adjust its\r\nsubmission rate for quality of service or due to device\r\nback-pressure indicating the OS should not send down\r\nadditional IO or risk overflowing the device’s buffering\r\ncapability.\r\n3.2 Our Architecture\r\nThe key insight to improved scalability in our multi-queue\r\ndesign is to distribute the lock contention on the single re\u0002quest queue lock to multiple queues through the use of two\r\nlevels of queues with distinct functionally as shown in Fig\u0002ure 5:\r\n• Software Staging Queues. Rather than staging IO for\r\ndispatch in a single software queue, block IO requests\r\nare now maintained in a collection of one or more re\u0002quest queues. These staging queues can be configured\r\nsuch that there is one such queue per socket, or per\r\ncore, on the system. So, on a NUMA system with 4\r\nsockets and 6 cores per socket, the staging area may\r\ncontain as few as 4 and as many as 24 queues. The\r\nvariable nature of the request queues decreases the pro\u0002liferation of locks if contention on a single queue is not\r\na bottleneck. With many CPU architectures offering\r\na large shared L3 cache per socket (typically a NUMA\r\nnode as well), having just a single queue per proces\u0002sor socket offers a good trade-off between duplicated\r\ndata structures which are cache unfriendly and lock\r\ncontention.\r\n• Hardware Dispatch Queues. After IO has entered the\r\nstaging queues, we introduce a new intermediate queu\u0002ing layer known as the hardware dispatch queues. Us\u0002ing these queues block IOs scheduled for dispatch are\r\nnot sent directly to the device driver, they are instead\r\nsent to the hardware dispatch queue. The number\r\nof hardware dispatch queues will typically match the\r\nnumber of hardware contexts supported by the device\r\ndriver. Device drivers may choose to support anywhere\r\nfrom one to 2048 queues as supported by the message\r\nsignaled interrupts standard MSI-X [25]. Because IO\r\nordering is not supported within the block layer any\r\nsoftware queue may feed any hardware queue without\r\nneeding to maintain a global ordering. This allows\r\nhardware to implement one or more queues that map\r\nonto NUMA nodes or CPU’s directly and provide a\r\nfast IO path from application to hardware that never\r\nhas to access remote memory on any other node.\r\nThis two level design explicitly separates the two buffering\r\nfunctions of the staging area that was previously merged\r\ninto a single queue in the Linux block layer: (i) support for\r\nIO scheduling (software level) and (ii) means to adjust the\r\nsubmission rate (hardware level) to prevent device buffer\r\nover run.\r\nThe number of entries in the software level queue can dy\u0002namically grow and shrink as needed to support the out\u0002standing queue depth maintained by the application, though\r\nqueue expansion and contraction is a relatively costly op\u0002eration compared to the memory overhead of maintaining\r\nenough free IO slots to support most application use. Con\u0002versely, the size of the hardware dispatch queue is bounded\r\nand correspond to the maximum queue depth that is sup\u0002ported by the device driver and hardware. Today many\r\nSSD’s that support native command queuing support a queue\r\ndepth of just 32, though high-end SSD storage devices may\r\nhave much deeper queue support to make use of the high\r\ninternal parallelism of their flash architecture. The 32 in\u0002flight request limit found on many consumer SSD’s is likely\r\nto increase substantially to support increased IOPS rates as",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/be6984a6-09fe-4f0f-88cc-98c29554dc84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5fc9a989adc94d2923db56bd95f5752817d2a6cd617860fd1b4dd5f8485b7db6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1016
      },
      {
        "segments": [
          {
            "segment_id": "be6984a6-09fe-4f0f-88cc-98c29554dc84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "socket architectures, the request queue lock contention is so\r\nhigh that multiple sockets issuing IOs reduces the through\u0002put of the Linux block layer to just about 125 thousand\r\nIOPS even though there have been high end solid state de\u0002vices on the market for several years able to achieve higher\r\nIOPS than this. The scalability of the Linux block layer is\r\nnot an issue that we might encounter in the future, it is a\r\nsignificant problem being faced by HPC in practice today.\r\n3 Multi-Queue Block Layer\r\nAs we have seen in Section 2.2, reducing lock contention\r\nand remote memory accesses are key challenges when re\u0002designing the block layer to scale on high NUMA-factor\r\narchitectures. Dealing efficiently with the high number of\r\nhardware interrupts is beyond the control of the block layer\r\n(more on this below) as the block layer cannot dictate how a\r\ndevice driver interacts with its hardware. In this Section, we\r\npropose a two-level multi-queue design for the Linux block\r\nlayer and discuss its key differences and advantages over the\r\ncurrent single queue block layer implementation. Before we\r\ndetail our design, we summarize the general block layer re\u0002quirements.\r\n3.1 Requirements\r\nBased on our analysis of the Linux block layer, we identify\r\nthree major requirements for a block layer:\r\n• Single Device Fairness\r\nMany application processes may use the same device.\r\nIt is important to enforce that a single process should\r\nnot be able to starve all others. This is a task for the\r\nblock layer. Traditionally, techniques such as CFQ or\r\ndeadline scheduling have been used to enforce fairness\r\nin the block layer. Without a centralized arbiter of de\u0002vice access, applications must either coordinate among\r\nthemselves for fairness or rely on the fairness policies\r\nimplemented in device drivers (which rarely exist).\r\n• Single and Multiple Device Accounting\r\nThe block layer should make it easy for system admin\u0002istrators to debug or simply monitor accesses to stor\u0002age devices. Having a uniform interface for system per\u0002formance monitoring and accounting enables applica\u0002tions and other operating system components to make\r\nintelligent decisions about application scheduling, load\r\nbalancing, and performance. If these were maintained\r\ndirectly by device drivers, it would be nearly impossi\u0002ble to enforce the convenience of consistency applica\u0002tion writers have become accustom to.\r\n• Single Device IO Staging Area\r\nTo improve performance and enforce fairness, the block\r\nlayer must be able to perform some form of IO schedul\u0002ing. To do this, the block layer requires a staging area,\r\nwhere IOs may be buffered before they are sent down\r\ninto the device driver. Using a staging area, the block\r\nlayer can reorder IOs, typically to promote sequential\r\naccesses over random ones, or it can group IOs, to sub\u0002mit larger IOs to the underlying device. In addition,\r\nthe staging area allows the block layer to adjust its\r\nsubmission rate for quality of service or due to device\r\nback-pressure indicating the OS should not send down\r\nadditional IO or risk overflowing the device’s buffering\r\ncapability.\r\n3.2 Our Architecture\r\nThe key insight to improved scalability in our multi-queue\r\ndesign is to distribute the lock contention on the single re\u0002quest queue lock to multiple queues through the use of two\r\nlevels of queues with distinct functionally as shown in Fig\u0002ure 5:\r\n• Software Staging Queues. Rather than staging IO for\r\ndispatch in a single software queue, block IO requests\r\nare now maintained in a collection of one or more re\u0002quest queues. These staging queues can be configured\r\nsuch that there is one such queue per socket, or per\r\ncore, on the system. So, on a NUMA system with 4\r\nsockets and 6 cores per socket, the staging area may\r\ncontain as few as 4 and as many as 24 queues. The\r\nvariable nature of the request queues decreases the pro\u0002liferation of locks if contention on a single queue is not\r\na bottleneck. With many CPU architectures offering\r\na large shared L3 cache per socket (typically a NUMA\r\nnode as well), having just a single queue per proces\u0002sor socket offers a good trade-off between duplicated\r\ndata structures which are cache unfriendly and lock\r\ncontention.\r\n• Hardware Dispatch Queues. After IO has entered the\r\nstaging queues, we introduce a new intermediate queu\u0002ing layer known as the hardware dispatch queues. Us\u0002ing these queues block IOs scheduled for dispatch are\r\nnot sent directly to the device driver, they are instead\r\nsent to the hardware dispatch queue. The number\r\nof hardware dispatch queues will typically match the\r\nnumber of hardware contexts supported by the device\r\ndriver. Device drivers may choose to support anywhere\r\nfrom one to 2048 queues as supported by the message\r\nsignaled interrupts standard MSI-X [25]. Because IO\r\nordering is not supported within the block layer any\r\nsoftware queue may feed any hardware queue without\r\nneeding to maintain a global ordering. This allows\r\nhardware to implement one or more queues that map\r\nonto NUMA nodes or CPU’s directly and provide a\r\nfast IO path from application to hardware that never\r\nhas to access remote memory on any other node.\r\nThis two level design explicitly separates the two buffering\r\nfunctions of the staging area that was previously merged\r\ninto a single queue in the Linux block layer: (i) support for\r\nIO scheduling (software level) and (ii) means to adjust the\r\nsubmission rate (hardware level) to prevent device buffer\r\nover run.\r\nThe number of entries in the software level queue can dy\u0002namically grow and shrink as needed to support the out\u0002standing queue depth maintained by the application, though\r\nqueue expansion and contraction is a relatively costly op\u0002eration compared to the memory overhead of maintaining\r\nenough free IO slots to support most application use. Con\u0002versely, the size of the hardware dispatch queue is bounded\r\nand correspond to the maximum queue depth that is sup\u0002ported by the device driver and hardware. Today many\r\nSSD’s that support native command queuing support a queue\r\ndepth of just 32, though high-end SSD storage devices may\r\nhave much deeper queue support to make use of the high\r\ninternal parallelism of their flash architecture. The 32 in\u0002flight request limit found on many consumer SSD’s is likely\r\nto increase substantially to support increased IOPS rates as",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/be6984a6-09fe-4f0f-88cc-98c29554dc84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5fc9a989adc94d2923db56bd95f5752817d2a6cd617860fd1b4dd5f8485b7db6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1016
      },
      {
        "segments": [
          {
            "segment_id": "e6cd91d4-f14a-4b22-bf1e-14a11aecd44e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 5: Proposed two level Linux block layer de\u0002sign.\r\na 1 million IOPS capable device will cause 31 thousand con\u0002text switches per second simply to process IO in batches of\r\n32. The CPU overhead of issuing IO to devices is inversely\r\nproportional to the amount of IO that is batched in each\r\nsubmission event.\r\n3.2.1 IO-Scheduling\r\nWithin the software queues, IOs can be shaped by per CPU\r\nor NUMA node policies that need not access local memory.\r\nAlternatively, policies may be implemented across software\r\nqueues to maintain global QoS metrics on IO, though at a\r\nperformance penalty. Once the IO has entered the hard\u0002ware dispatch queues, reordering i/Nums no longer possi\u0002ble. We eliminate this possibility so that the only con\u0002tenders for the hardware dispatch queue are inserted to the\r\nhead of the queue and removed from the tail by the device\r\ndriver, thus eliminating lock acquisitions for accounting or\r\nIO-scheduling. This improves the fast path cache locality\r\nwhen issuing IO’s in bulk to the device drivers.\r\nOur design has significant consequences on how IO may be\r\nissued to devices. Instead of inserting requests in the hard\u0002ware queue in sorted order to leverage sequential accesses\r\n(which was a main issue for hard drives), we simply follow\r\na FIFO policy: we insert the incoming block IO submitted\r\nby core i at the top of the request queue attached to core\r\ni or the NUMA socket this core resides on. Traditional IO\u0002schedulers have worked hard to turn random into sequential\r\naccess to optimize performance on traditional hard drives.\r\nOur two level queuing strategy relies on the fact that mod\u0002ern SSD’s have random read and write latency that is as fast\r\nas their sequential access. Thus interleaving IOs from multi\u0002ple software dispatch queues into a single hardware dispatch\r\nqueue does not hurt device performance. Also, by inserting\r\nrequests into the local software request queue, our design\r\nrespects thread locality for IOs and their completion.\r\nWhile global sequential re-ordering is still possible across\r\nthe multiple software queues, it is only necessary for HDD\r\nbased devices, where the additional latency and locking over\u0002head required to achieve total ordering does not hurt IOPS\r\nperformance. It can be argued that, for many users, it is\r\nno longer necessary to employ advanced fairness scheduling\r\nas the speed of the devices are often exceeding the ability\r\nof even multiple applications to saturate their performance.\r\nIf fairness is essential, it is possible to design a scheduler\r\nthat exploits the characteristics of SSDs at coarser granu\u0002larity to achieve lower performance overhead [23, 13, 19].\r\nWhether the scheduler should reside in the block layer or on\r\nthe SSD controller is an open issue. If the SSD is responsible\r\nfor fair IO scheduling, it can leverage internal device paral\u0002lelism, and lower latency, at the cost of additional interface\r\ncomplexity between disk and OS [8, 4].\r\n3.2.2 Number of Hardware Queues\r\nToday, most SATA, SAS and PCI-E based SSDs, support\r\njust a single hardware dispatch queue and a single comple\u0002tion queue using a single interrupt to signal completions.\r\nOne exception is the upcoming NVM Express (NVMe) [18]\r\ninterface which supports a flexible number of submission\r\nqueues and completion queues. For devices to scale IOPS\r\nperformance up, a single dispatch queue will result in cross\r\nCPU locking on the dispatch queue lock, much like the previ\u0002ous request queue lock. Providing multiple dispatch queues\r\nsuch that there is a local queue per NUMA node or CPU will\r\nallow NUMA local IO path between applications and hard\u0002ware, decreasing the need for remote memory access across\r\nall subsections of the block layer. In our design we have\r\nmoved IO-scheduling functionality into the software queues\r\nonly, thus even legacy devices that implement just a single\r\ndispatch queue see improved scaling from the new multi\u0002queue block layer.\r\n3.2.3 Tagged IO and IO Accounting\r\nIn addition to introducing a two-level queue based model,\r\nour design incoporates several other implementation improve\u0002ments. First, we introduce tag-based completions within the\r\nblock layer. Device command tagging was first introduced\r\nwith hardware supporting native command queuing. A tag\r\nis an integer value that uniquely identifies the position of the\r\nblock IO in the driver submission queue, so when completed\r\nthe tag is passed back from the device indicating which IO\r\nhas been completed. This eliminates the need to perform a\r\nlinear search of the in-flight window to determine which IO\r\nhas completed.\r\nIn our design, we build upon this tagging notion by al\u0002lowing the block layer to generate a unique tag associated\r\nwith an IO that is inserted into the hardware dispatch queue\r\n(between size 0 and the max dispatch queue size). This tag\r\nis then re-used by the device driver (rather than generating\r\na new one, as with NCQ). Upon completion this same tag\r\ncan then be used by both the device driver and the block\r\nlayer to identify completions without the need for redundant\r\ntagging. While the MQ implementation could maintain a\r\ntraditional in-flight list for legacy drivers, high IOPS drivers\r\nwill likely need to make use of tagged IO to scale well.\r\nSecond, to support fine grained IO accounting we have\r\nmodified the internal Linux accounting library to provide\r\nstatistics for the states of both the software queues and dis-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/e6cd91d4-f14a-4b22-bf1e-14a11aecd44e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9bdbfd5991424d0568a9bd54a33b57f1922f1efdd063ec41a11077fa31e38e0a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 866
      },
      {
        "segments": [
          {
            "segment_id": "e6cd91d4-f14a-4b22-bf1e-14a11aecd44e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 5: Proposed two level Linux block layer de\u0002sign.\r\na 1 million IOPS capable device will cause 31 thousand con\u0002text switches per second simply to process IO in batches of\r\n32. The CPU overhead of issuing IO to devices is inversely\r\nproportional to the amount of IO that is batched in each\r\nsubmission event.\r\n3.2.1 IO-Scheduling\r\nWithin the software queues, IOs can be shaped by per CPU\r\nor NUMA node policies that need not access local memory.\r\nAlternatively, policies may be implemented across software\r\nqueues to maintain global QoS metrics on IO, though at a\r\nperformance penalty. Once the IO has entered the hard\u0002ware dispatch queues, reordering i/Nums no longer possi\u0002ble. We eliminate this possibility so that the only con\u0002tenders for the hardware dispatch queue are inserted to the\r\nhead of the queue and removed from the tail by the device\r\ndriver, thus eliminating lock acquisitions for accounting or\r\nIO-scheduling. This improves the fast path cache locality\r\nwhen issuing IO’s in bulk to the device drivers.\r\nOur design has significant consequences on how IO may be\r\nissued to devices. Instead of inserting requests in the hard\u0002ware queue in sorted order to leverage sequential accesses\r\n(which was a main issue for hard drives), we simply follow\r\na FIFO policy: we insert the incoming block IO submitted\r\nby core i at the top of the request queue attached to core\r\ni or the NUMA socket this core resides on. Traditional IO\u0002schedulers have worked hard to turn random into sequential\r\naccess to optimize performance on traditional hard drives.\r\nOur two level queuing strategy relies on the fact that mod\u0002ern SSD’s have random read and write latency that is as fast\r\nas their sequential access. Thus interleaving IOs from multi\u0002ple software dispatch queues into a single hardware dispatch\r\nqueue does not hurt device performance. Also, by inserting\r\nrequests into the local software request queue, our design\r\nrespects thread locality for IOs and their completion.\r\nWhile global sequential re-ordering is still possible across\r\nthe multiple software queues, it is only necessary for HDD\r\nbased devices, where the additional latency and locking over\u0002head required to achieve total ordering does not hurt IOPS\r\nperformance. It can be argued that, for many users, it is\r\nno longer necessary to employ advanced fairness scheduling\r\nas the speed of the devices are often exceeding the ability\r\nof even multiple applications to saturate their performance.\r\nIf fairness is essential, it is possible to design a scheduler\r\nthat exploits the characteristics of SSDs at coarser granu\u0002larity to achieve lower performance overhead [23, 13, 19].\r\nWhether the scheduler should reside in the block layer or on\r\nthe SSD controller is an open issue. If the SSD is responsible\r\nfor fair IO scheduling, it can leverage internal device paral\u0002lelism, and lower latency, at the cost of additional interface\r\ncomplexity between disk and OS [8, 4].\r\n3.2.2 Number of Hardware Queues\r\nToday, most SATA, SAS and PCI-E based SSDs, support\r\njust a single hardware dispatch queue and a single comple\u0002tion queue using a single interrupt to signal completions.\r\nOne exception is the upcoming NVM Express (NVMe) [18]\r\ninterface which supports a flexible number of submission\r\nqueues and completion queues. For devices to scale IOPS\r\nperformance up, a single dispatch queue will result in cross\r\nCPU locking on the dispatch queue lock, much like the previ\u0002ous request queue lock. Providing multiple dispatch queues\r\nsuch that there is a local queue per NUMA node or CPU will\r\nallow NUMA local IO path between applications and hard\u0002ware, decreasing the need for remote memory access across\r\nall subsections of the block layer. In our design we have\r\nmoved IO-scheduling functionality into the software queues\r\nonly, thus even legacy devices that implement just a single\r\ndispatch queue see improved scaling from the new multi\u0002queue block layer.\r\n3.2.3 Tagged IO and IO Accounting\r\nIn addition to introducing a two-level queue based model,\r\nour design incoporates several other implementation improve\u0002ments. First, we introduce tag-based completions within the\r\nblock layer. Device command tagging was first introduced\r\nwith hardware supporting native command queuing. A tag\r\nis an integer value that uniquely identifies the position of the\r\nblock IO in the driver submission queue, so when completed\r\nthe tag is passed back from the device indicating which IO\r\nhas been completed. This eliminates the need to perform a\r\nlinear search of the in-flight window to determine which IO\r\nhas completed.\r\nIn our design, we build upon this tagging notion by al\u0002lowing the block layer to generate a unique tag associated\r\nwith an IO that is inserted into the hardware dispatch queue\r\n(between size 0 and the max dispatch queue size). This tag\r\nis then re-used by the device driver (rather than generating\r\na new one, as with NCQ). Upon completion this same tag\r\ncan then be used by both the device driver and the block\r\nlayer to identify completions without the need for redundant\r\ntagging. While the MQ implementation could maintain a\r\ntraditional in-flight list for legacy drivers, high IOPS drivers\r\nwill likely need to make use of tagged IO to scale well.\r\nSecond, to support fine grained IO accounting we have\r\nmodified the internal Linux accounting library to provide\r\nstatistics for the states of both the software queues and dis-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/e6cd91d4-f14a-4b22-bf1e-14a11aecd44e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9bdbfd5991424d0568a9bd54a33b57f1922f1efdd063ec41a11077fa31e38e0a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 866
      },
      {
        "segments": [
          {
            "segment_id": "2581a2ef-bfa9-4c49-92d2-041c6fa360aa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "patch queues. We have also modified the existing tracing\r\nand profiling mechanisms in blktrace, to support IO tracing\r\nfor future devices that are multi-queue aware. This will al\u0002low future device manufacturers to optimize their implemen\u0002tations and provide uniform global statistics to HPC cus\u0002tomers whose application performance is increasingly dom\u0002inated by the performance of the IO-subsystem.\r\n3.3 Multiqueue Impact on Device Manufac\u0002turers\r\nOne drawback of the our design is that it will require some\r\nextensions to the bottom edge device driver interface to\r\nachieve optimal performance. While the basic mechanisms\r\nfor driver registration and IO submission/completion remain\r\nunchanged, our design introduces these following require\u0002ments:\r\n• HW dispatch queue registration: The device driver must\r\nexport the number of submission queues that it sup\u0002ports as well as the size of these queues, so that the\r\nblock layer can allocate the matching hardware dis\u0002patch queues.\r\n• HW submission queue mapping function: The device\r\ndriver must export a function that returns a mapping\r\nbetween a given software level queue (associated to\r\ncore i or NUMA node i), and the appropriate hardware\r\ndispatch queue.\r\n• IO tag handling: The device driver tag management\r\nmechanism must be revised so that it accepts tags gen\u0002erated by the block layer. While not strictly required,\r\nusing a single data tag will result in optimal CPU us\u0002age between the device driver and block layer.\r\nThese changes are minimal and can be implemented in\r\nthe software driver, typically requiring no changes to exist\u0002ing hardware or software. While optimal performance will\r\ncome from maintaining multiple hardware submission and\r\ncompletion queues, legacy devices with only a single queue\r\ncan continue to operate under our new Linux block layer\r\nimplementation.\r\n4 Experimental Methodology\r\nIn the remaining of the paper, we denote the existing block\r\nlayer as single queue design (SQ), our design as the multi\u0002queue design (MQ), and a driver which bypasses the Linux\r\nblock layer as Raw. We implemented the MQ block layer as\r\na patch to the Linux kernel 3.103.\r\n4.1 Hardware Platforms\r\nTo conduct these comparisons, we rely on a null device\r\ndriver, i.e., a driver that is not connected to an underly\u0002ing storage device. This null driver simply receives IOs as\r\nfast as possible and acknowledges completion immediately.\r\nThis pseudo block device can acknowledge IO requests faster\r\nthan even a DRAM backed physical device, making the null\r\nblock device an ideal candidate for establishing an optimal\r\nbaseline for scalability and implementation efficiency.\r\nUsing the null block device, we experiment with 1, 2, 4\r\nand 8 sockets systems, i.e., Sandy Bridge-E, Westmere-EP,\r\n3Our implementation is available online at http:\r\n//git.kernel.dk/?p=linux-block.git;a=shortlog;\r\nh=refs/heads/new-queue\r\nPlatform/Intel Sandy\r\nBridge-E\r\nWestmere\u0002EPNehalem\u0002EXWestmere\u0002EX\r\nProcessor i7-3930K X5690 X7560 E7-2870\r\nNum. of Cores 6 12 32 80\r\nSpeed (Ghz) 3.2 3.46 2.66 2.4\r\nL3 Cache (MB) 12 12 24 30\r\nNUMA nodes 1 2 4 8\r\nTable 1: Architecture of Evaluation Systems\r\nNehalem-EX and Westmere-EX Intel platforms. Table 1\r\nsummarizes the characteristics of these four platforms. The\r\n1, 2 and 4-sockets systems use direct QPI links as intercon\u0002nect between sockets, while the 8-nodes system has a lower\r\nand upper CPU board (with 4 sockets each) and an intercon\u0002nect board for communication. We disabled the turbo boost\r\nand hyper-threading CPU features as well as any ACPI C\r\nand P-state throttling on our systems to decrease the vari\u0002ance in our measurements that would be caused by power\r\nsavings features.\r\n4.2 IO Load Generation\r\nWe focus our evaluations on latency and throughput. We\r\nexperiment with latency by issuing a single IO per partici\u0002pating core at a time using the pread/pwrite interface of the\r\nLinux kernel. We experiment with throughput by overlap\u0002ping the submission of asynchronous IOs. In the throughput\r\nexperiment we sustain 32 outstanding IOs per participating\r\ncore, i.e., if 8 cores are issuing IOs, then we maintain 256\r\noutstanding IOs. We use 32 IOs per process context because\r\nit matches the requirements of today’s SSD devices. Our IO\u0002load is generated using the flexible io generator (fio) [14] that\r\nallows us to carefully control the queue-depth, type, and dis\u0002tribution of IO onto a the LBA space of the block device. In\r\nall experiments we use 512 bytes read IO’s, though the type\r\nof IO is largely irrelevant since the null block driver does\r\nnot perform any computation or data transfer, it simply ac\u0002knowledges all requests immediately.\r\n4.3 Performance Metrics\r\nThe primary metrics for our experiments are absolute through\u0002put (IOPS) and latency (µ-seconds) of the block layer.\r\n5 Results\r\nIn a first phase, we compare our new block layer design\r\n(MQ) with the existing Linux block layer (SQ), and the op\u0002timal baseline (Raw). In a second phase, we investigate how\r\nour design allows the block layer to scale as the number of\r\navailable cores in the system increases. We leave a perfor\u0002mance tuning study of MQ (e.g., quality of the performance\r\noptimizations within the block layer) as a topic for future\r\nwork.\r\nFor each system configuration, we create as many fio pro\u0002cesses as there are cores and we ensure that all cores are\r\nutilized 100%. For the 1 socket system, the maximum num\u0002ber of cores is 6. For the 2 (resp., 4 and 8) sockets system,\r\nthe maximum number of core is 12 (resp., 32 and 80), and\r\nwe mark the separation between both 6 (resp., 8 and 10)\r\ncores sockets with a vertical dotted line. Unless otherwise\r\nnoted, for MQ, a software queue is associated to each core\r\nand a hardware dispatch queue is associated to each socket.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/2581a2ef-bfa9-4c49-92d2-041c6fa360aa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1803803664ac11e86c807e06f5c7b5f3b268c247db8750ac856bafa2f04eef06",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 903
      },
      {
        "segments": [
          {
            "segment_id": "2581a2ef-bfa9-4c49-92d2-041c6fa360aa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "patch queues. We have also modified the existing tracing\r\nand profiling mechanisms in blktrace, to support IO tracing\r\nfor future devices that are multi-queue aware. This will al\u0002low future device manufacturers to optimize their implemen\u0002tations and provide uniform global statistics to HPC cus\u0002tomers whose application performance is increasingly dom\u0002inated by the performance of the IO-subsystem.\r\n3.3 Multiqueue Impact on Device Manufac\u0002turers\r\nOne drawback of the our design is that it will require some\r\nextensions to the bottom edge device driver interface to\r\nachieve optimal performance. While the basic mechanisms\r\nfor driver registration and IO submission/completion remain\r\nunchanged, our design introduces these following require\u0002ments:\r\n• HW dispatch queue registration: The device driver must\r\nexport the number of submission queues that it sup\u0002ports as well as the size of these queues, so that the\r\nblock layer can allocate the matching hardware dis\u0002patch queues.\r\n• HW submission queue mapping function: The device\r\ndriver must export a function that returns a mapping\r\nbetween a given software level queue (associated to\r\ncore i or NUMA node i), and the appropriate hardware\r\ndispatch queue.\r\n• IO tag handling: The device driver tag management\r\nmechanism must be revised so that it accepts tags gen\u0002erated by the block layer. While not strictly required,\r\nusing a single data tag will result in optimal CPU us\u0002age between the device driver and block layer.\r\nThese changes are minimal and can be implemented in\r\nthe software driver, typically requiring no changes to exist\u0002ing hardware or software. While optimal performance will\r\ncome from maintaining multiple hardware submission and\r\ncompletion queues, legacy devices with only a single queue\r\ncan continue to operate under our new Linux block layer\r\nimplementation.\r\n4 Experimental Methodology\r\nIn the remaining of the paper, we denote the existing block\r\nlayer as single queue design (SQ), our design as the multi\u0002queue design (MQ), and a driver which bypasses the Linux\r\nblock layer as Raw. We implemented the MQ block layer as\r\na patch to the Linux kernel 3.103.\r\n4.1 Hardware Platforms\r\nTo conduct these comparisons, we rely on a null device\r\ndriver, i.e., a driver that is not connected to an underly\u0002ing storage device. This null driver simply receives IOs as\r\nfast as possible and acknowledges completion immediately.\r\nThis pseudo block device can acknowledge IO requests faster\r\nthan even a DRAM backed physical device, making the null\r\nblock device an ideal candidate for establishing an optimal\r\nbaseline for scalability and implementation efficiency.\r\nUsing the null block device, we experiment with 1, 2, 4\r\nand 8 sockets systems, i.e., Sandy Bridge-E, Westmere-EP,\r\n3Our implementation is available online at http:\r\n//git.kernel.dk/?p=linux-block.git;a=shortlog;\r\nh=refs/heads/new-queue\r\nPlatform/Intel Sandy\r\nBridge-E\r\nWestmere\u0002EPNehalem\u0002EXWestmere\u0002EX\r\nProcessor i7-3930K X5690 X7560 E7-2870\r\nNum. of Cores 6 12 32 80\r\nSpeed (Ghz) 3.2 3.46 2.66 2.4\r\nL3 Cache (MB) 12 12 24 30\r\nNUMA nodes 1 2 4 8\r\nTable 1: Architecture of Evaluation Systems\r\nNehalem-EX and Westmere-EX Intel platforms. Table 1\r\nsummarizes the characteristics of these four platforms. The\r\n1, 2 and 4-sockets systems use direct QPI links as intercon\u0002nect between sockets, while the 8-nodes system has a lower\r\nand upper CPU board (with 4 sockets each) and an intercon\u0002nect board for communication. We disabled the turbo boost\r\nand hyper-threading CPU features as well as any ACPI C\r\nand P-state throttling on our systems to decrease the vari\u0002ance in our measurements that would be caused by power\r\nsavings features.\r\n4.2 IO Load Generation\r\nWe focus our evaluations on latency and throughput. We\r\nexperiment with latency by issuing a single IO per partici\u0002pating core at a time using the pread/pwrite interface of the\r\nLinux kernel. We experiment with throughput by overlap\u0002ping the submission of asynchronous IOs. In the throughput\r\nexperiment we sustain 32 outstanding IOs per participating\r\ncore, i.e., if 8 cores are issuing IOs, then we maintain 256\r\noutstanding IOs. We use 32 IOs per process context because\r\nit matches the requirements of today’s SSD devices. Our IO\u0002load is generated using the flexible io generator (fio) [14] that\r\nallows us to carefully control the queue-depth, type, and dis\u0002tribution of IO onto a the LBA space of the block device. In\r\nall experiments we use 512 bytes read IO’s, though the type\r\nof IO is largely irrelevant since the null block driver does\r\nnot perform any computation or data transfer, it simply ac\u0002knowledges all requests immediately.\r\n4.3 Performance Metrics\r\nThe primary metrics for our experiments are absolute through\u0002put (IOPS) and latency (µ-seconds) of the block layer.\r\n5 Results\r\nIn a first phase, we compare our new block layer design\r\n(MQ) with the existing Linux block layer (SQ), and the op\u0002timal baseline (Raw). In a second phase, we investigate how\r\nour design allows the block layer to scale as the number of\r\navailable cores in the system increases. We leave a perfor\u0002mance tuning study of MQ (e.g., quality of the performance\r\noptimizations within the block layer) as a topic for future\r\nwork.\r\nFor each system configuration, we create as many fio pro\u0002cesses as there are cores and we ensure that all cores are\r\nutilized 100%. For the 1 socket system, the maximum num\u0002ber of cores is 6. For the 2 (resp., 4 and 8) sockets system,\r\nthe maximum number of core is 12 (resp., 32 and 80), and\r\nwe mark the separation between both 6 (resp., 8 and 10)\r\ncores sockets with a vertical dotted line. Unless otherwise\r\nnoted, for MQ, a software queue is associated to each core\r\nand a hardware dispatch queue is associated to each socket.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/2581a2ef-bfa9-4c49-92d2-041c6fa360aa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1803803664ac11e86c807e06f5c7b5f3b268c247db8750ac856bafa2f04eef06",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 903
      },
      {
        "segments": [
          {
            "segment_id": "9de31ef1-9fcb-425e-9475-34a19f45dc40",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "Number of Cores\r\nIOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\n1 2 3 4 5 6\r\n1 socket\r\n2 4 6 8 10 12\r\n2 socket 4 socket\r\n5 10 15 20 25 30\r\n8 socket\r\n10 20 30 40 50 60 70 80\r\nMQ\r\nSQ\r\nRaw\r\nFigure 6: IOPS for single/multi-queue and raw on the 1, 2, 4 and 8-nodes systems.\r\n5.1 Comparing MQ, SQ and Raw\r\nFigure 6 presents throughput (in IOPS) for SQ, MQ and\r\nRaw as a function of the number of cores available in the 1\r\nsocket, 2-sockets, 4-sockets, and 8-sockets systems respec\u0002tively. Overall, we can make the following observations.\r\nFirst, with the single queue block layer implementation,\r\nthroughput is limited below 1 million IOPS regardless of\r\nthe number of CPUs issuing IO or of the number of sockets\r\nin the system. The current Linux block layer implementa\u0002tion can not sustain more than 1 million IOPS on a single\r\nblock device.\r\nSecond, our new two layer multi-queue implementation\r\nimproves performance significantly. The system can sustain\r\nup to 3.5 million IOPS on a single socket system. However,\r\nin multi-socket systems scaling does not continue at nearly\r\nthe same rate.\r\nLet us analyze those results in more details:\r\n1. The scalability problems of SQ are evident as soon as\r\nmore than one core is used in the system. Additional\r\ncores spend most of their cycles acquiring and releasing\r\nspin locks for the single request queue and as such do\r\nnot contribute to improving throughput. This prob\u0002lem gets even worse on multi-socket systems, because\r\ntheir inter-connects and the need to maintain cache\u0002coherence.\r\n2. MQ performance is similar to SQ performance on a sin\u0002gle core. This shows that the overhead of introducing\r\nmultiple queues is minimal.\r\n3. MQ scales linearly within one socket. This is because\r\nwe removed the need for the block layer to rely on syn\u0002chronization between cores when block IOs are manip\u0002ulated inside the software level queues.\r\n4. For all systems, MQ follows the performance of Raw\r\nclosely. MQ is in fact a constant factor away from the\r\nraw measurements, respectively 22%, 19%, 13% and\r\n32% for the 1, 2, 4, 8-sockets systems. This overhead\r\nmight seem large, but the raw baseline does not im\u0002plement logic that is required in a real device driver.\r\nFor good scalability, the MQ performance just needs\r\nto follow the trend of the baseline.\r\n5. The scalability of MQ and raw exhibits a sharp dip\r\nwhen the number of sockets is higher than 1. We see\r\nthat throughput reaches 5 million IOPS (resp., 3.8 and\r\n4) for 6 cores (resp., 7 and 9) on a 2 sockets system\r\n(resp., 4 and 8 sockets system). This is far from the\r\n1 socket 2 sockets 4 sockets 8 sockets\r\nSQ 50 ms 50 ms 250 ms 750 ms\r\nMQ 50 ms 50 ms 50 ms 250 ms\r\nRaw 50 ms 50 ms 50 ms 250 ms\r\nTable 2: Maximum latency for each of the systems.\r\n10 million IOPS that we could have hoped for. Inter\u0002estingly, MQ follows roughly the raw baseline. There\r\nis thus a problem of scalability, whose root lies outside\r\nthe block layer, that has a significant impact on perfor\u0002mance. We focus on this problem in the next Section.\r\nLet us now turn our attention to latency. As we explained\r\nin the previous section, latency is measured through syn\u0002chronous IOs (with a single outstanding IO per participat\u0002ing core). The latency is measured as the time it takes to\r\ngo from the application, through the kernel system call, into\r\nthe block layer and driver and back again. Figure 7 shows\r\naverage latency (in µ-seconds) as a function of the number\r\nof cores available for the four systems that we study.\r\nIdeally, latency remains low regardless of the number of\r\ncores. In fact, remote memory accesses contribute to in\u0002crease latency on multi-sockets systems. For SQ, we observe\r\nthat latency increases linearly with the number of cores,\r\nslowly within one socket, and sharply when more than one\r\nsocket is active. For MQ, latency remains an order of mag\u0002nitude lower than for SQ. This is because, for MQ, the only\r\nremote memory accesses that are needed are those concern\u0002ing the hardware dispatch queue (there is no remote memory\r\naccesses for synchronizing the software level queues). Note\r\nthat, on 8 sockets system, the SQ graph illustrates the per\u0002formance penalty which is incurred when crossing the inter\u0002connect board (whenever 2, 4, 6 and 8 sockets are involved).\r\nTable 2 shows the maximum latency across all experi\u0002ments. With SQ, the maximum latency reaches 250 millisec\u0002onds in the 4 sockets system and 750 milliseconds on the 8\r\nsockets system. Interestingly, with SQ on a 8 sockets sys\u0002tems, 20% of the IO requests take more than 1 millisecond to\r\ncomplete. This is a very significant source of variability for\r\nIO performance. In contrast, with MQ, the number of IOs\r\nwhich take more than 1ms to complete only reaches 0.15%\r\nfor an 8 socket system, while it is below 0.01% for the other\r\nsystems. Note Raw exhibits minimal, but stable, variation\r\nacross all systems with around 0.02% of the IOs that take\r\nmore than 1ms to complete.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/9de31ef1-9fcb-425e-9475-34a19f45dc40.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2a8dadbef31a5d3d7bf266a1c3d0ba592582fcbf7b906a9a09afc08e43a8091a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 857
      },
      {
        "segments": [
          {
            "segment_id": "9de31ef1-9fcb-425e-9475-34a19f45dc40",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "Number of Cores\r\nIOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\n1 2 3 4 5 6\r\n1 socket\r\n2 4 6 8 10 12\r\n2 socket 4 socket\r\n5 10 15 20 25 30\r\n8 socket\r\n10 20 30 40 50 60 70 80\r\nMQ\r\nSQ\r\nRaw\r\nFigure 6: IOPS for single/multi-queue and raw on the 1, 2, 4 and 8-nodes systems.\r\n5.1 Comparing MQ, SQ and Raw\r\nFigure 6 presents throughput (in IOPS) for SQ, MQ and\r\nRaw as a function of the number of cores available in the 1\r\nsocket, 2-sockets, 4-sockets, and 8-sockets systems respec\u0002tively. Overall, we can make the following observations.\r\nFirst, with the single queue block layer implementation,\r\nthroughput is limited below 1 million IOPS regardless of\r\nthe number of CPUs issuing IO or of the number of sockets\r\nin the system. The current Linux block layer implementa\u0002tion can not sustain more than 1 million IOPS on a single\r\nblock device.\r\nSecond, our new two layer multi-queue implementation\r\nimproves performance significantly. The system can sustain\r\nup to 3.5 million IOPS on a single socket system. However,\r\nin multi-socket systems scaling does not continue at nearly\r\nthe same rate.\r\nLet us analyze those results in more details:\r\n1. The scalability problems of SQ are evident as soon as\r\nmore than one core is used in the system. Additional\r\ncores spend most of their cycles acquiring and releasing\r\nspin locks for the single request queue and as such do\r\nnot contribute to improving throughput. This prob\u0002lem gets even worse on multi-socket systems, because\r\ntheir inter-connects and the need to maintain cache\u0002coherence.\r\n2. MQ performance is similar to SQ performance on a sin\u0002gle core. This shows that the overhead of introducing\r\nmultiple queues is minimal.\r\n3. MQ scales linearly within one socket. This is because\r\nwe removed the need for the block layer to rely on syn\u0002chronization between cores when block IOs are manip\u0002ulated inside the software level queues.\r\n4. For all systems, MQ follows the performance of Raw\r\nclosely. MQ is in fact a constant factor away from the\r\nraw measurements, respectively 22%, 19%, 13% and\r\n32% for the 1, 2, 4, 8-sockets systems. This overhead\r\nmight seem large, but the raw baseline does not im\u0002plement logic that is required in a real device driver.\r\nFor good scalability, the MQ performance just needs\r\nto follow the trend of the baseline.\r\n5. The scalability of MQ and raw exhibits a sharp dip\r\nwhen the number of sockets is higher than 1. We see\r\nthat throughput reaches 5 million IOPS (resp., 3.8 and\r\n4) for 6 cores (resp., 7 and 9) on a 2 sockets system\r\n(resp., 4 and 8 sockets system). This is far from the\r\n1 socket 2 sockets 4 sockets 8 sockets\r\nSQ 50 ms 50 ms 250 ms 750 ms\r\nMQ 50 ms 50 ms 50 ms 250 ms\r\nRaw 50 ms 50 ms 50 ms 250 ms\r\nTable 2: Maximum latency for each of the systems.\r\n10 million IOPS that we could have hoped for. Inter\u0002estingly, MQ follows roughly the raw baseline. There\r\nis thus a problem of scalability, whose root lies outside\r\nthe block layer, that has a significant impact on perfor\u0002mance. We focus on this problem in the next Section.\r\nLet us now turn our attention to latency. As we explained\r\nin the previous section, latency is measured through syn\u0002chronous IOs (with a single outstanding IO per participat\u0002ing core). The latency is measured as the time it takes to\r\ngo from the application, through the kernel system call, into\r\nthe block layer and driver and back again. Figure 7 shows\r\naverage latency (in µ-seconds) as a function of the number\r\nof cores available for the four systems that we study.\r\nIdeally, latency remains low regardless of the number of\r\ncores. In fact, remote memory accesses contribute to in\u0002crease latency on multi-sockets systems. For SQ, we observe\r\nthat latency increases linearly with the number of cores,\r\nslowly within one socket, and sharply when more than one\r\nsocket is active. For MQ, latency remains an order of mag\u0002nitude lower than for SQ. This is because, for MQ, the only\r\nremote memory accesses that are needed are those concern\u0002ing the hardware dispatch queue (there is no remote memory\r\naccesses for synchronizing the software level queues). Note\r\nthat, on 8 sockets system, the SQ graph illustrates the per\u0002formance penalty which is incurred when crossing the inter\u0002connect board (whenever 2, 4, 6 and 8 sockets are involved).\r\nTable 2 shows the maximum latency across all experi\u0002ments. With SQ, the maximum latency reaches 250 millisec\u0002onds in the 4 sockets system and 750 milliseconds on the 8\r\nsockets system. Interestingly, with SQ on a 8 sockets sys\u0002tems, 20% of the IO requests take more than 1 millisecond to\r\ncomplete. This is a very significant source of variability for\r\nIO performance. In contrast, with MQ, the number of IOs\r\nwhich take more than 1ms to complete only reaches 0.15%\r\nfor an 8 socket system, while it is below 0.01% for the other\r\nsystems. Note Raw exhibits minimal, but stable, variation\r\nacross all systems with around 0.02% of the IOs that take\r\nmore than 1ms to complete.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/9de31ef1-9fcb-425e-9475-34a19f45dc40.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2a8dadbef31a5d3d7bf266a1c3d0ba592582fcbf7b906a9a09afc08e43a8091a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 857
      },
      {
        "segments": [
          {
            "segment_id": "7104fc13-c63b-4b79-970e-cb5d4b32f6cd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Number of Cores\r\n1 socket\r\nLatency (us)\r\n1\r\n10\r\n100\r\n1k\r\n10k\r\n1 2 3 4 5 6\r\nSQ\r\nMQ\r\nRaw\r\n2 socket\r\n2 4 6 8 10 12\r\n4 socket\r\n5 10 15 20 25 30 10 20 30 40 50 60 70 80\r\n8 socket\r\nFigure 7: Latency on the 1, 2, 4 and 8 node system using the null device.\r\nIOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\nNumber of Cores\r\n10 20 30 40 50 60 70 80\r\nRaw MQ\r\nRaw (Original)\r\nMQ (Original)\r\nFigure 8: IOPS for MQ and raw with libaio fixes\r\napplied on the 8-nodes systems.\r\n5.2 Improving Application Level IO Submis\u0002sion\r\nThe throughput graphs from the previous Section exposed\r\nscalability problem within the Linux stack, on top of the\r\nblock layer. Through profiling we were able to determine\r\nthat the asynchronous (libaio) and direct IO layers, used\r\nwithin the kernel to transfer block IOs from userspace into\r\nto the block layer, have several bottlenecks that have are\r\nfirst being exposed with the new MQ block layer implemen\u0002tation. These bottlenecks are: (i) a context list lock is is\u0002sued for each request, (ii) a completion ring in libaio used\r\nto manage sleep/wakeup cycles and (iii) a number of shared\r\nvariables are being updated throughout the library. We re\u0002moved these bottlenecks through a series of implementation\r\nimprovements.\r\nFirst, we replaced the context list lock with a lockless list,\r\nwhich instead of using mutexes to update a variable used the\r\ncompare-and-swap instruction of the processor to perform\r\nthe update. Second, we eliminated the use of the completion\r\nring as it caused an extra lock access when updating the\r\nnumber of elements in the completion list, which in the worst\r\ncase, could put the application process to sleep. Third, we\r\nused atomic compare-and-swap instructions to manipulate\r\nthe shared counters for internal data structures (e.g. the\r\nnumber of users of AIO context) instead of the native mutex\r\nstructures.\r\nFigure 8 demonstrates the IOPS of the raw and MQ de\u0002signs using this new userspace IO submission library, on the\r\n8-socket system, which has the hardest time maintaining IO\r\nscalability. We observe that both the MQ and Raw imple\u0002mentations, while still losing efficiency when moving to a\r\nsecond socket, are able to scale IOPS near linearly up to the\r\nmaximum number of cores within the system. The multi\u0002queue design proposed here allows the block layer to scale up\r\nIOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\nNumber of Cores\r\n10 20 30 40 50 60 70 80\r\nPer-core\r\nPer-node\r\nSingle\r\nFigure 9: IOPS for a single software queue with var\u0002ied number of mapped hardware dispatch queues on\r\nthe 8 socket system.\r\nto 10 million IOPS utilizing 70s cores on an 8 socket NUMA\r\nsystem while maintaining the conveniences of the block layer\r\nimplementation for application compatibility. We recognize\r\nthat the efficiency of the MQ (and Raw) implementations\r\ndrops significantly when moving from one socket onto a sec\u0002ond. This indicates that there are further bottlenecks to be\r\nimproved upon in Linux that lay outside the block layer.\r\nPossible candidates are interrupt handling, context switch\u0002ing improvements, and other core OS functions that we leave\r\nfor future work.\r\n5.3 Comparing Allocation of Software and\r\nHardware Dispatch Queues\r\nAs our design introduces two levels of queues (the soft\u0002ware and hardware dispatch queues), we must investigate\r\nhow number of queues defined for each level impacts perfor\u0002mance. We proceed in two steps. First, we fix the number\r\nof software level queues to one and we vary the number\r\nof hardware dispatch queues. Second, we fix the number\r\nof software level queues to one per core and we vary the\r\nnumber of hardware dispatch queues. In both experiments,\r\nthe number of hardware dispatch queues is either one, one\r\nper core (denoted per-core) or one per socket (denoted per\u0002socket). All experiments with the MQ block layer on on the\r\n8-socket system.\r\nFigure 9 presents throughput using a single software queue.\r\nWe observe that all configurations show a sharp performance\r\ndip when the second socket is introduced. Furthermore, we\r\nobserve that a single software and hardware dispatch queue\r\nperform significantly worse on the second socket, but follow\r\neach other when entering the third socket. Overall, this ex\u0002periment shows that a single software queue does not allow\r\nthe block layer to scape gracefully.\r\nWe show in Figure 10 the results of our experiments with",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/7104fc13-c63b-4b79-970e-cb5d4b32f6cd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a97e125aea7632671ef69b8f07be2fc6507403c2c6ce3f2b6b4be0221edf77e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 724
      },
      {
        "segments": [
          {
            "segment_id": "7104fc13-c63b-4b79-970e-cb5d4b32f6cd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Number of Cores\r\n1 socket\r\nLatency (us)\r\n1\r\n10\r\n100\r\n1k\r\n10k\r\n1 2 3 4 5 6\r\nSQ\r\nMQ\r\nRaw\r\n2 socket\r\n2 4 6 8 10 12\r\n4 socket\r\n5 10 15 20 25 30 10 20 30 40 50 60 70 80\r\n8 socket\r\nFigure 7: Latency on the 1, 2, 4 and 8 node system using the null device.\r\nIOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\nNumber of Cores\r\n10 20 30 40 50 60 70 80\r\nRaw MQ\r\nRaw (Original)\r\nMQ (Original)\r\nFigure 8: IOPS for MQ and raw with libaio fixes\r\napplied on the 8-nodes systems.\r\n5.2 Improving Application Level IO Submis\u0002sion\r\nThe throughput graphs from the previous Section exposed\r\nscalability problem within the Linux stack, on top of the\r\nblock layer. Through profiling we were able to determine\r\nthat the asynchronous (libaio) and direct IO layers, used\r\nwithin the kernel to transfer block IOs from userspace into\r\nto the block layer, have several bottlenecks that have are\r\nfirst being exposed with the new MQ block layer implemen\u0002tation. These bottlenecks are: (i) a context list lock is is\u0002sued for each request, (ii) a completion ring in libaio used\r\nto manage sleep/wakeup cycles and (iii) a number of shared\r\nvariables are being updated throughout the library. We re\u0002moved these bottlenecks through a series of implementation\r\nimprovements.\r\nFirst, we replaced the context list lock with a lockless list,\r\nwhich instead of using mutexes to update a variable used the\r\ncompare-and-swap instruction of the processor to perform\r\nthe update. Second, we eliminated the use of the completion\r\nring as it caused an extra lock access when updating the\r\nnumber of elements in the completion list, which in the worst\r\ncase, could put the application process to sleep. Third, we\r\nused atomic compare-and-swap instructions to manipulate\r\nthe shared counters for internal data structures (e.g. the\r\nnumber of users of AIO context) instead of the native mutex\r\nstructures.\r\nFigure 8 demonstrates the IOPS of the raw and MQ de\u0002signs using this new userspace IO submission library, on the\r\n8-socket system, which has the hardest time maintaining IO\r\nscalability. We observe that both the MQ and Raw imple\u0002mentations, while still losing efficiency when moving to a\r\nsecond socket, are able to scale IOPS near linearly up to the\r\nmaximum number of cores within the system. The multi\u0002queue design proposed here allows the block layer to scale up\r\nIOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\nNumber of Cores\r\n10 20 30 40 50 60 70 80\r\nPer-core\r\nPer-node\r\nSingle\r\nFigure 9: IOPS for a single software queue with var\u0002ied number of mapped hardware dispatch queues on\r\nthe 8 socket system.\r\nto 10 million IOPS utilizing 70s cores on an 8 socket NUMA\r\nsystem while maintaining the conveniences of the block layer\r\nimplementation for application compatibility. We recognize\r\nthat the efficiency of the MQ (and Raw) implementations\r\ndrops significantly when moving from one socket onto a sec\u0002ond. This indicates that there are further bottlenecks to be\r\nimproved upon in Linux that lay outside the block layer.\r\nPossible candidates are interrupt handling, context switch\u0002ing improvements, and other core OS functions that we leave\r\nfor future work.\r\n5.3 Comparing Allocation of Software and\r\nHardware Dispatch Queues\r\nAs our design introduces two levels of queues (the soft\u0002ware and hardware dispatch queues), we must investigate\r\nhow number of queues defined for each level impacts perfor\u0002mance. We proceed in two steps. First, we fix the number\r\nof software level queues to one and we vary the number\r\nof hardware dispatch queues. Second, we fix the number\r\nof software level queues to one per core and we vary the\r\nnumber of hardware dispatch queues. In both experiments,\r\nthe number of hardware dispatch queues is either one, one\r\nper core (denoted per-core) or one per socket (denoted per\u0002socket). All experiments with the MQ block layer on on the\r\n8-socket system.\r\nFigure 9 presents throughput using a single software queue.\r\nWe observe that all configurations show a sharp performance\r\ndip when the second socket is introduced. Furthermore, we\r\nobserve that a single software and hardware dispatch queue\r\nperform significantly worse on the second socket, but follow\r\neach other when entering the third socket. Overall, this ex\u0002periment shows that a single software queue does not allow\r\nthe block layer to scape gracefully.\r\nWe show in Figure 10 the results of our experiments with",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/7104fc13-c63b-4b79-970e-cb5d4b32f6cd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a97e125aea7632671ef69b8f07be2fc6507403c2c6ce3f2b6b4be0221edf77e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 724
      },
      {
        "segments": [
          {
            "segment_id": "077c8fb9-3fdc-4019-9c8a-56a69d71f236",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "IOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\nNumber of Cores\r\n10 20 30 40 50 60 70 80\r\nPer-core\r\nPer-node\r\nSingle\r\nFigure 10: IOPS for per-core software queue with a\r\ndifferent number of hardware dispatch queues.\r\na software queue per core. We see that combining multiple\r\nsoftware and hardware dispatch queues enable high perfor\u0002mance, reaching more than 15 million IOPS using the least\r\ncontended per-core/per-core queue mapping. The per-node\r\nhardware dispatch queue configuration also scales well up\r\nto the fifth socket, but then the per-node slowly decrease\r\nin throughput. This occur when the socket interconnect\r\nbecomes the bottleneck. Further performance is possible\r\nas more processes are added, but they slightly suffer from\r\nless available bandwidth. To achieve the highest through\u0002put, per-core queues for both software and hardware dis\u0002patch queues are advised. This is easily implemented on the\r\nsoftware queue side, while hardware queues must be imple\u0002mented by the device itself. Hardware vendors can restrict\r\nthe number of hardware queues to the system sockets avail\u0002able and still provide scalable performance.\r\n6 Related Work\r\nOur redesign of the block layer touch on network, hardware\r\ninterfaces and NUMA systems. Below we describe related\r\nwork in each of these fields.\r\n6.1 Network\r\nThe scalability of operating system network stacks has been\r\naddressed in the last 10 years by incorporating multiple\r\nsender and receiver queues within a single card to allow a\r\nproliferation of network ports [1, 21]. This allows a sin\u0002gle driver to manage multiple hardware devices and reduce\r\ncode and data structure duplication for common function\u0002ality. Our work builds upon the foundation of networking\r\nmulti-queue designs by allowing a single interface point, the\r\nblock device, with multiple queues within the software stack.\r\nOptimization of the kernel stack itself, with the purpose of\r\nremoving IO bottlenecks, has been studied using the Mon\u0002eta platform [7]. Caulfield et al. propose to bypass the\r\nblock layer and implement their own driver and single queue\r\nmechanism to increase performance. By bypassing the block\r\nlayer, each thread issues an IO and deals with its comple\u0002tion. Our approach is different, as we propose to redesign\r\nthe block layer thus improving performance across all de\u0002vices, for all applications.\r\n6.2 Hardware Interface\r\nThe NVMe interface [18] attempts to address many of the\r\nscalability problems within the block layer implementation.\r\nNVMe however proposes a new dynamic interface to accom\u0002modate the increased parallelism in NVM storage on which\r\neach process has its own submission and completion queue\r\nto a NVM storage device. While this is excellent for scalabil\u0002ity, it requires application modification and pushes much of\r\nthe complexity of maintaining storage synchronization out\r\nof the operating system into the application. This also ex\u0002poses security risks to the application such as denial of ser\u0002vice without a central trusted arbitrar of device access.\r\n6.3 NUMA\r\nThe affect of NUMA designs on parallel applications has\r\nbeen studied heavily in the HPC space [22, 30, 15, 20] and\r\nbig data communities [29, 3, 5]. We find that many of\r\nthese observations, disruptive interrupts, cache locality, and\r\nlock-contention have the same negative performance penalty\r\nwithin the operating system and block layer. Unfortunately,\r\nas a common implementation for all applications, some tech\u0002niques to avoid lock contention such as message passing sim\u0002ply are in-feasible to retrofit into a production operating\r\nsystem built around shared memory semantics.\r\nOne approach to improving IO performance is to access\r\ndevices via memory-mapped IO. While this does save some\r\nsystem call overhead, this does not fundamentally change or\r\nimprove the scalability of the operating system block layer.\r\nAdditionally, it introduces a non-powercut safe fault domain\r\n(the DRAM page-cache) that applications may be un-aware\r\nof while simultaneously requiring a large application re-write\r\nto take leverage.\r\n7 Conclusions and Future Work\r\nIn this paper, we have established that the current design\r\nof the Linux block layer does not scale beyond one million\r\nIOPS per device. This is sufficient for today’s SSD, but not\r\nfor tomorrow’s. We proposed a new design for the Linux\r\nblock layer. This design is based on two levels of queues in\r\norder to reduce contention and promote thread locality. Our\r\nexperiments have shown the superiority of our design and its\r\nscalability on multi-socket systems. Our multiqueue design\r\nleverages the new capabilities of NVM-Express or high-end\r\nPCI-E devices, while still providing the common interface\r\nand convenience features of the block layer.\r\nWe exposed limitations of the Linux IO stack beyond the\r\nblock layer. Locating and removing those additional bottle\u0002necks is a topic for future work. Future work also includes\r\nperformance tuning with multiple hardware queues, and ex\u0002periments with multiqueue capable hardware prototypes. As\r\none bottleneck is removed, a new choke point is quickly cre\u0002ated, creating an application through device NUMA-local\r\nIO-stack is an on-going process. We intend to work with\r\ndevice manufacturers and standards bodies to ratify the in\u0002clusion of hardware capabilities that will encourage adoption\r\nof the multiqueue interface and finalize this new block layer\r\nimplementation for possible inclusion in the mainline Linux\r\nkernel.\r\n8 References\r\n[1] Improving network performance in multi-core systems.\r\nIntel Corporation, 2007.\r\n[2] J. Axboe. Linux Block IO present and future. Ottawa\r\nLinux Symposium, 2004.\r\n[3] A. Baumann, P. Barham, P.-E. Dagand, T. Harris,\r\nR. Isaacs, S. Peter, T. Roscoe, A. Schupbach, and\r\nS. Akhilesh. The multikernel: a new OS architecture\r\nfor scalable multicore systems. Symposium on\r\nOperating Systems Principles, 2009.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/077c8fb9-3fdc-4019-9c8a-56a69d71f236.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=747968784a1265d74edaf0d29845506b0853942b2a466c707a912e733d1a9d6e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 882
      },
      {
        "segments": [
          {
            "segment_id": "077c8fb9-3fdc-4019-9c8a-56a69d71f236",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "IOPS\r\n0\r\n2.5M\r\n5M\r\n7.5M\r\n10M\r\n12.5M\r\n15M\r\nNumber of Cores\r\n10 20 30 40 50 60 70 80\r\nPer-core\r\nPer-node\r\nSingle\r\nFigure 10: IOPS for per-core software queue with a\r\ndifferent number of hardware dispatch queues.\r\na software queue per core. We see that combining multiple\r\nsoftware and hardware dispatch queues enable high perfor\u0002mance, reaching more than 15 million IOPS using the least\r\ncontended per-core/per-core queue mapping. The per-node\r\nhardware dispatch queue configuration also scales well up\r\nto the fifth socket, but then the per-node slowly decrease\r\nin throughput. This occur when the socket interconnect\r\nbecomes the bottleneck. Further performance is possible\r\nas more processes are added, but they slightly suffer from\r\nless available bandwidth. To achieve the highest through\u0002put, per-core queues for both software and hardware dis\u0002patch queues are advised. This is easily implemented on the\r\nsoftware queue side, while hardware queues must be imple\u0002mented by the device itself. Hardware vendors can restrict\r\nthe number of hardware queues to the system sockets avail\u0002able and still provide scalable performance.\r\n6 Related Work\r\nOur redesign of the block layer touch on network, hardware\r\ninterfaces and NUMA systems. Below we describe related\r\nwork in each of these fields.\r\n6.1 Network\r\nThe scalability of operating system network stacks has been\r\naddressed in the last 10 years by incorporating multiple\r\nsender and receiver queues within a single card to allow a\r\nproliferation of network ports [1, 21]. This allows a sin\u0002gle driver to manage multiple hardware devices and reduce\r\ncode and data structure duplication for common function\u0002ality. Our work builds upon the foundation of networking\r\nmulti-queue designs by allowing a single interface point, the\r\nblock device, with multiple queues within the software stack.\r\nOptimization of the kernel stack itself, with the purpose of\r\nremoving IO bottlenecks, has been studied using the Mon\u0002eta platform [7]. Caulfield et al. propose to bypass the\r\nblock layer and implement their own driver and single queue\r\nmechanism to increase performance. By bypassing the block\r\nlayer, each thread issues an IO and deals with its comple\u0002tion. Our approach is different, as we propose to redesign\r\nthe block layer thus improving performance across all de\u0002vices, for all applications.\r\n6.2 Hardware Interface\r\nThe NVMe interface [18] attempts to address many of the\r\nscalability problems within the block layer implementation.\r\nNVMe however proposes a new dynamic interface to accom\u0002modate the increased parallelism in NVM storage on which\r\neach process has its own submission and completion queue\r\nto a NVM storage device. While this is excellent for scalabil\u0002ity, it requires application modification and pushes much of\r\nthe complexity of maintaining storage synchronization out\r\nof the operating system into the application. This also ex\u0002poses security risks to the application such as denial of ser\u0002vice without a central trusted arbitrar of device access.\r\n6.3 NUMA\r\nThe affect of NUMA designs on parallel applications has\r\nbeen studied heavily in the HPC space [22, 30, 15, 20] and\r\nbig data communities [29, 3, 5]. We find that many of\r\nthese observations, disruptive interrupts, cache locality, and\r\nlock-contention have the same negative performance penalty\r\nwithin the operating system and block layer. Unfortunately,\r\nas a common implementation for all applications, some tech\u0002niques to avoid lock contention such as message passing sim\u0002ply are in-feasible to retrofit into a production operating\r\nsystem built around shared memory semantics.\r\nOne approach to improving IO performance is to access\r\ndevices via memory-mapped IO. While this does save some\r\nsystem call overhead, this does not fundamentally change or\r\nimprove the scalability of the operating system block layer.\r\nAdditionally, it introduces a non-powercut safe fault domain\r\n(the DRAM page-cache) that applications may be un-aware\r\nof while simultaneously requiring a large application re-write\r\nto take leverage.\r\n7 Conclusions and Future Work\r\nIn this paper, we have established that the current design\r\nof the Linux block layer does not scale beyond one million\r\nIOPS per device. This is sufficient for today’s SSD, but not\r\nfor tomorrow’s. We proposed a new design for the Linux\r\nblock layer. This design is based on two levels of queues in\r\norder to reduce contention and promote thread locality. Our\r\nexperiments have shown the superiority of our design and its\r\nscalability on multi-socket systems. Our multiqueue design\r\nleverages the new capabilities of NVM-Express or high-end\r\nPCI-E devices, while still providing the common interface\r\nand convenience features of the block layer.\r\nWe exposed limitations of the Linux IO stack beyond the\r\nblock layer. Locating and removing those additional bottle\u0002necks is a topic for future work. Future work also includes\r\nperformance tuning with multiple hardware queues, and ex\u0002periments with multiqueue capable hardware prototypes. As\r\none bottleneck is removed, a new choke point is quickly cre\u0002ated, creating an application through device NUMA-local\r\nIO-stack is an on-going process. We intend to work with\r\ndevice manufacturers and standards bodies to ratify the in\u0002clusion of hardware capabilities that will encourage adoption\r\nof the multiqueue interface and finalize this new block layer\r\nimplementation for possible inclusion in the mainline Linux\r\nkernel.\r\n8 References\r\n[1] Improving network performance in multi-core systems.\r\nIntel Corporation, 2007.\r\n[2] J. Axboe. Linux Block IO present and future. Ottawa\r\nLinux Symposium, 2004.\r\n[3] A. Baumann, P. Barham, P.-E. Dagand, T. Harris,\r\nR. Isaacs, S. Peter, T. Roscoe, A. Schupbach, and\r\nS. Akhilesh. The multikernel: a new OS architecture\r\nfor scalable multicore systems. Symposium on\r\nOperating Systems Principles, 2009.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/077c8fb9-3fdc-4019-9c8a-56a69d71f236.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=747968784a1265d74edaf0d29845506b0853942b2a466c707a912e733d1a9d6e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 882
      },
      {
        "segments": [
          {
            "segment_id": "e3b42117-7396-42aa-8d2c-a57c5b85df36",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "[4] M. Bjørling, P. Bonnet, L. Bouganim, and N. Dayan.\r\nThe necessary death of the block device interface. In\r\nConference on Innovative Data Systems Research,\r\n2013.\r\n[5] S. Boyd-wickizer, A. T. Clements, Y. Mao,\r\nA. Pesterev, M. F. Kaashoek, R. Morris, and\r\nN. Zeldovich. An Analysis of Linux Scalability to\r\nMany Cores. Operating Systems Design and\r\nImplementation, 2010.\r\n[6] G. W. Burr, M. J. Breitwisch, M. Franceschini,\r\nD. Garetto, K. Gopalakrishnan, B. Jackson, C. Lam,\r\nand A. Luis. Phase change memory technology.\r\nJournal of Vacuum Science and Technology B,\r\n28(2):223–262, 2010.\r\n[7] A. M. Caulfield, A. De, J. Coburn, T. I. Mollov, R. K.\r\nGupta, and S. Swanson. Moneta: A high-performance\r\nstorage array architecture for next-generation,\r\nnon-volatile memories. In Proceedings of The 43rd\r\nAnnual IEEE/ACM International Symposium on\r\nMicroarchitecture, 2010.\r\n[8] A. M. Caulfield, T. I. Mollov, L. A. Eisner, A. De,\r\nJ. Coburn, and S. Swanson. Providing safe, user space\r\naccess to fast, solid state disks. SIGARCH Comput.\r\nArchit. News, 40(1):387–400, Mar. 2012.\r\n[9] S. Cho, C. Park, H. Oh, S. Kim, Y. Y. Yi, and\r\nG. Ganger. Active Disk Meets Flash: A Case for\r\nIntelligent SSDs. Technical Report CMU-PDL-11-115,\r\n2011.\r\n[10] Completely Fair Queueing (CFQ) Scheduler.\r\nhttp://en.wikipedia.org/wiki/CFQ.\r\n[11] J. Condit, E. B. Nightingale, C. Frost, E. Ipek, B. Lee,\r\nD. Burger, and D. Coetzee. Better I/O through\r\nbyte-addressable, persistent memory. Symposium on\r\nOperating Systems Principles, page 133, 2009.\r\n[12] Deadline IO Scheduler. http:\r\n//en.wikipedia.org/wiki/Deadline_scheduler.\r\n[13] M. Dunn and A. L. N. Reddy. A new I/O scheduler\r\nfor solid state devices. Texas A&M University, 2010.\r\n[14] fio. http://freecode.com/projects/fio.\r\n[15] P. Foglia, C. A. Prete, M. Solinas, and F. Panicucci.\r\nInvestigating design tradeoffs in S-NUCA based CMP\r\nsystems. UCAS, 2009.\r\n[16] Fusion-io ioDrive2. http://www.fusionio.com/.\r\n[17] L. M. Grupp, J. D. David, and S. Swanson. The Bleak\r\nFuture of NAND Flash Memory. USENIX Conference\r\non File and Storage Technologies, 2012.\r\n[18] A. Huffman. NVM Express, Revision 1.0c. Intel\r\nCorporation, 2012.\r\n[19] J. Kim, Y. Oh, E. Kim, J. Choi, D. Lee, and S. H.\r\nNoh. Disk Schedulers for Solid State Drives. In\r\nEMSOFTˆaA˘Z09: 7th ACM Conf. on Embedded ´\r\nSoftware, pages 295–304, 2009.\r\n[20] F. Liu, X. Jiang, and Y. Solihin. Understanding How\r\nOff-Chip Memory Bandwidth Partitioning in Chip\r\nMultiprocessors Affects System Performance. High\r\nPerformance Computer Architecture, 2009.\r\n[21] S. Mangold, S. Choi, P. May, O. Klein, G. Hiertz, and\r\nL. Stibor. 802.11e Wireless LAN for Quality of\r\nService. IEEE, 2012.\r\n[22] J. Nieplocha, R. J. Harrison, and R. J. Littlefield.\r\nGlobal Arrays: A Non-Uniform-Memory-Access\r\nProgramming Model For High-Performance\r\nComputers. The Journal of Supercomputing, 1996.\r\n[23] S. Park and K. Shen. FIOS: A Fair, Efficient Flash\r\nI/O Scheduler. In USENIX Conference on File and\r\nStorage Technologies, 2010.\r\n[24] J. Parkhurst, J. Darringer, and B. Grundmann. From\r\nsingle core to multi-core: preparing for a new\r\nexponential. In Proceedings of the 2006 IEEE/ACM\r\ninternational conference on Computer-aided design,\r\n2006.\r\n[25] PCI-SIG. PCI Express Specification Revision 3.0.\r\nTechnical report, 2012.\r\n[26] L. Soares and M. Stumm. Flexsc: Flexible system call\r\nscheduling with exception-less system calls. In\r\nProceedings of the 9th USENIX conference on\r\nOperating systems design and implementation, 2010.\r\n[27] H. Sutter. The free lunch is over: A fundamental turn\r\ntoward concurrency in software. Dr. Dobb’s Journal,\r\n30(3):202–210, 2005.\r\n[28] V. Vasudevan, M. Kaminsky, and D. G. Andersen.\r\nUsing vector interfaces to deliver millions of iops from\r\na networked key-value storage server. In Proceedings of\r\nthe Third ACM Symposium on Cloud Computing,\r\n2012.\r\n[29] B. Verghese, S. Devine, A. Gupta, and M. Rosenblum.\r\nOperating System Support for Improving Data\r\nLocality on CC-NUMA Compute Servers. In\r\nInternational Conference on Architectural Support for\r\nProgramming Languages and Operating Systems, 1996.\r\n[30] J. Weinberg. Quantifying Locality In The Memory\r\nAccess Patterns of HPC Applications. PhD thesis,\r\n2005.\r\n[31] J. Yang, D. B. Minturn, and F. Hady. When Poll is\r\nBetter than Interrupt. In USENIX Conference on File\r\nand Storage Technologies, 2012.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/e3b42117-7396-42aa-8d2c-a57c5b85df36.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=abb9b500bf1d75aa4c833b5e7e6c4d38e6021bafef21addbbfff85c431250196",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 643
      },
      {
        "segments": [
          {
            "segment_id": "e3b42117-7396-42aa-8d2c-a57c5b85df36",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "[4] M. Bjørling, P. Bonnet, L. Bouganim, and N. Dayan.\r\nThe necessary death of the block device interface. In\r\nConference on Innovative Data Systems Research,\r\n2013.\r\n[5] S. Boyd-wickizer, A. T. Clements, Y. Mao,\r\nA. Pesterev, M. F. Kaashoek, R. Morris, and\r\nN. Zeldovich. An Analysis of Linux Scalability to\r\nMany Cores. Operating Systems Design and\r\nImplementation, 2010.\r\n[6] G. W. Burr, M. J. Breitwisch, M. Franceschini,\r\nD. Garetto, K. Gopalakrishnan, B. Jackson, C. Lam,\r\nand A. Luis. Phase change memory technology.\r\nJournal of Vacuum Science and Technology B,\r\n28(2):223–262, 2010.\r\n[7] A. M. Caulfield, A. De, J. Coburn, T. I. Mollov, R. K.\r\nGupta, and S. Swanson. Moneta: A high-performance\r\nstorage array architecture for next-generation,\r\nnon-volatile memories. In Proceedings of The 43rd\r\nAnnual IEEE/ACM International Symposium on\r\nMicroarchitecture, 2010.\r\n[8] A. M. Caulfield, T. I. Mollov, L. A. Eisner, A. De,\r\nJ. Coburn, and S. Swanson. Providing safe, user space\r\naccess to fast, solid state disks. SIGARCH Comput.\r\nArchit. News, 40(1):387–400, Mar. 2012.\r\n[9] S. Cho, C. Park, H. Oh, S. Kim, Y. Y. Yi, and\r\nG. Ganger. Active Disk Meets Flash: A Case for\r\nIntelligent SSDs. Technical Report CMU-PDL-11-115,\r\n2011.\r\n[10] Completely Fair Queueing (CFQ) Scheduler.\r\nhttp://en.wikipedia.org/wiki/CFQ.\r\n[11] J. Condit, E. B. Nightingale, C. Frost, E. Ipek, B. Lee,\r\nD. Burger, and D. Coetzee. Better I/O through\r\nbyte-addressable, persistent memory. Symposium on\r\nOperating Systems Principles, page 133, 2009.\r\n[12] Deadline IO Scheduler. http:\r\n//en.wikipedia.org/wiki/Deadline_scheduler.\r\n[13] M. Dunn and A. L. N. Reddy. A new I/O scheduler\r\nfor solid state devices. Texas A&M University, 2010.\r\n[14] fio. http://freecode.com/projects/fio.\r\n[15] P. Foglia, C. A. Prete, M. Solinas, and F. Panicucci.\r\nInvestigating design tradeoffs in S-NUCA based CMP\r\nsystems. UCAS, 2009.\r\n[16] Fusion-io ioDrive2. http://www.fusionio.com/.\r\n[17] L. M. Grupp, J. D. David, and S. Swanson. The Bleak\r\nFuture of NAND Flash Memory. USENIX Conference\r\non File and Storage Technologies, 2012.\r\n[18] A. Huffman. NVM Express, Revision 1.0c. Intel\r\nCorporation, 2012.\r\n[19] J. Kim, Y. Oh, E. Kim, J. Choi, D. Lee, and S. H.\r\nNoh. Disk Schedulers for Solid State Drives. In\r\nEMSOFTˆaA˘Z09: 7th ACM Conf. on Embedded ´\r\nSoftware, pages 295–304, 2009.\r\n[20] F. Liu, X. Jiang, and Y. Solihin. Understanding How\r\nOff-Chip Memory Bandwidth Partitioning in Chip\r\nMultiprocessors Affects System Performance. High\r\nPerformance Computer Architecture, 2009.\r\n[21] S. Mangold, S. Choi, P. May, O. Klein, G. Hiertz, and\r\nL. Stibor. 802.11e Wireless LAN for Quality of\r\nService. IEEE, 2012.\r\n[22] J. Nieplocha, R. J. Harrison, and R. J. Littlefield.\r\nGlobal Arrays: A Non-Uniform-Memory-Access\r\nProgramming Model For High-Performance\r\nComputers. The Journal of Supercomputing, 1996.\r\n[23] S. Park and K. Shen. FIOS: A Fair, Efficient Flash\r\nI/O Scheduler. In USENIX Conference on File and\r\nStorage Technologies, 2010.\r\n[24] J. Parkhurst, J. Darringer, and B. Grundmann. From\r\nsingle core to multi-core: preparing for a new\r\nexponential. In Proceedings of the 2006 IEEE/ACM\r\ninternational conference on Computer-aided design,\r\n2006.\r\n[25] PCI-SIG. PCI Express Specification Revision 3.0.\r\nTechnical report, 2012.\r\n[26] L. Soares and M. Stumm. Flexsc: Flexible system call\r\nscheduling with exception-less system calls. In\r\nProceedings of the 9th USENIX conference on\r\nOperating systems design and implementation, 2010.\r\n[27] H. Sutter. The free lunch is over: A fundamental turn\r\ntoward concurrency in software. Dr. Dobb’s Journal,\r\n30(3):202–210, 2005.\r\n[28] V. Vasudevan, M. Kaminsky, and D. G. Andersen.\r\nUsing vector interfaces to deliver millions of iops from\r\na networked key-value storage server. In Proceedings of\r\nthe Third ACM Symposium on Cloud Computing,\r\n2012.\r\n[29] B. Verghese, S. Devine, A. Gupta, and M. Rosenblum.\r\nOperating System Support for Improving Data\r\nLocality on CC-NUMA Compute Servers. In\r\nInternational Conference on Architectural Support for\r\nProgramming Languages and Operating Systems, 1996.\r\n[30] J. Weinberg. Quantifying Locality In The Memory\r\nAccess Patterns of HPC Applications. PhD thesis,\r\n2005.\r\n[31] J. Yang, D. B. Minturn, and F. Hady. When Poll is\r\nBetter than Interrupt. In USENIX Conference on File\r\nand Storage Technologies, 2012.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/2b161c03-d9f4-4898-ae29-34c0cd231f3f/images/e3b42117-7396-42aa-8d2c-a57c5b85df36.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041230Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=abb9b500bf1d75aa4c833b5e7e6c4d38e6021bafef21addbbfff85c431250196",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 643
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\n \"title\": \"Linux Block IO: Introducing Multi-queue SSD Access on Multi-core Systems\"\n}\n```"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "```string\nMatias Bjørling, Jens Axboe, David Nellans, Philippe Bonnet\n```"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "2013"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "Haifa, Israel\nOttawa, Canada\nTexas\n"
        }
      ]
    }
  }
}