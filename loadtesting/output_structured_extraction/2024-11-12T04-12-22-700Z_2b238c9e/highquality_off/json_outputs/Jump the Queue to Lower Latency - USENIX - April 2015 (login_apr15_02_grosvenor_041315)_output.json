{
  "file_name": "Jump the Queue to Lower Latency - USENIX - April 2015 (login_apr15_02_grosvenor_041315).pdf",
  "task_id": "3c51a980-0cd1-40d0-9840-fbc9988467a9",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "c15d2a53-1005-4cf0-9133-0d7b3b6eca5b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 1,
            "page_width": 621,
            "page_height": 801,
            "content": "6  APRIL 2015 VOL. 40, NO. 2 www.usenix.org\r\nDISTRIBUTED Jump the Queue to Lower Latency\r\nMAT THEW P. GROSVENOR , MALTE SCHWAR ZKOPF, IONEL GOG , AND ANDREW MOORE\r\nMatthew P. Grosvenor is a \r\nPhD student at the University \r\nof Cambridge Computer \r\nLaboratory. His interests lie in \r\ncross-layer optimizations of \r\nnetworks, with a particular focus on network \r\nlatency. He has completed research internships \r\nat NICTA (Sydney), Microsoft Research Silicon \r\nValley, and Microsoft Research Cambridge, \r\nand he maintains strong ties to the high-speed \r\nnetworking vendor Exablaze.\r\nmatthew.grosvenor@cl.cam.ac.uk\r\nMalte Schwarzkopf is \r\ncurrently finishing his PhD at \r\nthe University of Cambridge \r\nComputer Laboratory. \r\nHis research is primarily \r\non operating systems and scheduling for \r\ndatacenters, but he dallies in many a trade. He \r\ncompleted a research internship in Google’s \r\ncluster management group and will join the \r\nPDOS group at MIT after graduating.\r\nmalte.schwarzkopf@cl.cam.ac.uk\r\nIonel Gog is a PhD student in \r\nthe University of Cambridge \r\nComputer Laboratory. His \r\nresearch interests include \r\ndistributed systems, data \r\nprocessing systems, and scheduling. He \r\nreceived his MEng in computing from Imperial \r\nCollege London and has done internships at \r\nGoogle, Facebook, and Microsoft Research.\r\nionel.gog@cl.cam.ac.uk\r\nI\r\nn this article, we show that it is possible and practical to achieve \r\nbounded latency in datacenter networks using QJump, an open-source \r\ntool that we’ve been building at the University of Cambridge. Further\u0002more, we show how QJump can concurrently support a range of network \r\nservice levels, from strictly bounded latency through to line-rate throughput \r\nusing the prioritization features found in any datacenter switch. \r\nBringing Back Determinism\r\nIn a statistically multiplexed network, packets share network resources in a first come, \r\nfirst served manner. A packet arriving at a statistically multiplexed (“stat-mux”) switch \r\n(or router) is either forwarded immediately or forced to wait until the link is free. This makes \r\nit hard to determine how long the packet will take to cross the network. In other words, \r\nstat-mux networks do not provide latency determinism. \r\nThe desire to retrofit latency determinism onto Internet Protocol (IP) stat-mux networks \r\nsparked a glut of research in the mid-90s on “Quality of Service” (QoS) schemes. QoS tech\u0002nologies like DiffServ demonstrated that coarse-grained classification and rate-limiting\r\ncould be used to control Internet network latencies. However, these schemes were complex \r\nto deploy and often required cooperation between multiple competing entities. For these \r\nreasons (and many others) Internet QoS struggled for widespread deployment, and hence \r\nprovided limited benefits [1]. \r\nToday, the muscle behind the Internet is found in datacenters, with tens of thousands of \r\nnetworked compute nodes in each. Datacenter networks are constructed using the same \r\nfundamental building blocks as the Internet. Like the Internet, they use statistical multi\u0002plexing and Internet Protocol (IP) communication. Also like the Internet, datacenter \r\nnetworks suffer from lack of latency determinism, or “tail latency” problems. Worse still, \r\nthe close coupling of applications in datacenters magnifies tail-latency effects. Barroso and \r\nDean showed that, if as few as one machine in 10,000 is a straggler, up to 18% of user requests \r\ncan experience long tail latencies [2]. \r\nUnsurprisingly, the culprit for these tail latencies is once again statistical multiplexing. More \r\nprecisely, congestion from some applications causes queueing that delays traffic from other \r\napplications. We call the ability of networked applications to affect each others’ latencies \r\nnetwork interference. For example, Hadoop MapReduce can cause queueing that interferes \r\nwith memcached request latencies, causing latency increases of up to 85x. \r\nThe good news is that datacenters are also unlike the Internet. They have well-known \r\nnetwork structures, and the bulk of the network is under the control of a single authority. \r\nThe differences between datacenters and the Internet allow us to apply QoS schemes in new \r\nways, different and simpler than the Internet does. In datacenters, we can enforce a system\u0002wide policy, and, using known host counts and link rates, we can calculate specific rate \r\nlimits that allow us to provide a guaranteed bound on network latency. \r\nWe have implemented these ideas in QJump. QJump is a simple and immediately deployable \r\napproach to controlling network interference in datacenter networks. QJump is open source ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/c15d2a53-1005-4cf0-9133-0d7b3b6eca5b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e124aac02ca89116cb9202fe54a3914abcc00717fe107d3a1987dea1adedd8a5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 667
      },
      {
        "segments": [
          {
            "segment_id": "c15d2a53-1005-4cf0-9133-0d7b3b6eca5b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 1,
            "page_width": 621,
            "page_height": 801,
            "content": "6  APRIL 2015 VOL. 40, NO. 2 www.usenix.org\r\nDISTRIBUTED Jump the Queue to Lower Latency\r\nMAT THEW P. GROSVENOR , MALTE SCHWAR ZKOPF, IONEL GOG , AND ANDREW MOORE\r\nMatthew P. Grosvenor is a \r\nPhD student at the University \r\nof Cambridge Computer \r\nLaboratory. His interests lie in \r\ncross-layer optimizations of \r\nnetworks, with a particular focus on network \r\nlatency. He has completed research internships \r\nat NICTA (Sydney), Microsoft Research Silicon \r\nValley, and Microsoft Research Cambridge, \r\nand he maintains strong ties to the high-speed \r\nnetworking vendor Exablaze.\r\nmatthew.grosvenor@cl.cam.ac.uk\r\nMalte Schwarzkopf is \r\ncurrently finishing his PhD at \r\nthe University of Cambridge \r\nComputer Laboratory. \r\nHis research is primarily \r\non operating systems and scheduling for \r\ndatacenters, but he dallies in many a trade. He \r\ncompleted a research internship in Google’s \r\ncluster management group and will join the \r\nPDOS group at MIT after graduating.\r\nmalte.schwarzkopf@cl.cam.ac.uk\r\nIonel Gog is a PhD student in \r\nthe University of Cambridge \r\nComputer Laboratory. His \r\nresearch interests include \r\ndistributed systems, data \r\nprocessing systems, and scheduling. He \r\nreceived his MEng in computing from Imperial \r\nCollege London and has done internships at \r\nGoogle, Facebook, and Microsoft Research.\r\nionel.gog@cl.cam.ac.uk\r\nI\r\nn this article, we show that it is possible and practical to achieve \r\nbounded latency in datacenter networks using QJump, an open-source \r\ntool that we’ve been building at the University of Cambridge. Further\u0002more, we show how QJump can concurrently support a range of network \r\nservice levels, from strictly bounded latency through to line-rate throughput \r\nusing the prioritization features found in any datacenter switch. \r\nBringing Back Determinism\r\nIn a statistically multiplexed network, packets share network resources in a first come, \r\nfirst served manner. A packet arriving at a statistically multiplexed (“stat-mux”) switch \r\n(or router) is either forwarded immediately or forced to wait until the link is free. This makes \r\nit hard to determine how long the packet will take to cross the network. In other words, \r\nstat-mux networks do not provide latency determinism. \r\nThe desire to retrofit latency determinism onto Internet Protocol (IP) stat-mux networks \r\nsparked a glut of research in the mid-90s on “Quality of Service” (QoS) schemes. QoS tech\u0002nologies like DiffServ demonstrated that coarse-grained classification and rate-limiting\r\ncould be used to control Internet network latencies. However, these schemes were complex \r\nto deploy and often required cooperation between multiple competing entities. For these \r\nreasons (and many others) Internet QoS struggled for widespread deployment, and hence \r\nprovided limited benefits [1]. \r\nToday, the muscle behind the Internet is found in datacenters, with tens of thousands of \r\nnetworked compute nodes in each. Datacenter networks are constructed using the same \r\nfundamental building blocks as the Internet. Like the Internet, they use statistical multi\u0002plexing and Internet Protocol (IP) communication. Also like the Internet, datacenter \r\nnetworks suffer from lack of latency determinism, or “tail latency” problems. Worse still, \r\nthe close coupling of applications in datacenters magnifies tail-latency effects. Barroso and \r\nDean showed that, if as few as one machine in 10,000 is a straggler, up to 18% of user requests \r\ncan experience long tail latencies [2]. \r\nUnsurprisingly, the culprit for these tail latencies is once again statistical multiplexing. More \r\nprecisely, congestion from some applications causes queueing that delays traffic from other \r\napplications. We call the ability of networked applications to affect each others’ latencies \r\nnetwork interference. For example, Hadoop MapReduce can cause queueing that interferes \r\nwith memcached request latencies, causing latency increases of up to 85x. \r\nThe good news is that datacenters are also unlike the Internet. They have well-known \r\nnetwork structures, and the bulk of the network is under the control of a single authority. \r\nThe differences between datacenters and the Internet allow us to apply QoS schemes in new \r\nways, different and simpler than the Internet does. In datacenters, we can enforce a system\u0002wide policy, and, using known host counts and link rates, we can calculate specific rate \r\nlimits that allow us to provide a guaranteed bound on network latency. \r\nWe have implemented these ideas in QJump. QJump is a simple and immediately deployable \r\napproach to controlling network interference in datacenter networks. QJump is open source ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/c15d2a53-1005-4cf0-9133-0d7b3b6eca5b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e124aac02ca89116cb9202fe54a3914abcc00717fe107d3a1987dea1adedd8a5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 667
      },
      {
        "segments": [
          {
            "segment_id": "effffb24-723b-4362-b92c-0f343b0966cd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 2,
            "page_width": 621,
            "page_height": 801,
            "content": "www.usenix.org APRIL 2015 VOL. 40, NO. 2 7\r\nSYSTEMS\r\nFigure 1a shows a timeline of PTPd synchro\u0002nization offset. Figure 1b has a CDF of mem\u0002cached request latency, and Figure 1c has a \r\nCDF of Naiad synchronization time.\r\n300 400 500\r\nTime since start [sec]\r\n-400\r\n-200\r\n0\r\n200\r\n400\r\n600\r\nClock offset [\u001fs]\r\nalone\r\n+ Hadoop\r\n+ Had. w/ QJ\r\n1a\r\n0 500 1000 1500 2000\r\n0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1.0\r\nalone\r\n+ Hadoop\r\n+ Had. w/ QJ\r\nLatency in \u001fs\r\n1b\r\n0 500 1000 1500 2000\r\nLatency in \u001fs\r\n0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1.0\r\nalone\r\n+ Hadoop\r\n+ Had. w/ QJ\r\n1c\r\nand runs on unmodified hardware and software. A full paper describing QJump will appear \r\nin the 12th USENIX Symposium on Networked System Design and Implementation \r\n(NSDI ’15) [3]. Additional information including source code and data is available from our \r\naccompanying Web site: http://www.cl.cam.ac.uk/research/srg/netos/qjump. \r\nQJump in Action\r\nTo illustrate how bad network interference can get and how well QJump fixes it, we show the \r\nresults from a collection of experiments with latency-sensitive datacenter applications (see \r\nFigure 1). In each experiment, the application: (1) runs alone on the network, (2) shares the \r\nnetwork with Hadoop MapReduce, and (3) shares the network with Hadoop, but has QJump\r\nenabled. A complete evaluation of QJump, including full details of these experiments (and \r\nmany others), is available in the full paper.\r\n1. Clock Synchronization. Precise clock synchronization is important to distributed sys\u0002tems such as Google’s Spanner. PTPd offers microsecond-granularity time synchronization \r\nfrom a time server to machines on a local network. However, it assumes roughly constant \r\nnetwork delay. In Figure 1a, we show a timeline of PTPd synchronizing a host clock on both \r\nan idle network and when sharing the network with Hadoop. In the shared case, Hadoop \r\ncauses queueing which delays PTPd’s synchronization packets. This causes PTPd to tem\u0002porarily fall 200–500 s out of synchronization, 50x worse than on an idle network. With \r\nQJump enabled, the PTPd synchronization remains unaffected by Hadoop’s traffic. \r\n2. Key-Value Stores. Memcached is a popular in-memory key-value store used by Facebook \r\nand others to store small objects for quick retrieval. We benchmark memcached using the \r\nmemaslap load generator and measure the request latency. Figure 1b shows the distribution \r\nof request latencies on an idle network and a network shared with Hadoop. With Hadoop \r\nrunning, the 99th percentile request latency degrades by 1.5x from 779 s to 1196 s. Further\u0002more, around 1 in 6,000 requests takes over 200 ms to complete, over 85x worse than the \r\nmaximum latency on an idle network. With QJump enabled, these effects are mitigated. \r\n3. Big Data Computation. Naiad [4] is a framework for big data computation. In some \r\ncomputations, Naiad’s performance depends on low-latency synchronization between worker \r\nnodes. To test Naiad’s sensitivity to network interference, we execute a synchronization \r\nbenchmark (provided by the Naiad authors) with and without Hadoop running. Figure 1c \r\nshows the distribution of Naiad synchronization latencies in both situations. On an idle \r\nnetwork, Naiad takes around 500 s at the 99th percentile to perform a four-way synchro\u0002nization. With interference, this grows to 1.1–1.5 ms, a 2–3x performance degradation. With \r\nQJump running, the performance nearly exactly conforms to the interference-free situation. \r\nThese experiments cover just a small set of applications, but there are many others that can \r\nalso benefit from using QJump. Examples include coordination traffic for Software Defined \r\nNetworking (SDN), distributed locking/consensus services, and fast failure detectors.\r\nScheduling and Queueing Latency\r\nTo understand how QJump works, we first need to understand the two main sources of \r\nlatency nondeterminism in statistically multiplexed (stat-mux) networks: scheduling latency \r\nand queueing latency. In Figure 2a, a collection of packets (P) arrive at an idle switch S0. At \r\nAndrew W. Moore is a Senior \r\nLecturer at the University \r\nof Cambridge Computer \r\nLaboratory in England, where \r\nhe is part of the Systems \r\nResearch Group working on issues of network \r\nand systems architecture. His research \r\ninterests include enabling open-network \r\nresearch and education using the NetFPGA \r\nplatform. Other research pursuits include \r\nlow-power energy-aware networking and novel \r\nnetwork and systems datacenter architectures.\r\nandrew.moore@cl.cam.ac.uk",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/effffb24-723b-4362-b92c-0f343b0966cd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c52ba76ccb17ee595b4755f328e3a1dc28feba26cce0f022779afe8be80fda5b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 677
      },
      {
        "segments": [
          {
            "segment_id": "effffb24-723b-4362-b92c-0f343b0966cd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 2,
            "page_width": 621,
            "page_height": 801,
            "content": "www.usenix.org APRIL 2015 VOL. 40, NO. 2 7\r\nSYSTEMS\r\nFigure 1a shows a timeline of PTPd synchro\u0002nization offset. Figure 1b has a CDF of mem\u0002cached request latency, and Figure 1c has a \r\nCDF of Naiad synchronization time.\r\n300 400 500\r\nTime since start [sec]\r\n-400\r\n-200\r\n0\r\n200\r\n400\r\n600\r\nClock offset [\u001fs]\r\nalone\r\n+ Hadoop\r\n+ Had. w/ QJ\r\n1a\r\n0 500 1000 1500 2000\r\n0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1.0\r\nalone\r\n+ Hadoop\r\n+ Had. w/ QJ\r\nLatency in \u001fs\r\n1b\r\n0 500 1000 1500 2000\r\nLatency in \u001fs\r\n0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1.0\r\nalone\r\n+ Hadoop\r\n+ Had. w/ QJ\r\n1c\r\nand runs on unmodified hardware and software. A full paper describing QJump will appear \r\nin the 12th USENIX Symposium on Networked System Design and Implementation \r\n(NSDI ’15) [3]. Additional information including source code and data is available from our \r\naccompanying Web site: http://www.cl.cam.ac.uk/research/srg/netos/qjump. \r\nQJump in Action\r\nTo illustrate how bad network interference can get and how well QJump fixes it, we show the \r\nresults from a collection of experiments with latency-sensitive datacenter applications (see \r\nFigure 1). In each experiment, the application: (1) runs alone on the network, (2) shares the \r\nnetwork with Hadoop MapReduce, and (3) shares the network with Hadoop, but has QJump\r\nenabled. A complete evaluation of QJump, including full details of these experiments (and \r\nmany others), is available in the full paper.\r\n1. Clock Synchronization. Precise clock synchronization is important to distributed sys\u0002tems such as Google’s Spanner. PTPd offers microsecond-granularity time synchronization \r\nfrom a time server to machines on a local network. However, it assumes roughly constant \r\nnetwork delay. In Figure 1a, we show a timeline of PTPd synchronizing a host clock on both \r\nan idle network and when sharing the network with Hadoop. In the shared case, Hadoop \r\ncauses queueing which delays PTPd’s synchronization packets. This causes PTPd to tem\u0002porarily fall 200–500 s out of synchronization, 50x worse than on an idle network. With \r\nQJump enabled, the PTPd synchronization remains unaffected by Hadoop’s traffic. \r\n2. Key-Value Stores. Memcached is a popular in-memory key-value store used by Facebook \r\nand others to store small objects for quick retrieval. We benchmark memcached using the \r\nmemaslap load generator and measure the request latency. Figure 1b shows the distribution \r\nof request latencies on an idle network and a network shared with Hadoop. With Hadoop \r\nrunning, the 99th percentile request latency degrades by 1.5x from 779 s to 1196 s. Further\u0002more, around 1 in 6,000 requests takes over 200 ms to complete, over 85x worse than the \r\nmaximum latency on an idle network. With QJump enabled, these effects are mitigated. \r\n3. Big Data Computation. Naiad [4] is a framework for big data computation. In some \r\ncomputations, Naiad’s performance depends on low-latency synchronization between worker \r\nnodes. To test Naiad’s sensitivity to network interference, we execute a synchronization \r\nbenchmark (provided by the Naiad authors) with and without Hadoop running. Figure 1c \r\nshows the distribution of Naiad synchronization latencies in both situations. On an idle \r\nnetwork, Naiad takes around 500 s at the 99th percentile to perform a four-way synchro\u0002nization. With interference, this grows to 1.1–1.5 ms, a 2–3x performance degradation. With \r\nQJump running, the performance nearly exactly conforms to the interference-free situation. \r\nThese experiments cover just a small set of applications, but there are many others that can \r\nalso benefit from using QJump. Examples include coordination traffic for Software Defined \r\nNetworking (SDN), distributed locking/consensus services, and fast failure detectors.\r\nScheduling and Queueing Latency\r\nTo understand how QJump works, we first need to understand the two main sources of \r\nlatency nondeterminism in statistically multiplexed (stat-mux) networks: scheduling latency \r\nand queueing latency. In Figure 2a, a collection of packets (P) arrive at an idle switch S0. At \r\nAndrew W. Moore is a Senior \r\nLecturer at the University \r\nof Cambridge Computer \r\nLaboratory in England, where \r\nhe is part of the Systems \r\nResearch Group working on issues of network \r\nand systems architecture. His research \r\ninterests include enabling open-network \r\nresearch and education using the NetFPGA \r\nplatform. Other research pursuits include \r\nlow-power energy-aware networking and novel \r\nnetwork and systems datacenter architectures.\r\nandrew.moore@cl.cam.ac.uk",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/effffb24-723b-4362-b92c-0f343b0966cd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c52ba76ccb17ee595b4755f328e3a1dc28feba26cce0f022779afe8be80fda5b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 677
      },
      {
        "segments": [
          {
            "segment_id": "b4cd3439-e0ca-46f8-b332-74dc0aa6f8b9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 3,
            "page_width": 621,
            "page_height": 801,
            "content": "8  APRIL 2015 VOL. 40, NO. 2 www.usenix.org\r\nFigure 2: Latency causes (a) fan-in, packets waiting to be serviced by the \r\nswitch scheduler, or (b) queueing, packets waiting behind many other \r\npackets.\r\nS0 S1\r\nP P P L\r\nL\r\nP\r\nP\r\nP\r\nqueueing\r\nlatency\r\nscheduling latency\r\n1 2 3 4\r\n2\r\n3\r\n4\r\n1\r\nL\r\nP\r\nP\r\nP\r\nDISTRIBUTED SYSTEMS\r\nJump the Queue to Lower Latency\r\nthe same time, a latency sensitive packet (L) also arrives. The \r\nL packet experiences scheduling latency as it waits for other \r\nP packets to be serviced by the switch scheduler. Scheduling \r\nlatency is a consequence of fan-in, which happens when mul\u0002tiple packets contend for the same output port on the switch. If \r\nthe switch takes too long to output packets, then new packets \r\ncan queue behind existing ones. Figure 2b shows two latency \r\nsensitive packets (L) queued behind many other waiting packets \r\n(P). This is a kind of head-of-line blocking that we call queue\u0002ing latency. Queuing latency is caused by excessive scheduling \r\nlatency. We cannot eliminate scheduling latency in a stat-mux \r\nnetwork. However, using some simple math, we can put a bound \r\non it. By doing so, we can ensure that packets are issued into the \r\nnetwork at a rate that prevents them from queueing up behind \r\neach other, thus also control queueing latency. \r\nBounded Queues—Bounded Latency\r\nConsidering Figure 2a, in the worst case the L packet will need \r\nto wait for the switch scheduler to service all preceding P pack\u0002ets before it is serviced. For a switch with n ports, the worst-case \r\nwaiting time is n - 1 (approximately n) packets. As the number of \r\nports on the switch grows, the worst-case latency grows with it. \r\nWe can easily expand this understanding to cover multi-hop net\u0002works by treating the whole network as a single “big switch” (this \r\nis an application of the “hose-constraint” [4] model). Hence we \r\ncan apply the same calculation as above. Knowing that a packet \r\nof size P will take P/R seconds to transmit at link-rate R, we can \r\ntherefore bound the maximum interference delay at:\r\nwhere n is the number of hosts, P is the maximum packet size \r\n(in bits), and R is the rate of the slowest link in bits per second. \r\nEquation 1 assumes that hosts have only one (active) link to the \r\nnetwork and that the speed at the core of the network is never \r\nslower than the speed at the edge. We think that these are both \r\nsafe assumptions for any reasonable datacenter network.\r\nWe refer to the worst-case delay as a network epoch. A network \r\nepoch is the maximum time that an initially idle network will \r\ntake to service one packet from every sending host, regardless of \r\nthe source, destination, or timing of those packets. Intuitively, if \r\nwe imagine the network as a funnel, the network epoch repre\u0002sents the time that the funnel will take to drain when it is filled \r\nto the top. If all hosts are rate-limited so that they cannot issue \r\nmore than one packet per epoch, no permanent queues can build \r\nup, and the end-to-end network delay bound will be maintained \r\nforever. That is, we rate-limit hosts so that the funnel will never \r\noverflow. \r\nThe problem with a network epoch is that it is a global concept. \r\nTo maintain it, all hosts need to agree on when an epoch begins \r\nand when it ends. It would seem that this requires all hosts \r\nin the network to have tightly synchronized clocks. In fact, \r\nnetwork epochs can work even without clock synchronization. \r\nIf we assume that network epochs occur at the same frequency, \r\nbut not necessarily in the same phase, the network becomes \r\nmesochronous. This requires us to double the latency bound, but \r\nall other properties hold (see [3] for further details). The network \r\nepoch thus becomes:\r\nEquation 2 is the basis for QJump. QJump is based on the principle \r\nthat, if we rate-limit all hosts so that they can only issue one \r\npacket every network epoch, then no packet will take more than \r\none network epoch to be delivered to the destination even in the \r\nworst case.\r\nLatency Variance vs. Throughput\r\nAlthough the equation derived above provides an absolute upper \r\nbound on in-network delay, it also aggressively restricts through\u0002put. Formulating Equation 2 for throughput, we obtain:\r\nFor example, with 1,000 hosts and a 10 Gb/s edge, we obtain \r\nan effective throughput of 5 Mb/s per host. Clearly, this is not \r\nideal. We can improve this situation by making two observa\u0002tions. First, Equation 2 is pessimistic: it assumes that all hosts \r\ntransmit to one destination at the worst time, which is unlikely \r\ngiven a realistic network and traffic distribution. Second, some \r\napplications, like PTPd, are more sensitive to interference than \r\nothers—for example, memcached and Naiad—whereas still \r\nother applications, like Hadoop, are more sensitive to through\u0002put restrictions. From the first observation, we can relax the \r\nthroughput constraints in Equation 2 by assuming that fewer \r\nthan n hosts send to a single destination at the worst time. For \r\nexample, if we guess that only 500 of the 1,000 hosts concur-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/b4cd3439-e0ca-46f8-b332-74dc0aa6f8b9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9ba9fbf90506db167b2e32b15da6eb94ddc70f6731f94788cc6fed40159379d5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 846
      },
      {
        "segments": [
          {
            "segment_id": "b4cd3439-e0ca-46f8-b332-74dc0aa6f8b9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 3,
            "page_width": 621,
            "page_height": 801,
            "content": "8  APRIL 2015 VOL. 40, NO. 2 www.usenix.org\r\nFigure 2: Latency causes (a) fan-in, packets waiting to be serviced by the \r\nswitch scheduler, or (b) queueing, packets waiting behind many other \r\npackets.\r\nS0 S1\r\nP P P L\r\nL\r\nP\r\nP\r\nP\r\nqueueing\r\nlatency\r\nscheduling latency\r\n1 2 3 4\r\n2\r\n3\r\n4\r\n1\r\nL\r\nP\r\nP\r\nP\r\nDISTRIBUTED SYSTEMS\r\nJump the Queue to Lower Latency\r\nthe same time, a latency sensitive packet (L) also arrives. The \r\nL packet experiences scheduling latency as it waits for other \r\nP packets to be serviced by the switch scheduler. Scheduling \r\nlatency is a consequence of fan-in, which happens when mul\u0002tiple packets contend for the same output port on the switch. If \r\nthe switch takes too long to output packets, then new packets \r\ncan queue behind existing ones. Figure 2b shows two latency \r\nsensitive packets (L) queued behind many other waiting packets \r\n(P). This is a kind of head-of-line blocking that we call queue\u0002ing latency. Queuing latency is caused by excessive scheduling \r\nlatency. We cannot eliminate scheduling latency in a stat-mux \r\nnetwork. However, using some simple math, we can put a bound \r\non it. By doing so, we can ensure that packets are issued into the \r\nnetwork at a rate that prevents them from queueing up behind \r\neach other, thus also control queueing latency. \r\nBounded Queues—Bounded Latency\r\nConsidering Figure 2a, in the worst case the L packet will need \r\nto wait for the switch scheduler to service all preceding P pack\u0002ets before it is serviced. For a switch with n ports, the worst-case \r\nwaiting time is n - 1 (approximately n) packets. As the number of \r\nports on the switch grows, the worst-case latency grows with it. \r\nWe can easily expand this understanding to cover multi-hop net\u0002works by treating the whole network as a single “big switch” (this \r\nis an application of the “hose-constraint” [4] model). Hence we \r\ncan apply the same calculation as above. Knowing that a packet \r\nof size P will take P/R seconds to transmit at link-rate R, we can \r\ntherefore bound the maximum interference delay at:\r\nwhere n is the number of hosts, P is the maximum packet size \r\n(in bits), and R is the rate of the slowest link in bits per second. \r\nEquation 1 assumes that hosts have only one (active) link to the \r\nnetwork and that the speed at the core of the network is never \r\nslower than the speed at the edge. We think that these are both \r\nsafe assumptions for any reasonable datacenter network.\r\nWe refer to the worst-case delay as a network epoch. A network \r\nepoch is the maximum time that an initially idle network will \r\ntake to service one packet from every sending host, regardless of \r\nthe source, destination, or timing of those packets. Intuitively, if \r\nwe imagine the network as a funnel, the network epoch repre\u0002sents the time that the funnel will take to drain when it is filled \r\nto the top. If all hosts are rate-limited so that they cannot issue \r\nmore than one packet per epoch, no permanent queues can build \r\nup, and the end-to-end network delay bound will be maintained \r\nforever. That is, we rate-limit hosts so that the funnel will never \r\noverflow. \r\nThe problem with a network epoch is that it is a global concept. \r\nTo maintain it, all hosts need to agree on when an epoch begins \r\nand when it ends. It would seem that this requires all hosts \r\nin the network to have tightly synchronized clocks. In fact, \r\nnetwork epochs can work even without clock synchronization. \r\nIf we assume that network epochs occur at the same frequency, \r\nbut not necessarily in the same phase, the network becomes \r\nmesochronous. This requires us to double the latency bound, but \r\nall other properties hold (see [3] for further details). The network \r\nepoch thus becomes:\r\nEquation 2 is the basis for QJump. QJump is based on the principle \r\nthat, if we rate-limit all hosts so that they can only issue one \r\npacket every network epoch, then no packet will take more than \r\none network epoch to be delivered to the destination even in the \r\nworst case.\r\nLatency Variance vs. Throughput\r\nAlthough the equation derived above provides an absolute upper \r\nbound on in-network delay, it also aggressively restricts through\u0002put. Formulating Equation 2 for throughput, we obtain:\r\nFor example, with 1,000 hosts and a 10 Gb/s edge, we obtain \r\nan effective throughput of 5 Mb/s per host. Clearly, this is not \r\nideal. We can improve this situation by making two observa\u0002tions. First, Equation 2 is pessimistic: it assumes that all hosts \r\ntransmit to one destination at the worst time, which is unlikely \r\ngiven a realistic network and traffic distribution. Second, some \r\napplications, like PTPd, are more sensitive to interference than \r\nothers—for example, memcached and Naiad—whereas still \r\nother applications, like Hadoop, are more sensitive to through\u0002put restrictions. From the first observation, we can relax the \r\nthroughput constraints in Equation 2 by assuming that fewer \r\nthan n hosts send to a single destination at the worst time. For \r\nexample, if we guess that only 500 of the 1,000 hosts concur-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/b4cd3439-e0ca-46f8-b332-74dc0aa6f8b9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9ba9fbf90506db167b2e32b15da6eb94ddc70f6731f94788cc6fed40159379d5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 846
      },
      {
        "segments": [
          {
            "segment_id": "ef0c2155-da03-462d-9b36-7cc72899da19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 4,
            "page_width": 621,
            "page_height": 801,
            "content": "www.usenix.org APRIL 2015 VOL. 40, NO. 2 9\r\nIdeal\r\nContended\r\nEth. Flow Ctrl.\r\nECN\r\nDCTCP\r\nQJump\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\nNormalized RM\r\nS app. metric\r\n318\r\n12614\r\nHadoop\r\nruntime\r\nPTPd sync.\r\noffset\r\nmemcached\r\nreq. latency\r\nFigure 3: Comparison of QJump to several available congestion control \r\nalternatives\r\nDISTRIBUTED SYSTEMS\r\nJump the Queue to Lower Latency\r\nrently send to a single destination, then those 500 hosts can send \r\nat twice the rate and maintain the same network delay if our \r\nassumption holds. More generally, we define a scaling factor f so \r\nthat the assumed number of senders n′ is given by:\r\nIntuitively, f is a “throughput factor”: as the value of f grows, so \r\ndoes the available bandwidth. \r\nFrom the second observation, some (but not all) applications \r\ncan tolerate some degree of latency variance. Instead, for these \r\napplications we aim for a statistical reduction in latency vari\u0002ance. This reintroduces a degree of statistical multiplexing to \r\nthe network, albeit one that is more tightly controlled. When \r\nthe guess for f is too optimistic (the actual number of senders is \r\ngreater than n′), some queueing occurs, causing interference.\r\nThe probability that interference occurs increases with increas\u0002ing values of f. At the upper bound (f = n), latency variance is \r\nsimilar to existing networks and full network throughput is \r\navailable. At the lower bound (f = 1), latency is guaranteed, albeit \r\nwith reduced throughput. In essence, f quantifies the latency \r\nvariance vs. throughput tradeoff.\r\nJump the Queue with Prioritization\r\nWe would like to use multiple values of f concurrently, so that \r\ndifferent applications can benefit from the latency variance \r\nvs. throughput tradeoff that suits them best. To achieve this, \r\nwe partition the network so that traffic from latency-sensitive \r\napplications, like PTPd, memcached, and Naiad can “jump-the\u0002queue” over traffic from throughput-intensive applications like \r\nHadoop. Ethernet switches support the IEEE 802.1Q standard, \r\nwhich provides eight (0–7) hardware enforced “service classes” \r\nor “priorities.” \r\nThe problem with using priorities is that they can become \r\na “race to the top.” For example, memcached developers may \r\nassume that memcached traffic is the most important and \r\nshould receive the highest priority to minimize latency. Mean\u0002while, Hadoop developers may assume that Hadoop traffic is the \r\nmost important and should similarly receive the highest priority \r\nto maximize throughput. Since there are a limited number of \r\npriorities, neither can achieve an advantage and prioritization \r\nloses its value. QJump is different: it intentionally binds priority \r\nvalues to rate-limits. High priorities are given aggressive rate \r\nlimits (small f values), and priorities thus become useful because \r\nthey are no longer “free.” QJump users must choose between \r\nlow latency variance at low throughput (high priority) and high \r\nlatency variance at high throughput (low priority). We call the \r\nassignment of an f value to a priority a “QJump level.” The latency \r\nvariance of a given QJump level depends on the number of QJump\r\nlevels above it and their traffic patterns. \r\nImplementation\r\nQJump has two components: a rate-limiter to provide admission \r\ncontrol to the network, and an application utility to configure \r\nunmodified applications to use QJump levels. Our full paper \r\ndescribes the rate limiter and application utility in detail, and \r\nthe source code for both is available from our Web site. \r\nIn our prototype, we use our own high-performance rate limiter \r\nbuilt upon the queueing discipline (qdisc) mechanism offered by \r\nthe Linux kernel traffic control (TC). TC modules do not require \r\nkernel modifications and can be inserted and removed at run\u0002time, making them flexible and easy to deploy. \r\nTo support unmodified applications, we implemented a utility \r\nthat dynamically intercepts socket setup system calls and alters \r\ntheir options. We inject the utility into unmodified executables \r\nvia the Linux dynamic linker’s LD_PRELOAD support.\r\nPerformance Comparison\r\nWe have already demonstrated that QJump can resolve network \r\ninterference, but how does it compare to existing congestion \r\ncontrol mechanisms? To find out, we have tested QJump against \r\nseveral readily deployable congestion control schemes. In these \r\nexperiments, PTPd, memcached, and Hadoop are configured to \r\nrun on the same network for a 10-minute period. Since interfer\u0002ence is transient in these experiments, we measure the degree to \r\nwhich it affects applications using the root mean square (RMS) \r\nof each application-specific metric. For Hadoop, the metric of \r\ninterest is the job runtime, for PTPd it is the time synchroniza\u0002tion offset, and for memcached it is the request latency. Figure 3 \r\nshows six cases: an ideal case, a contended case, and one for each \r\nof the four comparison schemes. All cases are normalized to the \r\nideal case, which has each application running alone on an idle \r\nnetwork. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/ef0c2155-da03-462d-9b36-7cc72899da19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=999be0cf24c343eeb95b77a0e35e5c94f6dd439112ec72e3208025eb71f452f1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 758
      },
      {
        "segments": [
          {
            "segment_id": "ef0c2155-da03-462d-9b36-7cc72899da19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 4,
            "page_width": 621,
            "page_height": 801,
            "content": "www.usenix.org APRIL 2015 VOL. 40, NO. 2 9\r\nIdeal\r\nContended\r\nEth. Flow Ctrl.\r\nECN\r\nDCTCP\r\nQJump\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\nNormalized RM\r\nS app. metric\r\n318\r\n12614\r\nHadoop\r\nruntime\r\nPTPd sync.\r\noffset\r\nmemcached\r\nreq. latency\r\nFigure 3: Comparison of QJump to several available congestion control \r\nalternatives\r\nDISTRIBUTED SYSTEMS\r\nJump the Queue to Lower Latency\r\nrently send to a single destination, then those 500 hosts can send \r\nat twice the rate and maintain the same network delay if our \r\nassumption holds. More generally, we define a scaling factor f so \r\nthat the assumed number of senders n′ is given by:\r\nIntuitively, f is a “throughput factor”: as the value of f grows, so \r\ndoes the available bandwidth. \r\nFrom the second observation, some (but not all) applications \r\ncan tolerate some degree of latency variance. Instead, for these \r\napplications we aim for a statistical reduction in latency vari\u0002ance. This reintroduces a degree of statistical multiplexing to \r\nthe network, albeit one that is more tightly controlled. When \r\nthe guess for f is too optimistic (the actual number of senders is \r\ngreater than n′), some queueing occurs, causing interference.\r\nThe probability that interference occurs increases with increas\u0002ing values of f. At the upper bound (f = n), latency variance is \r\nsimilar to existing networks and full network throughput is \r\navailable. At the lower bound (f = 1), latency is guaranteed, albeit \r\nwith reduced throughput. In essence, f quantifies the latency \r\nvariance vs. throughput tradeoff.\r\nJump the Queue with Prioritization\r\nWe would like to use multiple values of f concurrently, so that \r\ndifferent applications can benefit from the latency variance \r\nvs. throughput tradeoff that suits them best. To achieve this, \r\nwe partition the network so that traffic from latency-sensitive \r\napplications, like PTPd, memcached, and Naiad can “jump-the\u0002queue” over traffic from throughput-intensive applications like \r\nHadoop. Ethernet switches support the IEEE 802.1Q standard, \r\nwhich provides eight (0–7) hardware enforced “service classes” \r\nor “priorities.” \r\nThe problem with using priorities is that they can become \r\na “race to the top.” For example, memcached developers may \r\nassume that memcached traffic is the most important and \r\nshould receive the highest priority to minimize latency. Mean\u0002while, Hadoop developers may assume that Hadoop traffic is the \r\nmost important and should similarly receive the highest priority \r\nto maximize throughput. Since there are a limited number of \r\npriorities, neither can achieve an advantage and prioritization \r\nloses its value. QJump is different: it intentionally binds priority \r\nvalues to rate-limits. High priorities are given aggressive rate \r\nlimits (small f values), and priorities thus become useful because \r\nthey are no longer “free.” QJump users must choose between \r\nlow latency variance at low throughput (high priority) and high \r\nlatency variance at high throughput (low priority). We call the \r\nassignment of an f value to a priority a “QJump level.” The latency \r\nvariance of a given QJump level depends on the number of QJump\r\nlevels above it and their traffic patterns. \r\nImplementation\r\nQJump has two components: a rate-limiter to provide admission \r\ncontrol to the network, and an application utility to configure \r\nunmodified applications to use QJump levels. Our full paper \r\ndescribes the rate limiter and application utility in detail, and \r\nthe source code for both is available from our Web site. \r\nIn our prototype, we use our own high-performance rate limiter \r\nbuilt upon the queueing discipline (qdisc) mechanism offered by \r\nthe Linux kernel traffic control (TC). TC modules do not require \r\nkernel modifications and can be inserted and removed at run\u0002time, making them flexible and easy to deploy. \r\nTo support unmodified applications, we implemented a utility \r\nthat dynamically intercepts socket setup system calls and alters \r\ntheir options. We inject the utility into unmodified executables \r\nvia the Linux dynamic linker’s LD_PRELOAD support.\r\nPerformance Comparison\r\nWe have already demonstrated that QJump can resolve network \r\ninterference, but how does it compare to existing congestion \r\ncontrol mechanisms? To find out, we have tested QJump against \r\nseveral readily deployable congestion control schemes. In these \r\nexperiments, PTPd, memcached, and Hadoop are configured to \r\nrun on the same network for a 10-minute period. Since interfer\u0002ence is transient in these experiments, we measure the degree to \r\nwhich it affects applications using the root mean square (RMS) \r\nof each application-specific metric. For Hadoop, the metric of \r\ninterest is the job runtime, for PTPd it is the time synchroniza\u0002tion offset, and for memcached it is the request latency. Figure 3 \r\nshows six cases: an ideal case, a contended case, and one for each \r\nof the four comparison schemes. All cases are normalized to the \r\nideal case, which has each application running alone on an idle \r\nnetwork. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/ef0c2155-da03-462d-9b36-7cc72899da19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=999be0cf24c343eeb95b77a0e35e5c94f6dd439112ec72e3208025eb71f452f1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 758
      },
      {
        "segments": [
          {
            "segment_id": "fe6e690d-ecad-4d7a-986d-63750c2865c8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 5,
            "page_width": 621,
            "page_height": 801,
            "content": "10  APRIL 2015 VOL. 40, NO. 2 www.usenix.org\r\nDISTRIBUTED SYSTEMS\r\nJump the Queue to Lower Latency\r\nEthernet Flow Control \r\nLike QJump, Ethernet Flow Control is a data link layer conges\u0002tion control mechanism. Hosts and switches issue special pause\r\nmessages when their queues are nearly full, alerting senders \r\nto slow down. Figure 3 shows that Ethernet Flow Control \r\n(Pause frames) has a limited positive impact on memcached \r\nbut increases the RMS offset for PTPd. Hadoop’s performance \r\nremains unaffected.\r\nEarly Congestion Notification (ECN) \r\nECN is a network-layer mechanism in which switches indicate \r\nqueueing to end hosts by marking TCP packets. Our Arista 7050 \r\nswitches implement ECN with weighted random early detection \r\n(WRED). The effectiveness of WRED depends on an administra\u0002tor correctly configuring upper and lower marking thresholds. \r\nWe investigated 10 different marking threshold pairs, ranging \r\nbetween [5, 10] and [2560, 5120], in packets. None of these \r\nsettings achieved ideal performance for all three applications, \r\nbut the best compromise was [40, 80]. With this configuration, \r\nECN very effectively resolves the interference experienced by \r\nPTPd and memcached. However, this comes at the expense of \r\nincreased Hadoop job runtimes.\r\nDatacenter TCP (DCTCP) \r\nDCTCP uses the rate at which ECN markings are received to \r\nbuild an estimate of network congestion. It applies this to a new \r\nTCP congestion avoidance algorithm to achieve lower queue\u0002ing delays. We configured DCTCP with the recommended ECN \r\nmarking thresholds of [65, 65]. Figure 3 shows that DCTCP \r\nreduces the variance in PTPd synchronization and memcached \r\nlatency compared to the contended case. However, this comes \r\nat an increase in Hadoop job runtimes, as Hadoop’s bulk data \r\ntransfers are affected by DCTCP’s congestion avoidance.\r\nQJump\r\nFigure 3 shows that QJump achieves the best results. The vari\u0002ance in Hadoop, PTPd, and memcached performance is close to \r\nthe uncontended ideal case.\r\nConclusion\r\nQJump applies QoS-inspired concepts to datacenter applications \r\nto mitigate network interference. It offers multiple QJump levels \r\nwith different latency variance vs. throughput tradeoffs, includ\u0002ing bounded latency (at low rate) and full utilization (at high \r\nlatency variance). QJump is readily deployable, open source, and \r\nrequires no hardware, protocol, or application changes. \r\nOur source code and all experimental data sets are available at \r\nhttp://www.cl.cam.ac.uk/research/srg/netos/qjump.\r\nReferences\r\n[1] J. Crowcroft, S. Hand, R. Mortier, T. Roscoe, and A. Warfield, \r\n“QoS’s Downfall: At the Bottom, or Not at All!” in Proceedings \r\nof the ACM SIGCOMM Workshop on Revisiting IP QoS, 2003, \r\npp. 109–114.\r\n[2] J. Dean and L. A. Barroso, “The Tail at Scale: Managing \r\nLatency Variability in Large-Scale Online Services,” Commu\u0002nications of the ACM, vol. 56, no. 2 (Feb. 2013), pp. 74–80.\r\n[3] M. P. Grosvenor, M. Schwarzkopf, I. Gog, R. N. M. Watson, \r\nA. W. Moore, S. Hand, and J. Crowcroft, “Queues Don’t Matter \r\nif You Can JUMP Them!” forthcoming in Proceedings of the \r\n12th USENIX Symposium on Networked Systems Design and \r\nImplementation (NSDI ’15), May 2015.\r\n[4] D. G. Murray, F. McSherry, R. Isaacs, M. Isard, and P. Bar\u0002ham, “Naiad: A Timely Dataflow System,” in Proceedings of \r\nthe ACM Symposium on Operating Systems Principles (SOSP),\r\n2013, pp. 439–455.\r\n[5] N. G. Duffield, P. Goyal, A. Greenberg, P. Mishra, K. K. \r\nRamakrishnan, and J. E. van der Merive, “A Flexible Model for \r\nResource Management in Virtual Private Networks,” in Pro\u0002ceedings of the ACM Conference on Applications, Technologies, \r\nArchitectures, and Protocols for Computer Communication \r\n(SIGCOMM), Aug. 1999, pp. 95–108.\r\nAcknowledgments\r\nThe full paper version of this work includes contributions from \r\nJon Crowcroft, Steven Hand, and Robert N. M. Watson. This \r\nwork was jointly supported by a Google Fellowship, EPSRC \r\nINTERNET Project EP/H040536/1, the Defense Advanced \r\nResearch Projects Agency (DARPA), and the Air Force Research \r\nLaboratory (AFRL), under contract FA8750-11-C-0249. The \r\nviews, opinions, and/or findings contained in this article are \r\nthose of the authors and should not be interpreted as represent\u0002ing the official views or policies, either expressed or implied, of \r\nthe Defense Advanced Research Projects Agency or the Depart\u0002ment of Defense. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/fe6e690d-ecad-4d7a-986d-63750c2865c8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=05870837038ba02d3c7843390775c15e87ff66b6dcd331ee05a38f1341373158",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 644
      },
      {
        "segments": [
          {
            "segment_id": "fe6e690d-ecad-4d7a-986d-63750c2865c8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 621,
              "height": 801
            },
            "page_number": 5,
            "page_width": 621,
            "page_height": 801,
            "content": "10  APRIL 2015 VOL. 40, NO. 2 www.usenix.org\r\nDISTRIBUTED SYSTEMS\r\nJump the Queue to Lower Latency\r\nEthernet Flow Control \r\nLike QJump, Ethernet Flow Control is a data link layer conges\u0002tion control mechanism. Hosts and switches issue special pause\r\nmessages when their queues are nearly full, alerting senders \r\nto slow down. Figure 3 shows that Ethernet Flow Control \r\n(Pause frames) has a limited positive impact on memcached \r\nbut increases the RMS offset for PTPd. Hadoop’s performance \r\nremains unaffected.\r\nEarly Congestion Notification (ECN) \r\nECN is a network-layer mechanism in which switches indicate \r\nqueueing to end hosts by marking TCP packets. Our Arista 7050 \r\nswitches implement ECN with weighted random early detection \r\n(WRED). The effectiveness of WRED depends on an administra\u0002tor correctly configuring upper and lower marking thresholds. \r\nWe investigated 10 different marking threshold pairs, ranging \r\nbetween [5, 10] and [2560, 5120], in packets. None of these \r\nsettings achieved ideal performance for all three applications, \r\nbut the best compromise was [40, 80]. With this configuration, \r\nECN very effectively resolves the interference experienced by \r\nPTPd and memcached. However, this comes at the expense of \r\nincreased Hadoop job runtimes.\r\nDatacenter TCP (DCTCP) \r\nDCTCP uses the rate at which ECN markings are received to \r\nbuild an estimate of network congestion. It applies this to a new \r\nTCP congestion avoidance algorithm to achieve lower queue\u0002ing delays. We configured DCTCP with the recommended ECN \r\nmarking thresholds of [65, 65]. Figure 3 shows that DCTCP \r\nreduces the variance in PTPd synchronization and memcached \r\nlatency compared to the contended case. However, this comes \r\nat an increase in Hadoop job runtimes, as Hadoop’s bulk data \r\ntransfers are affected by DCTCP’s congestion avoidance.\r\nQJump\r\nFigure 3 shows that QJump achieves the best results. The vari\u0002ance in Hadoop, PTPd, and memcached performance is close to \r\nthe uncontended ideal case.\r\nConclusion\r\nQJump applies QoS-inspired concepts to datacenter applications \r\nto mitigate network interference. It offers multiple QJump levels \r\nwith different latency variance vs. throughput tradeoffs, includ\u0002ing bounded latency (at low rate) and full utilization (at high \r\nlatency variance). QJump is readily deployable, open source, and \r\nrequires no hardware, protocol, or application changes. \r\nOur source code and all experimental data sets are available at \r\nhttp://www.cl.cam.ac.uk/research/srg/netos/qjump.\r\nReferences\r\n[1] J. Crowcroft, S. Hand, R. Mortier, T. Roscoe, and A. Warfield, \r\n“QoS’s Downfall: At the Bottom, or Not at All!” in Proceedings \r\nof the ACM SIGCOMM Workshop on Revisiting IP QoS, 2003, \r\npp. 109–114.\r\n[2] J. Dean and L. A. Barroso, “The Tail at Scale: Managing \r\nLatency Variability in Large-Scale Online Services,” Commu\u0002nications of the ACM, vol. 56, no. 2 (Feb. 2013), pp. 74–80.\r\n[3] M. P. Grosvenor, M. Schwarzkopf, I. Gog, R. N. M. Watson, \r\nA. W. Moore, S. Hand, and J. Crowcroft, “Queues Don’t Matter \r\nif You Can JUMP Them!” forthcoming in Proceedings of the \r\n12th USENIX Symposium on Networked Systems Design and \r\nImplementation (NSDI ’15), May 2015.\r\n[4] D. G. Murray, F. McSherry, R. Isaacs, M. Isard, and P. Bar\u0002ham, “Naiad: A Timely Dataflow System,” in Proceedings of \r\nthe ACM Symposium on Operating Systems Principles (SOSP),\r\n2013, pp. 439–455.\r\n[5] N. G. Duffield, P. Goyal, A. Greenberg, P. Mishra, K. K. \r\nRamakrishnan, and J. E. van der Merive, “A Flexible Model for \r\nResource Management in Virtual Private Networks,” in Pro\u0002ceedings of the ACM Conference on Applications, Technologies, \r\nArchitectures, and Protocols for Computer Communication \r\n(SIGCOMM), Aug. 1999, pp. 95–108.\r\nAcknowledgments\r\nThe full paper version of this work includes contributions from \r\nJon Crowcroft, Steven Hand, and Robert N. M. Watson. This \r\nwork was jointly supported by a Google Fellowship, EPSRC \r\nINTERNET Project EP/H040536/1, the Defense Advanced \r\nResearch Projects Agency (DARPA), and the Air Force Research \r\nLaboratory (AFRL), under contract FA8750-11-C-0249. The \r\nviews, opinions, and/or findings contained in this article are \r\nthose of the authors and should not be interpreted as represent\u0002ing the official views or policies, either expressed or implied, of \r\nthe Defense Advanced Research Projects Agency or the Depart\u0002ment of Defense. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/fe6e690d-ecad-4d7a-986d-63750c2865c8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=05870837038ba02d3c7843390775c15e87ff66b6dcd331ee05a38f1341373158",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 644
      },
      {
        "segments": [
          {
            "segment_id": "f038a583-bb3a-4981-86ae-4f9ad623ca84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 603,
              "height": 783
            },
            "page_number": 6,
            "page_width": 603,
            "page_height": 783,
            "content": "USENIX Awards\r\nUSENIX honors members of the community with three prestigious annual awards \r\nwhich recognize public service and technical excellence. The winners of these \r\nawards are selected by the USENIX Awards Committee. The USENIX membership \r\nmay submit nominations for any or all three of the awards to the committee.\r\nThe USENIX Lifetime Achievement (Flame) Award\r\nThe USENIX Lifetime Achievement Award recognizes and celebrates singular contri\u0002butions to the UNIX community in both intellectual achievement and service that \r\nare not recognized in any other forum. The award itself is in the form of an original \r\nglass sculpture called “The Flame,” and in the case of a team based at a single place, \r\na plaque for the team office.\r\nDetails and a list of past recipients are available at www.usenix.org/about/flame.\r\nThe Software Tools Users Group (STUG) Award\r\nThe Software Tools Users Group Award recognizes significant contributions to \r\nthe general community that reflect the spirit and character of those who came \r\ntogether to form the Software Tools Users Group (STUG). This is a cash award.\r\nSTUG and the Software Tools effort were characterized by two important tenets. \r\nThe first was an extraordinary focus on building portable, reusable libraries of code \r\nshared among multiple applications on wildly disparate systems. The other tenet, \r\nshared with the UNIX community, is “renegade empowerment.”\r\nThe Software Tools Users Group gave users the power to improve their environment \r\nwhen their platform provider proved inadequate, even when local management \r\nsided with the platform provider. Therefore, nominees for the STUG Award should exhibit one or both of these traits in a conspicuous \r\nmanner: a contribution to the reusable code-base available to all or the provision of a significant enabling technology directly to users \r\nin a widely available form.\r\nDetails and a list of past recipients are available at www.usenix.org/about/stug.\r\nThe LISA Award for Outstanding Achievement in System Administration\r\nThis annual award goes to someone whose professional contributions to the system administration community over a number of \r\nyears merit special recognition.\r\nDetails and a list of past recipients are available at www.usenix.org/lisa/awards/outstanding.\r\nwww.usenix.org/about/usenix-awards\r\nCall for Award Nominations\r\nUSENIX requests nominations for these \r\nthree awards; they may be from any \r\nmember of the community. Nominations \r\nshould be sent to the Chair of the Awards \r\nCommittee via awards@usenix.org by \r\nMay 1 each year. A nomination should \r\ninclude:\r\n1. Name and contact information of \r\nthe person making the nomination\r\n2. Name(s) and contact information of \r\nthe nominee(s)\r\n3. A citation, approximately 100 words \r\nlong\r\n4. A statement, at most one page long, \r\non why the candidate(s) should receive \r\nthe award\r\n5. Between two and four supporting \r\nletters, no longer than one page each\r\nNOMINATIONS DUE MAY 1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/3c51a980-0cd1-40d0-9840-fbc9988467a9/images/f038a583-bb3a-4981-86ae-4f9ad623ca84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041239Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=adcba4f977e85bc6b472eaea1fc1f41e264687e3008f565f23ddffccc0f834aa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 441
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "Jump the Queue to Lower Latency\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, and Andrew Moore\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "APRIL 2015"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "University of Cambridge"
        }
      ]
    }
  }
}