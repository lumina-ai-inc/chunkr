{
  "file_name": "LightGBM - A Highly-Efficient Gradient Boosting Decision Tree (2017).pdf",
  "task_id": "74b3a5fa-d7fa-44a7-b103-b0891defcb11",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "60cf5b17-12be-4d0d-9174-f02d4dde538d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "LightGBM: A Highly Efficient Gradient Boosting\r\nDecision Tree\r\nGuolin Ke1, Qi Meng2, Thomas Finley3, Taifeng Wang1,\r\nWei Chen1, Weidong Ma1, Qiwei Ye1, Tie-Yan Liu1\r\n1Microsoft Research 2Peking University 3 Microsoft Redmond\r\n1\r\n{guolin.ke, taifengw, wche, weima, qiwye, tie-yan.liu}@microsoft.com;\r\n2qimeng13@pku.edu.cn; 3\r\ntfinely@microsoft.com;\r\nAbstract\r\nGradient Boosting Decision Tree (GBDT) is a popular machine learning algo\u0002rithm, and has quite a few effective implementations such as XGBoost and pGBRT.\r\nAlthough many engineering optimizations have been adopted in these implemen\u0002tations, the efficiency and scalability are still unsatisfactory when the feature\r\ndimension is high and data size is large. A major reason is that for each feature,\r\nthey need to scan all the data instances to estimate the information gain of all\r\npossible split points, which is very time consuming. To tackle this problem, we\r\npropose two novel techniques: Gradient-based One-Side Sampling (GOSS) and\r\nExclusive Feature Bundling (EFB). With GOSS, we exclude a significant propor\u0002tion of data instances with small gradients, and only use the rest to estimate the\r\ninformation gain. We prove that, since the data instances with larger gradients play\r\na more important role in the computation of information gain, GOSS can obtain\r\nquite accurate estimation of the information gain with a much smaller data size.\r\nWith EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero\r\nvalues simultaneously), to reduce the number of features. We prove that finding\r\nthe optimal bundling of exclusive features is NP-hard, but a greedy algorithm\r\ncan achieve quite good approximation ratio (and thus can effectively reduce the\r\nnumber of features without hurting the accuracy of split point determination by\r\nmuch). We call our new GBDT implementation with GOSS and EFB LightGBM.\r\nOur experiments on multiple public datasets show that, LightGBM speeds up the\r\ntraining process of conventional GBDT by up to over 20 times while achieving\r\nalmost the same accuracy.\r\n1 Introduction\r\nGradient boosting decision tree (GBDT) [1] is a widely-used machine learning algorithm, due to\r\nits efficiency, accuracy, and interpretability. GBDT achieves state-of-the-art performances in many\r\nmachine learning tasks, such as multi-class classification [2], click prediction [3], and learning to\r\nrank [4]. In recent years, with the emergence of big data (in terms of both the number of features\r\nand the number of instances), GBDT is facing new challenges, especially in the tradeoff between\r\naccuracy and efficiency. Conventional implementations of GBDT need to, for every feature, scan all\r\nthe data instances to estimate the information gain of all the possible split points. Therefore, their\r\ncomputational complexities will be proportional to both the number of features and the number of\r\ninstances. This makes these implementations very time consuming when handling big data.\r\nTo tackle this challenge, a straightforward idea is to reduce the number of data instances and the\r\nnumber of features. However, this turns out to be highly non-trivial. For example, it is unclear how to\r\nperform data sampling for GBDT. While there are some works that sample data according to their\r\nweights to speed up the training process of boosting [5, 6, 7], they cannot be directly applied to GBDT\r\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/60cf5b17-12be-4d0d-9174-f02d4dde538d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=17db53146a05d1590558d74006fd442daaf5206510b2829efcced6a788bb7b5f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 518
      },
      {
        "segments": [
          {
            "segment_id": "60cf5b17-12be-4d0d-9174-f02d4dde538d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "LightGBM: A Highly Efficient Gradient Boosting\r\nDecision Tree\r\nGuolin Ke1, Qi Meng2, Thomas Finley3, Taifeng Wang1,\r\nWei Chen1, Weidong Ma1, Qiwei Ye1, Tie-Yan Liu1\r\n1Microsoft Research 2Peking University 3 Microsoft Redmond\r\n1\r\n{guolin.ke, taifengw, wche, weima, qiwye, tie-yan.liu}@microsoft.com;\r\n2qimeng13@pku.edu.cn; 3\r\ntfinely@microsoft.com;\r\nAbstract\r\nGradient Boosting Decision Tree (GBDT) is a popular machine learning algo\u0002rithm, and has quite a few effective implementations such as XGBoost and pGBRT.\r\nAlthough many engineering optimizations have been adopted in these implemen\u0002tations, the efficiency and scalability are still unsatisfactory when the feature\r\ndimension is high and data size is large. A major reason is that for each feature,\r\nthey need to scan all the data instances to estimate the information gain of all\r\npossible split points, which is very time consuming. To tackle this problem, we\r\npropose two novel techniques: Gradient-based One-Side Sampling (GOSS) and\r\nExclusive Feature Bundling (EFB). With GOSS, we exclude a significant propor\u0002tion of data instances with small gradients, and only use the rest to estimate the\r\ninformation gain. We prove that, since the data instances with larger gradients play\r\na more important role in the computation of information gain, GOSS can obtain\r\nquite accurate estimation of the information gain with a much smaller data size.\r\nWith EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero\r\nvalues simultaneously), to reduce the number of features. We prove that finding\r\nthe optimal bundling of exclusive features is NP-hard, but a greedy algorithm\r\ncan achieve quite good approximation ratio (and thus can effectively reduce the\r\nnumber of features without hurting the accuracy of split point determination by\r\nmuch). We call our new GBDT implementation with GOSS and EFB LightGBM.\r\nOur experiments on multiple public datasets show that, LightGBM speeds up the\r\ntraining process of conventional GBDT by up to over 20 times while achieving\r\nalmost the same accuracy.\r\n1 Introduction\r\nGradient boosting decision tree (GBDT) [1] is a widely-used machine learning algorithm, due to\r\nits efficiency, accuracy, and interpretability. GBDT achieves state-of-the-art performances in many\r\nmachine learning tasks, such as multi-class classification [2], click prediction [3], and learning to\r\nrank [4]. In recent years, with the emergence of big data (in terms of both the number of features\r\nand the number of instances), GBDT is facing new challenges, especially in the tradeoff between\r\naccuracy and efficiency. Conventional implementations of GBDT need to, for every feature, scan all\r\nthe data instances to estimate the information gain of all the possible split points. Therefore, their\r\ncomputational complexities will be proportional to both the number of features and the number of\r\ninstances. This makes these implementations very time consuming when handling big data.\r\nTo tackle this challenge, a straightforward idea is to reduce the number of data instances and the\r\nnumber of features. However, this turns out to be highly non-trivial. For example, it is unclear how to\r\nperform data sampling for GBDT. While there are some works that sample data according to their\r\nweights to speed up the training process of boosting [5, 6, 7], they cannot be directly applied to GBDT\r\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/60cf5b17-12be-4d0d-9174-f02d4dde538d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=17db53146a05d1590558d74006fd442daaf5206510b2829efcced6a788bb7b5f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 518
      },
      {
        "segments": [
          {
            "segment_id": "623ede5c-b500-4516-967e-bbe4bc04f52d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "since there is no sample weight in GBDT at all. In this paper, we propose two novel techniques\r\ntowards this goal, as elaborated below.\r\nGradient-based One-Side Sampling (GOSS). While there is no native weight for data instance in\r\nGBDT, we notice that data instances with different gradients play different roles in the computation\r\nof information gain. In particular, according to the definition of information gain, those instances\r\nwith larger gradients1(i.e., under-trained instances) will contribute more to the information gain.\r\nTherefore, when down sampling the data instances, in order to retain the accuracy of information gain\r\nestimation, we should better keep those instances with large gradients (e.g., larger than a pre-defined\r\nthreshold, or among the top percentiles), and only randomly drop those instances with small gradients.\r\nWe prove that such a treatment can lead to a more accurate gain estimation than uniformly random\r\nsampling, with the same target sampling rate, especially when the value of information gain has a\r\nlarge range.\r\nExclusive Feature Bundling (EFB). Usually in real applications, although there are a large number\r\nof features, the feature space is quite sparse, which provides us a possibility of designing a nearly\r\nlossless approach to reduce the number of effective features. Specifically, in a sparse feature space,\r\nmany features are (almost) exclusive, i.e., they rarely take nonzero values simultaneously. Examples\r\ninclude the one-hot features (e.g., one-hot word representation in text mining). We can safely bundle\r\nsuch exclusive features. To this end, we design an efficient algorithm by reducing the optimal\r\nbundling problem to a graph coloring problem (by taking features as vertices and adding edges for\r\nevery two features if they are not mutually exclusive), and solving it by a greedy algorithm with a\r\nconstant approximation ratio.\r\nWe call the new GBDT algorithm with GOSS and EFB LightGBM2. Our experiments on multiple\r\npublic datasets show that LightGBM can accelerate the training process by up to over 20 times while\r\nachieving almost the same accuracy.\r\nThe remaining of this paper is organized as follows. At first, we review GBDT algorithms and related\r\nwork in Sec. 2. Then, we introduce the details of GOSS in Sec. 3 and EFB in Sec. 4. Our experiments\r\nfor LightGBM on public datasets are presented in Sec. 5. Finally, we conclude the paper in Sec. 6.\r\n2 Preliminaries\r\n2.1 GBDT and Its Complexity Analysis\r\nGBDT is an ensemble model of decision trees, which are trained in sequence [1]. In each iteration,\r\nGBDT learns the decision trees by fitting the negative gradients (also known as residual errors).\r\nThe main cost in GBDT lies in learning the decision trees, and the most time-consuming part in\r\nlearning a decision tree is to find the best split points. One of the most popular algorithms to find split\r\npoints is the pre-sorted algorithm [8, 9], which enumerates all possible split points on the pre-sorted\r\nfeature values. This algorithm is simple and can find the optimal split points, however, it is inefficient\r\nin both training speed and memory consumption. Another popular algorithm is the histogram-based\r\nalgorithm [10, 11, 12], as shown in Alg. 13. Instead of finding the split points on the sorted feature\r\nvalues, histogram-based algorithm buckets continuous feature values into discrete bins and uses these\r\nbins to construct feature histograms during training. Since the histogram-based algorithm is more\r\nefficient in both memory consumption and training speed, we will develop our work on its basis.\r\nAs shown in Alg. 1, the histogram-based algorithm finds the best split points based on the feature\r\nhistograms. It costs O(#data × #feature) for histogram building and O(#bin × #feature) for\r\nsplit point finding. Since #bin is usually much smaller than #data, histogram building will dominate\r\nthe computational complexity. If we can reduce #data or #feature, we will be able to substantially\r\nspeed up the training of GBDT.\r\n2.2 Related Work\r\nThere have been quite a few implementations of GBDT in the literature, including XGBoost [13],\r\npGBRT [14], scikit-learn [15], and gbm in R [16]\r\n4\r\n. Scikit-learn and gbm in R implements the pre\u0002sorted algorithm, and pGBRT implements the histogram-based algorithm. XGBoost supports both\r\n1When we say larger or smaller gradients in this paper, we refer to their absolute values.\r\n2The code is available at GitHub: https://github.com/Microsoft/LightGBM.\r\n3Due to space restriction, high level pseudo code is used. The details could be found in our open-source code.\r\n4There are some other works speed up GBDT training via GPU [17, 18], or parallel training [19]. However,\r\nthey are out of the scope of this paper.\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/623ede5c-b500-4516-967e-bbe4bc04f52d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4ac84082e9cfe08ecdcf366a7e9bc057556e4075b23acef56abf36aefbb5054a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 748
      },
      {
        "segments": [
          {
            "segment_id": "623ede5c-b500-4516-967e-bbe4bc04f52d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "since there is no sample weight in GBDT at all. In this paper, we propose two novel techniques\r\ntowards this goal, as elaborated below.\r\nGradient-based One-Side Sampling (GOSS). While there is no native weight for data instance in\r\nGBDT, we notice that data instances with different gradients play different roles in the computation\r\nof information gain. In particular, according to the definition of information gain, those instances\r\nwith larger gradients1(i.e., under-trained instances) will contribute more to the information gain.\r\nTherefore, when down sampling the data instances, in order to retain the accuracy of information gain\r\nestimation, we should better keep those instances with large gradients (e.g., larger than a pre-defined\r\nthreshold, or among the top percentiles), and only randomly drop those instances with small gradients.\r\nWe prove that such a treatment can lead to a more accurate gain estimation than uniformly random\r\nsampling, with the same target sampling rate, especially when the value of information gain has a\r\nlarge range.\r\nExclusive Feature Bundling (EFB). Usually in real applications, although there are a large number\r\nof features, the feature space is quite sparse, which provides us a possibility of designing a nearly\r\nlossless approach to reduce the number of effective features. Specifically, in a sparse feature space,\r\nmany features are (almost) exclusive, i.e., they rarely take nonzero values simultaneously. Examples\r\ninclude the one-hot features (e.g., one-hot word representation in text mining). We can safely bundle\r\nsuch exclusive features. To this end, we design an efficient algorithm by reducing the optimal\r\nbundling problem to a graph coloring problem (by taking features as vertices and adding edges for\r\nevery two features if they are not mutually exclusive), and solving it by a greedy algorithm with a\r\nconstant approximation ratio.\r\nWe call the new GBDT algorithm with GOSS and EFB LightGBM2. Our experiments on multiple\r\npublic datasets show that LightGBM can accelerate the training process by up to over 20 times while\r\nachieving almost the same accuracy.\r\nThe remaining of this paper is organized as follows. At first, we review GBDT algorithms and related\r\nwork in Sec. 2. Then, we introduce the details of GOSS in Sec. 3 and EFB in Sec. 4. Our experiments\r\nfor LightGBM on public datasets are presented in Sec. 5. Finally, we conclude the paper in Sec. 6.\r\n2 Preliminaries\r\n2.1 GBDT and Its Complexity Analysis\r\nGBDT is an ensemble model of decision trees, which are trained in sequence [1]. In each iteration,\r\nGBDT learns the decision trees by fitting the negative gradients (also known as residual errors).\r\nThe main cost in GBDT lies in learning the decision trees, and the most time-consuming part in\r\nlearning a decision tree is to find the best split points. One of the most popular algorithms to find split\r\npoints is the pre-sorted algorithm [8, 9], which enumerates all possible split points on the pre-sorted\r\nfeature values. This algorithm is simple and can find the optimal split points, however, it is inefficient\r\nin both training speed and memory consumption. Another popular algorithm is the histogram-based\r\nalgorithm [10, 11, 12], as shown in Alg. 13. Instead of finding the split points on the sorted feature\r\nvalues, histogram-based algorithm buckets continuous feature values into discrete bins and uses these\r\nbins to construct feature histograms during training. Since the histogram-based algorithm is more\r\nefficient in both memory consumption and training speed, we will develop our work on its basis.\r\nAs shown in Alg. 1, the histogram-based algorithm finds the best split points based on the feature\r\nhistograms. It costs O(#data × #feature) for histogram building and O(#bin × #feature) for\r\nsplit point finding. Since #bin is usually much smaller than #data, histogram building will dominate\r\nthe computational complexity. If we can reduce #data or #feature, we will be able to substantially\r\nspeed up the training of GBDT.\r\n2.2 Related Work\r\nThere have been quite a few implementations of GBDT in the literature, including XGBoost [13],\r\npGBRT [14], scikit-learn [15], and gbm in R [16]\r\n4\r\n. Scikit-learn and gbm in R implements the pre\u0002sorted algorithm, and pGBRT implements the histogram-based algorithm. XGBoost supports both\r\n1When we say larger or smaller gradients in this paper, we refer to their absolute values.\r\n2The code is available at GitHub: https://github.com/Microsoft/LightGBM.\r\n3Due to space restriction, high level pseudo code is used. The details could be found in our open-source code.\r\n4There are some other works speed up GBDT training via GPU [17, 18], or parallel training [19]. However,\r\nthey are out of the scope of this paper.\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/623ede5c-b500-4516-967e-bbe4bc04f52d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4ac84082e9cfe08ecdcf366a7e9bc057556e4075b23acef56abf36aefbb5054a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 748
      },
      {
        "segments": [
          {
            "segment_id": "b24c6b13-283f-47d7-b7d0-743636525489",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "the pre-sorted algorithm and histogram-based algorithm. As shown in [13], XGBoost outperforms\r\nthe other tools. So, we use XGBoost as our baseline in the experiment section.\r\nTo reduce the size of the training data, a common approach is to down sample the data instances. For\r\nexample, in [5], data instances are filtered if their weights are smaller than a fixed threshold. SGB\r\n[20] uses a random subset to train the weak learners in every iteration. In [6], the sampling ratio are\r\ndynamically adjusted in the training progress. However, all these works except SGB [20] are based\r\non AdaBoost [21], and cannot be directly applied to GBDT since there are no native weights for data\r\ninstances in GBDT. Though SGB can be applied to GBDT, it usually hurts accuracy and thus it is not\r\na desirable choice.\r\nSimilarly, to reduce the number of features, it is natural to filter weak features [22, 23, 7, 24]. This\r\nis usually done by principle component analysis or projection pursuit. However, these approaches\r\nhighly rely on the assumption that features contain significant redundancy, which might not always\r\nbe true in practice (features are usually designed with their unique contributions and removing any of\r\nthem may affect the training accuracy to some degree).\r\nThe large-scale datasets used in real applications are usually quite sparse. GBDT with the pre-sorted\r\nalgorithm can reduce the training cost by ignoring the features with zero values [13]. However,\r\nGBDT with the histogram-based algorithm does not have efficient sparse optimization solutions. The\r\nreason is that the histogram-based algorithm needs to retrieve feature bin values (refer to Alg. 1) for\r\neach data instance no matter the feature value is zero or not. It is highly preferred that GBDT with\r\nthe histogram-based algorithm can effectively leverage such sparse property.\r\nTo address the limitations of previous works, we propose two new novel techniques called Gradient\u0002based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). More details will be\r\nintroduced in the next sections.\r\nAlgorithm 1: Histogram-based Algorithm\r\nInput: I: training data, d: max depth\r\nInput: m: feature dimension\r\nnodeSet ← {0} . tree nodes in current level\r\nrowSet ← {{0, 1, 2, ...}} . data indices in tree nodes\r\nfor i = 1 to d do\r\nfor node in nodeSet do\r\nusedRows ← rowSet[node]\r\nfor k = 1 to m do\r\nH ← new Histogram()\r\n. Build histogram\r\nfor j in usedRows do\r\nbin ← I.f[k][j].bin\r\nH[bin].y ← H[bin].y + I.y[j]\r\nH[bin].n ← H[bin].n + 1\r\nFind the best split on histogram H.\r\n...\r\nUpdate rowSet and nodeSet according to the best\r\nsplit points.\r\n...\r\nAlgorithm 2: Gradient-based One-Side Sampling\r\nInput: I: training data, d: iterations\r\nInput: a: sampling ratio of large gradient data\r\nInput: b: sampling ratio of small gradient data\r\nInput: loss: loss function, L: weak learner\r\nmodels ← {}, fact ← 1−a\r\nb\r\ntopN ← a × len(I) , randN ← b × len(I)\r\nfor i = 1 to d do\r\npreds ← models.predict(I)\r\ng ← loss(I, preds), w ← {1,1,...}\r\nsorted ← GetSortedIndices(abs(g))\r\ntopSet ← sorted[1:topN]\r\nrandSet ← RandomPick(sorted[topN:len(I)],\r\nrandN)\r\nusedSet ← topSet + randSet\r\nw[randSet] × = fact . Assign weight f act to the\r\nsmall gradient data.\r\nnewModel ← L(I[usedSet], − g[usedSet],\r\nw[usedSet])\r\nmodels.append(newModel)\r\n3 Gradient-based One-Side Sampling\r\nIn this section, we propose a novel sampling method for GBDT that can achieve a good balance\r\nbetween reducing the number of data instances and keeping the accuracy for learned decision trees.\r\n3.1 Algorithm Description\r\nIn AdaBoost, the sample weight serves as a good indicator for the importance of data instances.\r\nHowever, in GBDT, there are no native sample weights, and thus the sampling methods proposed for\r\nAdaBoost cannot be directly applied. Fortunately, we notice that the gradient for each data instance\r\nin GBDT provides us with useful information for data sampling. That is, if an instance is associated\r\nwith a small gradient, the training error for this instance is small and it is already well-trained.\r\nA straightforward idea is to discard those data instances with small gradients. However, the data\r\ndistribution will be changed by doing so, which will hurt the accuracy of the learned model. To avoid\r\nthis problem, we propose a new method called Gradient-based One-Side Sampling (GOSS).\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/b24c6b13-283f-47d7-b7d0-743636525489.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d7d3e648b0d1c08af3dc1b20b28d78e875e6d9f69d4e3d09f691be88023aea9b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 700
      },
      {
        "segments": [
          {
            "segment_id": "b24c6b13-283f-47d7-b7d0-743636525489",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "the pre-sorted algorithm and histogram-based algorithm. As shown in [13], XGBoost outperforms\r\nthe other tools. So, we use XGBoost as our baseline in the experiment section.\r\nTo reduce the size of the training data, a common approach is to down sample the data instances. For\r\nexample, in [5], data instances are filtered if their weights are smaller than a fixed threshold. SGB\r\n[20] uses a random subset to train the weak learners in every iteration. In [6], the sampling ratio are\r\ndynamically adjusted in the training progress. However, all these works except SGB [20] are based\r\non AdaBoost [21], and cannot be directly applied to GBDT since there are no native weights for data\r\ninstances in GBDT. Though SGB can be applied to GBDT, it usually hurts accuracy and thus it is not\r\na desirable choice.\r\nSimilarly, to reduce the number of features, it is natural to filter weak features [22, 23, 7, 24]. This\r\nis usually done by principle component analysis or projection pursuit. However, these approaches\r\nhighly rely on the assumption that features contain significant redundancy, which might not always\r\nbe true in practice (features are usually designed with their unique contributions and removing any of\r\nthem may affect the training accuracy to some degree).\r\nThe large-scale datasets used in real applications are usually quite sparse. GBDT with the pre-sorted\r\nalgorithm can reduce the training cost by ignoring the features with zero values [13]. However,\r\nGBDT with the histogram-based algorithm does not have efficient sparse optimization solutions. The\r\nreason is that the histogram-based algorithm needs to retrieve feature bin values (refer to Alg. 1) for\r\neach data instance no matter the feature value is zero or not. It is highly preferred that GBDT with\r\nthe histogram-based algorithm can effectively leverage such sparse property.\r\nTo address the limitations of previous works, we propose two new novel techniques called Gradient\u0002based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). More details will be\r\nintroduced in the next sections.\r\nAlgorithm 1: Histogram-based Algorithm\r\nInput: I: training data, d: max depth\r\nInput: m: feature dimension\r\nnodeSet ← {0} . tree nodes in current level\r\nrowSet ← {{0, 1, 2, ...}} . data indices in tree nodes\r\nfor i = 1 to d do\r\nfor node in nodeSet do\r\nusedRows ← rowSet[node]\r\nfor k = 1 to m do\r\nH ← new Histogram()\r\n. Build histogram\r\nfor j in usedRows do\r\nbin ← I.f[k][j].bin\r\nH[bin].y ← H[bin].y + I.y[j]\r\nH[bin].n ← H[bin].n + 1\r\nFind the best split on histogram H.\r\n...\r\nUpdate rowSet and nodeSet according to the best\r\nsplit points.\r\n...\r\nAlgorithm 2: Gradient-based One-Side Sampling\r\nInput: I: training data, d: iterations\r\nInput: a: sampling ratio of large gradient data\r\nInput: b: sampling ratio of small gradient data\r\nInput: loss: loss function, L: weak learner\r\nmodels ← {}, fact ← 1−a\r\nb\r\ntopN ← a × len(I) , randN ← b × len(I)\r\nfor i = 1 to d do\r\npreds ← models.predict(I)\r\ng ← loss(I, preds), w ← {1,1,...}\r\nsorted ← GetSortedIndices(abs(g))\r\ntopSet ← sorted[1:topN]\r\nrandSet ← RandomPick(sorted[topN:len(I)],\r\nrandN)\r\nusedSet ← topSet + randSet\r\nw[randSet] × = fact . Assign weight f act to the\r\nsmall gradient data.\r\nnewModel ← L(I[usedSet], − g[usedSet],\r\nw[usedSet])\r\nmodels.append(newModel)\r\n3 Gradient-based One-Side Sampling\r\nIn this section, we propose a novel sampling method for GBDT that can achieve a good balance\r\nbetween reducing the number of data instances and keeping the accuracy for learned decision trees.\r\n3.1 Algorithm Description\r\nIn AdaBoost, the sample weight serves as a good indicator for the importance of data instances.\r\nHowever, in GBDT, there are no native sample weights, and thus the sampling methods proposed for\r\nAdaBoost cannot be directly applied. Fortunately, we notice that the gradient for each data instance\r\nin GBDT provides us with useful information for data sampling. That is, if an instance is associated\r\nwith a small gradient, the training error for this instance is small and it is already well-trained.\r\nA straightforward idea is to discard those data instances with small gradients. However, the data\r\ndistribution will be changed by doing so, which will hurt the accuracy of the learned model. To avoid\r\nthis problem, we propose a new method called Gradient-based One-Side Sampling (GOSS).\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/b24c6b13-283f-47d7-b7d0-743636525489.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d7d3e648b0d1c08af3dc1b20b28d78e875e6d9f69d4e3d09f691be88023aea9b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 700
      },
      {
        "segments": [
          {
            "segment_id": "a1ab4fef-4e83-438c-9ae9-a190c7c13f19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "GOSS keeps all the instances with large gradients and performs random sampling on the instances\r\nwith small gradients. In order to compensate the influence to the data distribution, when computing the\r\ninformation gain, GOSS introduces a constant multiplier for the data instances with small gradients\r\n(see Alg. 2). Specifically, GOSS firstly sorts the data instances according to the absolute value of their\r\ngradients and selects the top a×100% instances. Then it randomly samples b×100% instances from\r\nthe rest of the data. After that, GOSS amplifies the sampled data with small gradients by a constant\r\n1−a\r\nb when calculating the information gain. By doing so, we put more focus on the under-trained\r\ninstances without changing the original data distribution by much.\r\n3.2 Theoretical Analysis\r\nGBDT uses decision trees to learn a function from the input space X\r\ns\r\nto the gradient space G [1].\r\nSuppose that we have a training set with n i.i.d. instances {x1, · · · , xn}, where each xiis a vector\r\nwith dimension s in space X\r\ns\r\n. In each iteration of gradient boosting, the negative gradients of the\r\nloss function with respect to the output of the model are denoted as {g1, · · · , gn}. The decision tree\r\nmodel splits each node at the most informative feature (with the largest information gain). For GBDT,\r\nthe information gain is usually measured by the variance after splitting, which is defined as below.\r\nDefinition 3.1 Let O be the training dataset on a fixed node of the decision tree. The variance gain\r\nof splitting feature j at point d for this node is defined as\r\nVj|O(d) = 1\r\nnO\r\n \r\n(\r\nP\r\n{xi∈O:xij≤d}\r\ngi)\r\n2\r\nn\r\nj\r\nl|O\r\n(d)\r\n+\r\n(\r\nP\r\n{xi∈O:xij>d}\r\ngi)\r\n2\r\nn\r\nj\r\nr|O\r\n(d)\r\n!\r\n,\r\nwhere nO =\r\nPI[xi ∈ O], n\r\nj\r\nl|O\r\n(d) = PI[xi ∈ O : xij ≤ d] and n\r\nj\r\nr|O\r\n(d) = PI[xi ∈ O : xij > d].\r\nFor feature j, the decision tree algorithm selects d\r\n∗\r\nj = argmaxdVj (d) and calculates the largest gain\r\nVj (d\r\n∗\r\nj ).\r\n5 Then, the data are split according feature j∗\r\nat point dj\r\n∗ into the left and right child nodes.\r\nIn our proposed GOSS method, first, we rank the training instances according to their absolute values\r\nof their gradients in the descending order; second, we keep the top-a × 100% instances with the larger\r\ngradients and get an instance subset A; then, for the remaining set Acconsisting (1 − a) × 100%\r\ninstances with smaller gradients, we further randomly sample a subset B with size b × |Ac|; finally,\r\nwe split the instances according to the estimated variance gain V˜\r\nj (d) over the subset A ∪ B, i.e.,\r\nV˜j (d) = 1\r\nn\r\n \r\n(\r\nP\r\nxi∈Al\r\ngi +\r\n1−a\r\nb\r\nP\r\nxi∈Bl\r\ngi)\r\n2\r\nn\r\nj\r\nl\r\n(d)\r\n+\r\n(\r\nP\r\nxi∈Ar\r\ngi +\r\n1−a\r\nb\r\nP\r\nxi∈Br\r\ngi)\r\n2\r\nn\r\nj\r\nr(d)\r\n!\r\n, (1)\r\nwhere Al = {xi ∈ A : xij ≤ d},Ar = {xi ∈ A : xij > d},Bl = {xi ∈ B : xij ≤ d},Br = {xi ∈ B :\r\nxij > d}, and the coefficient 1−a\r\nb\r\nis used to normalize the sum of the gradients over B back to the\r\nsize of Ac.\r\nThus, in GOSS, we use the estimated V˜\r\nj (d) over a smaller instance subset, instead of the accurate\r\nVj (d) over all the instances to determine the split point, and the computation cost can be largely\r\nreduced. More importantly, the following theorem indicates that GOSS will not lose much training\r\naccuracy and will outperform random sampling. Due to space restrictions, we leave the proof of the\r\ntheorem to the supplementary materials.\r\nTheorem 3.2 We denote the approximation error in GOSS as E(d) = |V˜j (d) − Vj (d)| and g¯\r\nj\r\nl P\r\n(d) =\r\nxi∈(A∪Ac)l\r\n|gi|\r\nn\r\nj\r\nl\r\n(d)\r\n, g¯\r\nj\r\nr(d) =\r\nP\r\nxi∈(A∪Ac)r\r\n|gi|\r\nn\r\nj\r\nr(d)\r\n. With probability at least 1 − δ, we have\r\nE(d) ≤ C\r\n2\r\na,b ln 1/δ · max (\r\n1\r\nn\r\nj\r\nl\r\n(d)\r\n,\r\n1\r\nn\r\nj\r\nr(d)\r\n)\r\n+ 2DCa,br\r\nln 1/δ\r\nn\r\n, (2)\r\nwhere Ca,b =\r\n1√−a\r\nb\r\nmaxxi∈Ac |gi|, and D = max(¯g\r\nj\r\nl\r\n(d), g¯\r\nj\r\nr(d)).\r\nAccording to the theorem, we have the following discussions: (1) The asymptotic approximation ratio\r\nof GOSS is O\r\n\u0012\r\n1\r\nn\r\nj\r\nl\r\n(d)\r\n+\r\n1\r\nn\r\nj\r\nr(d)\r\n+ √1\r\nn\r\n\u0013\r\n. If the split is not too unbalanced (i.e., n\r\nj\r\nl\r\n(d) ≥ O(\r\n√\r\nn) and\r\nn\r\nj\r\nr(d) ≥ O(\r\n√\r\nn)), the approximation error will be dominated by the second term of Ineq.(2) which\r\n5Our following analysis holds for arbitrary node. For simplicity and without confusion, we omit the sub-index\r\nO in all the notations.\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/a1ab4fef-4e83-438c-9ae9-a190c7c13f19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8559d9644712b5da7765f00d72d6da373d30e6f31fed393700672f19ea30e398",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 814
      },
      {
        "segments": [
          {
            "segment_id": "a1ab4fef-4e83-438c-9ae9-a190c7c13f19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "GOSS keeps all the instances with large gradients and performs random sampling on the instances\r\nwith small gradients. In order to compensate the influence to the data distribution, when computing the\r\ninformation gain, GOSS introduces a constant multiplier for the data instances with small gradients\r\n(see Alg. 2). Specifically, GOSS firstly sorts the data instances according to the absolute value of their\r\ngradients and selects the top a×100% instances. Then it randomly samples b×100% instances from\r\nthe rest of the data. After that, GOSS amplifies the sampled data with small gradients by a constant\r\n1−a\r\nb when calculating the information gain. By doing so, we put more focus on the under-trained\r\ninstances without changing the original data distribution by much.\r\n3.2 Theoretical Analysis\r\nGBDT uses decision trees to learn a function from the input space X\r\ns\r\nto the gradient space G [1].\r\nSuppose that we have a training set with n i.i.d. instances {x1, · · · , xn}, where each xiis a vector\r\nwith dimension s in space X\r\ns\r\n. In each iteration of gradient boosting, the negative gradients of the\r\nloss function with respect to the output of the model are denoted as {g1, · · · , gn}. The decision tree\r\nmodel splits each node at the most informative feature (with the largest information gain). For GBDT,\r\nthe information gain is usually measured by the variance after splitting, which is defined as below.\r\nDefinition 3.1 Let O be the training dataset on a fixed node of the decision tree. The variance gain\r\nof splitting feature j at point d for this node is defined as\r\nVj|O(d) = 1\r\nnO\r\n \r\n(\r\nP\r\n{xi∈O:xij≤d}\r\ngi)\r\n2\r\nn\r\nj\r\nl|O\r\n(d)\r\n+\r\n(\r\nP\r\n{xi∈O:xij>d}\r\ngi)\r\n2\r\nn\r\nj\r\nr|O\r\n(d)\r\n!\r\n,\r\nwhere nO =\r\nPI[xi ∈ O], n\r\nj\r\nl|O\r\n(d) = PI[xi ∈ O : xij ≤ d] and n\r\nj\r\nr|O\r\n(d) = PI[xi ∈ O : xij > d].\r\nFor feature j, the decision tree algorithm selects d\r\n∗\r\nj = argmaxdVj (d) and calculates the largest gain\r\nVj (d\r\n∗\r\nj ).\r\n5 Then, the data are split according feature j∗\r\nat point dj\r\n∗ into the left and right child nodes.\r\nIn our proposed GOSS method, first, we rank the training instances according to their absolute values\r\nof their gradients in the descending order; second, we keep the top-a × 100% instances with the larger\r\ngradients and get an instance subset A; then, for the remaining set Acconsisting (1 − a) × 100%\r\ninstances with smaller gradients, we further randomly sample a subset B with size b × |Ac|; finally,\r\nwe split the instances according to the estimated variance gain V˜\r\nj (d) over the subset A ∪ B, i.e.,\r\nV˜j (d) = 1\r\nn\r\n \r\n(\r\nP\r\nxi∈Al\r\ngi +\r\n1−a\r\nb\r\nP\r\nxi∈Bl\r\ngi)\r\n2\r\nn\r\nj\r\nl\r\n(d)\r\n+\r\n(\r\nP\r\nxi∈Ar\r\ngi +\r\n1−a\r\nb\r\nP\r\nxi∈Br\r\ngi)\r\n2\r\nn\r\nj\r\nr(d)\r\n!\r\n, (1)\r\nwhere Al = {xi ∈ A : xij ≤ d},Ar = {xi ∈ A : xij > d},Bl = {xi ∈ B : xij ≤ d},Br = {xi ∈ B :\r\nxij > d}, and the coefficient 1−a\r\nb\r\nis used to normalize the sum of the gradients over B back to the\r\nsize of Ac.\r\nThus, in GOSS, we use the estimated V˜\r\nj (d) over a smaller instance subset, instead of the accurate\r\nVj (d) over all the instances to determine the split point, and the computation cost can be largely\r\nreduced. More importantly, the following theorem indicates that GOSS will not lose much training\r\naccuracy and will outperform random sampling. Due to space restrictions, we leave the proof of the\r\ntheorem to the supplementary materials.\r\nTheorem 3.2 We denote the approximation error in GOSS as E(d) = |V˜j (d) − Vj (d)| and g¯\r\nj\r\nl P\r\n(d) =\r\nxi∈(A∪Ac)l\r\n|gi|\r\nn\r\nj\r\nl\r\n(d)\r\n, g¯\r\nj\r\nr(d) =\r\nP\r\nxi∈(A∪Ac)r\r\n|gi|\r\nn\r\nj\r\nr(d)\r\n. With probability at least 1 − δ, we have\r\nE(d) ≤ C\r\n2\r\na,b ln 1/δ · max (\r\n1\r\nn\r\nj\r\nl\r\n(d)\r\n,\r\n1\r\nn\r\nj\r\nr(d)\r\n)\r\n+ 2DCa,br\r\nln 1/δ\r\nn\r\n, (2)\r\nwhere Ca,b =\r\n1√−a\r\nb\r\nmaxxi∈Ac |gi|, and D = max(¯g\r\nj\r\nl\r\n(d), g¯\r\nj\r\nr(d)).\r\nAccording to the theorem, we have the following discussions: (1) The asymptotic approximation ratio\r\nof GOSS is O\r\n\u0012\r\n1\r\nn\r\nj\r\nl\r\n(d)\r\n+\r\n1\r\nn\r\nj\r\nr(d)\r\n+ √1\r\nn\r\n\u0013\r\n. If the split is not too unbalanced (i.e., n\r\nj\r\nl\r\n(d) ≥ O(\r\n√\r\nn) and\r\nn\r\nj\r\nr(d) ≥ O(\r\n√\r\nn)), the approximation error will be dominated by the second term of Ineq.(2) which\r\n5Our following analysis holds for arbitrary node. For simplicity and without confusion, we omit the sub-index\r\nO in all the notations.\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/a1ab4fef-4e83-438c-9ae9-a190c7c13f19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8559d9644712b5da7765f00d72d6da373d30e6f31fed393700672f19ea30e398",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 814
      },
      {
        "segments": [
          {
            "segment_id": "2ff752af-1465-4ac4-b5a0-d2ba817869b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "decreases to 0 in O(\r\n√\r\nn) with n → ∞. That means when number of data is large, the approximation\r\nis quite accurate. (2) Random sampling is a special case of GOSS with a = 0. In many cases,\r\nGOSS could outperform random sampling, under the condition C0,β > Ca,β−a, which is equivalent\r\nto √αa\r\nβ > √1−aβ−a with αa = maxxi∈A∪Ac |gi|/ maxxi∈Ac |gi|.\r\nNext, we analyze the generalization performance in GOSS. We consider the generalization error in\r\nGOSS E\r\nGOSS\r\ngen (d) = |V˜j (d) − V∗(d)|, which is the gap between the variance gain calculated by the\r\nsampled training instances in GOSS and the true variance gain for the underlying distribution. We\r\nhave E\r\nGOSS\r\ngen (d) ≤ |V˜j (d) − Vj (d)| + |Vj (d) − V∗(d)|\r\n∆= EGOSS(d) + Egen(d). Thus, the generalization\r\nerror with GOSS will be close to that calculated by using the full data instances if the GOSS\r\napproximation is accurate. On the other hand, sampling will increase the diversity of the base learners,\r\nwhich potentially help to improve the generalization performance [24].\r\n4 Exclusive Feature Bundling\r\nIn this section, we propose a novel method to effectively reduce the number of features.\r\nAlgorithm 3: Greedy Bundling\r\nInput: F: features, K: max conflict count\r\nConstruct graph G\r\nsearchOrder ← G.sortByDegree()\r\nbundles ← {}, bundlesConflict ← {}\r\nfor i in searchOrder do\r\nneedNew ← True\r\nfor j = 1 to len(bundles) do\r\ncnt ← ConflictCnt(bundles[j],F[i])\r\nif cnt + bundlesConflict[i] ≤ K then\r\nbundles[j].add(F[i]), needNew ← False\r\nbreak\r\nif needNew then\r\nAdd F[i] as a new bundle to bundles\r\nOutput: bundles\r\nAlgorithm 4: Merge Exclusive Features\r\nInput: numData: number of data\r\nInput: F: One bundle of exclusive features\r\nbinRanges ← {0}, totalBin ← 0\r\nfor f in F do\r\ntotalBin += f.numBin\r\nbinRanges.append(totalBin)\r\nnewBin ← new Bin(numData)\r\nfor i = 1 to numData do\r\nnewBin[i] ← 0\r\nfor j = 1 to len(F) do\r\nif F[j].bin[i] 6= 0 then\r\nnewBin[i] ← F[j].bin[i] + binRanges[j]\r\nOutput: newBin, binRanges\r\nHigh-dimensional data are usually very sparse. The sparsity of the feature space provides us a\r\npossibility of designing a nearly lossless approach to reduce the number of features. Specifically, in\r\na sparse feature space, many features are mutually exclusive, i.e., they never take nonzero values\r\nsimultaneously. We can safely bundle exclusive features into a single feature (which we call an\r\nexclusive feature bundle). By a carefully designed feature scanning algorithm, we can build the\r\nsame feature histograms from the feature bundles as those from individual features. In this way, the\r\ncomplexity of histogram building changes from O(#data × #feature) to O(#data × #bundle),\r\nwhile #bundle << #feature. Then we can significantly speed up the training of GBDT without\r\nhurting the accuracy. In the following, we will show how to achieve this in detail.\r\nThere are two issues to be addressed. The first one is to determine which features should be bundled\r\ntogether. The second is how to construct the bundle.\r\nTheorem 4.1 The problem of partitioning features into a smallest number of exclusive bundles is\r\nNP-hard.\r\nProof: We will reduce the graph coloring problem [25] to our problem. Since graph coloring problem\r\nis NP-hard, we can then deduce our conclusion.\r\nGiven any instance G = (V, E) of the graph coloring problem. We construct an instance of our\r\nproblem as follows. Take each row of the incidence matrix of G as a feature, and get an instance of\r\nour problem with |V | features. It is easy to see that an exclusive bundle of features in our problem\r\ncorresponds to a set of vertices with the same color, and vice versa. \u0003\r\nFor the first issue, we prove in Theorem 4.1 that it is NP-Hard to find the optimal bundling strategy,\r\nwhich indicates that it is impossible to find an exact solution within polynomial time. In order to\r\nfind a good approximation algorithm, we first reduce the optimal bundling problem to the graph\r\ncoloring problem by taking features as vertices and adding edges for every two features if they are\r\nnot mutually exclusive, then we use a greedy algorithm which can produce reasonably good results\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/2ff752af-1465-4ac4-b5a0-d2ba817869b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fafd2ea0020a663666f516623b4da73520e9f12345470d2bc29947b2c44f9650",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 689
      },
      {
        "segments": [
          {
            "segment_id": "2ff752af-1465-4ac4-b5a0-d2ba817869b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "decreases to 0 in O(\r\n√\r\nn) with n → ∞. That means when number of data is large, the approximation\r\nis quite accurate. (2) Random sampling is a special case of GOSS with a = 0. In many cases,\r\nGOSS could outperform random sampling, under the condition C0,β > Ca,β−a, which is equivalent\r\nto √αa\r\nβ > √1−aβ−a with αa = maxxi∈A∪Ac |gi|/ maxxi∈Ac |gi|.\r\nNext, we analyze the generalization performance in GOSS. We consider the generalization error in\r\nGOSS E\r\nGOSS\r\ngen (d) = |V˜j (d) − V∗(d)|, which is the gap between the variance gain calculated by the\r\nsampled training instances in GOSS and the true variance gain for the underlying distribution. We\r\nhave E\r\nGOSS\r\ngen (d) ≤ |V˜j (d) − Vj (d)| + |Vj (d) − V∗(d)|\r\n∆= EGOSS(d) + Egen(d). Thus, the generalization\r\nerror with GOSS will be close to that calculated by using the full data instances if the GOSS\r\napproximation is accurate. On the other hand, sampling will increase the diversity of the base learners,\r\nwhich potentially help to improve the generalization performance [24].\r\n4 Exclusive Feature Bundling\r\nIn this section, we propose a novel method to effectively reduce the number of features.\r\nAlgorithm 3: Greedy Bundling\r\nInput: F: features, K: max conflict count\r\nConstruct graph G\r\nsearchOrder ← G.sortByDegree()\r\nbundles ← {}, bundlesConflict ← {}\r\nfor i in searchOrder do\r\nneedNew ← True\r\nfor j = 1 to len(bundles) do\r\ncnt ← ConflictCnt(bundles[j],F[i])\r\nif cnt + bundlesConflict[i] ≤ K then\r\nbundles[j].add(F[i]), needNew ← False\r\nbreak\r\nif needNew then\r\nAdd F[i] as a new bundle to bundles\r\nOutput: bundles\r\nAlgorithm 4: Merge Exclusive Features\r\nInput: numData: number of data\r\nInput: F: One bundle of exclusive features\r\nbinRanges ← {0}, totalBin ← 0\r\nfor f in F do\r\ntotalBin += f.numBin\r\nbinRanges.append(totalBin)\r\nnewBin ← new Bin(numData)\r\nfor i = 1 to numData do\r\nnewBin[i] ← 0\r\nfor j = 1 to len(F) do\r\nif F[j].bin[i] 6= 0 then\r\nnewBin[i] ← F[j].bin[i] + binRanges[j]\r\nOutput: newBin, binRanges\r\nHigh-dimensional data are usually very sparse. The sparsity of the feature space provides us a\r\npossibility of designing a nearly lossless approach to reduce the number of features. Specifically, in\r\na sparse feature space, many features are mutually exclusive, i.e., they never take nonzero values\r\nsimultaneously. We can safely bundle exclusive features into a single feature (which we call an\r\nexclusive feature bundle). By a carefully designed feature scanning algorithm, we can build the\r\nsame feature histograms from the feature bundles as those from individual features. In this way, the\r\ncomplexity of histogram building changes from O(#data × #feature) to O(#data × #bundle),\r\nwhile #bundle << #feature. Then we can significantly speed up the training of GBDT without\r\nhurting the accuracy. In the following, we will show how to achieve this in detail.\r\nThere are two issues to be addressed. The first one is to determine which features should be bundled\r\ntogether. The second is how to construct the bundle.\r\nTheorem 4.1 The problem of partitioning features into a smallest number of exclusive bundles is\r\nNP-hard.\r\nProof: We will reduce the graph coloring problem [25] to our problem. Since graph coloring problem\r\nis NP-hard, we can then deduce our conclusion.\r\nGiven any instance G = (V, E) of the graph coloring problem. We construct an instance of our\r\nproblem as follows. Take each row of the incidence matrix of G as a feature, and get an instance of\r\nour problem with |V | features. It is easy to see that an exclusive bundle of features in our problem\r\ncorresponds to a set of vertices with the same color, and vice versa. \u0003\r\nFor the first issue, we prove in Theorem 4.1 that it is NP-Hard to find the optimal bundling strategy,\r\nwhich indicates that it is impossible to find an exact solution within polynomial time. In order to\r\nfind a good approximation algorithm, we first reduce the optimal bundling problem to the graph\r\ncoloring problem by taking features as vertices and adding edges for every two features if they are\r\nnot mutually exclusive, then we use a greedy algorithm which can produce reasonably good results\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/2ff752af-1465-4ac4-b5a0-d2ba817869b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fafd2ea0020a663666f516623b4da73520e9f12345470d2bc29947b2c44f9650",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 689
      },
      {
        "segments": [
          {
            "segment_id": "441c8efe-5095-4367-b0de-22aeac3e3245",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "(with a constant approximation ratio) for graph coloring to produce the bundles. Furthermore, we\r\nnotice that there are usually quite a few features, although not 100% mutually exclusive, also rarely\r\ntake nonzero values simultaneously. If our algorithm can allow a small fraction of conflicts, we can\r\nget an even smaller number of feature bundles and further improve the computational efficiency.\r\nBy simple calculation, random polluting a small fraction of feature values will affect the training\r\naccuracy by at most O([(1 − γ)n]\r\n−2/3\r\n)(See Proposition 2.1 in the supplementary materials), where\r\nγ is the maximal conflict rate in each bundle. So, if we choose a relatively small γ, we will be able to\r\nachieve a good balance between accuracy and efficiency.\r\nBased on the above discussions, we design an algorithm for exclusive feature bundling as shown\r\nin Alg. 3. First, we construct a graph with weighted edges, whose weights correspond to the total\r\nconflicts between features. Second, we sort the features by their degrees in the graph in the descending\r\norder. Finally, we check each feature in the ordered list, and either assign it to an existing bundle\r\nwith a small conflict (controlled by γ), or create a new bundle. The time complexity of Alg. 3 is\r\nO(#feature2) and it is processed only once before training. This complexity is acceptable when the\r\nnumber of features is not very large, but may still suffer if there are millions of features. To further\r\nimprove the efficiency, we propose a more efficient ordering strategy without building the graph:\r\nordering by the count of nonzero values, which is similar to ordering by degrees since more nonzero\r\nvalues usually leads to higher probability of conflicts. Since we only alter the ordering strategies in\r\nAlg. 3, the details of the new algorithm are omitted to avoid duplication.\r\nFor the second issues, we need a good way of merging the features in the same bundle in order to\r\nreduce the corresponding training complexity. The key is to ensure that the values of the original\r\nfeatures can be identified from the feature bundles. Since the histogram-based algorithm stores\r\ndiscrete bins instead of continuous values of the features, we can construct a feature bundle by letting\r\nexclusive features reside in different bins. This can be done by adding offsets to the original values of\r\nthe features. For example, suppose we have two features in a feature bundle. Originally, feature A\r\ntakes value from [0, 10) and feature B takes value [0, 20). We then add an offset of 10 to the values of\r\nfeature B so that the refined feature takes values from [10, 30). After that, it is safe to merge features\r\nA and B, and use a feature bundle with range [0, 30] to replace the original features A and B. The\r\ndetailed algorithm is shown in Alg. 4.\r\nEFB algorithm can bundle many exclusive features to the much fewer dense features, which can\r\neffectively avoid unnecessary computation for zero feature values. Actually, we can also optimize\r\nthe basic histogram-based algorithm towards ignoring the zero feature values by using a table for\r\neach feature to record the data with nonzero values. By scanning the data in this table, the cost of\r\nhistogram building for a feature will change from O(#data) to O(#non_zero_data). However,\r\nthis method needs additional memory and computation cost to maintain these per-feature tables in the\r\nwhole tree growth process. We implement this optimization in LightGBM as a basic function. Note,\r\nthis optimization does not conflict with EFB since we can still use it when the bundles are sparse.\r\n5 Experiments\r\nIn this section, we report the experimental results regarding our proposed LightGBM algorithm. We\r\nuse five different datasets which are all publicly available. The details of these datasets are listed\r\nin Table 1. Among them, the Microsoft Learning to Rank (LETOR) [26] dataset contains 30K web\r\nsearch queries. The features used in this dataset are mostly dense numerical features. The Allstate\r\nInsurance Claim [27] and the Flight Delay [28] datasets both contain a lot of one-hot coding features.\r\nAnd the last two datasets are from KDD CUP 2010 and KDD CUP 2012. We directly use the features\r\nused by the winning solution from NTU [29, 30, 31], which contains both dense and sparse features,\r\nand these two datasets are very large. These datasets are large, include both sparse and dense features,\r\nand cover many real-world tasks. Thus, we can use them to test our algorithm thoroughly.\r\nOur experimental environment is a Linux server with two E5-2670 v3 CPUs (in total 24 cores) and\r\n256GB memories. All experiments run with multi-threading and the number of threads is fixed to 16.\r\n5.1 Overall Comparison\r\nWe present the overall comparisons in this subsection. XGBoost [13] and LightGBM without GOSS\r\nand EFB (called lgb_baselline) are used as baselines. For XGBoost, we used two versions, xgb_exa\r\n(pre-sorted algorithm) and xgb_his (histogram-based algorithm). For xgb_his, lgb_baseline, and\r\nLightGBM, we used the leaf-wise tree growth strategy [32]. For xgb_exa, since it only supports\r\nlayer-wise growth strategy, we tuned the parameters for xgb_exa to let it grow similar trees like other\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/441c8efe-5095-4367-b0de-22aeac3e3245.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e6eb5c57aacce16a2a81a6118964bb5f36f66f4636e76d5c6d2e7e91166f6c01",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 850
      },
      {
        "segments": [
          {
            "segment_id": "441c8efe-5095-4367-b0de-22aeac3e3245",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "(with a constant approximation ratio) for graph coloring to produce the bundles. Furthermore, we\r\nnotice that there are usually quite a few features, although not 100% mutually exclusive, also rarely\r\ntake nonzero values simultaneously. If our algorithm can allow a small fraction of conflicts, we can\r\nget an even smaller number of feature bundles and further improve the computational efficiency.\r\nBy simple calculation, random polluting a small fraction of feature values will affect the training\r\naccuracy by at most O([(1 − γ)n]\r\n−2/3\r\n)(See Proposition 2.1 in the supplementary materials), where\r\nγ is the maximal conflict rate in each bundle. So, if we choose a relatively small γ, we will be able to\r\nachieve a good balance between accuracy and efficiency.\r\nBased on the above discussions, we design an algorithm for exclusive feature bundling as shown\r\nin Alg. 3. First, we construct a graph with weighted edges, whose weights correspond to the total\r\nconflicts between features. Second, we sort the features by their degrees in the graph in the descending\r\norder. Finally, we check each feature in the ordered list, and either assign it to an existing bundle\r\nwith a small conflict (controlled by γ), or create a new bundle. The time complexity of Alg. 3 is\r\nO(#feature2) and it is processed only once before training. This complexity is acceptable when the\r\nnumber of features is not very large, but may still suffer if there are millions of features. To further\r\nimprove the efficiency, we propose a more efficient ordering strategy without building the graph:\r\nordering by the count of nonzero values, which is similar to ordering by degrees since more nonzero\r\nvalues usually leads to higher probability of conflicts. Since we only alter the ordering strategies in\r\nAlg. 3, the details of the new algorithm are omitted to avoid duplication.\r\nFor the second issues, we need a good way of merging the features in the same bundle in order to\r\nreduce the corresponding training complexity. The key is to ensure that the values of the original\r\nfeatures can be identified from the feature bundles. Since the histogram-based algorithm stores\r\ndiscrete bins instead of continuous values of the features, we can construct a feature bundle by letting\r\nexclusive features reside in different bins. This can be done by adding offsets to the original values of\r\nthe features. For example, suppose we have two features in a feature bundle. Originally, feature A\r\ntakes value from [0, 10) and feature B takes value [0, 20). We then add an offset of 10 to the values of\r\nfeature B so that the refined feature takes values from [10, 30). After that, it is safe to merge features\r\nA and B, and use a feature bundle with range [0, 30] to replace the original features A and B. The\r\ndetailed algorithm is shown in Alg. 4.\r\nEFB algorithm can bundle many exclusive features to the much fewer dense features, which can\r\neffectively avoid unnecessary computation for zero feature values. Actually, we can also optimize\r\nthe basic histogram-based algorithm towards ignoring the zero feature values by using a table for\r\neach feature to record the data with nonzero values. By scanning the data in this table, the cost of\r\nhistogram building for a feature will change from O(#data) to O(#non_zero_data). However,\r\nthis method needs additional memory and computation cost to maintain these per-feature tables in the\r\nwhole tree growth process. We implement this optimization in LightGBM as a basic function. Note,\r\nthis optimization does not conflict with EFB since we can still use it when the bundles are sparse.\r\n5 Experiments\r\nIn this section, we report the experimental results regarding our proposed LightGBM algorithm. We\r\nuse five different datasets which are all publicly available. The details of these datasets are listed\r\nin Table 1. Among them, the Microsoft Learning to Rank (LETOR) [26] dataset contains 30K web\r\nsearch queries. The features used in this dataset are mostly dense numerical features. The Allstate\r\nInsurance Claim [27] and the Flight Delay [28] datasets both contain a lot of one-hot coding features.\r\nAnd the last two datasets are from KDD CUP 2010 and KDD CUP 2012. We directly use the features\r\nused by the winning solution from NTU [29, 30, 31], which contains both dense and sparse features,\r\nand these two datasets are very large. These datasets are large, include both sparse and dense features,\r\nand cover many real-world tasks. Thus, we can use them to test our algorithm thoroughly.\r\nOur experimental environment is a Linux server with two E5-2670 v3 CPUs (in total 24 cores) and\r\n256GB memories. All experiments run with multi-threading and the number of threads is fixed to 16.\r\n5.1 Overall Comparison\r\nWe present the overall comparisons in this subsection. XGBoost [13] and LightGBM without GOSS\r\nand EFB (called lgb_baselline) are used as baselines. For XGBoost, we used two versions, xgb_exa\r\n(pre-sorted algorithm) and xgb_his (histogram-based algorithm). For xgb_his, lgb_baseline, and\r\nLightGBM, we used the leaf-wise tree growth strategy [32]. For xgb_exa, since it only supports\r\nlayer-wise growth strategy, we tuned the parameters for xgb_exa to let it grow similar trees like other\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/441c8efe-5095-4367-b0de-22aeac3e3245.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e6eb5c57aacce16a2a81a6118964bb5f36f66f4636e76d5c6d2e7e91166f6c01",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 850
      },
      {
        "segments": [
          {
            "segment_id": "852bd70d-00b3-494e-8eb8-28dd3bb59a47",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "Table 1: Datasets used in the experiments.\r\nName #data #feature Description Task Metric\r\nAllstate 12 M 4228 Sparse Binary classification AUC\r\nFlight Delay 10 M 700 Sparse Binary classification AUC\r\nLETOR 2M 136 Dense Ranking NDCG [4]\r\nKDD10 19M 29M Sparse Binary classification AUC\r\nKDD12 119M 54M Sparse Binary classification AUC\r\nTable 2: Overall training time cost comparison. LightGBM is lgb_baseline with GOSS and EFB.\r\nEFB_only is lgb_baseline with EFB. The values in the table are the average time cost (seconds) for\r\ntraining one iteration.\r\nxgb_exa xgb_his lgb_baseline EFB_only LightGBM\r\nAllstate 10.85 2.63 6.07 0.71 0.28\r\nFlight Delay 5.94 1.05 1.39 0.27 0.22\r\nLETOR 5.55 0.63 0.49 0.46 0.31\r\nKDD10 108.27 OOM 39.85 6.33 2.85\r\nKDD12 191.99 OOM 168.26 20.23 12.67\r\nTable 3: Overall accuracy comparison on test datasets. Use AUC for classification task and\r\nNDCG@10 for ranking task. SGB is lgb_baseline with Stochastic Gradient Boosting, and its\r\nsampling ratio is the same as LightGBM.\r\nxgb_exa xgb_his lgb_baseline SGB LightGBM\r\nAllstate 0.6070 0.6089 0.6093 0.6064 ± 7e-4 0.6093 ± 9e-5\r\nFlight Delay 0.7601 0.7840 0.7847 0.7780 ± 8e-4 0.7846 ± 4e-5\r\nLETOR 0.4977 0.4982 0.5277 0.5239 ± 6e-4 0.5275 ± 5e-4\r\nKDD10 0.7796 OOM 0.78735 0.7759 ± 3e-4 0.78732 ± 1e-4\r\nKDD12 0.7029 OOM 0.7049 0.6989 ± 8e-4 0.7051 ± 5e-5\r\nmethods. And we also tuned the parameters for all datasets towards a better balancing between speed\r\nand accuracy. We set a = 0.05, b = 0.05 for Allstate, KDD10 and KDD12, and set a = 0.1, b = 0.1\r\nfor Flight Delay and LETOR. We set γ = 0 in EFB. All algorithms are run for fixed iterations, and\r\nwe get the accuracy results from the iteration with the best score.6\r\n0 200 400 600 800 1000\r\nTime(s)\r\n0.73 0.74 0.75 0.76 0.77 0.78 0.79\r\nAUC\r\nLightGBM\r\nlgb_baseline\r\nxgb_his\r\nxgb_exa\r\nFigure 1: Time-AUC curve on Flight Delay.\r\n0 50 100 150 200 250 300 350 400\r\nTime(s)\r\n0.40 0.42 0.44 0.46 0.48 0.50 0.52\r\nNDCG@10\r\nLightGBM\r\nlgb_baseline\r\nxgb_his\r\nxgb_exa\r\nFigure 2: Time-NDCG curve on LETOR.\r\nThe training time and test accuracy are summarized in Table 2 and Table 3 respectively. From these\r\nresults, we can see that LightGBM is the fastest while maintaining almost the same accuracy as\r\nbaselines. The xgb_exa is based on the pre-sorted algorithm, which is quite slow comparing with\r\nhistogram-base algorithms. By comparing with lgb_baseline, LightGBM speed up 21x, 6x, 1.6x,\r\n14x and 13x respectively on the Allstate, Flight Delay, LETOR, KDD10 and KDD12 datasets. Since\r\nxgb_his is quite memory consuming, it cannot run successfully on KDD10 and KDD12 datasets\r\ndue to out-of-memory. On the remaining datasets, LightGBM are all faster, up to 9x speed-up is\r\nachieved on the Allstate dataset. The speed-up is calculated based on training time per iteration since\r\nall algorithms converge after similar number of iterations. To demonstrate the overall training process,\r\nwe also show the training curves based on wall clock time on Flight Delay and LETOR in the Fig. 1\r\n6Due to space restrictions, we leave the details of parameter settings to the supplementary material.\r\n7",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/852bd70d-00b3-494e-8eb8-28dd3bb59a47.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=de85651143f1c7ca9789e27f13dbdcfabdfd1792797c999185919cb67fdf83ba",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 507
      },
      {
        "segments": [
          {
            "segment_id": "0414e018-2029-456d-8c90-92f892a85b38",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Table 4: Accuracy comparison on LETOR dataset for GOSS and SGB under different sampling ratios.\r\nWe ensure all experiments reach the convergence points by using large iterations with early stopping.\r\nThe standard deviations on different settings are small. The settings of a and b for GOSS can be\r\nfound in the supplementary materials.\r\nSampling ratio 0.1 0.15 0.2 0.25 0.3 0.35 0.4\r\nSGB 0.5182 0.5216 0.5239 0.5249 0.5252 0.5263 0.5267\r\nGOSS 0.5224 0.5256 0.5275 0.5284 0.5289 0.5293 0.5296\r\nand Fig. 2, respectively. To save space, we put the remaining training curves of the other datasets in\r\nthe supplementary material.\r\nOn all datasets, LightGBM can achieve almost the same test accuracy as the baselines. This indicates\r\nthat both GOSS and EFB will not hurt accuracy while bringing significant speed-up. It is consistent\r\nwith our theoretical analysis in Sec. 3.2 and Sec. 4.\r\nLightGBM achieves quite different speed-up ratios on these datasets. The overall speed-up comes\r\nfrom the combination of GOSS and EFB, we will break down the contribution and discuss the\r\neffectiveness of GOSS and EFB separately in the next sections.\r\n5.2 Analysis on GOSS\r\nFirst, we study the speed-up ability of GOSS. From the comparison of LightGBM and EFB_only\r\n(LightGBM without GOSS) in Table 2, we can see that GOSS can bring nearly 2x speed-up by its\r\nown with using 10% - 20% data. GOSS can learn trees by only using the sampled data. However, it\r\nretains some computations on the full dataset, such as conducting the predictions and computing the\r\ngradients. Thus, we can find that the overall speed-up is not linearly correlated with the percentage of\r\nsampled data. However, the speed-up brought by GOSS is still very significant and the technique is\r\nuniversally applicable to different datasets.\r\nSecond, we evaluate the accuracy of GOSS by comparing with Stochastic Gradient Boosting (SGB)\r\n[20]. Without loss of generality, we use the LETOR dataset for the test. We tune the sampling ratio\r\nby choosing different a and b in GOSS, and use the same overall sampling ratio for SGB. We run\r\nthese settings until convergence by using early stopping. The results are shown in Table 4. We can\r\nsee the accuracy of GOSS is always better than SGB when using the same sampling ratio. These\r\nresults are consistent with our discussions in Sec. 3.2. All the experiments demonstrate that GOSS is\r\na more effective sampling method than stochastic sampling.\r\n5.3 Analysis on EFB\r\nWe check the contribution of EFB to the speed-up by comparing lgb_baseline with EFB_only. The\r\nresults are shown in Table 2. Here we do not allow the confliction in the bundle finding process (i.e.,\r\nγ = 0).7 We find that EFB can help achieve significant speed-up on those large-scale datasets.\r\nPlease note lgb_baseline has been optimized for the sparse features, and EFB can still speed up\r\nthe training by a large factor. It is because EFB merges many sparse features (both the one-hot\r\ncoding features and implicitly exclusive features) into much fewer features. The basic sparse feature\r\noptimization is included in the bundling process. However, the EFB does not have the additional cost\r\non maintaining nonzero data table for each feature in the tree learning process. What is more, since\r\nmany previously isolated features are bundled together, it can increase spatial locality and improve\r\ncache hit rate significantly. Therefore, the overall improvement on efficiency is dramatic. With\r\nabove analysis, EFB is a very effective algorithm to leverage sparse property in the histogram-based\r\nalgorithm, and it can bring a significant speed-up for GBDT training process.\r\n6 Conclusion\r\nIn this paper, we have proposed a novel GBDT algorithm called LightGBM, which contains two\r\nnovel techniques: Gradient-based One-Side Sampling and Exclusive Feature Bundling to deal with\r\nlarge number of data instances and large number of features respectively. We have performed both\r\ntheoretical analysis and experimental studies on these two techniques. The experimental results are\r\nconsistent with the theory and show that with the help of GOSS and EFB, LightGBM can significantly\r\noutperform XGBoost and SGB in terms of computational speed and memory consumption. For the\r\nfuture work, we will study the optimal selection of a and b in Gradient-based One-Side Sampling\r\nand continue improving the performance of Exclusive Feature Bundling to deal with large number of\r\nfeatures no matter they are sparse or not.\r\n7We put our detailed study on γ tuning in the supplementary materials.\r\n8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/0414e018-2029-456d-8c90-92f892a85b38.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5854193f80d656a085768883fc7a999b10456c1c9b3e9073575d1755847a373d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 726
      },
      {
        "segments": [
          {
            "segment_id": "0414e018-2029-456d-8c90-92f892a85b38",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Table 4: Accuracy comparison on LETOR dataset for GOSS and SGB under different sampling ratios.\r\nWe ensure all experiments reach the convergence points by using large iterations with early stopping.\r\nThe standard deviations on different settings are small. The settings of a and b for GOSS can be\r\nfound in the supplementary materials.\r\nSampling ratio 0.1 0.15 0.2 0.25 0.3 0.35 0.4\r\nSGB 0.5182 0.5216 0.5239 0.5249 0.5252 0.5263 0.5267\r\nGOSS 0.5224 0.5256 0.5275 0.5284 0.5289 0.5293 0.5296\r\nand Fig. 2, respectively. To save space, we put the remaining training curves of the other datasets in\r\nthe supplementary material.\r\nOn all datasets, LightGBM can achieve almost the same test accuracy as the baselines. This indicates\r\nthat both GOSS and EFB will not hurt accuracy while bringing significant speed-up. It is consistent\r\nwith our theoretical analysis in Sec. 3.2 and Sec. 4.\r\nLightGBM achieves quite different speed-up ratios on these datasets. The overall speed-up comes\r\nfrom the combination of GOSS and EFB, we will break down the contribution and discuss the\r\neffectiveness of GOSS and EFB separately in the next sections.\r\n5.2 Analysis on GOSS\r\nFirst, we study the speed-up ability of GOSS. From the comparison of LightGBM and EFB_only\r\n(LightGBM without GOSS) in Table 2, we can see that GOSS can bring nearly 2x speed-up by its\r\nown with using 10% - 20% data. GOSS can learn trees by only using the sampled data. However, it\r\nretains some computations on the full dataset, such as conducting the predictions and computing the\r\ngradients. Thus, we can find that the overall speed-up is not linearly correlated with the percentage of\r\nsampled data. However, the speed-up brought by GOSS is still very significant and the technique is\r\nuniversally applicable to different datasets.\r\nSecond, we evaluate the accuracy of GOSS by comparing with Stochastic Gradient Boosting (SGB)\r\n[20]. Without loss of generality, we use the LETOR dataset for the test. We tune the sampling ratio\r\nby choosing different a and b in GOSS, and use the same overall sampling ratio for SGB. We run\r\nthese settings until convergence by using early stopping. The results are shown in Table 4. We can\r\nsee the accuracy of GOSS is always better than SGB when using the same sampling ratio. These\r\nresults are consistent with our discussions in Sec. 3.2. All the experiments demonstrate that GOSS is\r\na more effective sampling method than stochastic sampling.\r\n5.3 Analysis on EFB\r\nWe check the contribution of EFB to the speed-up by comparing lgb_baseline with EFB_only. The\r\nresults are shown in Table 2. Here we do not allow the confliction in the bundle finding process (i.e.,\r\nγ = 0).7 We find that EFB can help achieve significant speed-up on those large-scale datasets.\r\nPlease note lgb_baseline has been optimized for the sparse features, and EFB can still speed up\r\nthe training by a large factor. It is because EFB merges many sparse features (both the one-hot\r\ncoding features and implicitly exclusive features) into much fewer features. The basic sparse feature\r\noptimization is included in the bundling process. However, the EFB does not have the additional cost\r\non maintaining nonzero data table for each feature in the tree learning process. What is more, since\r\nmany previously isolated features are bundled together, it can increase spatial locality and improve\r\ncache hit rate significantly. Therefore, the overall improvement on efficiency is dramatic. With\r\nabove analysis, EFB is a very effective algorithm to leverage sparse property in the histogram-based\r\nalgorithm, and it can bring a significant speed-up for GBDT training process.\r\n6 Conclusion\r\nIn this paper, we have proposed a novel GBDT algorithm called LightGBM, which contains two\r\nnovel techniques: Gradient-based One-Side Sampling and Exclusive Feature Bundling to deal with\r\nlarge number of data instances and large number of features respectively. We have performed both\r\ntheoretical analysis and experimental studies on these two techniques. The experimental results are\r\nconsistent with the theory and show that with the help of GOSS and EFB, LightGBM can significantly\r\noutperform XGBoost and SGB in terms of computational speed and memory consumption. For the\r\nfuture work, we will study the optimal selection of a and b in Gradient-based One-Side Sampling\r\nand continue improving the performance of Exclusive Feature Bundling to deal with large number of\r\nfeatures no matter they are sparse or not.\r\n7We put our detailed study on γ tuning in the supplementary materials.\r\n8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/0414e018-2029-456d-8c90-92f892a85b38.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5854193f80d656a085768883fc7a999b10456c1c9b3e9073575d1755847a373d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 726
      },
      {
        "segments": [
          {
            "segment_id": "8390c1bc-ff9a-4636-8b6a-51fa24bd27a2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "References\r\n[1] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189–1232, 2001.\r\n[2] Ping Li. Robust logitboost and adaptive base class (abc) logitboost. arXiv preprint arXiv:1203.3491, 2012.\r\n[3] Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-through rate for new ads. In\r\nProceedings of the 16th international conference on World Wide Web, pages 521–530. ACM, 2007.\r\n[4] Christopher JC Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581):81, 2010.\r\n[5] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression: a statistical view of boosting (with discussion and\r\na rejoinder by the authors). The annals of statistics, 28(2):337–407, 2000.\r\n[6] Charles Dubout and François Fleuret. Boosting with maximum adaptive sampling. In Advances in Neural Information Processing\r\nSystems, pages 1332–1340, 2011.\r\n[7] Ron Appel, Thomas J Fuchs, Piotr Dollár, and Pietro Perona. Quickly boosting decision trees-pruning underachieving features early. In\r\nICML (3), pages 594–602, 2013.\r\n[8] Manish Mehta, Rakesh Agrawal, and Jorma Rissanen. Sliq: A fast scalable classifier for data mining. In International Conference on\r\nExtending Database Technology, pages 18–32. Springer, 1996.\r\n[9] John Shafer, Rakesh Agrawal, and Manish Mehta. Sprint: A scalable parallel classi er for data mining. In Proc. 1996 Int. Conf. Very\r\nLarge Data Bases, pages 544–555. Citeseer, 1996.\r\n[10] Sanjay Ranka and V Singh. Clouds: A decision tree classifier for large datasets. In Proceedings of the 4th Knowledge Discovery and\r\nData Mining Conference, pages 2–8, 1998.\r\n[11] Ruoming Jin and Gagan Agrawal. Communication and memory efficient parallel decision tree construction. In Proceedings of the 2003\r\nSIAM International Conference on Data Mining, pages 119–129. SIAM, 2003.\r\n[12] Ping Li, Christopher JC Burges, Qiang Wu, JC Platt, D Koller, Y Singer, and S Roweis. Mcrank: Learning to rank using multiple\r\nclassification and gradient boosting. In NIPS, volume 7, pages 845–852, 2007.\r\n[13] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22Nd ACM SIGKDD International\r\nConference on Knowledge Discovery and Data Mining, pages 785–794. ACM, 2016.\r\n[14] Stephen Tyree, Kilian Q Weinberger, Kunal Agrawal, and Jennifer Paykin. Parallel boosted regression trees for web search ranking. In\r\nProceedings of the 20th international conference on World wide web, pages 387–396. ACM, 2011.\r\n[15] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter\r\nPrettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research,\r\n12(Oct):2825–2830, 2011.\r\n[16] Greg Ridgeway. Generalized boosted models: A guide to the gbm package. Update, 1(1):2007, 2007.\r\n[17] Huan Zhang, Si Si, and Cho-Jui Hsieh. Gpu-acceleration for large-scale tree boosting. arXiv preprint arXiv:1706.08359, 2017.\r\n[18] Rory Mitchell and Eibe Frank. Accelerating the xgboost algorithm using gpu computing. PeerJ Preprints, 5:e2911v1, 2017.\r\n[19] Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, and Tieyan Liu. A communication-efficient parallel algorithm\r\nfor decision tree. In Advances in Neural Information Processing Systems, pages 1271–1279, 2016.\r\n[20] Jerome H Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4):367–378, 2002.\r\n[21] Michael Collins, Robert E Schapire, and Yoram Singer. Logistic regression, adaboost and bregman distances. Machine Learning,\r\n48(1-3):253–285, 2002.\r\n[22] Ian Jolliffe. Principal component analysis. Wiley Online Library, 2002.\r\n[23] Luis O Jimenez and David A Landgrebe. Hyperspectral data analysis and supervised feature reduction via projection pursuit. IEEE\r\nTransactions on Geoscience and Remote Sensing, 37(6):2653–2667, 1999.\r\n[24] Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. CRC press, 2012.\r\n[25] Tommy R Jensen and Bjarne Toft. Graph coloring problems, volume 39. John Wiley & Sons, 2011.\r\n[26] Tao Qin and Tie-Yan Liu. Introducing LETOR 4.0 datasets. CoRR, abs/1306.2597, 2013.\r\n[27] Allstate claim data, https://www.kaggle.com/c/ClaimPredictionChallenge.\r\n[28] Flight delay data, https://github.com/szilard/benchm-ml#data.\r\n[29] Hsiang-Fu Yu, Hung-Yi Lo, Hsun-Ping Hsieh, Jing-Kai Lou, Todd G McKenzie, Jung-Wei Chou, Po-Han Chung, Chia-Hua Ho, Chun-Fu\r\nChang, Yin-Hsuan Wei, et al. Feature engineering and classifier ensemble for kdd cup 2010. In KDD Cup, 2010.\r\n[30] Kuan-Wei Wu, Chun-Sung Ferng, Chia-Hua Ho, An-Chun Liang, Chun-Heng Huang, Wei-Yuan Shen, Jyun-Yu Jiang, Ming-Hao Yang,\r\nTing-Wei Lin, Ching-Pei Lee, et al. A two-stage ensemble of diverse models for advertisement ranking in kdd cup 2012. In KDDCup,\r\n2012.\r\n[31] Libsvm binary classification data, https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html.\r\n[32] Haijian Shi. Best-first decision tree learning. PhD thesis, The University of Waikato, 2007.\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/8390c1bc-ff9a-4636-8b6a-51fa24bd27a2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7c3f3f4916d6dd85a904f4b46efb4d8c686f439a11e6bb07f1f9a09f6fb91bd0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 704
      },
      {
        "segments": [
          {
            "segment_id": "8390c1bc-ff9a-4636-8b6a-51fa24bd27a2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "References\r\n[1] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189–1232, 2001.\r\n[2] Ping Li. Robust logitboost and adaptive base class (abc) logitboost. arXiv preprint arXiv:1203.3491, 2012.\r\n[3] Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-through rate for new ads. In\r\nProceedings of the 16th international conference on World Wide Web, pages 521–530. ACM, 2007.\r\n[4] Christopher JC Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581):81, 2010.\r\n[5] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression: a statistical view of boosting (with discussion and\r\na rejoinder by the authors). The annals of statistics, 28(2):337–407, 2000.\r\n[6] Charles Dubout and François Fleuret. Boosting with maximum adaptive sampling. In Advances in Neural Information Processing\r\nSystems, pages 1332–1340, 2011.\r\n[7] Ron Appel, Thomas J Fuchs, Piotr Dollár, and Pietro Perona. Quickly boosting decision trees-pruning underachieving features early. In\r\nICML (3), pages 594–602, 2013.\r\n[8] Manish Mehta, Rakesh Agrawal, and Jorma Rissanen. Sliq: A fast scalable classifier for data mining. In International Conference on\r\nExtending Database Technology, pages 18–32. Springer, 1996.\r\n[9] John Shafer, Rakesh Agrawal, and Manish Mehta. Sprint: A scalable parallel classi er for data mining. In Proc. 1996 Int. Conf. Very\r\nLarge Data Bases, pages 544–555. Citeseer, 1996.\r\n[10] Sanjay Ranka and V Singh. Clouds: A decision tree classifier for large datasets. In Proceedings of the 4th Knowledge Discovery and\r\nData Mining Conference, pages 2–8, 1998.\r\n[11] Ruoming Jin and Gagan Agrawal. Communication and memory efficient parallel decision tree construction. In Proceedings of the 2003\r\nSIAM International Conference on Data Mining, pages 119–129. SIAM, 2003.\r\n[12] Ping Li, Christopher JC Burges, Qiang Wu, JC Platt, D Koller, Y Singer, and S Roweis. Mcrank: Learning to rank using multiple\r\nclassification and gradient boosting. In NIPS, volume 7, pages 845–852, 2007.\r\n[13] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22Nd ACM SIGKDD International\r\nConference on Knowledge Discovery and Data Mining, pages 785–794. ACM, 2016.\r\n[14] Stephen Tyree, Kilian Q Weinberger, Kunal Agrawal, and Jennifer Paykin. Parallel boosted regression trees for web search ranking. In\r\nProceedings of the 20th international conference on World wide web, pages 387–396. ACM, 2011.\r\n[15] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter\r\nPrettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research,\r\n12(Oct):2825–2830, 2011.\r\n[16] Greg Ridgeway. Generalized boosted models: A guide to the gbm package. Update, 1(1):2007, 2007.\r\n[17] Huan Zhang, Si Si, and Cho-Jui Hsieh. Gpu-acceleration for large-scale tree boosting. arXiv preprint arXiv:1706.08359, 2017.\r\n[18] Rory Mitchell and Eibe Frank. Accelerating the xgboost algorithm using gpu computing. PeerJ Preprints, 5:e2911v1, 2017.\r\n[19] Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, and Tieyan Liu. A communication-efficient parallel algorithm\r\nfor decision tree. In Advances in Neural Information Processing Systems, pages 1271–1279, 2016.\r\n[20] Jerome H Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4):367–378, 2002.\r\n[21] Michael Collins, Robert E Schapire, and Yoram Singer. Logistic regression, adaboost and bregman distances. Machine Learning,\r\n48(1-3):253–285, 2002.\r\n[22] Ian Jolliffe. Principal component analysis. Wiley Online Library, 2002.\r\n[23] Luis O Jimenez and David A Landgrebe. Hyperspectral data analysis and supervised feature reduction via projection pursuit. IEEE\r\nTransactions on Geoscience and Remote Sensing, 37(6):2653–2667, 1999.\r\n[24] Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. CRC press, 2012.\r\n[25] Tommy R Jensen and Bjarne Toft. Graph coloring problems, volume 39. John Wiley & Sons, 2011.\r\n[26] Tao Qin and Tie-Yan Liu. Introducing LETOR 4.0 datasets. CoRR, abs/1306.2597, 2013.\r\n[27] Allstate claim data, https://www.kaggle.com/c/ClaimPredictionChallenge.\r\n[28] Flight delay data, https://github.com/szilard/benchm-ml#data.\r\n[29] Hsiang-Fu Yu, Hung-Yi Lo, Hsun-Ping Hsieh, Jing-Kai Lou, Todd G McKenzie, Jung-Wei Chou, Po-Han Chung, Chia-Hua Ho, Chun-Fu\r\nChang, Yin-Hsuan Wei, et al. Feature engineering and classifier ensemble for kdd cup 2010. In KDD Cup, 2010.\r\n[30] Kuan-Wei Wu, Chun-Sung Ferng, Chia-Hua Ho, An-Chun Liang, Chun-Heng Huang, Wei-Yuan Shen, Jyun-Yu Jiang, Ming-Hao Yang,\r\nTing-Wei Lin, Ching-Pei Lee, et al. A two-stage ensemble of diverse models for advertisement ranking in kdd cup 2012. In KDDCup,\r\n2012.\r\n[31] Libsvm binary classification data, https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html.\r\n[32] Haijian Shi. Best-first decision tree learning. PhD thesis, The University of Waikato, 2007.\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/74b3a5fa-d7fa-44a7-b103-b0891defcb11/images/8390c1bc-ff9a-4636-8b6a-51fa24bd27a2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7c3f3f4916d6dd85a904f4b46efb4d8c686f439a11e6bb07f1f9a09f6fb91bd0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 704
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "```json\n{\"date_published\": \"2017\"}\n```"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "Long Beach, CA, USA"
        }
      ]
    }
  }
}