{
  "file_name": "Intel AVX-512 - Permuting Data within and Between AVX Registers - Technology Guide (1668169807).pdf",
  "task_id": "100afd0c-e35c-4775-a300-87e110334352",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "e20fd375-3189-4bc0-92c2-2cee007efa7e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 1,
            "page_width": 594,
            "page_height": 792,
            "content": "1 \r\nTechnology Guide \r\nIntel® Advanced Vector Extensions 512 \r\n(Intel® AVX-512) - Permuting Data Within \r\nand Between AVX Registers \r\nAuthor\r\nDaniel Towner\r\n1 Introduction\r\nThe Intel® Advanced Vector Extensions (Intel® AVX) family of instruction sets on Intel \r\nprocessors provides a rich variety of capabilities for supporting many different single\r\ninstruction, multiple data (SIMD) instructions and data types. Like many other SIMD \r\ninstruction sets, Intel AVX instructions are predominantly vertical, or map, instructions, \r\nwhere one or more SIMD values are converted to another SIMD value element-by-element. \r\nHowever, it can be desirable to reorder elements within or across SIMD values as well, and the \r\nIntel AVX families of instructions have many clever ways of achieving this. This document \r\ndiscusses the many different ways to perform permutations, describes the trade-offs, and \r\nshows how the techniques described can be used to implement versions of a few selected \r\nalgorithms.\r\nThis document is part of the Network Transformation Experience Kits. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/e20fd375-3189-4bc0-92c2-2cee007efa7e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=65d62f036ac5b432959ca4da2a38d7ec8ee565fe3f46f315f169040b8204da29",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c2a486bd-b278-465b-b637-3196b0d6314f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 2,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n2\r\nTable of Contents\r\n1 Introduction................................................................................................................................................................................................................1\r\n1.1 Terminology..............................................................................................................................................................................................................4\r\n1.2 Reference Documentation .................................................................................................................................................................................4\r\n2 Background...............................................................................................................................................................................................................4\r\n2.1 Terminology..............................................................................................................................................................................................................5\r\n2.2 Execution Performance .......................................................................................................................................................................................5\r\n2.3 Element Data Type Support..............................................................................................................................................................................6\r\n3 AVX Permutation Instructions..........................................................................................................................................................................7\r\n3.1 General Purpose Permutes ................................................................................................................................................................................7\r\n3.1.1 In-Lane General-Purpose Permute ................................................................................................................................................................8\r\n3.1.2 Single Source Cross-Lane General-Purpose Permutes ........................................................................................................................ 9\r\n3.1.3 Dual Source Cross-Lane General-Purpose Permutes............................................................................................................................ 9\r\n3.1.4 Bit-Level Byte Shuffle (VBMI).........................................................................................................................................................................10\r\n3.1.5 Clean and Dirty Indexing....................................................................................................................................................................................10\r\n3.1.6 Summary of Relative Costs for Different Permutes................................................................................................................................11\r\n3.2 Fixed Purpose Permutes ....................................................................................................................................................................................11\r\n3.2.1 Broadcast..................................................................................................................................................................................................................11\r\n3.2.2 Duplicate Low/High.............................................................................................................................................................................................12\r\n3.2.3 Insert/Extract.........................................................................................................................................................................................................13\r\n3.3 Alignment Operations ........................................................................................................................................................................................15\r\n3.3.1 Per-Register Alignment Instructions............................................................................................................................................................16\r\n3.3.2 Per-Lane Alignment Instructions...................................................................................................................................................................16\r\n3.3.3 Per-element Alignment Instructions (VBMI2) .........................................................................................................................................16\r\n3.3.4 Element Rotation..................................................................................................................................................................................................17\r\n3.4 Bit-Wise Expansion and Compression.........................................................................................................................................................17\r\n3.5 Gather and Scatter...............................................................................................................................................................................................18\r\n3.6 Reductions...............................................................................................................................................................................................................19\r\n3.7 Fake Permutes.......................................................................................................................................................................................................19\r\n3.7.1 Shifts and Rotates ..............................................................................................................................................................................................20\r\n3.7.2 Data Conversions................................................................................................................................................................................................20\r\n4 Worked Examples................................................................................................................................................................................................20\r\n4.1 Matrix Transpose.................................................................................................................................................................................................20\r\n4.2 Multi-data-set Reductions................................................................................................................................................................................22\r\n4.2.1 Iterative Reduction Intrinsic ............................................................................................................................................................................22\r\n4.2.2 Using Transpose to Improve a Reduction .................................................................................................................................................23\r\n4.2.3 Integrated Transpose-and-Reduce..............................................................................................................................................................24\r\n4.3 Full-Register-Width Shifts (and Rotates) Using VBMI........................................................................................................................25\r\n5 Summary.................................................................................................................................................................................................................26\r\nCompiler-Assisted Permute Generation ...................................................................................................................................................27\r\nFigures\r\nFigure 1. Layout of Various Sizes of SIMD Register and How Each Can Be Broken Down into Smaller Subgroups of Elements\r\n.......................................................................................................................................................................................................................................5\r\nFigure 2. Function to Implement the 16-bit Compress Operation on FP16 Vector Elements....................................................................7\r\nFigure 3. General Operation of a Permute.......................................................................................................................................................................7\r\nFigure 4. Operation of Two Different Types of In-Lane Permute.........................................................................................................................8\r\nFigure 5. How In-Lane Dynamic Shuffle Instruction Permits a Conditional 0 to be Inserted Instead of an Index Element.......... 8\r\nFigure 6. A Full-Register Single-Source Cross-Lane Permute.............................................................................................................................. 9\r\nFigure 7. A Dual-Source Cross-Lane Permute ............................................................................................................................................................10\r\nFigure 8. How _mmX_multishift_epi64_epi8 Works for Each 64-bit Half-Lane..................................................................................10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/c2a486bd-b278-465b-b637-3196b0d6314f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=07be07bf5cdc56f75fb455977a4c318a6b509fbc0af5fd41b388a90f44f5e092",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 433
      },
      {
        "segments": [
          {
            "segment_id": "17033df7-7c85-4531-8ff6-f4fa5fd3972d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 3,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n3 \r\nFigure 9. How Dirty Indexing Works in _mm_permutevar_ps ...............................................................................................................................11\r\nFigure 10. Broadcast of a Single 16-bit Element into a 512-bit SIMD Value........................................................................................................12\r\nFigure 11. Broadcast of Four 32-bit Elements from Memory into a 512-bit SIMD Value..............................................................................12\r\nFigure 12. How Duplicate Low/High Instructions Work.............................................................................................................................................13\r\nFigure 13. Behavior of Scalar/SIMD Value Insert and Extract Operations.........................................................................................................13\r\nFigure 14. Behavior of Inter-Lane Insert and Extract Operations...........................................................................................................................14\r\nFigure 15. Behavior of Inter-Lane Insert and Extract Operations...........................................................................................................................14\r\nFigure 16. How to Perform Alignment of Two Data Objects ...................................................................................................................................15\r\nFigure 17. Per-lane Alignment..............................................................................................................................................................................................16\r\nFigure 18. Per-element Alignment......................................................................................................................................................................................17\r\nFigure 19. Element Rotation within a Single SIMD Value Using the Alignment Instruction .......................................................................17\r\nFigure 20. How the Bit-Wise Expansion and Compression Instructions Work, using a Zeroing Bit Mask.............................................18\r\nFigure 21. How Memory Gather Instruction Operates ...............................................................................................................................................18\r\nFigure 22. Basic Behavior of a VNNI Dot-Product-Like Instruction (e.g., _mXXX_madd_epi16) ..............................................................19\r\nFigure 23. Use of Truncation-Conversion to Perform a Stride-By-2 Permutation........................................................................................ 20\r\nFigure 24. Effect of an 8x8 Matrix Transpose................................................................................................................................................................. 21\r\nFigure 25. How to Perform a Transpose as a Series of Sub-Transposes of Different Sizes.......................................................................22\r\nFigure 26. Example Data Structure Showing a Series of Values That Must Be Reduced...........................................................................22\r\nFigure 27. Source Code Showing How to Perform a Reduction as a Series of Permute and Reduce Steps ......................................23\r\nFigure 28. How Multiple Reduce-Add Groups Can Be Transposed to Convert the Horizontal Reduction Operation into a \r\nVertical Summation............................................................................................................................................................................................24\r\nFigure 29. How Combined Transpose-And-Reduce Can Result in the Amount of Data Being Processed at Each Stage of the \r\nTranspose Can Be Reduced............................................................................................................................................................................24\r\nFigure 30. Transposition and Summation of Two Rows of Data ...........................................................................................................................25\r\nFigure 31. An Alternative Form of Transposition and summation That Exploits Commutativity Addition Reduction ..................25\r\nFigure 32. A Bit-Level Shift by a Large Offset in an Intel AVX-512 Register.....................................................................................................25\r\nFigure 33. How 64-bit Element Shifts Destroy Data Bits .........................................................................................................................................26\r\nFigure 34. Implementing Compile-Time Shift Using Lane-Wise and Element-Wise valign{d,q} Instructions from VBMI...........26\r\nFigure 35. Source Code Showing a C++20 Code Fragment that uses a Compile-Time Lambda Function to Generate an Index \r\nSequence for Use in the Compiler’s Own Built-In Permute ...............................................................................................................28\r\nTables\r\nTable 1. Terminology............................................................................................................................................................................................................. 4\r\nTable 2. Reference Documents ........................................................................................................................................................................................ 4\r\nTable 3. Dispatch Port and Execution Stacks of the Ice Lake Client Microarchitecture............................................................................ 6\r\nTable 4. Latency and Throughput Costs for a Variety of General Purpose Permute Instructions........................................................11\r\nTable 5. A Selection of Shuffle Indexes and the Intel AVX Instruction Sequence Generated by the Compiler............................27\r\nTable 6. A Selection of Compile-Time Permute Function Calls and the Intel AVX Instruction Sequence Generated by the \r\nCompiler..................................................................................................................................................................................................................28\r\nDocument Revision History\r\nRevision Date Description\r\n001 October 2022 Initial release.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/17033df7-7c85-4531-8ff6-f4fa5fd3972d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5235af27dcaf2ea34d29082da982a8c95d026d77f619f778f85a2a423f613082",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 446
      },
      {
        "segments": [
          {
            "segment_id": "7222307a-8063-416f-a2fc-c540a8c2f36b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 4,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n4 \r\n1.1 Terminology\r\nTable 1. Terminology\r\nAbbreviation Description\r\nBLAS Basic Linear Algebra Subprograms\r\nGFNI Galois Field New Instructions\r\nIntel® AVX Intel® Advanced Vector Extensions (Intel® AVX)\r\nIntel® AVX2 Intel® Advanced Vector Extensions 2 (Intel® AVX2)\r\nIntel® AVX-512 Intel® Advanced Vector Extensions 512 (Intel® AVX-512)\r\nISA Instruction set architecture. A definition of a processor, its instructions, and its data storage. ISA is typically \r\nused to refer to a family of related instructions thatimplement a particular class of operations.\r\nLLVM LLVM is a set of compiler and toolchain technologies that can be used to develop a front end for any \r\nprogramming language and a back end for any instruction set architecture.\r\nLSB Least significant bit. \r\nMSB Most significant bit\r\nSIMD Single instruction, multiple data. A single instruction operates on registers that can contain more than one data \r\nelement. As contrasted to Single Instruction, Single Data (SISD), where each instruction operates on registers \r\nthat store exactly one data value.\r\nVBMI Vector Bit Manipulation Instructions\r\nVNNI Vector Neural Network Instructions\r\n1.2 Reference Documentation\r\nTable 2. Reference Documents \r\nReference Source\r\nIntel® AVX-512 - FP16 Instruction Set for Intel® Xeon® \r\nProcessor Based Products Technology Guide\r\nhttps://networkbuilders.intel.com/solutionslibrary/intel-avx-512-fp16-instruction\u0002set-for-intel-xeon-processor-based-products-technology-guide\r\nIntel® AVX-512 - Instruction Set for Packet Processing \r\nTechnology Guide\r\nhttps://networkbuilders.intel.com/solutionslibrary/intel-avx-512-instruction-set\u0002for-packet-processing-technology-guide\r\nIntel® AVX-512 - Packet Processing with Intel® AVX\u0002512 Instruction Set Solution Briefhttps://networkbuilders.intel.com/solutionslibrary/intel-avx-512-packet\u0002processing-with-intel-avx-512-instruction-set-solution-brief\r\nIntel® AVX-512 -Writing Packet Processing Software \r\nwith Intel® AVX-512 Instruction Set Technology Guide\r\nhttps://networkbuilders.intel.com/solutionslibrary/intel-avx-512-writing-packet\u0002processing-software-with-intel-avx-512-instruction-set-technology-guide\r\nIntel® AVX-512 – Accelerate Packet Processing Using \r\nIntel® Advanced Vector Extensions 512 (Intel® AVX\u0002512) Training Video\r\nhttps://networkbuilders.intel.com/accelerate-packet-processing-using-intel\u0002advanced-vector-extensions-512-intel-avx-512\r\nIntel® AVX-512-FP16 Architecture Specification https://software.intel.com/content/www/us/en/develop/download/intel-avx512-\r\nfp16-architecture-specification.html\r\nIntel® 64 and IA-32 Architectures Optimization \r\nReference Manual\r\nhttps://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-\r\nia-32-architectures-optimization-manual.pdf\r\nIntel® 64 and IA-32 Architectures Software Developer \r\nManuals\r\nhttps://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html\r\nIntel® Intrinsics Guide, an online reference guide https://software.intel.com/sites/landingpage/IntrinsicsGuide/\r\nGalois Field New Instructions (GFNI) Technology \r\nGuide\r\nhttps://networkbuilders.intel.com/solutionslibrary/galois-field-new-instructions\u0002gfni-technology-guide\r\n2 Background \r\nPermute instructions can be used in many ways, including:\r\n Transpositions of matrices in numeric processing (e.g., Basic Linear Algebra Subprograms, or BLAS)\r\n Reformatting multiple independent data sets to make them more suitable for SIMD processing (also known as array-of\u0002struct and struct-of-array conversions)\r\n Horizontal reductions (e.g., determine the minimum value element in a SIMD value)\r\n In-register look-up tables",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/7222307a-8063-416f-a2fc-c540a8c2f36b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a9b1680037cd00ef96d1bcdce33c9931f97236c63e0a491b300b45747420ba55",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 366
      },
      {
        "segments": [
          {
            "segment_id": "f5fe176b-c624-472b-b454-ba5a2f3f53dc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 5,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n5 \r\nAll of these examples depend in some critical way upon being able to move data within or across SIMD values, and consequently\r\nunderstanding the most efficient ways possible to achieve this can make a considerable impact on the execution performance. \r\nSection 4 has worked examples that show ways to implement some of these functions.\r\n2.1 Terminology\r\nBefore we discuss the classes of permute instructions, it is necessary to set out how AVX registers are organized to store \r\ndifferent data elements since the layout of the register has many implications for the performance and utility of the instructions. \r\nFigure 1 shows how the data within a variety of registers may be organized. Throughout this document we adopted the \r\nconvention that bits are numbered so that the least-significant bit is on the right and the most significant bit is on the left.\r\nAt the highest level, SIMD values may be SSE-style 128-bit, AVX/AXV2-style 256-bit, or Intel AVX-512 style 512-bit. All registers \r\ncan be divided into 128-bit groups known as lanes. Lanes are an important concept since they have a direct impact on execution \r\nperformance; any permute operation thattakes place within a lane will be faster or more efficient than an operation that crosses \r\nlanes. Note that an SSE register is effectively an entire lane in its own right. Once again, the convention of putting the least \r\nsignificant bit on the right means that Lane 0 is the rightmost lane, and subsequent lanes are numbered ascendingly from right to \r\nleft.\r\nRegisters may be subdivided further into individual elements. Such elements may be 8, 16, 32, or 64-bits in size. Those elements \r\nrepresent the smallest unit at which an operation may be applied to a SIMD value. For some permutation operations, elements \r\nare grouped to form small clusters of data that are worked on together. For example, some permute instructions operate on \r\nformats known as`f32x4’ or ‘i64x2’, which represent four 32-bit values or two 64-bit values respectively, although it should be \r\nnoted that such clusters are essentially a 128-bit lane by a different name.\r\nIn an Intel AVX-512 processor, instructions can access registers of 128-bit, 256-bit, or 512-bit. The different sizes of registers are \r\noverlaid on top of each other. For example, the 128-bit xmm register is the lowest part of a 256-bit ymm register, and that in turn \r\nis the lowest half of a 512-bit zmm register. If a zmm register is used in an SSE instruction, then the instruction only operates on \r\nthe lowest 128-bits.\r\n512-bit AVX-512 register\r\n256-bit AVX/AVX2 register\r\n128-bit SSE register\r\n7 6\r\n15 14 13 12\r\n31 30 29 28 27 26 25 24\r\n5 4\r\n11 10 9 8\r\n23 22 21 20 19 18 17 16\r\n3 2\r\n7 6 5 4\r\n15 14 13 12 11 10 9 8\r\n1 0\r\n3 2 1 0\r\n7 6 5 4 3 2 1 0\r\n64-bit (e.g., double, int64)\r\n8-bit\r\n16-bit (e.g., _Float16, int16)\r\n32-bit (e.g., float, int32)\r\nLane 3\r\n128-bit\r\nLane 2\r\n128-bit\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\nBit 512 Bit 384 Bit 256 Bit 128 Bit 0\r\nFigure 1. Layout of Various Sizes of SIMD Register and How Each Can Be Broken Down into Smaller Subgroups of Elements\r\nNote that the terms shuffle or swizzle are also common names for the operation of permuting data, but we shall prefer the term \r\npermute in this document. Shuffle is used in the names of some older pre-Intel AVX-512 instructions, although there are some \r\nexceptions where shuffle issued more recently than this too. \r\n2.2 Execution Performance\r\nPermutes have a number of orthogonal properties that describe or influence their behavior and performance: \r\nIn-lane or cross-lane Permutes that operate within 128-bit lanes are faster, but less flexible, than their wider counterparts\r\n1-source or 2-source Some permutes can read data from two source registers, thus allowing inter-register permutation.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/f5fe176b-c624-472b-b454-ba5a2f3f53dc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=103cf3e272c6623d14894ab37854d5b37895ee536b89bd6e80b32c612d763cbb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 657
      },
      {
        "segments": [
          {
            "segment_id": "f5fe176b-c624-472b-b454-ba5a2f3f53dc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 5,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n5 \r\nAll of these examples depend in some critical way upon being able to move data within or across SIMD values, and consequently\r\nunderstanding the most efficient ways possible to achieve this can make a considerable impact on the execution performance. \r\nSection 4 has worked examples that show ways to implement some of these functions.\r\n2.1 Terminology\r\nBefore we discuss the classes of permute instructions, it is necessary to set out how AVX registers are organized to store \r\ndifferent data elements since the layout of the register has many implications for the performance and utility of the instructions. \r\nFigure 1 shows how the data within a variety of registers may be organized. Throughout this document we adopted the \r\nconvention that bits are numbered so that the least-significant bit is on the right and the most significant bit is on the left.\r\nAt the highest level, SIMD values may be SSE-style 128-bit, AVX/AXV2-style 256-bit, or Intel AVX-512 style 512-bit. All registers \r\ncan be divided into 128-bit groups known as lanes. Lanes are an important concept since they have a direct impact on execution \r\nperformance; any permute operation thattakes place within a lane will be faster or more efficient than an operation that crosses \r\nlanes. Note that an SSE register is effectively an entire lane in its own right. Once again, the convention of putting the least \r\nsignificant bit on the right means that Lane 0 is the rightmost lane, and subsequent lanes are numbered ascendingly from right to \r\nleft.\r\nRegisters may be subdivided further into individual elements. Such elements may be 8, 16, 32, or 64-bits in size. Those elements \r\nrepresent the smallest unit at which an operation may be applied to a SIMD value. For some permutation operations, elements \r\nare grouped to form small clusters of data that are worked on together. For example, some permute instructions operate on \r\nformats known as`f32x4’ or ‘i64x2’, which represent four 32-bit values or two 64-bit values respectively, although it should be \r\nnoted that such clusters are essentially a 128-bit lane by a different name.\r\nIn an Intel AVX-512 processor, instructions can access registers of 128-bit, 256-bit, or 512-bit. The different sizes of registers are \r\noverlaid on top of each other. For example, the 128-bit xmm register is the lowest part of a 256-bit ymm register, and that in turn \r\nis the lowest half of a 512-bit zmm register. If a zmm register is used in an SSE instruction, then the instruction only operates on \r\nthe lowest 128-bits.\r\n512-bit AVX-512 register\r\n256-bit AVX/AVX2 register\r\n128-bit SSE register\r\n7 6\r\n15 14 13 12\r\n31 30 29 28 27 26 25 24\r\n5 4\r\n11 10 9 8\r\n23 22 21 20 19 18 17 16\r\n3 2\r\n7 6 5 4\r\n15 14 13 12 11 10 9 8\r\n1 0\r\n3 2 1 0\r\n7 6 5 4 3 2 1 0\r\n64-bit (e.g., double, int64)\r\n8-bit\r\n16-bit (e.g., _Float16, int16)\r\n32-bit (e.g., float, int32)\r\nLane 3\r\n128-bit\r\nLane 2\r\n128-bit\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\nBit 512 Bit 384 Bit 256 Bit 128 Bit 0\r\nFigure 1. Layout of Various Sizes of SIMD Register and How Each Can Be Broken Down into Smaller Subgroups of Elements\r\nNote that the terms shuffle or swizzle are also common names for the operation of permuting data, but we shall prefer the term \r\npermute in this document. Shuffle is used in the names of some older pre-Intel AVX-512 instructions, although there are some \r\nexceptions where shuffle issued more recently than this too. \r\n2.2 Execution Performance\r\nPermutes have a number of orthogonal properties that describe or influence their behavior and performance: \r\nIn-lane or cross-lane Permutes that operate within 128-bit lanes are faster, but less flexible, than their wider counterparts\r\n1-source or 2-source Some permutes can read data from two source registers, thus allowing inter-register permutation.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/f5fe176b-c624-472b-b454-ba5a2f3f53dc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=103cf3e272c6623d14894ab37854d5b37895ee536b89bd6e80b32c612d763cbb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 657
      },
      {
        "segments": [
          {
            "segment_id": "2f09aad0-e883-4819-8206-48f93a25b75a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 6,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n6 \r\nStatic or dynamic Some permutes encode their permutation pattern into the instruction itself or use an immediate thatis \r\nsupplied to the instruction. Encoding the pattern into the instruction at compile-time can remove the \r\nneed for a separate index register but also will be less flexible in its capabilities. For example, \r\nimmediate values that are encoded into an instruction have a limited number of bits, and hence can \r\nonly configure a limited set of permutation indexes. In contrast, dynamic indexing is extremely flexible \r\nbut is more expensive due to the need to load and use an extra index register.\r\nGranularity The granularity of the units of data to which the indexes apply can vary, from entire 128-bit lanes and \r\ndown to individual bytes in some of the more recent Intel AVX-512 instruction set variants. Typically, \r\nthe large granularity permutes are cheaper to execute than variants that operate at the smaller \r\ngranularities. \r\nIn all the cases above, the choice for each property impacts the performance of the corresponding permute. The cheapest \r\npermutes would be those that operate within a lane using a static index and address large elements, while at the expensive end of \r\nthat extreme would be the cross-lane, multi-source dynamic byte permutes. When optimizing code, it is wise to try to use the \r\ncheapest possible permute to do the required job.\r\nWhile there are many instructions dedicated to moving data across or within SIMD values, it is useful to be able to perform data \r\nmovement in other ways too in order to improve the performance of the processor. This is because the processor is only capable \r\nof executing dedicated permute instructions on port 5 as shown in Table 31\r\n. It can be useful to use non-permute instructions in \r\nways that mimic permutes, so in a later section we also describe some of those instructions. \r\nTable 3. Dispatch Port and Execution Stacks of the Ice Lake Client Microarchitecture \r\n2.3 Element Data Type Support\r\nAs shown in Figure 1 above, SIMD values may be divided into smaller groups of elements. In the earlier SSE and Intel AVX \r\ninstruction sets, most instructions operated primarily on 32-bit and 64-bit elements, in both integer and floating-point forms. At \r\nthe time, these data elements were the most common and consequently Intel AVX permutation support was skewed towards \r\nthose data types in preference to other less common sizes, for both vertical and horizontal operations. There was some support \r\nfor the less common 8-bit operations, and very little support for 16-bit operations. Note that the 8-bit operations could be used\r\nreadily to implement 16-bit permutations where necessary, although with some slight inconvenience (e.g., a 16-bit dynamic \r\npermute requires the individual 8-bit permute elements to be generated).\r\nWith the evolution of ISAs from SSE through Intel AVX2 and into Intel AVX-512, other data sizes become more common and \r\nthese instruction sets started to gain additional 8- and 16-bit capabilities for all types of operation. These instructions were still \r\nslightly disadvantaged as they often had lower execution performance, but at least they provided ways of handling these \r\noperations more efficiently than synthesized code sequences. However, support for some of the more exotic instructions, such \r\nas compress and expand, still lacked 16-bit and 8-bit support.\r\nIn the 3rd Gen Intel® Xeon® Scalable processor, a major upgrade of many instructions occurred. 8- and 16-bit permutations were \r\nfinally handled with virtually the same ease as the other data types. For example, the compress and expand instructions that we \r\ndescribe in Section 3.4 gained variants that operated at 8- and 16-bit granularity.\r\n1 Note that on 3rd Gen Intel® Xeon® Scalable processors (codenamed Ice Lake) and 4th Gen Intel® Xeon® Scalable processors (codenamed \r\nSapphire Rapids), port 1 also can execute some shuffle instructions, but not the full range of general purpose permutes described in this \r\ndocument, so we do not consider that dimension further in this document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/2f09aad0-e883-4819-8206-48f93a25b75a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=dbd97521a85a41c6ee1309921b1c4c706a9269df4608806f937c3de456a9c8fb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 663
      },
      {
        "segments": [
          {
            "segment_id": "2f09aad0-e883-4819-8206-48f93a25b75a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 6,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n6 \r\nStatic or dynamic Some permutes encode their permutation pattern into the instruction itself or use an immediate thatis \r\nsupplied to the instruction. Encoding the pattern into the instruction at compile-time can remove the \r\nneed for a separate index register but also will be less flexible in its capabilities. For example, \r\nimmediate values that are encoded into an instruction have a limited number of bits, and hence can \r\nonly configure a limited set of permutation indexes. In contrast, dynamic indexing is extremely flexible \r\nbut is more expensive due to the need to load and use an extra index register.\r\nGranularity The granularity of the units of data to which the indexes apply can vary, from entire 128-bit lanes and \r\ndown to individual bytes in some of the more recent Intel AVX-512 instruction set variants. Typically, \r\nthe large granularity permutes are cheaper to execute than variants that operate at the smaller \r\ngranularities. \r\nIn all the cases above, the choice for each property impacts the performance of the corresponding permute. The cheapest \r\npermutes would be those that operate within a lane using a static index and address large elements, while at the expensive end of \r\nthat extreme would be the cross-lane, multi-source dynamic byte permutes. When optimizing code, it is wise to try to use the \r\ncheapest possible permute to do the required job.\r\nWhile there are many instructions dedicated to moving data across or within SIMD values, it is useful to be able to perform data \r\nmovement in other ways too in order to improve the performance of the processor. This is because the processor is only capable \r\nof executing dedicated permute instructions on port 5 as shown in Table 31\r\n. It can be useful to use non-permute instructions in \r\nways that mimic permutes, so in a later section we also describe some of those instructions. \r\nTable 3. Dispatch Port and Execution Stacks of the Ice Lake Client Microarchitecture \r\n2.3 Element Data Type Support\r\nAs shown in Figure 1 above, SIMD values may be divided into smaller groups of elements. In the earlier SSE and Intel AVX \r\ninstruction sets, most instructions operated primarily on 32-bit and 64-bit elements, in both integer and floating-point forms. At \r\nthe time, these data elements were the most common and consequently Intel AVX permutation support was skewed towards \r\nthose data types in preference to other less common sizes, for both vertical and horizontal operations. There was some support \r\nfor the less common 8-bit operations, and very little support for 16-bit operations. Note that the 8-bit operations could be used\r\nreadily to implement 16-bit permutations where necessary, although with some slight inconvenience (e.g., a 16-bit dynamic \r\npermute requires the individual 8-bit permute elements to be generated).\r\nWith the evolution of ISAs from SSE through Intel AVX2 and into Intel AVX-512, other data sizes become more common and \r\nthese instruction sets started to gain additional 8- and 16-bit capabilities for all types of operation. These instructions were still \r\nslightly disadvantaged as they often had lower execution performance, but at least they provided ways of handling these \r\noperations more efficiently than synthesized code sequences. However, support for some of the more exotic instructions, such \r\nas compress and expand, still lacked 16-bit and 8-bit support.\r\nIn the 3rd Gen Intel® Xeon® Scalable processor, a major upgrade of many instructions occurred. 8- and 16-bit permutations were \r\nfinally handled with virtually the same ease as the other data types. For example, the compress and expand instructions that we \r\ndescribe in Section 3.4 gained variants that operated at 8- and 16-bit granularity.\r\n1 Note that on 3rd Gen Intel® Xeon® Scalable processors (codenamed Ice Lake) and 4th Gen Intel® Xeon® Scalable processors (codenamed \r\nSapphire Rapids), port 1 also can execute some shuffle instructions, but not the full range of general purpose permutes described in this \r\ndocument, so we do not consider that dimension further in this document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/2f09aad0-e883-4819-8206-48f93a25b75a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=dbd97521a85a41c6ee1309921b1c4c706a9269df4608806f937c3de456a9c8fb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 663
      },
      {
        "segments": [
          {
            "segment_id": "d27f608a-8fe0-462c-9f98-c8b8c3cc3d6c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 7,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n7 \r\nAt the time of writing, 3rd Gen and 4th Gen Intel Xeon Scalable processors support essentially every type of permute at every \r\nelement granularity. There may still be some performance differences (especially for small element types) but there is no lack of \r\ninstructions available to perform every possible instruction that could be required.\r\nIn a few places an instruction is available for only some data types of a particular size and not for others. For example, the recently \r\nintroduced AVX512-FP16 instruction set, described in Intel® AVX-512 - FP16 Instruction Set for Intel® Xeon® Processor Based \r\nProducts Technology Guide, provides support for half-precision floating-point operations but does not provide any way to \r\npermute such values. However, since the FP16 data format uses 16-bits for storage, we can implement variants of those \r\ninstructions using the existing epi16 permute support, as illustrated in Figure 2. In that example, the incoming FP16 data is cast to \r\nepi16, the compress instruction performed on it, and then the value cast back to FP16 on completion. The cast instructions cost \r\nnothing to implement and are only there to tell the compiler the intent. \r\nFigure 2. Function to Implement the 16-bit Compress Operation on FP16 Vector Elements\r\n3 AVX Permutation Instructions\r\nIn this major section we describe each of the common classes of permute instructions, starting with general purpose \r\ninstructions that can essentially implement any permute, and working through some of the more unusual specialized instructions \r\nthat can be used to perform particular types of permute more efficiently or faster.\r\n3.1 General Purpose Permutes\r\nThe general purpose permutes are the most flexible way to perform almost any type of horizontal movement of data within or \r\nacross multiple AVX SIMD values. Practically any type of desired permutation of data could be implemented using the \r\ninstructions described in this section, with the only exception being the compress and expand instructions from Section 3.4. All \r\npermutes operate on the general principle shown in Figure 3. A set of elements from some input SIMD value is reordered using \r\nan index to form a new SIMD value. Individual elements may be duplicated, moved, or omitted entirely from the output result.\r\n7 6 5 4 3 2 1 0\r\n7 4 3 7 2 6 1 1\r\nIndexes\r\nFigure 3. General Operation of a Permute\r\nLater in this document we describe many specialized permute instructions. The general purpose permute instructions are \r\nsufficiently powerful that they can often behave in the same way as the specialized permutes, but there are some disadvantages \r\nto using a general permute in such a way. One reason is that the more powerful general purpose permutes often require an index \r\nregister to be used to store the desired permutation. Using a register to store the index increases register pressure (i.e., how \r\nmany registers are needed to store all in-use values), which can limit some optimizations, and also requires an extra instruction to \r\nload the desired index values into the register. Another reason for preferring a specialized instruction is that it may use dedicated \r\nhardware to make the operation more efficient. For example, if a single value has to be broadcast from memory into all elements \r\nof a register, one way to achieve this is by loading a value from memory into a register in one instruction, and then broadcasting it \r\nin the register using a general purpose permute in another instruction. However, the load execution unit in all recent Intel \r\nprocessors has a special broadcast unit built into the memory load unit, which means that a single instruction can perform the \r\nwork of both the load and the broadcast more cheaply.\r\nThe remainder of this section describes the main variants of permute, working from the simplest and cheapest, through to the \r\nmost comprehensive and expensive.\r\n__m512h compress_ph(__mmask32 mask, __m512h value) \r\n{ \r\n const auto asInt16 = _mm512_castph_si512(value);\r\n const auto comp = _mm512_mask_compress_epi16(mask, asInt16);\r\n return _mm512_castsi512_ph(comp);\r\n}",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/d27f608a-8fe0-462c-9f98-c8b8c3cc3d6c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=faafcb5bb7080309fe4e592ca1400627df33abd443150d6437601986633ab474",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 664
      },
      {
        "segments": [
          {
            "segment_id": "d27f608a-8fe0-462c-9f98-c8b8c3cc3d6c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 7,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n7 \r\nAt the time of writing, 3rd Gen and 4th Gen Intel Xeon Scalable processors support essentially every type of permute at every \r\nelement granularity. There may still be some performance differences (especially for small element types) but there is no lack of \r\ninstructions available to perform every possible instruction that could be required.\r\nIn a few places an instruction is available for only some data types of a particular size and not for others. For example, the recently \r\nintroduced AVX512-FP16 instruction set, described in Intel® AVX-512 - FP16 Instruction Set for Intel® Xeon® Processor Based \r\nProducts Technology Guide, provides support for half-precision floating-point operations but does not provide any way to \r\npermute such values. However, since the FP16 data format uses 16-bits for storage, we can implement variants of those \r\ninstructions using the existing epi16 permute support, as illustrated in Figure 2. In that example, the incoming FP16 data is cast to \r\nepi16, the compress instruction performed on it, and then the value cast back to FP16 on completion. The cast instructions cost \r\nnothing to implement and are only there to tell the compiler the intent. \r\nFigure 2. Function to Implement the 16-bit Compress Operation on FP16 Vector Elements\r\n3 AVX Permutation Instructions\r\nIn this major section we describe each of the common classes of permute instructions, starting with general purpose \r\ninstructions that can essentially implement any permute, and working through some of the more unusual specialized instructions \r\nthat can be used to perform particular types of permute more efficiently or faster.\r\n3.1 General Purpose Permutes\r\nThe general purpose permutes are the most flexible way to perform almost any type of horizontal movement of data within or \r\nacross multiple AVX SIMD values. Practically any type of desired permutation of data could be implemented using the \r\ninstructions described in this section, with the only exception being the compress and expand instructions from Section 3.4. All \r\npermutes operate on the general principle shown in Figure 3. A set of elements from some input SIMD value is reordered using \r\nan index to form a new SIMD value. Individual elements may be duplicated, moved, or omitted entirely from the output result.\r\n7 6 5 4 3 2 1 0\r\n7 4 3 7 2 6 1 1\r\nIndexes\r\nFigure 3. General Operation of a Permute\r\nLater in this document we describe many specialized permute instructions. The general purpose permute instructions are \r\nsufficiently powerful that they can often behave in the same way as the specialized permutes, but there are some disadvantages \r\nto using a general permute in such a way. One reason is that the more powerful general purpose permutes often require an index \r\nregister to be used to store the desired permutation. Using a register to store the index increases register pressure (i.e., how \r\nmany registers are needed to store all in-use values), which can limit some optimizations, and also requires an extra instruction to \r\nload the desired index values into the register. Another reason for preferring a specialized instruction is that it may use dedicated \r\nhardware to make the operation more efficient. For example, if a single value has to be broadcast from memory into all elements \r\nof a register, one way to achieve this is by loading a value from memory into a register in one instruction, and then broadcasting it \r\nin the register using a general purpose permute in another instruction. However, the load execution unit in all recent Intel \r\nprocessors has a special broadcast unit built into the memory load unit, which means that a single instruction can perform the \r\nwork of both the load and the broadcast more cheaply.\r\nThe remainder of this section describes the main variants of permute, working from the simplest and cheapest, through to the \r\nmost comprehensive and expensive.\r\n__m512h compress_ph(__mmask32 mask, __m512h value) \r\n{ \r\n const auto asInt16 = _mm512_castph_si512(value);\r\n const auto comp = _mm512_mask_compress_epi16(mask, asInt16);\r\n return _mm512_castsi512_ph(comp);\r\n}",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/d27f608a-8fe0-462c-9f98-c8b8c3cc3d6c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=faafcb5bb7080309fe4e592ca1400627df33abd443150d6437601986633ab474",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 664
      },
      {
        "segments": [
          {
            "segment_id": "00ee59af-dfc3-479c-925e-5b2e29c146a7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 8,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n8 \r\n3.1.1 In-Lane General-Purpose Permute\r\nThe in-lane permutes can only move data within a single 128-bit lane of data, and historically they all derive from the original \r\ninstructions found in the 128-bit SSE-family of instructions. The 256-bit and 512-bit variants that were introduced in Intel AVX \r\nand Intel AVX-512 are implemented by duplicating the underlying 128-bitlane function across every lane in the wider registers. \r\nAn illustration of their general behavior for 256-bit registers is shown in Figure 4. On the left, the permute is operating on 32-bit \r\ndata elements. Because each lane has only four such elements and they can only permute to four possible locations within the \r\noutput lane, the desired index can be expressed in a single 8-bit immediate. Because there is only one immediate value however, \r\nall the lanes must perform the same permutation. In contrast, the right-hand example of that figure shows how a byte-level \r\npermute is implemented. In this, every byte can be moved somewhere else, but because the indexing is more complicated, the \r\nindex can only come from a second index register. Because a separate index register is used it does mean that each lane can use \r\na different permutation index.\r\n256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n_mm256_shuffle_epi32(sourceReg, imm) _mm256_shuffle_epi8(sourceReg, indexReg)\r\nBit 256 Bit 128 Bit 0 Bit 256 Bit 128 Bit 0\r\nFigure 4. Operation of Two Different Types of In-Lane Permute \r\nNote that the limited size of the immediate value means that it is impossible to encode 8- or 16-bit permutes using an immediate. \r\nThe only way to permute at this granularity is by using the shuffle_epi8 instruction with a dynamic index. In the case of 16-bit \r\npermutes, adjacent 8-bit indexes must be configured to move pairs of bytes, rather than having a single index representing the \r\nentire 16-bit index.\r\nOne note of warning is that the index register used for _mm[256]_shuffle_epi8 has a slight difference in behavior compared \r\nwith many of the other general-purpose indexed permutes described in this document. Normally a permute would use the least \r\nnumber of bits required to form the index (e.g., an in-lane shuffle would require four bits to store any possible index position for a \r\nbyte within a lane). Any extra bits would be ignored by most of the AVX permute instructions, forming what we shall call a dirty \r\nindex (see Section 3.1.5 for more details). In the case of the shuffle_epi8 instructions however, the MSB of each index is used to \r\ndecide whether to insert a zero value into the output position, as illustrated in Figure 5. Note how the hexadecimal index uses the \r\nlowest 4-bits as the output index, but if the MSB of the index is set, a zero value (gray in this diagram) will be inserted instead. \r\n \r\n128 64 0\r\nSource f e d c b a 9 8 7 6 5 4 3 2 1 0\r\nHEX Index\r\nResult\r\nf2 04 f2 8a 03 09 8c 05 07 09 8a 80 ff 2 66 00\r\n0 [4] 0 0 [3] [9] 0 [5] [7] [9] 0 0 0 [2] [6] [0]\r\nFigure 5. How In-Lane Dynamic Shuffle Instruction Permits a Conditional 0 to be Inserted Instead of an Index Element\r\nNote that the in-lane permutes are named shuffles because they come from the original SSE instruction set that used that \r\nterminology, and the same naming convention has been used for their 256-bit and 512-bit in-lane counterparts.\r\nAll variants of in-lane permutes are cheap and can operate in 1 cycle, with a throughput of 1 instruction per cycle.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/00ee59af-dfc3-479c-925e-5b2e29c146a7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1be16cdca053452f7d90eedee405bd1afb86417314d3489b19480ad15bb834c7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 617
      },
      {
        "segments": [
          {
            "segment_id": "00ee59af-dfc3-479c-925e-5b2e29c146a7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 8,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n8 \r\n3.1.1 In-Lane General-Purpose Permute\r\nThe in-lane permutes can only move data within a single 128-bit lane of data, and historically they all derive from the original \r\ninstructions found in the 128-bit SSE-family of instructions. The 256-bit and 512-bit variants that were introduced in Intel AVX \r\nand Intel AVX-512 are implemented by duplicating the underlying 128-bitlane function across every lane in the wider registers. \r\nAn illustration of their general behavior for 256-bit registers is shown in Figure 4. On the left, the permute is operating on 32-bit \r\ndata elements. Because each lane has only four such elements and they can only permute to four possible locations within the \r\noutput lane, the desired index can be expressed in a single 8-bit immediate. Because there is only one immediate value however, \r\nall the lanes must perform the same permutation. In contrast, the right-hand example of that figure shows how a byte-level \r\npermute is implemented. In this, every byte can be moved somewhere else, but because the indexing is more complicated, the \r\nindex can only come from a second index register. Because a separate index register is used it does mean that each lane can use \r\na different permutation index.\r\n256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n_mm256_shuffle_epi32(sourceReg, imm) _mm256_shuffle_epi8(sourceReg, indexReg)\r\nBit 256 Bit 128 Bit 0 Bit 256 Bit 128 Bit 0\r\nFigure 4. Operation of Two Different Types of In-Lane Permute \r\nNote that the limited size of the immediate value means that it is impossible to encode 8- or 16-bit permutes using an immediate. \r\nThe only way to permute at this granularity is by using the shuffle_epi8 instruction with a dynamic index. In the case of 16-bit \r\npermutes, adjacent 8-bit indexes must be configured to move pairs of bytes, rather than having a single index representing the \r\nentire 16-bit index.\r\nOne note of warning is that the index register used for _mm[256]_shuffle_epi8 has a slight difference in behavior compared \r\nwith many of the other general-purpose indexed permutes described in this document. Normally a permute would use the least \r\nnumber of bits required to form the index (e.g., an in-lane shuffle would require four bits to store any possible index position for a \r\nbyte within a lane). Any extra bits would be ignored by most of the AVX permute instructions, forming what we shall call a dirty \r\nindex (see Section 3.1.5 for more details). In the case of the shuffle_epi8 instructions however, the MSB of each index is used to \r\ndecide whether to insert a zero value into the output position, as illustrated in Figure 5. Note how the hexadecimal index uses the \r\nlowest 4-bits as the output index, but if the MSB of the index is set, a zero value (gray in this diagram) will be inserted instead. \r\n \r\n128 64 0\r\nSource f e d c b a 9 8 7 6 5 4 3 2 1 0\r\nHEX Index\r\nResult\r\nf2 04 f2 8a 03 09 8c 05 07 09 8a 80 ff 2 66 00\r\n0 [4] 0 0 [3] [9] 0 [5] [7] [9] 0 0 0 [2] [6] [0]\r\nFigure 5. How In-Lane Dynamic Shuffle Instruction Permits a Conditional 0 to be Inserted Instead of an Index Element\r\nNote that the in-lane permutes are named shuffles because they come from the original SSE instruction set that used that \r\nterminology, and the same naming convention has been used for their 256-bit and 512-bit in-lane counterparts.\r\nAll variants of in-lane permutes are cheap and can operate in 1 cycle, with a throughput of 1 instruction per cycle.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/00ee59af-dfc3-479c-925e-5b2e29c146a7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1be16cdca053452f7d90eedee405bd1afb86417314d3489b19480ad15bb834c7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 617
      },
      {
        "segments": [
          {
            "segment_id": "a3fec51d-ce99-476c-a17c-511ba1179dfd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 9,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n9 \r\n3.1.2 Single Source Cross-Lane General-Purpose Permutes\r\nFull register width, cross-lane permutes operate as illustrated in Figure 6. Cross-lane permutes allow any input element to be \r\ncopied to any output element position. Because of the large set of possible indexes, an immediate cannot be used and the index \r\nis supplied in another register.\r\n_mm256_permutexvar_epi32(sourceReg, indexReg)\r\n256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n7 6 5 4 3 2 1 0\r\n5 7 3 0 6 1 2 4\r\nBit 256 Bit 128 Bit 0\r\nFigure 6. A Full-Register Single-Source Cross-Lane Permute\r\nCross-lane permutes typically have a latency of 3 cycles and a throughput of 1 instruction per cycle.\r\nThe Intel AVX-512 foundation instruction set has support for 32-bit and 64-bit elements cross-lane permutes. 8-bit and 16-bit \r\nsupport for cross-lane permutes was only added in the VBMI ISA available in 3rd Gen Intel Xeon Scalable processors and \r\nonwards. Note also that while 8-bit element permutes have the same performance as their 32- and 64-bit counterparts, 16-bit \r\ncross-lane permute is more expensive and should be avoided where possible or replaced with an 8-bit variant instead.\r\n3.1.3 Dual Source Cross-Lane General-Purpose Permutes\r\nDual source permutes allow two source registers to be used as though they were indexed as a single register of twice the width. \r\nThis is illustrated in Figure 7, where the first source register (on the right, following our LSB-on-the-right convention) provides \r\nindexes [0..7] and the left-hand register provides indexes [8..15]. The output register can therefore not only choose which \r\nelement from the register to use in any given output position, but it may even select from which register the element should be \r\ntaken. This is a powerful instruction for performing any permute where data from several sources needs to be mixed together \r\n(e.g., transpose).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/a3fec51d-ce99-476c-a17c-511ba1179dfd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8906ebced7fef6dafff1f5550b382f089191c0ed19fd50c880987669e307e6cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 314
      },
      {
        "segments": [
          {
            "segment_id": "1979e525-4e0f-47eb-9313-93fc2cae487d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 10,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n10\r\n_mm256_permutex2var_epi32(source0Reg, indexReg, source1Reg)\r\nSource 1 - 256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n15 14 13 12 11 10 9 8\r\nSource 0 - 256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n7 6 5 4 3 2 1 0\r\n13 15 5 12 7 0 6 8\r\nBit 256 Bit 128 Bit 0 Bit 256 Bit 128 Bit 0\r\nFigure 7. A Dual-Source Cross-Lane Permute\r\nThe 32-bit and 64-bit variants available in the Intel AVX-512 foundation ISA have the same performance as their single-source \r\ncounterparts: 3 cycle latency and a throughput of 1.\r\n8-bit and 16-bit dual source permutes were introduced in the VBMI ISA added in 3rd Gen Intel Xeon Scalable processors. They \r\nare both slightly more expensive than their single-source counterparts.\r\n3.1.4 Bit-Level Byte Shuffle (VBMI)\r\nThe general-purpose instructions discussed so far operate at an element granularity thatis no less than the byte, but it can \r\nsometimes be useful to be able to permute data at the bit-level. This would be an expensive operation if applied at the full\u0002register size (i.e., moving a set of bits from any one location in an Intel AVX-512 register to anywhere else would require very \r\nexpensive hardware), and also defining the index would be very tricky. However, by imposing a limitation that data movement is \r\nrestricted to within a half-lane of 64-bits, it becomes possible to define a special purpose bit-level permutation, and this was \r\nintroduced to the 3rd Gen Intel Xeon Scalable processors VBMI ISA. \r\nThe basic operation of the VBMI multishift byte instruction is shown in Figure 8. The input is a set of 64-bit elements. A source \r\nindex register is supplied thatis broken down into 8-bit groups, and within each 8-bit group is a single index value thatrepresents \r\nthe bit-level index of the 8-bit block to read into that byte output. For example, the first byte of the output comes from bits [1,9) \r\nof the source value, and the second byte of the output reads bits [11,19). Outputs are written in bytes, but each byte may come \r\nfrom completely arbitrary bit positions from the input.\r\n63 56 48 40 32 24 16 8 0\r\n64-bit half lane\r\n56 56 43 26 31 11 11 1\r\nFigure 8. How _mmX_multishift_epi64_epi8 Works for Each 64-bit Half-Lane\r\nThe multishift byte instruction performs the same basic operation for every 64-bit block within the source register (i.e., for 128-\r\nbit registers, it would perform two such permutes, for 256-bit it would perform four such permutes, and so on).\r\nThe instruction takes 3 cycles and has a throughput of 1 on 3rd Gen Intel Xeon Scalable processors. \r\n3.1.5 Clean and Dirty Indexing\r\nAll of the dynamic permute instructions described in this section require the indexes to be specified using a separate SIMD \r\nregister, but the instructions vary a little in how they interpret the contents of that register. Mostly, each element is indexed by \r\nthe least number of bits required to represent the index. An example is shown in Figure 9, which illustrates the behavior of the ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/1979e525-4e0f-47eb-9313-93fc2cae487d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8e2b6b27a66e322123a84eb8c9e48283edc76ff0f43bc5ac7d9cd6335915b1df",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 525
      },
      {
        "segments": [
          {
            "segment_id": "1979e525-4e0f-47eb-9313-93fc2cae487d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 10,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n10\r\n_mm256_permutex2var_epi32(source0Reg, indexReg, source1Reg)\r\nSource 1 - 256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n15 14 13 12 11 10 9 8\r\nSource 0 - 256-bit AVX/AVX2 register\r\nLane 1\r\n128-bit\r\nLane 0\r\n128-bit\r\n7 6 5 4 3 2 1 0\r\n13 15 5 12 7 0 6 8\r\nBit 256 Bit 128 Bit 0 Bit 256 Bit 128 Bit 0\r\nFigure 7. A Dual-Source Cross-Lane Permute\r\nThe 32-bit and 64-bit variants available in the Intel AVX-512 foundation ISA have the same performance as their single-source \r\ncounterparts: 3 cycle latency and a throughput of 1.\r\n8-bit and 16-bit dual source permutes were introduced in the VBMI ISA added in 3rd Gen Intel Xeon Scalable processors. They \r\nare both slightly more expensive than their single-source counterparts.\r\n3.1.4 Bit-Level Byte Shuffle (VBMI)\r\nThe general-purpose instructions discussed so far operate at an element granularity thatis no less than the byte, but it can \r\nsometimes be useful to be able to permute data at the bit-level. This would be an expensive operation if applied at the full\u0002register size (i.e., moving a set of bits from any one location in an Intel AVX-512 register to anywhere else would require very \r\nexpensive hardware), and also defining the index would be very tricky. However, by imposing a limitation that data movement is \r\nrestricted to within a half-lane of 64-bits, it becomes possible to define a special purpose bit-level permutation, and this was \r\nintroduced to the 3rd Gen Intel Xeon Scalable processors VBMI ISA. \r\nThe basic operation of the VBMI multishift byte instruction is shown in Figure 8. The input is a set of 64-bit elements. A source \r\nindex register is supplied thatis broken down into 8-bit groups, and within each 8-bit group is a single index value thatrepresents \r\nthe bit-level index of the 8-bit block to read into that byte output. For example, the first byte of the output comes from bits [1,9) \r\nof the source value, and the second byte of the output reads bits [11,19). Outputs are written in bytes, but each byte may come \r\nfrom completely arbitrary bit positions from the input.\r\n63 56 48 40 32 24 16 8 0\r\n64-bit half lane\r\n56 56 43 26 31 11 11 1\r\nFigure 8. How _mmX_multishift_epi64_epi8 Works for Each 64-bit Half-Lane\r\nThe multishift byte instruction performs the same basic operation for every 64-bit block within the source register (i.e., for 128-\r\nbit registers, it would perform two such permutes, for 256-bit it would perform four such permutes, and so on).\r\nThe instruction takes 3 cycles and has a throughput of 1 on 3rd Gen Intel Xeon Scalable processors. \r\n3.1.5 Clean and Dirty Indexing\r\nAll of the dynamic permute instructions described in this section require the indexes to be specified using a separate SIMD \r\nregister, but the instructions vary a little in how they interpret the contents of that register. Mostly, each element is indexed by \r\nthe least number of bits required to represent the index. An example is shown in Figure 9, which illustrates the behavior of the ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/1979e525-4e0f-47eb-9313-93fc2cae487d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8e2b6b27a66e322123a84eb8c9e48283edc76ff0f43bc5ac7d9cd6335915b1df",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 525
      },
      {
        "segments": [
          {
            "segment_id": "8dfe6b13-395d-4678-b2a3-a2522edba5b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 11,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n11\r\n_mm_permutevar_ps instruction. There are four possible input elements in the source SIMD value, so two index bits are \r\nrequired. Since each index is represented by the same number of bits as the elements being permuted (i.e., 32-bits in this case), \r\nit follows that the upper bits of each index element contain bits that are not used by the index operation itself. Most dynamic \r\npermute instructions allow those bits to be completely arbitrary and they only use the lowest bits that are actually required; we \r\nshall call this a dirty index. Dirty indexes are useful when computing index values, rather than preloading them. For example, if the \r\nindex is computed by shifting or rotating values within an index to put the required index bits into the lowest bit position,there is \r\nno need to do further work to mask out the unwanted higher bits since the instruction will ignore them anyway.\r\n...11111 ...0100 ...01010\r\nc d a c\r\nBit 128 Bit 0\r\nd c b a\r\n...11110\r\n2 3 0 2\r\nSource\r\nIndex\r\nResult\r\nFigure 9. How Dirty Indexing Works in _mm_permutevar_ps\r\nIn contrast, a few of the general purpose permute instructions (e.g., _mm_shuffle_epi8) require what we shall call clean \r\nindexes. In those indexes the lowest bits provide the index, but the other bits also convey some meaning to the instruction (see \r\nSection 3.1.1 for more details). This can be extremely useful in some circumstances, but it must be accounted for when \r\ncomputing indexes using other instruction sequences, since bits may be left behind in places other than the lowest actual index \r\nbits and these change the behavior of the instruction. In such a case, care must be taken to mask out the unwanted bits.\r\n3.1.6 Summary of Relative Costs for Different Permutes\r\nIn Table 4 we show some example costs for different general-purpose instructions running on a 3rd Gen Intel Xeon Scalable \r\nprocessor. These illustrate how it is best to use in-lane permutes where possible, and for cross-lane permutes to try to use the \r\nlarger granularities. \r\nFor true 16-bit permutes, the instructions should be used since there is no easy alternative, but, where the permutation pattern is \r\nknown at compile-time, it is cheaper to use the 8-bit permute instead, with pairs of indexes to represent a 16-bit permute.\r\nNote that future processors may have different permutation costs.\r\nTable 4. Latency and Throughput Costs for a Variety of General Purpose Permute Instructions\r\nElement Granularity In-Lane Single-Source Cross-Lane Dual-Source Cross-Lane\r\n8-bit 1/1 3/1 (VBMI only) 4/2 (VBMI only)\r\n16-bit n/a 4/1 (VBMI only) 7/2 (VBMI only)\r\n32-bit 1/1 3/1 3/1\r\n64-bit 1/1 3/1 3/1\r\n3.2 Fixed Purpose Permutes\r\nThere are a few common types of permutations or shuffles2 – broadcasting, duplicating packing and unpacking – that are \r\ndirectly supported using specific instructions since this typically confers some performance advantages.\r\n3.2.1 Broadcast \r\nA broadcast operation takes small group of elements – either a single element or a few contiguous elements – and puts \r\nduplicated copies of that element group across an entire SIMD register value. For example, Figure 10 illustrates how a single 16-\r\nbit element has been broadcast from the lowest position of one SIMD data value into all positions in a different SIMD data value.\r\n2 Historically many of these fixed-purpose permutes can be found in pre-Intel AVX-512 instruction sets where they were called \r\nshuffles instead. Where they are also available in Intel AVX-512 instruction sets the term shuffle is still used to show how they \r\nrelate to their pre-Intel AVX-512 counterparts. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/8dfe6b13-395d-4678-b2a3-a2522edba5b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9228feef4461991832f6d8b9e6e3536e821f442e4b24d5ec8c540712341bf483",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 599
      },
      {
        "segments": [
          {
            "segment_id": "8dfe6b13-395d-4678-b2a3-a2522edba5b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 11,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n11\r\n_mm_permutevar_ps instruction. There are four possible input elements in the source SIMD value, so two index bits are \r\nrequired. Since each index is represented by the same number of bits as the elements being permuted (i.e., 32-bits in this case), \r\nit follows that the upper bits of each index element contain bits that are not used by the index operation itself. Most dynamic \r\npermute instructions allow those bits to be completely arbitrary and they only use the lowest bits that are actually required; we \r\nshall call this a dirty index. Dirty indexes are useful when computing index values, rather than preloading them. For example, if the \r\nindex is computed by shifting or rotating values within an index to put the required index bits into the lowest bit position,there is \r\nno need to do further work to mask out the unwanted higher bits since the instruction will ignore them anyway.\r\n...11111 ...0100 ...01010\r\nc d a c\r\nBit 128 Bit 0\r\nd c b a\r\n...11110\r\n2 3 0 2\r\nSource\r\nIndex\r\nResult\r\nFigure 9. How Dirty Indexing Works in _mm_permutevar_ps\r\nIn contrast, a few of the general purpose permute instructions (e.g., _mm_shuffle_epi8) require what we shall call clean \r\nindexes. In those indexes the lowest bits provide the index, but the other bits also convey some meaning to the instruction (see \r\nSection 3.1.1 for more details). This can be extremely useful in some circumstances, but it must be accounted for when \r\ncomputing indexes using other instruction sequences, since bits may be left behind in places other than the lowest actual index \r\nbits and these change the behavior of the instruction. In such a case, care must be taken to mask out the unwanted bits.\r\n3.1.6 Summary of Relative Costs for Different Permutes\r\nIn Table 4 we show some example costs for different general-purpose instructions running on a 3rd Gen Intel Xeon Scalable \r\nprocessor. These illustrate how it is best to use in-lane permutes where possible, and for cross-lane permutes to try to use the \r\nlarger granularities. \r\nFor true 16-bit permutes, the instructions should be used since there is no easy alternative, but, where the permutation pattern is \r\nknown at compile-time, it is cheaper to use the 8-bit permute instead, with pairs of indexes to represent a 16-bit permute.\r\nNote that future processors may have different permutation costs.\r\nTable 4. Latency and Throughput Costs for a Variety of General Purpose Permute Instructions\r\nElement Granularity In-Lane Single-Source Cross-Lane Dual-Source Cross-Lane\r\n8-bit 1/1 3/1 (VBMI only) 4/2 (VBMI only)\r\n16-bit n/a 4/1 (VBMI only) 7/2 (VBMI only)\r\n32-bit 1/1 3/1 3/1\r\n64-bit 1/1 3/1 3/1\r\n3.2 Fixed Purpose Permutes\r\nThere are a few common types of permutations or shuffles2 – broadcasting, duplicating packing and unpacking – that are \r\ndirectly supported using specific instructions since this typically confers some performance advantages.\r\n3.2.1 Broadcast \r\nA broadcast operation takes small group of elements – either a single element or a few contiguous elements – and puts \r\nduplicated copies of that element group across an entire SIMD register value. For example, Figure 10 illustrates how a single 16-\r\nbit element has been broadcast from the lowest position of one SIMD data value into all positions in a different SIMD data value.\r\n2 Historically many of these fixed-purpose permutes can be found in pre-Intel AVX-512 instruction sets where they were called \r\nshuffles instead. Where they are also available in Intel AVX-512 instruction sets the term shuffle is still used to show how they \r\nrelate to their pre-Intel AVX-512 counterparts. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/8dfe6b13-395d-4678-b2a3-a2522edba5b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9228feef4461991832f6d8b9e6e3536e821f442e4b24d5ec8c540712341bf483",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 599
      },
      {
        "segments": [
          {
            "segment_id": "c159ff2d-3319-4162-b426-06f93d7516c6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 12,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n12\r\nq z y x\r\nx x x x x x x x x x x x x x x x\r\nFigure 10. Broadcast of a Single 16-bit Element into a 512-bit SIMD Value\r\nA broadcast from memory is very common and consequently it has been given dedicated hardware support for 32-bit elements \r\nand above to enable it to happen as part of a normal load operation and with the same throughput and latency as a normal load. It \r\nis therefore advantageous to try to optimize code to broadcast data from memory rather than a register wherever possible. 8- \r\nand 16-bit broadcasts cannot be broadcast directly from memory and require a separate broadcast-from-register instruction to \r\nbe used instead. Figure 11 shows how a group of 4x32-bit integer values stored in memory are loaded and broadcast into every \r\nsuch group within a larger 512-bit SIMD value (this is equivalent to a 128-bit lane broadcast). Note that it can sometimes be \r\ncheaper to use the load unit’s embedded hardware and perform multiple broadcast loads than it would be to perform a single \r\nload followed by several permutes of the loaded value. \r\n0 1 2 3\r\nptr\r\nMemory\r\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3\r\nFigure 11. Broadcast of Four 32-bit Elements from Memory into a 512-bit SIMD Value3\r\nA broadcast from a register is effectively a cross-lane permute and has the same performance as a general permute that would \r\ndo the same thing (i.e., 3 cycle latency and a throughput of 1). The advantage of the broadcast instruction is that it does not \r\nrequire an index register to be configured first. Only the lowest element or group of elements may be broadcast. Broadcasting a \r\ndifferent element of a register either requires a general purpose permute to be set up or it should be arranged that the value is in \r\nmemory instead. \r\nIn Intel AVX-512 broadcasts of 32-bit elements or wider may be embedded into the instruction itself using a special assembly \r\ninstruction syntax. These may be handled more efficiently than separate broadcast-and-use instructions since they require \r\nfewer registers and increase the possibility of instruction micro-fusion in the processor (see section 18.9 of the Intel® 64 and IA\u000232 Architectures Optimization Reference Manual). For compiled code, you can use separate broadcastintrinsics followed by an \r\ninstruction that uses the broadcast value, and the compiler handles the embedding of the broadcast into the instruction by itself. \r\n3.2.2 Duplicate Low/High\r\nThe duplicate instructions allow one element from each pair of elements in a SIMD value to be duplicated into both elements in \r\nthe output pair. This is shown in Figure 12, where on the left the lower element in each pair is duplicated, and on the right the \r\nupper element of each pair is duplicated.\r\n3 Convention has changed for this diagram since memory is often depicted as being numbered left to right, and SIMD values as right-to-left. In an \r\nattempt to make this diagram simpler, the SIMD order has been reversed to match memory convention.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/c159ff2d-3319-4162-b426-06f93d7516c6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=60c37430ca155d23559bbd1a916018d2b68b171beccbd9a40208e9c2f6e1fcff",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "c159ff2d-3319-4162-b426-06f93d7516c6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 12,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n12\r\nq z y x\r\nx x x x x x x x x x x x x x x x\r\nFigure 10. Broadcast of a Single 16-bit Element into a 512-bit SIMD Value\r\nA broadcast from memory is very common and consequently it has been given dedicated hardware support for 32-bit elements \r\nand above to enable it to happen as part of a normal load operation and with the same throughput and latency as a normal load. It \r\nis therefore advantageous to try to optimize code to broadcast data from memory rather than a register wherever possible. 8- \r\nand 16-bit broadcasts cannot be broadcast directly from memory and require a separate broadcast-from-register instruction to \r\nbe used instead. Figure 11 shows how a group of 4x32-bit integer values stored in memory are loaded and broadcast into every \r\nsuch group within a larger 512-bit SIMD value (this is equivalent to a 128-bit lane broadcast). Note that it can sometimes be \r\ncheaper to use the load unit’s embedded hardware and perform multiple broadcast loads than it would be to perform a single \r\nload followed by several permutes of the loaded value. \r\n0 1 2 3\r\nptr\r\nMemory\r\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3\r\nFigure 11. Broadcast of Four 32-bit Elements from Memory into a 512-bit SIMD Value3\r\nA broadcast from a register is effectively a cross-lane permute and has the same performance as a general permute that would \r\ndo the same thing (i.e., 3 cycle latency and a throughput of 1). The advantage of the broadcast instruction is that it does not \r\nrequire an index register to be configured first. Only the lowest element or group of elements may be broadcast. Broadcasting a \r\ndifferent element of a register either requires a general purpose permute to be set up or it should be arranged that the value is in \r\nmemory instead. \r\nIn Intel AVX-512 broadcasts of 32-bit elements or wider may be embedded into the instruction itself using a special assembly \r\ninstruction syntax. These may be handled more efficiently than separate broadcast-and-use instructions since they require \r\nfewer registers and increase the possibility of instruction micro-fusion in the processor (see section 18.9 of the Intel® 64 and IA\u000232 Architectures Optimization Reference Manual). For compiled code, you can use separate broadcastintrinsics followed by an \r\ninstruction that uses the broadcast value, and the compiler handles the embedding of the broadcast into the instruction by itself. \r\n3.2.2 Duplicate Low/High\r\nThe duplicate instructions allow one element from each pair of elements in a SIMD value to be duplicated into both elements in \r\nthe output pair. This is shown in Figure 12, where on the left the lower element in each pair is duplicated, and on the right the \r\nupper element of each pair is duplicated.\r\n3 Convention has changed for this diagram since memory is often depicted as being numbered left to right, and SIMD values as right-to-left. In an \r\nattempt to make this diagram simpler, the SIMD order has been reversed to match memory convention.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/c159ff2d-3319-4162-b426-06f93d7516c6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=60c37430ca155d23559bbd1a916018d2b68b171beccbd9a40208e9c2f6e1fcff",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "b5059fde-73db-45a2-a796-6f1096505f13",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 13,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n13\r\n7 6 5 4 3 2 1 0 7 6 5 4 3 2 1 0\r\n6 6 4 4 2 2 0 0 7 7 5 5 3 3 1 1\r\nDuplicate Low Duplicate High\r\nFigure 12. How Duplicate Low/High Instructions Work\r\nThere are only three general forms of duplicate: \r\n_mXXX_moveldup_ps Duplicate the lower 32-bit values corresponding to a floating-point element\r\n_mXXX_movehdup_ps Duplicate the higher 32-bit values corresponding to a floating-point element\r\n_mXXX_movedup_pd Duplicate the even elements corresponding to a 64-bit floating-point value\r\nNote that there is no 64-bit duplicate high instruction. Other 32-bit and 64-bit data types can be duplicated by casting to \r\nFP32/FP64 element types (zero cost), performing the duplication, and casting back again. There are no duplicates for 8- or 16-\r\nbit elements.\r\nBecause duplication is an in-lane operation it has latency of 1 cycle and a throughput of 1.\r\n3.2.3 Insert/Extract \r\nInsert and extract operations are conceptually very simple; one or a few contiguous elements from a position in one register are \r\nmoved to a different position in another register. However, these basic operations are implemented using several different \r\nbuilding blocks that achieve smaller pieces of this behavior, and often the compiler is used to synthesize the necessary code \r\nsequence. In this section we describe these building blocks, how they can be combined, and why there are often faster \r\nalternatives to the obvious implementation.\r\n3.2.3.1 Scalar to/from Lane\r\nThe simplest type of insert and extract operations work on individual elements. For example, a 16-bit data element might be \r\ninserted into a SIMD value at a given position, or a 16-bit value could be extracted from a given position in a SIMD value. In such \r\ncases, the 16-bit element would be represented in a scalar register, and the SIMD value in these cases would be limited to a single \r\nlane of data. This is illustrated in Figure 13, where a scalar register is inserted into a SIMD 128-bit lane on the left, and a scalar \r\nregister extracted from a 128-bit SIMD lane on the right.\r\nScalar register\r\n128-bit simd\r\nInsert (e.g., pinsrw)\r\nScalar register\r\n128-bit simd\r\nExtract (e.g., pextrw)\r\nFigure 13. Behavior of Scalar/SIMD Value Insert and Extract Operations\r\nNote that there is no instruction that allows a scalar register to be inserted or extracted using a 256-bit or 512-bit SIMD register. \r\nSuch operations must be synthesized by you or the compiler, as described in the following sections.\r\n3.2.3.2 Lane to/from register\r\nThe next step up in the toolkit of insert and extract operations is to allow a 128-bit lane of data to be inserted or extracted from \r\nanother register, as shown in Figure 14. The instructions, which do go by many different names, are all essentially doing the same \r\nthing. For example, _mm256_insertf128_ps, _mm256_insertf32x4, _mm256_insertf64x2, and so on. The reason for having \r\nthese variants is to allow the mask behavior to be changed. By specifying the granularity within each lane for the insert or extract \r\noperation, individual elements can be zeroed or copied using a bit mask.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/b5059fde-73db-45a2-a796-6f1096505f13.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=08e3eea4a1d2b3973783b47c3c718b8b246e076c630103d6faf6ded60da1432f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 521
      },
      {
        "segments": [
          {
            "segment_id": "b5059fde-73db-45a2-a796-6f1096505f13",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 13,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n13\r\n7 6 5 4 3 2 1 0 7 6 5 4 3 2 1 0\r\n6 6 4 4 2 2 0 0 7 7 5 5 3 3 1 1\r\nDuplicate Low Duplicate High\r\nFigure 12. How Duplicate Low/High Instructions Work\r\nThere are only three general forms of duplicate: \r\n_mXXX_moveldup_ps Duplicate the lower 32-bit values corresponding to a floating-point element\r\n_mXXX_movehdup_ps Duplicate the higher 32-bit values corresponding to a floating-point element\r\n_mXXX_movedup_pd Duplicate the even elements corresponding to a 64-bit floating-point value\r\nNote that there is no 64-bit duplicate high instruction. Other 32-bit and 64-bit data types can be duplicated by casting to \r\nFP32/FP64 element types (zero cost), performing the duplication, and casting back again. There are no duplicates for 8- or 16-\r\nbit elements.\r\nBecause duplication is an in-lane operation it has latency of 1 cycle and a throughput of 1.\r\n3.2.3 Insert/Extract \r\nInsert and extract operations are conceptually very simple; one or a few contiguous elements from a position in one register are \r\nmoved to a different position in another register. However, these basic operations are implemented using several different \r\nbuilding blocks that achieve smaller pieces of this behavior, and often the compiler is used to synthesize the necessary code \r\nsequence. In this section we describe these building blocks, how they can be combined, and why there are often faster \r\nalternatives to the obvious implementation.\r\n3.2.3.1 Scalar to/from Lane\r\nThe simplest type of insert and extract operations work on individual elements. For example, a 16-bit data element might be \r\ninserted into a SIMD value at a given position, or a 16-bit value could be extracted from a given position in a SIMD value. In such \r\ncases, the 16-bit element would be represented in a scalar register, and the SIMD value in these cases would be limited to a single \r\nlane of data. This is illustrated in Figure 13, where a scalar register is inserted into a SIMD 128-bit lane on the left, and a scalar \r\nregister extracted from a 128-bit SIMD lane on the right.\r\nScalar register\r\n128-bit simd\r\nInsert (e.g., pinsrw)\r\nScalar register\r\n128-bit simd\r\nExtract (e.g., pextrw)\r\nFigure 13. Behavior of Scalar/SIMD Value Insert and Extract Operations\r\nNote that there is no instruction that allows a scalar register to be inserted or extracted using a 256-bit or 512-bit SIMD register. \r\nSuch operations must be synthesized by you or the compiler, as described in the following sections.\r\n3.2.3.2 Lane to/from register\r\nThe next step up in the toolkit of insert and extract operations is to allow a 128-bit lane of data to be inserted or extracted from \r\nanother register, as shown in Figure 14. The instructions, which do go by many different names, are all essentially doing the same \r\nthing. For example, _mm256_insertf128_ps, _mm256_insertf32x4, _mm256_insertf64x2, and so on. The reason for having \r\nthese variants is to allow the mask behavior to be changed. By specifying the granularity within each lane for the insert or extract \r\noperation, individual elements can be zeroed or copied using a bit mask.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/b5059fde-73db-45a2-a796-6f1096505f13.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=08e3eea4a1d2b3973783b47c3c718b8b246e076c630103d6faf6ded60da1432f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 521
      },
      {
        "segments": [
          {
            "segment_id": "bd1e1026-8608-4bb5-853c-c4f78f01d64a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 14,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n14\r\nBit 256 Bit 128 Bit 0\r\nBit 256 Bit 128 Bit 0\r\n_mm256_insertf32x4\r\nLane 0\r\n128-bit\r\n0 1 2 3\r\n0 1 2 3 2 0 4 7\r\n0 1 2 3 2 0 4 7\r\nLane 0\r\n128-bit\r\n0 1 2 3\r\n_mm256_extract_i32x4_epi32\r\nFigure 14. Behavior of Inter-Lane Insert and Extract Operations \r\nThe ability to pass a bit mask to the insert operations makes for an interesting possibility; a permute from 3 different sources. For \r\nexample, _mm512_mask_insertf32x4 allows a single 128-bit lane to be inserted into another register, and then mask combined \r\nwith a third register.\r\nIntel AVX-512 also allows a double-lane of 256-bits to be inserted and extracted using, for example, _mm512_insert_f32x8_ps. \r\n3.2.3.3 Compiler Sequences for Scalar to/from Register\r\nAs noted above, there are no instructions for inserting scalar values into SIMD registers wider than a single lane or extracting a \r\nscalar value from a SIMD register wider than a single lane. There are C/C++ intrinsics to do this job but they are synthesized by \r\nthe compiler.\r\nFor example, consider the _mm256_insert_epi16 intrinsic. This can insert a scalar register anywhere within a parent 256-bit \r\nregister, but contemporary compilers (e.g., gcc 11) generate code that behaves as shown in Figure 15, which illustrates the \r\ninsertion of a value into position 13. The code sequence is a read-modify-write, since elements other than 13 must not be \r\nchanged; the top lane is extracted first, then the desired element of that lane overwritten by the new scalar register, leaving the \r\noriginal elements in that lane unaltered, and then the entire lane written back to the desired position.\r\nBit 256 Bit 128 Bit 0\r\nf e d c b a 9 8 7 6 5 4 3 2 1 0\r\nf e d c b a 9 8\r\n_mm_insert_epi16 f e x c b a 9 8\r\nx\r\n_mm256_extracti128_si256\r\n_mm256_inserti128_si256 f e x c b a 9 8 7 6 5 4 3 2 1 0\r\nFigure 15. Behavior of Inter-Lane Insert and Extract Operations\r\nThere is more than one way to achieve this particular synthesis of the insert (or extract) intrinsics and at time-of-writing LLVM \r\nand Intel® oneAPI compilers synthesized this in a slightly different way than GCC.\r\nWhile the compiler can implement what the programmer desires, as we have just seen, this can result in a synthesized instruction \r\nthat may be less performant than expected. In such cases it may be worth considering the contents of the next section in which \r\nwe look at a few ways to avoid insert and extract instructions entirely.\r\n3.2.3.4 Alternative Implementation of Insert and Extract\r\nInsertion and extraction can often be somewhat expensive, so in this section we shall look at ways of reducing that expense.\r\nThe first and most obvious way to reduce the expense is notto do the operation at all! It has been noted already that the different \r\nsizes of register are overlaid on top of each other; the xmm0 register is the lowest 128-bits of the ymm0 register, which in turn is \r\nthe lowest 256-bits of the zmm0 register. Thus, rather than inserting and extracting to lane 0 (e.g., _mm512_insertf32x4(x, \r\n0)) the SIMD value can simply be cast to a register of a different size instead (e.g., _mm512_castps256_ps512(x)). The \r\ncompiler does that with no cost whatsoever.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/bd1e1026-8608-4bb5-853c-c4f78f01d64a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=325e19f7ea955c8e256bd2b24a1593e1e15461dcfc2544f6d6861cc3598acd70",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 564
      },
      {
        "segments": [
          {
            "segment_id": "bd1e1026-8608-4bb5-853c-c4f78f01d64a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 14,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n14\r\nBit 256 Bit 128 Bit 0\r\nBit 256 Bit 128 Bit 0\r\n_mm256_insertf32x4\r\nLane 0\r\n128-bit\r\n0 1 2 3\r\n0 1 2 3 2 0 4 7\r\n0 1 2 3 2 0 4 7\r\nLane 0\r\n128-bit\r\n0 1 2 3\r\n_mm256_extract_i32x4_epi32\r\nFigure 14. Behavior of Inter-Lane Insert and Extract Operations \r\nThe ability to pass a bit mask to the insert operations makes for an interesting possibility; a permute from 3 different sources. For \r\nexample, _mm512_mask_insertf32x4 allows a single 128-bit lane to be inserted into another register, and then mask combined \r\nwith a third register.\r\nIntel AVX-512 also allows a double-lane of 256-bits to be inserted and extracted using, for example, _mm512_insert_f32x8_ps. \r\n3.2.3.3 Compiler Sequences for Scalar to/from Register\r\nAs noted above, there are no instructions for inserting scalar values into SIMD registers wider than a single lane or extracting a \r\nscalar value from a SIMD register wider than a single lane. There are C/C++ intrinsics to do this job but they are synthesized by \r\nthe compiler.\r\nFor example, consider the _mm256_insert_epi16 intrinsic. This can insert a scalar register anywhere within a parent 256-bit \r\nregister, but contemporary compilers (e.g., gcc 11) generate code that behaves as shown in Figure 15, which illustrates the \r\ninsertion of a value into position 13. The code sequence is a read-modify-write, since elements other than 13 must not be \r\nchanged; the top lane is extracted first, then the desired element of that lane overwritten by the new scalar register, leaving the \r\noriginal elements in that lane unaltered, and then the entire lane written back to the desired position.\r\nBit 256 Bit 128 Bit 0\r\nf e d c b a 9 8 7 6 5 4 3 2 1 0\r\nf e d c b a 9 8\r\n_mm_insert_epi16 f e x c b a 9 8\r\nx\r\n_mm256_extracti128_si256\r\n_mm256_inserti128_si256 f e x c b a 9 8 7 6 5 4 3 2 1 0\r\nFigure 15. Behavior of Inter-Lane Insert and Extract Operations\r\nThere is more than one way to achieve this particular synthesis of the insert (or extract) intrinsics and at time-of-writing LLVM \r\nand Intel® oneAPI compilers synthesized this in a slightly different way than GCC.\r\nWhile the compiler can implement what the programmer desires, as we have just seen, this can result in a synthesized instruction \r\nthat may be less performant than expected. In such cases it may be worth considering the contents of the next section in which \r\nwe look at a few ways to avoid insert and extract instructions entirely.\r\n3.2.3.4 Alternative Implementation of Insert and Extract\r\nInsertion and extraction can often be somewhat expensive, so in this section we shall look at ways of reducing that expense.\r\nThe first and most obvious way to reduce the expense is notto do the operation at all! It has been noted already that the different \r\nsizes of register are overlaid on top of each other; the xmm0 register is the lowest 128-bits of the ymm0 register, which in turn is \r\nthe lowest 256-bits of the zmm0 register. Thus, rather than inserting and extracting to lane 0 (e.g., _mm512_insertf32x4(x, \r\n0)) the SIMD value can simply be cast to a register of a different size instead (e.g., _mm512_castps256_ps512(x)). The \r\ncompiler does that with no cost whatsoever.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/bd1e1026-8608-4bb5-853c-c4f78f01d64a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=325e19f7ea955c8e256bd2b24a1593e1e15461dcfc2544f6d6861cc3598acd70",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 564
      },
      {
        "segments": [
          {
            "segment_id": "dbf6eb4e-55b5-4e91-b8b9-6f95aee1b782",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 15,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n15\r\nThe second way of reducing expense is to try to avoid using the scalar registers. Although the scalar registers are a natural way \r\nto store individual values, moving data to and from those registers from SIMD values (particularly multi-lane values) can be \r\nexpensive. For example, consider the following code fragment that extracts a value from one SIMD register, and inserts it back \r\ninto another:\r\n__m256i moveElement(__m256i x, __m256i y) \r\n{ \r\n auto e = _mm256_extract_epi16(x, 6);\r\n return _mm256_insert_epi16(y, e, 2);\r\n} \r\nIn this scenario, it would be cheaper to perform a permutation directly, where a two-source permute is used to copy all the values \r\nfrom y to the same position except for element 2 thatis copied from the other source. This becomes a direct SIMD-to-SIMD\r\ninstruction thatis much more efficient. Unfortunately, the contemporary compilers at time of writing did not spot that \r\nopportunity in the code above, so the programmer needs to be aware of this optimization. Note that even when the scalar value\r\nbeing inserted or extracted is manipulated in some way, it may still be cheaper to do that by pretending that it is part of a simd \r\nvalue and operating on the complete SIMD than converting to a scalar, operating on it as a scalar, and then inserting it back again.\r\nThe final technique is to use a broadcast instruction instead of an insert instruction. A broadcast instruction, such as \r\nvpbroadcastd, can operate from a scalar register, and broadcast the scalar value to all elements of a SIMD register. Even better, \r\nit can be given a mask register to decide exactly which elements receive the broadcast value. If a mask with only a single set bit is \r\npresented, it inserts the element value into exactly one place. A single instruction can insert into any element position of an Intel \r\nAVX or Intel AVX-512 register without being converted into a synthesized multi-instruction sequence.\r\n3.3 Alignment Operations\r\nAlignment instructions all perform the generic operation illustrated in Figure 16, where two source objects are concatenated, and \r\npart of the resulting object extracted. They allow the upper part of one data object to be contiguously combined with the lower \r\npart of another data object. \r\nBit N Bit 0\r\nBit 2*N Bit N Bit 0\r\nSource B Source A\r\nSource B Source A\r\nBit N Bit 0 Bit N Bit 0\r\noffset\r\nFigure 16. How to Perform Alignment of Two Data Objects \r\nAll Intel AVX-512 alignment instructions follow this basic form but can differ in the granularity at which the alignment is \r\nperformed: register, lane, or element. Most alignment instructions are immediate-valued and can only extract the subset of \r\nvalues (i.e., the red box in Figure 16) from a fixed compile-time position, although a few of the more recent instructions \r\nintroduced in 3rd Gen Intel Xeon Scalable processors are capable of dynamic (runtime) selection. A selection of the available \r\nalignment instructions is described in the following sections. In many of the cases the instruction used is called `alignr’. Here, the \r\n`r’ suffix indicates that the combined value is being shifted to the right by a given offset to move data into the lowest part of the \r\noutput register.\r\nNote that a general-purpose permute instruction could replace most of the alignment instructions listed here, but these \r\ninstructions are useful because they often have smaller encodings, can operate without having to load an indexing register first, \r\nand reduce register pressure since they do not require an index register.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/dbf6eb4e-55b5-4e91-b8b9-6f95aee1b782.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1ea9bd1a7c1a7024e3ef2ce5ea287bd27b280642d12906cdbc3c3fc13067e483",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 592
      },
      {
        "segments": [
          {
            "segment_id": "dbf6eb4e-55b5-4e91-b8b9-6f95aee1b782",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 15,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n15\r\nThe second way of reducing expense is to try to avoid using the scalar registers. Although the scalar registers are a natural way \r\nto store individual values, moving data to and from those registers from SIMD values (particularly multi-lane values) can be \r\nexpensive. For example, consider the following code fragment that extracts a value from one SIMD register, and inserts it back \r\ninto another:\r\n__m256i moveElement(__m256i x, __m256i y) \r\n{ \r\n auto e = _mm256_extract_epi16(x, 6);\r\n return _mm256_insert_epi16(y, e, 2);\r\n} \r\nIn this scenario, it would be cheaper to perform a permutation directly, where a two-source permute is used to copy all the values \r\nfrom y to the same position except for element 2 thatis copied from the other source. This becomes a direct SIMD-to-SIMD\r\ninstruction thatis much more efficient. Unfortunately, the contemporary compilers at time of writing did not spot that \r\nopportunity in the code above, so the programmer needs to be aware of this optimization. Note that even when the scalar value\r\nbeing inserted or extracted is manipulated in some way, it may still be cheaper to do that by pretending that it is part of a simd \r\nvalue and operating on the complete SIMD than converting to a scalar, operating on it as a scalar, and then inserting it back again.\r\nThe final technique is to use a broadcast instruction instead of an insert instruction. A broadcast instruction, such as \r\nvpbroadcastd, can operate from a scalar register, and broadcast the scalar value to all elements of a SIMD register. Even better, \r\nit can be given a mask register to decide exactly which elements receive the broadcast value. If a mask with only a single set bit is \r\npresented, it inserts the element value into exactly one place. A single instruction can insert into any element position of an Intel \r\nAVX or Intel AVX-512 register without being converted into a synthesized multi-instruction sequence.\r\n3.3 Alignment Operations\r\nAlignment instructions all perform the generic operation illustrated in Figure 16, where two source objects are concatenated, and \r\npart of the resulting object extracted. They allow the upper part of one data object to be contiguously combined with the lower \r\npart of another data object. \r\nBit N Bit 0\r\nBit 2*N Bit N Bit 0\r\nSource B Source A\r\nSource B Source A\r\nBit N Bit 0 Bit N Bit 0\r\noffset\r\nFigure 16. How to Perform Alignment of Two Data Objects \r\nAll Intel AVX-512 alignment instructions follow this basic form but can differ in the granularity at which the alignment is \r\nperformed: register, lane, or element. Most alignment instructions are immediate-valued and can only extract the subset of \r\nvalues (i.e., the red box in Figure 16) from a fixed compile-time position, although a few of the more recent instructions \r\nintroduced in 3rd Gen Intel Xeon Scalable processors are capable of dynamic (runtime) selection. A selection of the available \r\nalignment instructions is described in the following sections. In many of the cases the instruction used is called `alignr’. Here, the \r\n`r’ suffix indicates that the combined value is being shifted to the right by a given offset to move data into the lowest part of the \r\noutput register.\r\nNote that a general-purpose permute instruction could replace most of the alignment instructions listed here, but these \r\ninstructions are useful because they often have smaller encodings, can operate without having to load an indexing register first, \r\nand reduce register pressure since they do not require an index register.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/dbf6eb4e-55b5-4e91-b8b9-6f95aee1b782.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1ea9bd1a7c1a7024e3ef2ce5ea287bd27b280642d12906cdbc3c3fc13067e483",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 592
      },
      {
        "segments": [
          {
            "segment_id": "79b6ee4b-8da2-4eb9-85c0-387c1244a091",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 16,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n16\r\n3.3.1 Per-Register Alignment Instructions\r\nPer-register alignment instructions allow the original source values to be entire registers of any supported SIMD bit-width (i.e., \r\n128-bit, 256-bit, or 512-bit). They operate as per Figure 16, where the data sources are whole registers.\r\nPer-register alignment instructions may only shift by offsets that are known at compile-time, and only at the granularity of whole \r\n32-bit or 64-bit elements. This granularity restriction is imposed because an immediate value (the compile-time shift offset) has \r\na limited range and would not be large enough to express a shift at a smaller granularity. Note that only shift-right is supported, \r\nbut with suitable manipulation shift-left is possible too.\r\nThe per-register instructions have the same performance as a cross-lane permute instruction of the same size (e.g., 3 cycle \r\nlatency and throughput of 1 for _mm512_alignr_epi32). \r\nExamples of per-register alignment instructions include _mm512_alignr_epi32, and _mm256_alignr_epi64. \r\n3.3.2 Per-Lane Alignment Instructions\r\nPer-lane alignment instructions can perform as many alignments as there are lanes within the source data objects. A per-lane \r\nalignment is illustrated in Figure 17, where, in this case, the original source objects are 256-bit Intel AVX registers, each \r\ncontaining two 128-bit lanes. Respective lanes from both sources are aligned with the same lane in the other object, and the \r\nresulting output written to the respective lane of the output data object.\r\nSource B Source A\r\nBit 128 Bit 0 Bit 128 Bit 0\r\nBit 128 Bit 0\r\nBit 256 Bit 256\r\nBit 256\r\nFigure 17. Per-lane Alignment\r\nPer-lane alignment instructions may only shift by offsets that are known at compile-time but they may do so at byte granularity, \r\nwhich means they effectively allow any element granularity shift to be handled. Per-lane alignment may only shift right, but with \r\nsuitable manipulation of the shift, left shift is also possible. All lanes are constrained to shift by the same offset.\r\nThe per-lane instructions have the same performance as an in-lane permute instruction of the same size (e.g., 1 cycle latency and \r\nthroughput of 1). \r\nExamples of per-lane alignment instructions include _mm512_alignr_epi8 and _mm256_alignr_epi8. \r\n3.3.3 Per-element Alignment Instructions (VBMI2)\r\nThe VBMI2 family of Intel AVX-512 instructions introduced in 3rd Gen Intel Xeon Scalable processors allows for finer grained \r\nalignment instructions than the previous alignment instructions described above, adding 16-, 32-, and 64-bit alignment \r\ngranularities. An example of per-element alignment is shown in Figure 18. In this case each source value contains four data \r\nelements (e.g., 64-bit elements in a 256-bit SIMD value). Each respective pair of elements taken from the two sources is \r\ncombined into a single contiguous value and part of that new value extracted and inserted into the respective element of the \r\nresult. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/79b6ee4b-8da2-4eb9-85c0-387c1244a091.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb0217876aafbc44b207150ba413379ececdf43804ba115d8481945e8ae5cf43",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 459
      },
      {
        "segments": [
          {
            "segment_id": "65054537-6de0-40e3-9ee2-f89c3eb3dfb5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 17,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n17\r\nSource B Source A\r\n256 128 0 256 128 0\r\n256 128 0\r\nFigure 18. Per-element Alignment\r\nPer-lane alignment instructions may shift by offsets that are either static (through an immediate) or dynamic (by using an \r\nadditional register to store the shift offset for each element). These per-element alignments may also shift both left and right\r\nand with a bit-level granularity. Only 16-, 32-, and 64-bit elements are supported.\r\nThe per-element instructions have the same performance as other shifts and rotations (i.e., 1 cycle latency and throughput of 1). \r\nExamples of per-lane alignment instructions include _mm512_shdi_epi16 (shift left in 16-bit elements by an immediate) and \r\n_mm256_shrdv_epi32 (shift right in 32-bit elements by a variable index).\r\n3.3.4 Element Rotation\r\nAll of the alignment instructions discussed above can be used to perform an element rotation by using the incoming source \r\nvalue for both operands of the instruction. An example is shown in Figure 19 where an incoming SIMD value containing four \r\nelements has the elements rotated from the original order [0, 1, 2, 3] to the new order [3, 0, 1, 2]. \r\n \r\n3 2 1 0\r\nSource\r\n3 2 1 0 3 2 1 0\r\n0 3 2 1\r\nFigure 19. Element Rotation within a Single SIMD Value Using the Alignment Instruction \r\n3.4 Bit-Wise Expansion and Compression\r\nThe expansion and compression instructions are special types of permute that use a bit mask to specify the indexes, rather than \r\nusing a numeric index. An illustration of these instructions is shown in Figure 20. On the left, the expansion instruction writes \r\nconsecutive elements from a source value into the output elements, which have an active (i.e., set) mask bit. Any inactive output \r\nelements are zeroed in this example, because the maskz variant is used, although a second source register may be used to supply \r\nthe values to write into those elements. The compress instruction operates in the opposite sense, taking active masked values\r\nfrom an input source register and writing them into contiguous positions in the output register. Compress is useful for \r\nimplementing operations such as C++ remove_if or copy_if functions from the standard algorithms (e.g., \r\nhttps://en.cppreference.com/w/cpp/algorithm/copy).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/65054537-6de0-40e3-9ee2-f89c3eb3dfb5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d54577b368d5f2f79387a96a922b8ee0ac8ccf0169c5118855d844db50285610",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 372
      },
      {
        "segments": [
          {
            "segment_id": "b13a3a2e-d214-4f28-a956-856c7f88e8a4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 18,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n18\r\n1 0 1 0 0 0 1 1\r\n0 0 0 0\r\nmaskz_expand\r\n1 0 1 0 0 0 1 1\r\n0 0 0\r\nmaskz_compress\r\n0\r\nSource Register\r\nMask selection\r\nOutput register\r\nh g f e d c b a\r\nd c b a\r\nh g f e d c b a\r\nh f b a\r\nFigure 20. How the Bit-Wise Expansion and Compression Instructions Work, using a Zeroing Bit Mask \r\nThe compression and expansion instructions were first added in Intel AVX-512, and then only for 32-bit and 64-bit granularity \r\nelements. Support for 8- and 16-bit granularity elements was added in the 3rd Gen Intel Xeon Scalable processors VBMI2 \r\ninstruction set. If a compress or expand operation must operate on 8- or 16-bit granularity on machines that do not support \r\nVBMI2, then you should convert the values from one format to another. For example, 16-bit values could be converted to 32-bit, \r\nthen the compress or expand operation performed, and the 32-bit result turned back into the final 16-bit output. Note that such a \r\nconversion means that only one half or one quarter as many elements can be processed. \r\nThe compress and expand instructions are more expensive compared to the other permutes described in this document. We do \r\nnot recommend using these instructions when other permutations can perform the same operation. For example, rather than \r\nusing the bit mask 100100100100 to expand or compress every third element it would be more effective to set up an indexed \r\npermute with the index [0, 3, 6, 9, …].\r\nCompression and expansion of values without Intel AVX-512 is more difficult, and code sequences to synthesis these operations \r\nmust be used instead. A future document will describe how to achieve those operations.\r\n3.5 Gather and Scatter\r\nThe gather and scatter instructions allow data to be permuted into or out of memory using a supplied SIMD index value. An \r\nexample of a gather instruction is shown in Figure 21. A SIMD of index values is supplied, along with a base pointer that addresses \r\na region of memory. Each index is then used to dereference the memory allocation at the index’ offset from the base pointer. The \r\nvalue at that memory location is then placed into the respective element in the output register. This continues for every index \r\nelement until all the result elements have been read. \r\nptr\r\nMemory x\r\nptr+a\r\nx\r\ns\r\nptr+c\r\nt\r\nptr+d\r\ny\r\nptr+b\r\nIndex SIMD a b c d\r\nResult SIMD x y s t\r\nFigure 21. How Memory Gather Instruction Operates\r\nThe scatter instruction works using the same general principle but writing the value of a source register element into the \r\nmemory at the given index from a base pointer.\r\nThe gather and scatter instructions are excellent for working with data that is stored in arbitrary dynamic memory locations. The \r\ninstructions handle all the necessary insert/extract operations of the indexes and the values that are passed back-and-forth to ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/b13a3a2e-d214-4f28-a956-856c7f88e8a4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c4ef5cdf21c55af12a2b95e885c3dccdb76b93dbe8682a987821509d9f88edc5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 506
      },
      {
        "segments": [
          {
            "segment_id": "b57e31df-9247-4dce-b4ac-9e92ab85ddf6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 19,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n19\r\nthe memory system. However, the performance of the gather and scatter instructions is comparable to using individual read and\r\nwrite operations. For example, a gather operation with 16 indexes requires 16 individual load instructions. The gather handles the \r\nindexing, which adds a little efficiency, but it still needs to perform each of those 16 load operations.\r\nWhen the data being read or written is stored in memory in a structured pattern it is often faster to use a software sequence \r\ninstead. For example, if data were to be read at a stride of 4, this could be done using a single gather instruction and an index \r\nsequence of [0, 4, 8, 12, …]. However, performing fewer loads of the data into registers and then using register permute \r\ninstructions to extract every fourth elementis faster. \r\n3.6 Reductions\r\nA common use for permutation instructions is to be able to perform a reduction operation. For example, given a SIMD value of \r\nmultiple elements, compute a single value representing the sum of all elements in that register. One way of achieving this \r\noperation is through a reduction tree, which can be generated automatically by the compiler. For example, the intrinsic named \r\n_mm512_reduce_add_ps is turned into a sequence of operations that repeatedly split a SIMD into two pieces, combine them, \r\nsplit them again, and so on. There are a number of such reduction intrinsics available from the compiler to cover minimum, \r\nmaximum, addition, and multiplication reductions.\r\nThe Intel AVX-512 instruction set family does not currently have any instructions that perform any type of full reduction across \r\nan entire SIMD data value. However, there are a few instructions that perform partial reductions, typically on pairs of adjacent \r\nvalues. Figure 22 shows an example where all the elements in one SIMD value are multiplied by their respective element in \r\nanother SIMD source value, and then adjacent pairs of data added together. The multiplication and the addition take place in a \r\ndata type that is twice as large as the original elements, thereby allowing high precision to be accumulated. An example of such \r\nan instruction is _mm512_madd_epi16, which takes 16-bit input elements and generates 32-bit accumulated output elements.\r\nh g f e d c b a\r\np o n m l k j i\r\nmul mul mul mul mul mul mul mul\r\ngo*hp em*fn ck+dl ai+bj\r\nSource A\r\nSource B\r\nResult\r\nFigure 22. Basic Behavior of a VNNI Dot-Product-Like Instruction (e.g., _mXXX_madd_epi16) \r\nIn addition to the older instructions that operate on this principle, many of the newer Vector Neural Network Instructions (VNNI) \r\navailable as part of Intel’s DLBoost technology also work in this way, although they also are able to accumulate their values as \r\nwell (i.e., act as a fused-multiply-add). Some of the new VNNI instructions can accumulate up to four horizontal values.\r\nPartial reduction operations can be used simplify bigger full-width reductions by replacing some of the reduction tree stages \r\nwith a partial reduction instruction.\r\n3.7 Fake Permutes \r\nThis document mostly considers real permutation instructions, but sometimes it is possible to achieve the same effect as a \r\npermutation instruction by using another instruction entirely. This may have one of two advantages: \r\nExecution Parallelism If the alternative instruction executes on a different port, then it can execute in parallel with other \r\npermute instructions. For example, Table 3 shows that there is only one 512-bit execution port capable\r\nof performing permutes, so using an instruction that has the same effect as such an instruction but \r\nruns on a different execution port increases parallelism and performance.\r\nEncoding No need to use a register to store an index to achieve a particular permutation\r\nThe following subsections describe a few common ways to use other instructions to achieve permutations.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/b57e31df-9247-4dce-b4ac-9e92ab85ddf6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1620bdc18e616e4620ecdf67704cac8fcba959a443870e8e123f385bbc9cfc47",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 636
      },
      {
        "segments": [
          {
            "segment_id": "b57e31df-9247-4dce-b4ac-9e92ab85ddf6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 19,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n19\r\nthe memory system. However, the performance of the gather and scatter instructions is comparable to using individual read and\r\nwrite operations. For example, a gather operation with 16 indexes requires 16 individual load instructions. The gather handles the \r\nindexing, which adds a little efficiency, but it still needs to perform each of those 16 load operations.\r\nWhen the data being read or written is stored in memory in a structured pattern it is often faster to use a software sequence \r\ninstead. For example, if data were to be read at a stride of 4, this could be done using a single gather instruction and an index \r\nsequence of [0, 4, 8, 12, …]. However, performing fewer loads of the data into registers and then using register permute \r\ninstructions to extract every fourth elementis faster. \r\n3.6 Reductions\r\nA common use for permutation instructions is to be able to perform a reduction operation. For example, given a SIMD value of \r\nmultiple elements, compute a single value representing the sum of all elements in that register. One way of achieving this \r\noperation is through a reduction tree, which can be generated automatically by the compiler. For example, the intrinsic named \r\n_mm512_reduce_add_ps is turned into a sequence of operations that repeatedly split a SIMD into two pieces, combine them, \r\nsplit them again, and so on. There are a number of such reduction intrinsics available from the compiler to cover minimum, \r\nmaximum, addition, and multiplication reductions.\r\nThe Intel AVX-512 instruction set family does not currently have any instructions that perform any type of full reduction across \r\nan entire SIMD data value. However, there are a few instructions that perform partial reductions, typically on pairs of adjacent \r\nvalues. Figure 22 shows an example where all the elements in one SIMD value are multiplied by their respective element in \r\nanother SIMD source value, and then adjacent pairs of data added together. The multiplication and the addition take place in a \r\ndata type that is twice as large as the original elements, thereby allowing high precision to be accumulated. An example of such \r\nan instruction is _mm512_madd_epi16, which takes 16-bit input elements and generates 32-bit accumulated output elements.\r\nh g f e d c b a\r\np o n m l k j i\r\nmul mul mul mul mul mul mul mul\r\ngo*hp em*fn ck+dl ai+bj\r\nSource A\r\nSource B\r\nResult\r\nFigure 22. Basic Behavior of a VNNI Dot-Product-Like Instruction (e.g., _mXXX_madd_epi16) \r\nIn addition to the older instructions that operate on this principle, many of the newer Vector Neural Network Instructions (VNNI) \r\navailable as part of Intel’s DLBoost technology also work in this way, although they also are able to accumulate their values as \r\nwell (i.e., act as a fused-multiply-add). Some of the new VNNI instructions can accumulate up to four horizontal values.\r\nPartial reduction operations can be used simplify bigger full-width reductions by replacing some of the reduction tree stages \r\nwith a partial reduction instruction.\r\n3.7 Fake Permutes \r\nThis document mostly considers real permutation instructions, but sometimes it is possible to achieve the same effect as a \r\npermutation instruction by using another instruction entirely. This may have one of two advantages: \r\nExecution Parallelism If the alternative instruction executes on a different port, then it can execute in parallel with other \r\npermute instructions. For example, Table 3 shows that there is only one 512-bit execution port capable\r\nof performing permutes, so using an instruction that has the same effect as such an instruction but \r\nruns on a different execution port increases parallelism and performance.\r\nEncoding No need to use a register to store an index to achieve a particular permutation\r\nThe following subsections describe a few common ways to use other instructions to achieve permutations.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/b57e31df-9247-4dce-b4ac-9e92ab85ddf6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1620bdc18e616e4620ecdf67704cac8fcba959a443870e8e123f385bbc9cfc47",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 636
      },
      {
        "segments": [
          {
            "segment_id": "527c7a58-34cb-4c38-bf87-062b4d3f4264",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 20,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n20\r\n3.7.1 Shifts and Rotates\r\nShifts and rotates allow data to be moved back and forth with a lane or element at the bit-level, rather than the byte or element \r\nlevel. This can be very useful when operating on data that has an irregular size. Shifts and rotates do not allow data to be easily \r\ncombined with other sources (with the exception of mask-like operations), so they are more useful in moving data within a \r\nregister rather than across registers, but this is still very useful. Shifts and rotates exist at all levels from entire 128-bit lanes (e.g., \r\n_mm256_bslli_epi128), and down through most element sizes (i.e., 64-, 32-, and 16-bit). Shifts and rotates are not directly \r\nsupported for 8-bit elements, but in 3rd Gen Intel Xeon Scalable processors onwards the GFNI ISA can be used to provide those \r\ntoo. For more information, see Galois Field New Instructions (GFNI) Technology Guide. \r\n3.7.2 Data Conversions\r\nInteger data conversions provide a way to expand and contract data on what is effectively a power-of-2 stride. In Figure 23 an \r\nillustration of a stride-by-2 permute (extract even elements) is performed. It works by treating the original set of 32-bit values as \r\nthough they were actually 64-bit, and then extracts the lower 32-bits of each 64-bit element.\r\n7 6 5 4\r\nSource (256-bit)\r\n3 2 1 0\r\n7 6 5 4 3 2 1 0\r\nOriginal 8 x 32-bit elements\r\nCast to 4 x 64-bit elements\r\n6 4 2 0 Truncate to 32-bit (cvtepi64_epi32)\r\n \r\nFigure 23. Use of Truncation-Conversion to Perform a Stride-By-2 Permutation\r\nNote that conversions to very small granularities (e.g., using cvtepi64_epi8 to perform a stride-by-8 conversion) may be \r\nimplemented in a series of smaller steps by the compiler or processor, so you should bear this in mind while optimizing SIMD \r\nreordering algorithms that use them. \r\n4 Worked Examples\r\nThis document has so far described many different ways to exploit different types of permutation and data-movement \r\ninstructions, but without concrete examples it may be difficult to fully understand the advantage of using some permutation \r\ninstructions over others. In this section we go through some case studies of common permutation algorithms, showing how \r\ndifferent combinations of instructions may be used to good effect, and highlighting some of the factors about permutation that \r\nshould be considered. We look at matrix transpose, horizontal reductions, and full-width SIMD bit shifts.\r\n4.1 Matrix Transpose\r\nMatrix transpose operations are useful in many different basic linear algebra subprograms (BLAS) and also for performing array\u0002to-struct and struct-to-array conversion for enabling high-performance SIMD algorithms to be implemented on multiple data \r\nsets. The transpose is also very similar to building blocks for operations such as reductions (examined in more detail in Section \r\n4.2) and an understanding of how they work is useful there too. An example of a simple 8x8 matrix transpose is shown in \r\nFigure 24. Note that conventional mathematical element ordering is used (left-to-right) rather than the reversed ordering used \r\nin the other diagrams in this document. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/527c7a58-34cb-4c38-bf87-062b4d3f4264.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c4ec5f141ad4826bb42280e4a70893065f0d89bf25843643cf0bfd605d2a8bae",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 513
      },
      {
        "segments": [
          {
            "segment_id": "527c7a58-34cb-4c38-bf87-062b4d3f4264",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 20,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n20\r\n3.7.1 Shifts and Rotates\r\nShifts and rotates allow data to be moved back and forth with a lane or element at the bit-level, rather than the byte or element \r\nlevel. This can be very useful when operating on data that has an irregular size. Shifts and rotates do not allow data to be easily \r\ncombined with other sources (with the exception of mask-like operations), so they are more useful in moving data within a \r\nregister rather than across registers, but this is still very useful. Shifts and rotates exist at all levels from entire 128-bit lanes (e.g., \r\n_mm256_bslli_epi128), and down through most element sizes (i.e., 64-, 32-, and 16-bit). Shifts and rotates are not directly \r\nsupported for 8-bit elements, but in 3rd Gen Intel Xeon Scalable processors onwards the GFNI ISA can be used to provide those \r\ntoo. For more information, see Galois Field New Instructions (GFNI) Technology Guide. \r\n3.7.2 Data Conversions\r\nInteger data conversions provide a way to expand and contract data on what is effectively a power-of-2 stride. In Figure 23 an \r\nillustration of a stride-by-2 permute (extract even elements) is performed. It works by treating the original set of 32-bit values as \r\nthough they were actually 64-bit, and then extracts the lower 32-bits of each 64-bit element.\r\n7 6 5 4\r\nSource (256-bit)\r\n3 2 1 0\r\n7 6 5 4 3 2 1 0\r\nOriginal 8 x 32-bit elements\r\nCast to 4 x 64-bit elements\r\n6 4 2 0 Truncate to 32-bit (cvtepi64_epi32)\r\n \r\nFigure 23. Use of Truncation-Conversion to Perform a Stride-By-2 Permutation\r\nNote that conversions to very small granularities (e.g., using cvtepi64_epi8 to perform a stride-by-8 conversion) may be \r\nimplemented in a series of smaller steps by the compiler or processor, so you should bear this in mind while optimizing SIMD \r\nreordering algorithms that use them. \r\n4 Worked Examples\r\nThis document has so far described many different ways to exploit different types of permutation and data-movement \r\ninstructions, but without concrete examples it may be difficult to fully understand the advantage of using some permutation \r\ninstructions over others. In this section we go through some case studies of common permutation algorithms, showing how \r\ndifferent combinations of instructions may be used to good effect, and highlighting some of the factors about permutation that \r\nshould be considered. We look at matrix transpose, horizontal reductions, and full-width SIMD bit shifts.\r\n4.1 Matrix Transpose\r\nMatrix transpose operations are useful in many different basic linear algebra subprograms (BLAS) and also for performing array\u0002to-struct and struct-to-array conversion for enabling high-performance SIMD algorithms to be implemented on multiple data \r\nsets. The transpose is also very similar to building blocks for operations such as reductions (examined in more detail in Section \r\n4.2) and an understanding of how they work is useful there too. An example of a simple 8x8 matrix transpose is shown in \r\nFigure 24. Note that conventional mathematical element ordering is used (left-to-right) rather than the reversed ordering used \r\nin the other diagrams in this document. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/527c7a58-34cb-4c38-bf87-062b4d3f4264.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c4ec5f141ad4826bb42280e4a70893065f0d89bf25843643cf0bfd605d2a8bae",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 513
      },
      {
        "segments": [
          {
            "segment_id": "93cf01c0-f9ee-4eee-b69c-fa61836734d3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 21,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n21\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n512-bits per row, as 8 elements of 64-bit\r\n8 rows\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n512-bits per row, as 8 elements of 64-bit\r\n8 rows\r\nFigure 24. Effect of an 8x8 Matrix Transpose\r\nThere are many different ways to perform transpose. The exact method used can vary depending upon different factors, such as \r\nmatrix size and shape, the granularity of transposed elements, and whether the matrix is in memory or registers. In this section \r\nwe illustrate one way to transpose an 8x8 matrix of 64-bit elements (e.g., 64-bit double, 32-bit complex<float>) from memory \r\nand back into memory. We shall use a hierarchical decomposition method for performing the transpose, which is efficient, works \r\nwell across a range of Intel processor generations, is easy to comprehend, and is easy to adapt for different use cases. It is \r\nexpected that for specific use cases the code could be optimized further to exploit the properties of those specific scenarios.\r\nA matrix transpose can be implemented as a series of sub-transposes, and this is illustrated in Figure 25. On the left-hand-side, \r\nthe original matrix is laid out, with colors showing each different row. The data is then transposed in a series of stages, where \r\neach stage performs a transpose at a different granularity. For example, the top row shows how the matrix is originally treated as \r\na 2x2 matrix, and elements (0,1) and (1,0) are transposed with each other, leaving the diagonal elements where they are. Each \r\nquadrant of that transpose then is treated as smaller matrices that are themselves transposed again. This continues until in \r\nStage 3 the final transpose at the smallest element granularity results in the total transposing being completed. The bottom row \r\nof that same diagram shows that the reverse sequencing of transposes could be performed with the same effect. In this case the \r\nsmallest transposes are done in Stage 1, leading through to the full transposes in Stage 3. The overall effect is the same.\r\nThe choice of whether to use a big-to-small or small-to-big series of transposes can affect the overall performance of the \r\nfunction. The small transposes (i.e., Stage 3 of the upper flow or Stage 1 of the lower flow) only move the data within a lane and \r\ntend to be cheaper than the multi-element inter-lane transposes at the opposite end of their respective flows. For a complete \r\ntranspose, the total number of instructions needed to do the cheap or expensive stages remains the same, so the order does not \r\nmatter for this example. However, it can sometimes be desirable to choose one order over another if that exposes useful \r\ninstructions. For example, it may be cheaper to do a small-to-big flow orderif that means that the VNNI unit could be used to \r\nperform an initial partial reduction, or the load unit’s element-duplication feature can be exploited to shift the data by one \r\nelement position when reading from memory. The choice of which flow order to use for different scenarios is left as an exercise \r\nfor the reader according to the precise size, type, and context of transpose required.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/93cf01c0-f9ee-4eee-b69c-fa61836734d3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f254f96a00b5cd2be21009cc0a65ee4c72e8b9e0a753161001843089d97f33ff",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 654
      },
      {
        "segments": [
          {
            "segment_id": "93cf01c0-f9ee-4eee-b69c-fa61836734d3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 21,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n21\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n512-bits per row, as 8 elements of 64-bit\r\n8 rows\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n512-bits per row, as 8 elements of 64-bit\r\n8 rows\r\nFigure 24. Effect of an 8x8 Matrix Transpose\r\nThere are many different ways to perform transpose. The exact method used can vary depending upon different factors, such as \r\nmatrix size and shape, the granularity of transposed elements, and whether the matrix is in memory or registers. In this section \r\nwe illustrate one way to transpose an 8x8 matrix of 64-bit elements (e.g., 64-bit double, 32-bit complex<float>) from memory \r\nand back into memory. We shall use a hierarchical decomposition method for performing the transpose, which is efficient, works \r\nwell across a range of Intel processor generations, is easy to comprehend, and is easy to adapt for different use cases. It is \r\nexpected that for specific use cases the code could be optimized further to exploit the properties of those specific scenarios.\r\nA matrix transpose can be implemented as a series of sub-transposes, and this is illustrated in Figure 25. On the left-hand-side, \r\nthe original matrix is laid out, with colors showing each different row. The data is then transposed in a series of stages, where \r\neach stage performs a transpose at a different granularity. For example, the top row shows how the matrix is originally treated as \r\na 2x2 matrix, and elements (0,1) and (1,0) are transposed with each other, leaving the diagonal elements where they are. Each \r\nquadrant of that transpose then is treated as smaller matrices that are themselves transposed again. This continues until in \r\nStage 3 the final transpose at the smallest element granularity results in the total transposing being completed. The bottom row \r\nof that same diagram shows that the reverse sequencing of transposes could be performed with the same effect. In this case the \r\nsmallest transposes are done in Stage 1, leading through to the full transposes in Stage 3. The overall effect is the same.\r\nThe choice of whether to use a big-to-small or small-to-big series of transposes can affect the overall performance of the \r\nfunction. The small transposes (i.e., Stage 3 of the upper flow or Stage 1 of the lower flow) only move the data within a lane and \r\ntend to be cheaper than the multi-element inter-lane transposes at the opposite end of their respective flows. For a complete \r\ntranspose, the total number of instructions needed to do the cheap or expensive stages remains the same, so the order does not \r\nmatter for this example. However, it can sometimes be desirable to choose one order over another if that exposes useful \r\ninstructions. For example, it may be cheaper to do a small-to-big flow orderif that means that the VNNI unit could be used to \r\nperform an initial partial reduction, or the load unit’s element-duplication feature can be exploited to shift the data by one \r\nelement position when reading from memory. The choice of which flow order to use for different scenarios is left as an exercise \r\nfor the reader according to the precise size, type, and context of transpose required.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/93cf01c0-f9ee-4eee-b69c-fa61836734d3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f254f96a00b5cd2be21009cc0a65ee4c72e8b9e0a753161001843089d97f33ff",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 654
      },
      {
        "segments": [
          {
            "segment_id": "ca2ba358-cee5-47a4-8715-717b6c7d724e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 22,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n22\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n0 1 2 3\r\n4 5 6 7\r\n8 9 10 11\r\n12 13 14 15\r\n16 17 18 19\r\n20 21 22 23\r\n24 25 26 27\r\n28 29 30 31\r\n36 37 38 39\r\n44 45 46 47\r\n52 53 54 55\r\n60 61 62 63\r\n32 33 34 35\r\n40 41 42 43\r\n48 49 50 51\r\n56 57 58 59\r\n0 1\r\n2 3\r\n4 5 6 7\r\n8 9\r\n10 11\r\n12 13 14 15\r\n16 17\r\n18 19\r\n20 21 22 23\r\n24 25\r\n26 27\r\n28 29 30 31\r\n36 37\r\n38 39\r\n44 45\r\n46 47\r\n52 53\r\n54 55\r\n60 61\r\n62 63\r\n32 33\r\n34 35\r\n40 41\r\n42 43\r\n48 49\r\n50 51\r\n56 57\r\n58 59\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\nBig to little transpose\r\nLittle-to-big transpose\r\nFigure 25. How to Perform a Transpose as a Series of Sub-Transposes of Different Sizes \r\nThe idea of using a series of smaller transposes to transpose bigger matrices works for matrices of any size, shape, element type, \r\nand storage medium (register of memory). \r\nIt is possible to use the technique illustrated in this section to build transposes for matrix objects that are bigger than would fit \r\ninto registers too. Such an example is beyond the scope of this document.\r\n4.2 Multi-data-set Reductions\r\nConsider the data structure illustrated in Figure 26. This shows an array of values in memory, where the elements are divided into \r\nsmall groups of 8 contiguous elements (we assume that each element is 64-bits) and each group must be reduced into a single \r\nscalar value by summing all the values in the group. Note that the reduction could equally well be another type of operation (e.g., \r\nminimum). \r\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\r\ns0 s1 s2 s3\r\nSum()\r\nSum()\r\nSum()\r\nSum()\r\nFigure 26. Example Data Structure Showing a Series of Values That Must Be Reduced\r\nLeaving aside the most trivial implementation - reading each element one-by-one and adding it to the appropriate output \r\naccumulation element - there are three increasingly sophisticated ways to implement this code.\r\n4.2.1 Iterative Reduction Intrinsic\r\nMost modern C/C++ compilers implement reduction intrinsics and these allow the code to be written as shown in Figure 27. A \r\nsimple loop reads each group into a single Intel AVX-512 register (8 x 64-bit elements) and then directly calls the appropriate \r\nreduction intrinsic (https://godbolt.org/z/soEnGqhvz). The code this generates is illustrated in the same diagram.\r\n \r\n for (int i=0; i<n; ++i)\r\n { ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/ca2ba358-cee5-47a4-8715-717b6c7d724e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c6bdf77874b866aaa495170251c9d131c4b22502798298564fb613511ae8e56d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 773
      },
      {
        "segments": [
          {
            "segment_id": "ca2ba358-cee5-47a4-8715-717b6c7d724e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 22,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n22\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n0 1 2 3\r\n4 5 6 7\r\n8 9 10 11\r\n12 13 14 15\r\n16 17 18 19\r\n20 21 22 23\r\n24 25 26 27\r\n28 29 30 31\r\n36 37 38 39\r\n44 45 46 47\r\n52 53 54 55\r\n60 61 62 63\r\n32 33 34 35\r\n40 41 42 43\r\n48 49 50 51\r\n56 57 58 59\r\n0 1\r\n2 3\r\n4 5 6 7\r\n8 9\r\n10 11\r\n12 13 14 15\r\n16 17\r\n18 19\r\n20 21 22 23\r\n24 25\r\n26 27\r\n28 29 30 31\r\n36 37\r\n38 39\r\n44 45\r\n46 47\r\n52 53\r\n54 55\r\n60 61\r\n62 63\r\n32 33\r\n34 35\r\n40 41\r\n42 43\r\n48 49\r\n50 51\r\n56 57\r\n58 59\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\nBig to little transpose\r\nLittle-to-big transpose\r\nFigure 25. How to Perform a Transpose as a Series of Sub-Transposes of Different Sizes \r\nThe idea of using a series of smaller transposes to transpose bigger matrices works for matrices of any size, shape, element type, \r\nand storage medium (register of memory). \r\nIt is possible to use the technique illustrated in this section to build transposes for matrix objects that are bigger than would fit \r\ninto registers too. Such an example is beyond the scope of this document.\r\n4.2 Multi-data-set Reductions\r\nConsider the data structure illustrated in Figure 26. This shows an array of values in memory, where the elements are divided into \r\nsmall groups of 8 contiguous elements (we assume that each element is 64-bits) and each group must be reduced into a single \r\nscalar value by summing all the values in the group. Note that the reduction could equally well be another type of operation (e.g., \r\nminimum). \r\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\r\ns0 s1 s2 s3\r\nSum()\r\nSum()\r\nSum()\r\nSum()\r\nFigure 26. Example Data Structure Showing a Series of Values That Must Be Reduced\r\nLeaving aside the most trivial implementation - reading each element one-by-one and adding it to the appropriate output \r\naccumulation element - there are three increasingly sophisticated ways to implement this code.\r\n4.2.1 Iterative Reduction Intrinsic\r\nMost modern C/C++ compilers implement reduction intrinsics and these allow the code to be written as shown in Figure 27. A \r\nsimple loop reads each group into a single Intel AVX-512 register (8 x 64-bit elements) and then directly calls the appropriate \r\nreduction intrinsic (https://godbolt.org/z/soEnGqhvz). The code this generates is illustrated in the same diagram.\r\n \r\n for (int i=0; i<n; ++i)\r\n { ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/ca2ba358-cee5-47a4-8715-717b6c7d724e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c6bdf77874b866aaa495170251c9d131c4b22502798298564fb613511ae8e56d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 773
      },
      {
        "segments": [
          {
            "segment_id": "684f2434-8f9a-43d1-b115-8de72a57b4df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 23,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n23\r\n __m512i values = _mm512_loadu_si512(values_to_sum + i * 8);\r\n reductions[i] = _mm512_reduce_add_epi64(values);\r\n } \r\n.LBB0_5:\r\n vmovdqu ymm0, ymmword ptr [rdi + 8*rcx + 32] \r\n vpaddq zmm0, zmm0, zmmword ptr [rdi + 8*rcx] \r\n vextracti128 xmm1, ymm0, 1\r\n vpaddq xmm0, xmm0, xmm1\r\n vpshufd xmm1, xmm0, 238\r\n vpaddq xmm0, xmm0, xmm1\r\n vmovq qword ptr [rax + rcx], xmm0\r\n add rcx, 8\r\n cmp r8, rcx\r\n jne .LBB0_5\r\nFigure 27. Source Code Showing How to Perform a Reduction as a Series of Permute and Reduce Steps\r\nNotice that there is no single instruction that can implement a horizontal instruction addition so the compiler has synthesized the \r\noperation using a series of permutes and adds. The code is organized into a reduction network, where the SIMD value is\r\nsuccessively split into two pieces that are added together, continuing until only one element remains.\r\nThis code performs reasonably well but there are two sources of inefficiency:\r\n1. The full width of the processor’s SIMD capabilities is left unused. The first stage only uses 256-bits of the full Intel AVX-512 \r\nregister, the second stage only 64-bits, and so on. Ideally, SIMD processors should do something with the entire width of a \r\nregister at any given point to maximize efficiency.\r\n2. There is a critical path exposed in this code caused by each permute-and-add stage being dependent on the results of the \r\nprevious stage. There is no opportunity to exploit parallelism, and the full latency of the permute instructions is exposed, \r\nwhich leads to processor stalls\r\nBy unrolling the loop, it becomes possible to overlap multiple iterations, which hides some of the effects of the stalls on the \r\ncritical path, but it does not fix the SIMD inefficiency issue. Our next code fragment attempts to address that.\r\n4.2.2 Using Transpose to Improve a Reduction\r\nTo improve the efficiency of the code, we could exploit the fact that multiple groups are being reduced, and each group is \r\nindependent of the others. This opens the way to allow us to reduce several groups in parallel with each other. \r\nSIMD processors are typically better at vertical (map-like) operations than horizontal (reduce-like) operations, so we could \r\nimprove the efficiency of this code by turning the horizontal reduction operation into a vertical summation by using a transpose \r\non the data first. This is illustrated in Figure 28. On the left we have our 8 original rows of data, and we call reduce_add_epi64 on \r\neach row. This in turn generates the reduce-add network described in the previous section. In contrast, if we first transpose the \r\ndata so that respective elements are positioned in the same index of each of a series of SIMD rows then we can trivially add all the \r\nrows together using the efficient _add_epi64 vertical operations, which make full use of SIMD. This is faster overall because a \r\nsingle large transpose of multiple data sets uses the entire SIMD data width, while the series of individual row reductions only \r\nuses partially filled registers and requires more instructions overall. Note that the larger the matrix, or the smaller the granularity \r\nof the individual elements, the more efficient it is to perform one large transpose than many smaller reductions.\r\n ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/684f2434-8f9a-43d1-b115-8de72a57b4df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=36d9ec2dcd0bc8ca1d1ddd74168e5a3255fefb60b4f0ca88f4397e237f85f1cf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 542
      },
      {
        "segments": [
          {
            "segment_id": "684f2434-8f9a-43d1-b115-8de72a57b4df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 23,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n23\r\n __m512i values = _mm512_loadu_si512(values_to_sum + i * 8);\r\n reductions[i] = _mm512_reduce_add_epi64(values);\r\n } \r\n.LBB0_5:\r\n vmovdqu ymm0, ymmword ptr [rdi + 8*rcx + 32] \r\n vpaddq zmm0, zmm0, zmmword ptr [rdi + 8*rcx] \r\n vextracti128 xmm1, ymm0, 1\r\n vpaddq xmm0, xmm0, xmm1\r\n vpshufd xmm1, xmm0, 238\r\n vpaddq xmm0, xmm0, xmm1\r\n vmovq qword ptr [rax + rcx], xmm0\r\n add rcx, 8\r\n cmp r8, rcx\r\n jne .LBB0_5\r\nFigure 27. Source Code Showing How to Perform a Reduction as a Series of Permute and Reduce Steps\r\nNotice that there is no single instruction that can implement a horizontal instruction addition so the compiler has synthesized the \r\noperation using a series of permutes and adds. The code is organized into a reduction network, where the SIMD value is\r\nsuccessively split into two pieces that are added together, continuing until only one element remains.\r\nThis code performs reasonably well but there are two sources of inefficiency:\r\n1. The full width of the processor’s SIMD capabilities is left unused. The first stage only uses 256-bits of the full Intel AVX-512 \r\nregister, the second stage only 64-bits, and so on. Ideally, SIMD processors should do something with the entire width of a \r\nregister at any given point to maximize efficiency.\r\n2. There is a critical path exposed in this code caused by each permute-and-add stage being dependent on the results of the \r\nprevious stage. There is no opportunity to exploit parallelism, and the full latency of the permute instructions is exposed, \r\nwhich leads to processor stalls\r\nBy unrolling the loop, it becomes possible to overlap multiple iterations, which hides some of the effects of the stalls on the \r\ncritical path, but it does not fix the SIMD inefficiency issue. Our next code fragment attempts to address that.\r\n4.2.2 Using Transpose to Improve a Reduction\r\nTo improve the efficiency of the code, we could exploit the fact that multiple groups are being reduced, and each group is \r\nindependent of the others. This opens the way to allow us to reduce several groups in parallel with each other. \r\nSIMD processors are typically better at vertical (map-like) operations than horizontal (reduce-like) operations, so we could \r\nimprove the efficiency of this code by turning the horizontal reduction operation into a vertical summation by using a transpose \r\non the data first. This is illustrated in Figure 28. On the left we have our 8 original rows of data, and we call reduce_add_epi64 on \r\neach row. This in turn generates the reduce-add network described in the previous section. In contrast, if we first transpose the \r\ndata so that respective elements are positioned in the same index of each of a series of SIMD rows then we can trivially add all the \r\nrows together using the efficient _add_epi64 vertical operations, which make full use of SIMD. This is faster overall because a \r\nsingle large transpose of multiple data sets uses the entire SIMD data width, while the series of individual row reductions only \r\nuses partially filled registers and requires more instructions overall. Note that the larger the matrix, or the smaller the granularity \r\nof the individual elements, the more efficient it is to perform one large transpose than many smaller reductions.\r\n ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/684f2434-8f9a-43d1-b115-8de72a57b4df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=36d9ec2dcd0bc8ca1d1ddd74168e5a3255fefb60b4f0ca88f4397e237f85f1cf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 542
      },
      {
        "segments": [
          {
            "segment_id": "7fdd0cf6-177a-47cd-ba73-181b88cc0a71",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 24,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n24\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\nreduce_add_epi64 s0\r\nreduce_add_epi64 s1\r\nreduce_add_epi64 s2\r\nreduce_add_epi64 s3\r\nreduce_add_epi64 s4\r\nreduce_add_epi64 s5\r\nreduce_add_epi64 s6\r\nreduce_add_epi64 s7\r\ns0 s1 s2 s3 s4 s5 s6 s7\r\nadd_epi64 x 8\r\nFigure 28. How Multiple Reduce-Add Groups Can Be Transposed to Convert the Horizontal Reduction Operation into a Vertical \r\nSummation\r\nNote that we leave the mechanics of handling multiple iterations to the reader. In practice this code might need to be able to \r\nhandle arbitrary numbers of groups, and such code would require suitable masking, unrolling, or specialization to deal with those \r\nissues. Also note thatif it were possible to arrange for the data to be transposed while it was generated, or during a storage phase \r\nof computation, so that the data as in an efficient storage format to begin with, the cost of this code is entirely eliminated.\r\nYou should also beware of the destination of the transpose. If the data is transposed into memory, and then read back again for \r\nthe summation, this may end up wiping out any performance gains because the memory system could potentially introduce a \r\nbottleneck. Itis preferable to sum the values after they have been transposed into registers and before they get stored back to \r\nmemory.\r\n4.2.3 Integrated Transpose-and-Reduce\r\nWe can further optimize the code from the previous section by exploiting the knowledge that the data is undergoing a reduction. \r\nAt each stage of the transpose, rather than simply transposing the data, we can reduce it too. The reduction decreases the \r\namount of data in flight, making the computation of subsequent stages cheaper. This is illustrated in Figure 29. The original data \r\nstarts out on the left-hand-side, where we are computing the sum of all the values in each row to form a single output value. In \r\nStage 1 we transpose with adjacent rows and sum the resulting data. This means that every pair of rows uses a summing \r\nreduction to create one output row. In Stage 2 we now do the same transpose-and-sum operation again. In Stage 3, the number \r\nof rows of data has reduced again, and we continue to transpose-and-reduce until we end up with the final answer.\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\nStage 1 Stage 2 Stage 3\r\nFigure 29. How Combined Transpose-And-Reduce Can Resultin the Amount of Data Being Processed at Each Stage of the \r\nTranspose Can Be Reduced\r\nIn the previous section we described how we could transpose the entire data matrix first, and then sum it. This required three \r\nstages, each containing 8 permutes, giving a total of 24 permute instructions. In the new combined transpose-and-reduce \r\nversion, the first stage needs 8 permutes, the second 4, and the third only 2, giving a total of 14 permutes. This is almost a halving \r\nin the number of required permutes.\r\nIn the previous section we also showed how the transpose could be implemented from little-to-big or big-to-little, as illustrated in \r\nFigure 25. Some of the stages are more expensive than others, but the overall transpose has to perform all the permutes at each \r\nstage, so it ultimately does not really matter in what order the data is processed. In contrast, it does make a difference in the \r\ncombined transpose-and-reduce. Since we have more data in the Stage 1, it makes sense to use the cheapest permute to start ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/7fdd0cf6-177a-47cd-ba73-181b88cc0a71.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e49a832bbc09e7ec0c4aeba1fc6fc4250c50db91dab2a9df51a823e030b7fc0e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 816
      },
      {
        "segments": [
          {
            "segment_id": "7fdd0cf6-177a-47cd-ba73-181b88cc0a71",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 24,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n24\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\nreduce_add_epi64 s0\r\nreduce_add_epi64 s1\r\nreduce_add_epi64 s2\r\nreduce_add_epi64 s3\r\nreduce_add_epi64 s4\r\nreduce_add_epi64 s5\r\nreduce_add_epi64 s6\r\nreduce_add_epi64 s7\r\ns0 s1 s2 s3 s4 s5 s6 s7\r\nadd_epi64 x 8\r\nFigure 28. How Multiple Reduce-Add Groups Can Be Transposed to Convert the Horizontal Reduction Operation into a Vertical \r\nSummation\r\nNote that we leave the mechanics of handling multiple iterations to the reader. In practice this code might need to be able to \r\nhandle arbitrary numbers of groups, and such code would require suitable masking, unrolling, or specialization to deal with those \r\nissues. Also note thatif it were possible to arrange for the data to be transposed while it was generated, or during a storage phase \r\nof computation, so that the data as in an efficient storage format to begin with, the cost of this code is entirely eliminated.\r\nYou should also beware of the destination of the transpose. If the data is transposed into memory, and then read back again for \r\nthe summation, this may end up wiping out any performance gains because the memory system could potentially introduce a \r\nbottleneck. Itis preferable to sum the values after they have been transposed into registers and before they get stored back to \r\nmemory.\r\n4.2.3 Integrated Transpose-and-Reduce\r\nWe can further optimize the code from the previous section by exploiting the knowledge that the data is undergoing a reduction. \r\nAt each stage of the transpose, rather than simply transposing the data, we can reduce it too. The reduction decreases the \r\namount of data in flight, making the computation of subsequent stages cheaper. This is illustrated in Figure 29. The original data \r\nstarts out on the left-hand-side, where we are computing the sum of all the values in each row to form a single output value. In \r\nStage 1 we transpose with adjacent rows and sum the resulting data. This means that every pair of rows uses a summing \r\nreduction to create one output row. In Stage 2 we now do the same transpose-and-sum operation again. In Stage 3, the number \r\nof rows of data has reduced again, and we continue to transpose-and-reduce until we end up with the final answer.\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n32 33 34 35 36 37 38 39\r\n40 41 42 43 44 45 46 47\r\n48 49 50 51 52 53 54 55\r\n56 57 58 59 60 61 62 63\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\nStage 1 Stage 2 Stage 3\r\nFigure 29. How Combined Transpose-And-Reduce Can Resultin the Amount of Data Being Processed at Each Stage of the \r\nTranspose Can Be Reduced\r\nIn the previous section we described how we could transpose the entire data matrix first, and then sum it. This required three \r\nstages, each containing 8 permutes, giving a total of 24 permute instructions. In the new combined transpose-and-reduce \r\nversion, the first stage needs 8 permutes, the second 4, and the third only 2, giving a total of 14 permutes. This is almost a halving \r\nin the number of required permutes.\r\nIn the previous section we also showed how the transpose could be implemented from little-to-big or big-to-little, as illustrated in \r\nFigure 25. Some of the stages are more expensive than others, but the overall transpose has to perform all the permutes at each \r\nstage, so it ultimately does not really matter in what order the data is processed. In contrast, it does make a difference in the \r\ncombined transpose-and-reduce. Since we have more data in the Stage 1, it makes sense to use the cheapest permute to start ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/7fdd0cf6-177a-47cd-ba73-181b88cc0a71.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e49a832bbc09e7ec0c4aeba1fc6fc4250c50db91dab2a9df51a823e030b7fc0e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 816
      },
      {
        "segments": [
          {
            "segment_id": "f2fa8a25-7c89-4976-8b5c-c9d1beadd399",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 25,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n25\r\nwith when we have many permutes to do, and to use the more expensive permutes in the later stages when we have fewer of \r\nthem.\r\nWe can go a tiny bit further still in optimizing this code by exploiting another property of the reduction: each reduction stage for \r\nthis example (and others like a minimum) is commutative. To illustrate why this is important first consider the original \r\nimplementation described above. In Figure 30 we show the transpose-and-sum of the first two rows. This operation requires two \r\npermutes and an add. \r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n0 1 2 3 4 5 6 7 0+1 8+9\r\n8 9 10 11 12 13 14 15\r\n2+3 10+11 4+5 12+13 14+15\r\nFigure 30. Transposition and Summation of Two Rows of Data \r\nHowever, the addition step does not require that the values in Stage 1 are necessarily in the order that these are currently given. \r\nWe could change the order of the Stage 1 values to be slightly different, as shown in Figure 31. The difference is very subtle. \r\nEvery odd column contains the same elements as previously, but in reversed order. This slight change means that the \r\ninstructions needed to implement this operation are a mask, a permute, and an add. One permute has been swapped for a \r\npotentially cheaper mask instruction, giving a slight performance improvement. \r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n8 7\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n0 1 2 3 4 5 6 7 15 0+1 9+8\r\n8 9 10 11 12 13 14 15\r\n2+3 11+10 4+5 13+12 6+7 15+14\r\nFigure 31. An Alternative Form of Transposition and summation That Exploits Commutativity Addition Reduction\r\n4.3 Full-Register-Width Shifts (and Rotates) Using VBMI\r\nSuppose that you wish to shift (or rotate) the bits across an entire 512-bit register. There are many ways to achieve this, taking \r\nadvantage of properties such as whether the shift is dynamic or compile-time, whether it is a multiple of some number of bytes or \r\nis at the bit granularity, and whether it is logical or arithmetic. Specific types of shift or rotate can be coded up using fast \r\nspecialist sequences, and optimizing specific sizes and shifts is left as an exercise for the reader. However, it is useful to illustrate \r\nthe thinking behind the implementation of one specific type of shift and the problems that have to be solved, and to this end we \r\nshall implement a shift of an Intel AVX-512 value by the compile-time offset of 12 bits, as illustrated in Figure 32. That figure \r\nshows the 64 bytes within a register, and how the value has been shifted right by 42 bits.\r\n512 448 384 Bit 128 256 192 128 64 0\r\n42-bits\r\nFigure 32. A Bit-Level Shift by a Large Offset in an Intel AVX-512 Register\r\nThe main issue with a bit-level shift of this type is that bit-level shifts operate within elements. The naïve use of a plain shift in 64-\r\nbit elements would result in bits being destroyed by shifting in zeros, as illustrated in Figure 33. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/f2fa8a25-7c89-4976-8b5c-c9d1beadd399.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4272497edec9ba7659dfac0ceb533adf08fef2046c53aef59171024adda134d6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 540
      },
      {
        "segments": [
          {
            "segment_id": "f2fa8a25-7c89-4976-8b5c-c9d1beadd399",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 25,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n25\r\nwith when we have many permutes to do, and to use the more expensive permutes in the later stages when we have fewer of \r\nthem.\r\nWe can go a tiny bit further still in optimizing this code by exploiting another property of the reduction: each reduction stage for \r\nthis example (and others like a minimum) is commutative. To illustrate why this is important first consider the original \r\nimplementation described above. In Figure 30 we show the transpose-and-sum of the first two rows. This operation requires two \r\npermutes and an add. \r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n0 1 2 3 4 5 6 7 0+1 8+9\r\n8 9 10 11 12 13 14 15\r\n2+3 10+11 4+5 12+13 14+15\r\nFigure 30. Transposition and Summation of Two Rows of Data \r\nHowever, the addition step does not require that the values in Stage 1 are necessarily in the order that these are currently given. \r\nWe could change the order of the Stage 1 values to be slightly different, as shown in Figure 31. The difference is very subtle. \r\nEvery odd column contains the same elements as previously, but in reversed order. This slight change means that the \r\ninstructions needed to implement this operation are a mask, a permute, and an add. One permute has been swapped for a \r\npotentially cheaper mask instruction, giving a slight performance improvement. \r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n8 7\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n0 1 2 3 4 5 6 7 15 0+1 9+8\r\n8 9 10 11 12 13 14 15\r\n2+3 11+10 4+5 13+12 6+7 15+14\r\nFigure 31. An Alternative Form of Transposition and summation That Exploits Commutativity Addition Reduction\r\n4.3 Full-Register-Width Shifts (and Rotates) Using VBMI\r\nSuppose that you wish to shift (or rotate) the bits across an entire 512-bit register. There are many ways to achieve this, taking \r\nadvantage of properties such as whether the shift is dynamic or compile-time, whether it is a multiple of some number of bytes or \r\nis at the bit granularity, and whether it is logical or arithmetic. Specific types of shift or rotate can be coded up using fast \r\nspecialist sequences, and optimizing specific sizes and shifts is left as an exercise for the reader. However, it is useful to illustrate \r\nthe thinking behind the implementation of one specific type of shift and the problems that have to be solved, and to this end we \r\nshall implement a shift of an Intel AVX-512 value by the compile-time offset of 12 bits, as illustrated in Figure 32. That figure \r\nshows the 64 bytes within a register, and how the value has been shifted right by 42 bits.\r\n512 448 384 Bit 128 256 192 128 64 0\r\n42-bits\r\nFigure 32. A Bit-Level Shift by a Large Offset in an Intel AVX-512 Register\r\nThe main issue with a bit-level shift of this type is that bit-level shifts operate within elements. The naïve use of a plain shift in 64-\r\nbit elements would result in bits being destroyed by shifting in zeros, as illustrated in Figure 33. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/f2fa8a25-7c89-4976-8b5c-c9d1beadd399.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4272497edec9ba7659dfac0ceb533adf08fef2046c53aef59171024adda134d6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 540
      },
      {
        "segments": [
          {
            "segment_id": "b38d786d-f850-4c1e-9fa9-3af139ef9c73",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 26,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n26\r\n512 448 384 Bit 128 256 192 128 64 0\r\n42-bits\r\nFigure 33. How 64-bit Element Shifts Destroy Data Bits\r\nTo fix this issue, it is necessary to take the bits from the adjacent 64-bit element, shift those in the opposite direction, and then \r\ncombine the top sets of bits together. However, with the VBMI instruction set this can be conveniently done using the new \r\nelement-wise alignment instructions (e.g., VPSHLDQ) as illustrated in Figure 34. \r\n512 448 384 Bit 128 256 192 128 64 0\r\n42-bits\r\nalignr\r\nvpshldq\r\nFigure 34. Implementing Compile-Time Shift Using Lane-Wise and Element-Wise valign{d,q} Instructions from VBMI\r\nIn this example, the bit shift is less than the size of the 64-bit element, so we begin by moving all the 64-bit half-lanes to the right \r\nby one element offset, so that two adjacent 64-bit groups from our original SIMD value are now positioned in the same 64-bit \r\nelement position in two different registers. Thus, the top bits of one half-lane and the bottom bits of the adjacent half-lane that \r\nwill be shifted in are now in the same element position. Finally, we use the VBMI 64-bit aligned shift to combine the two sets of \r\nbits – shown in yellow and blue boxes – into a contiguous block of bits that represents the final output. This is of course repeated \r\nacross every 64-bit element.\r\nShifts by variable bit-level offsets are a little harder since the initial alignment instructions to get the correct 64-bit half-lanes in \r\nplace do not allow dynamic shifts, and a permute should be used instead. After the data is in the correct half-lane, the element\u0002wise align can be used as that allows a shift by a dynamic offset. Building the dynamic alignment is left as an exercise for the \r\nreader.\r\n5 Summary\r\nIn this document we have looked at the many and varied ways that Intel AVX-512 allows data elements within a SIMD value to be \r\nreordered, moved, and combined with each other. At one extreme, elements can be moved very short distances in fixed \r\npatterns, while at the other extreme, data can be moved across entire registers in essentially arbitrary patterns, albeit at greater\r\ncomputation cost.\r\nWe have provided a few worked examples showing how different types of permute can be layered together to build up complex \r\npermutations such as transposes, reductions, and very wide shift operations.\r\nOther examples of how permute instructions can be used are provided in the Intel® 64 and IA-32 Architectures Optimization \r\nReference Manual. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/b38d786d-f850-4c1e-9fa9-3af139ef9c73.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1d37a87890686b92ce50e8822d1894cae5348ab8c5f9eb818f1e5695f53c43be",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 433
      },
      {
        "segments": [
          {
            "segment_id": "bd557ef4-f1e0-4ca0-a27d-968462441589",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 27,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n27\r\nCompiler-Assisted Permute Generation\r\nThroughout this document we have described the various types of permutation possible, and the intrinsics necessary to use \r\nthem. We recommended that the programmer should try to use the cheapest and most efficient types of permute instructions \r\nthat solve the task at hand, rather than relying on using the general purpose permute. However, knowing about and being able to \r\nuse the best instruction for the job can be difficult due to the size and complexity of the Intel AVX instruction sets. To solve this \r\nissue, we can enlist the compiler to help us.\r\nRecent versions of GCC, LLVM, and Intel oneAPI compilers include some support for SIMD vector convenience built-ins, data \r\ntypes, and a convenient syntax for manipulating SIMD values. One little-appreciated built-in compiler function is \r\n__builtin_shufflevector. This built-in allows the programmer to specify what permutation they desire, and the compiler \r\ntries to determine the cheapest way to provide it. Table 5 shows a small selection of permute index sequences and the code \r\ngenerated for each and demonstrates how the programmer specifies the operation they want and is given an efficient way to \r\nachieve it.\r\nTable 5. A Selection of Shuffle Indexes and the Intel AVX Instruction Sequence Generated by the Compiler\r\nShuffle Pattern Given to \r\n__builtin_vector_shuffle\r\nfor 32-bit Index Elements\r\nPurpose Output from Clang 14.0.0\r\n0, 0, 2, 2, 4, 4, 6, 6 Duplicate even elements vmovsldup ymm0, ymm0\r\n1, 2, 3, 0, 5, 6, 7, 4 Rotate in lane vpermilps ymm0, ymm0, 57\r\n3, 4, 7, 1, 2, 3, 4, 5 Arbitrary permute vmovups ymm1, ymmword ptr [$idx]\r\nvpermps ymm0, ymm1, ymm0\r\nYou should beware that the compiler is not perfect, and it sometimes misses opportunities that would provide faster code \r\nsequences. It is hoped that over time the compiler’s understanding of what instruction sequences are fast will improve and any \r\ncode written to use index sequences will get faster with future compilers. Also note that the Clang 14.0.0 and Intel oneAPI \r\n2022.0 compilers at time of writing were more sophisticated than their contemporary GCC 11 and selected better permute \r\ninstructions.\r\nThis section seems to have rendered the rest of the document obsolete. If the compiler can choose for itself which is the best \r\ninstruction to use, what is the purpose of this document? There are several reasons, including:\r\n As already noted, the compiler is not perfect – there are some performance issues at time of writing, and falling back to \r\nintrinsics may still be necessary in places if the compiler is missing an opportunity. \r\n The programmer may want to be able to interpret the assembly generated by the compiler to understand why it has chosen \r\na particular sequence for a given type of permutation. \r\n Understanding what can be done efficiently in the processor can help guide the programmer to use certain index sequences \r\nover others. The compiler is aiding the programmer by allowing a more obvious syntax to be used and avoiding the need to \r\nmemorize intrinsic names, but the compiler is ultimately guided by the programmer to combine the permutes together in \r\nparticular ways.\r\n The built-in shuffle syntax opens up the path to C++ compile-time permutation functions!\r\nThe last of these bullet points is interesting because it permits a way of programming in C++ that allows the index sequence to be \r\nspecified at compile-time using a function rather than having to manually compute each index.\r\nTo illustrate why compile-time permute functions are interesting, consider what would be required to write a numerics library \r\nthat needs to be able to transpose matrices of different data types and matrix dimensions. If conventional intrinsics are used, \r\nthen the programmer needs to write many different versions of the function, parameterized for different types and sizes. In C++ \r\nit might be a little easier than in C due to the ability to use templates, but at some level the source code needs to know how to \r\nmove elements around within a SIMD value using a named intrinsic. This explicit naming breaks portability since it ties the source \r\ncode to a specific instruction, width, element size, and target. However, by using the __builtin_vectorshuffle function the \r\ncompiler can be told to generate a certain permutation, and not only will it be applied portably to any data type and size, but it will \r\ndo so efficiently without the programmer having to specify which intrinsic to use.\r\nTo aid us in building compile-time permute functions, consider the C++204 code fragment shown in Figure 35. This code \r\nfragment takes a pair of source SIMD values of any type and size (although they must be the same) and an index generation \r\nfunction that converts from an input index to an output index. The index generator is called repeatedly for all values between \r\n4 C++20 is required because this code uses template parameters on the lambda and the lambda is also written as an Immediately Invoked Function \r\nExpression (IIFE). This can be implemented in C++14 or 17 as well at the expense of more verbosity.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/bd557ef4-f1e0-4ca0-a27d-968462441589.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=977aa1ccf7457d5995609724c15e9996cf21775c03563acab412d3c0d6015e44",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 853
      },
      {
        "segments": [
          {
            "segment_id": "bd557ef4-f1e0-4ca0-a27d-968462441589",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 27,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n27\r\nCompiler-Assisted Permute Generation\r\nThroughout this document we have described the various types of permutation possible, and the intrinsics necessary to use \r\nthem. We recommended that the programmer should try to use the cheapest and most efficient types of permute instructions \r\nthat solve the task at hand, rather than relying on using the general purpose permute. However, knowing about and being able to \r\nuse the best instruction for the job can be difficult due to the size and complexity of the Intel AVX instruction sets. To solve this \r\nissue, we can enlist the compiler to help us.\r\nRecent versions of GCC, LLVM, and Intel oneAPI compilers include some support for SIMD vector convenience built-ins, data \r\ntypes, and a convenient syntax for manipulating SIMD values. One little-appreciated built-in compiler function is \r\n__builtin_shufflevector. This built-in allows the programmer to specify what permutation they desire, and the compiler \r\ntries to determine the cheapest way to provide it. Table 5 shows a small selection of permute index sequences and the code \r\ngenerated for each and demonstrates how the programmer specifies the operation they want and is given an efficient way to \r\nachieve it.\r\nTable 5. A Selection of Shuffle Indexes and the Intel AVX Instruction Sequence Generated by the Compiler\r\nShuffle Pattern Given to \r\n__builtin_vector_shuffle\r\nfor 32-bit Index Elements\r\nPurpose Output from Clang 14.0.0\r\n0, 0, 2, 2, 4, 4, 6, 6 Duplicate even elements vmovsldup ymm0, ymm0\r\n1, 2, 3, 0, 5, 6, 7, 4 Rotate in lane vpermilps ymm0, ymm0, 57\r\n3, 4, 7, 1, 2, 3, 4, 5 Arbitrary permute vmovups ymm1, ymmword ptr [$idx]\r\nvpermps ymm0, ymm1, ymm0\r\nYou should beware that the compiler is not perfect, and it sometimes misses opportunities that would provide faster code \r\nsequences. It is hoped that over time the compiler’s understanding of what instruction sequences are fast will improve and any \r\ncode written to use index sequences will get faster with future compilers. Also note that the Clang 14.0.0 and Intel oneAPI \r\n2022.0 compilers at time of writing were more sophisticated than their contemporary GCC 11 and selected better permute \r\ninstructions.\r\nThis section seems to have rendered the rest of the document obsolete. If the compiler can choose for itself which is the best \r\ninstruction to use, what is the purpose of this document? There are several reasons, including:\r\n As already noted, the compiler is not perfect – there are some performance issues at time of writing, and falling back to \r\nintrinsics may still be necessary in places if the compiler is missing an opportunity. \r\n The programmer may want to be able to interpret the assembly generated by the compiler to understand why it has chosen \r\na particular sequence for a given type of permutation. \r\n Understanding what can be done efficiently in the processor can help guide the programmer to use certain index sequences \r\nover others. The compiler is aiding the programmer by allowing a more obvious syntax to be used and avoiding the need to \r\nmemorize intrinsic names, but the compiler is ultimately guided by the programmer to combine the permutes together in \r\nparticular ways.\r\n The built-in shuffle syntax opens up the path to C++ compile-time permutation functions!\r\nThe last of these bullet points is interesting because it permits a way of programming in C++ that allows the index sequence to be \r\nspecified at compile-time using a function rather than having to manually compute each index.\r\nTo illustrate why compile-time permute functions are interesting, consider what would be required to write a numerics library \r\nthat needs to be able to transpose matrices of different data types and matrix dimensions. If conventional intrinsics are used, \r\nthen the programmer needs to write many different versions of the function, parameterized for different types and sizes. In C++ \r\nit might be a little easier than in C due to the ability to use templates, but at some level the source code needs to know how to \r\nmove elements around within a SIMD value using a named intrinsic. This explicit naming breaks portability since it ties the source \r\ncode to a specific instruction, width, element size, and target. However, by using the __builtin_vectorshuffle function the \r\ncompiler can be told to generate a certain permutation, and not only will it be applied portably to any data type and size, but it will \r\ndo so efficiently without the programmer having to specify which intrinsic to use.\r\nTo aid us in building compile-time permute functions, consider the C++204 code fragment shown in Figure 35. This code \r\nfragment takes a pair of source SIMD values of any type and size (although they must be the same) and an index generation \r\nfunction that converts from an input index to an output index. The index generator is called repeatedly for all values between \r\n4 C++20 is required because this code uses template parameters on the lambda and the lambda is also written as an Immediately Invoked Function \r\nExpression (IIFE). This can be implemented in C++14 or 17 as well at the expense of more verbosity.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/bd557ef4-f1e0-4ca0-a27d-968462441589.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=977aa1ccf7457d5995609724c15e9996cf21775c03563acab412d3c0d6015e44",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 853
      },
      {
        "segments": [
          {
            "segment_id": "113ba18d-331e-4fd9-acc8-1052f26ad7a1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 28,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n28\r\n[0..N), where N is the size of the result vector. For each index it returns a new index representing the source index from which \r\nthat index takes its value (e.g., given a function that does “index * 2” the sequential `iota’ indexes [0, 1, 2, 3…] are mapped to \r\nthe new index sequence [0, 2, 4, 6, …). Those new index values are then passed to the __builtin_shufflevector function to \r\nperform the actual permute.\r\ntemplate <typename _Result = void, typename _Vec, typename IndexGenerator> \r\nconstexpr auto permute(_Vec v0, _Vec v1, IndexGenerator fn) {\r\n // If the user explicitly specified an output type use that, otherwise assume it is\r\n // the same as an input SIMD.\r\n using _outVec = std::conditional_t<std::is_same_v<void, _Result>, _Vec, _Result>;\r\n constexpr auto _numElements = sizeof(_outVec) / sizeof(decltype(_outVec()[0]));\r\n return [=] <std::size_t... _Idx> (std::index_sequence<_Idx...>)\r\n { \r\n return __builtin_shufflevector(v0, v1, fn(_Idx)...);\r\n } (std::make_index_sequence<_numElements>{});\r\n} \r\nFigure 35. Source Code Showing a C++20 Code Fragmentthat uses a Compile-Time Lambda Function to Generate an Index \r\nSequence for Use in the Compiler’s Own Built-In Permute\r\nSome examples of how to use this compile-time permute function are shown in Table 6. \r\nTable 6. A Selection of Compile-Time Permute Function Calls and the Intel AVX Instruction Sequence Generated by the \r\nCompiler\r\nPermute Call Purpose Output from Clang 14.0.05\r\npermute (x, x, [](auto idx) {\r\nreturn idx & ~1; \r\n});\r\nDuplicate even elements vmovsldup zmm0, zmm0\r\npermute (x, x, [](auto idx) {\r\n return idx ^ 1; \r\n});\r\nSwap even/odd elements in each pair \r\n(complex-valued IQ swap) vpermilps ymm0, ymm0, \r\n177\r\npermute (x, x, [](auto idx) { \r\nreturn idx + 8; \r\n});\r\nExtract upper half of a 16-element \r\nvector. Note thatthe instruction \r\nsequence accepts a zmm input and \r\nreturns a ymm output.\r\nvextractf64x4 ymm0, zmm0, 1\r\npermute(x, y, [](size_t idx) {\r\n return idx % 2 \r\n ? idx / 2 + 16\r\n : idx;\r\n});\r\nInsert a set of 8 contiguous values from \r\none vector into the odd elements of a 16-\r\nelement vector.\r\nvmovups zmm2, ptr [rip + .LC]\r\n# zmm2 = \r\n[0,16,2,17,4,18,6,19,8,20,10,2\r\n1,12,22,14,23]\r\nvpermt2ps zmm0, zmm2, zmm1\r\nNotice how any two vectors can be combined and even resized into a new vector, and the compiler efficiently determines how to \r\ndo this for each new function it encounters. This compile-time permute instruction is extremely powerful and flexible and is very \r\nuseful for building generic SIMD manipulation routines.\r\n5 https://godbolt.org/z/a5bhK7d6j",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/113ba18d-331e-4fd9-acc8-1052f26ad7a1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=377b29b0744a3714b396aa4be3e8e235401371b60d10ef1c0681ecd264bde54d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 407
      },
      {
        "segments": [
          {
            "segment_id": "5c3b594b-a6a2-403b-a700-a7465e032450",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 594,
              "height": 792
            },
            "page_number": 29,
            "page_width": 594,
            "page_height": 792,
            "content": "Technology Guide | Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX \r\nRegisters \r\n29\r\nPerformance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex. \r\nPerformance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for \r\nconfiguration details. No product or component can be absolutely secure.\r\nIntel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular \r\npurpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade.\r\nIntel technologies may require enabled hardware, software or service activation.\r\nIntel does not control or audit third-party data. You should consult other sources to evaluate accuracy.\r\nThe products described may contain design defects or errors known as errata which may cause the product to deviate from published \r\nspecifications. Current characterized errata are available on request.\r\n© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands \r\nmay be claimed as the property of others. \r\n1022/DN/WIPRO/PDF 742407-001US",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/100afd0c-e35c-4775-a300-87e110334352/images/5c3b594b-a6a2-403b-a700-a7465e032450.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042435Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=45b4b96a92bb154746318743575bd13d8be8599b54c086d585813c7020eb78ba",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 191
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "Intel® Advanced Vector Extensions 512 (Intel® AVX-512) - Permuting Data Within and Between AVX Registers\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "No response"
        }
      ]
    }
  }
}