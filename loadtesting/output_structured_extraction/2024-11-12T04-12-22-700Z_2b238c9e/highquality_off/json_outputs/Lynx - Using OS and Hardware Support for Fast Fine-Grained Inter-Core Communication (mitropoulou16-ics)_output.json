{
  "file_name": "Lynx - Using OS and Hardware Support for Fast Fine-Grained Inter-Core Communication (mitropoulou16-ics).pdf",
  "task_id": "248c5e86-bf0b-4ea9-8819-9ce920ac645f",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "f25721a1-4fca-455c-905f-227a7b55dc9b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Lynx: Using OS and Hardware Support for Fast\r\nFine-Grained Inter-Core Communication\r\nKonstantina Mitropoulou, Vasileios Porpodas1, Xiaochun Zhang and Timothy M. Jones\r\nComputer Laboratory\r\nUniversity of Cambridge, UK\r\nfirstname.lastname@cl.cam.ac.uk\r\nABSTRACT\r\nDesigning high-performance software queues for fast inter\u0002core communication is challenging, but critical for maximis\u0002ing software parallelism. State-of-the-art single-producer /\r\nsingle-consumer queues for streaming applications contain\r\nmultiple sections, requiring the producer and consumer to\r\noperate independently on different sections from each other.\r\nWhile these queues perform well for coarse-grained data\r\ntransfers, they perform poorly in the fine-grained case.\r\nThis paper proposes Lynx, a novel SP/SC queue, specif\u0002ically tuned for fine-grained communication. Lynx is built\r\nfrom the ground up, reducing the generated code on the\r\ncritical-path to just two operations per enqueue and de\u0002queue. To achieve this it relies on existing commodity pro\u0002cessor hardware and operating system exception handling\r\nsupport to deal with infrequent queue maintenance opera\u0002tions. Lynx outperforms the state-of-the art by up to 1.57×\r\nin total 64-bit throughput reaching a peak throughput of\r\n15.7GB/s on a common desktop system. Real applications\r\nusing Lynx get a performance improvement of up to 1.4×.\r\nCCS Concepts\r\n•Software and its engineering → Buffering;\r\nKeywords\r\nSingle-Producer / Single-Consumer Software Queue, Fine\u0002grained Communication, Hardware Exceptions\r\n1. INTRODUCTION\r\nHigh-performance parallel applications rely on fast inter\u0002core communication to share data between tasks. Existing\r\ncommodity processors implement cache coherence protocols\r\nto maintain a consistent shared memory for this purpose.\r\nSoftware then builds upon this memory model with data\r\nstructures that facilitate the transfer. For asynchronous\r\n1Currently at Intel, Santa Clara.\r\nPermission to make digital or hard copies of all or part of this work for personal or\r\nclassroom use is granted without fee provided that copies are not made or distributed\r\nfor profit or commercial advantage and that copies bear this notice and the full cita\u0002tion on the first page. Copyrights for components of this work owned by others than\r\nACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re\u0002publish, to post on servers or to redistribute to lists, requires prior specific permission\r\nand/or a fee. Request permissions from permissions@acm.org.\r\nICS ’16, June 01-03, 2016, Istanbul, Turkey\r\n\rc 2016 ACM. ISBN 978-1-4503-4361-9/16/05. . . $15.00\r\nDOI: http://dx.doi.org/10.1145/2925426.2926274\r\ncommunication of large volumes of data, software queues are\r\nthe most common programming abstraction, which provide\r\na first-in first-out (FIFO) buffer with simple enqueue and de\u0002queue operations. However, building high-performance soft\u0002ware queues has proven to be a major challenge and there\r\nhas been significant work on improving their efficiency [8,\r\n14, 15, 22, 24, 25].\r\nSingle-producer / single-consumer (SP/SC) queues are a\r\nsubset of the generic multiple-producer / multiple-consumer\r\n(MP/MC) model. This more specialised type of queue is\r\nwidely used to aid pipeline parallelism, where each stage of\r\nthe pipeline produces data for the next. The performance\r\nof the SP/SC queue is critical in determining the amount\r\nand granularity of parallelism that can be extracted. Even\r\nthe fastest state-of-the-art queues have a prohibitively high\r\noverhead for transferring small amounts of data. To amor\u0002tise this cost, programmers typically only use a queue for\r\ncoarse-grained communication, thus limiting the potential\r\nfor parallelism. However, there are application domains that\r\nrely on extremely fast SP/SC queues for fine-grained data\r\ntransfers, such as automatic parallelization [9, 20], software\u0002based error detection [24, 25, 28] and fast line-rate network\r\ntraffic monitoring [16].\r\nSP/SC queues require numerous major innovations to\r\nachieve high performance. These involved lock-free imple\u0002mentations [7, 14]; minimising or completely avoiding fre\u0002quent bidirectional inter-core communication (a.k.a. cache\r\nping-pong [11]) of the queue control variables [8, 15, 16,\r\n24, 25]; and avoiding cache thrashing using specialised non\u0002temporal memory instructions [11]. However, our analysis of\r\na state-of-the-art queue shows that there is still performance\r\nleft on the table.\r\nIn this work we propose a new SP/SC queue design that\r\nprovides extremely fast inter-core communication even at\r\na very fine granularity. We show that current queues dis\u0002play poor fine-grained performance due to the execution of\r\ninfrequently-required code for producer/consumer synchro\u0002nisation, and for reaching the end of the queue. We then\r\ndevelop Lynx, a novel architecture that makes use of ex\u0002isting processor hardware and operating system support for\r\nexception handling to minimise enqueue and dequeue oper\u0002ation overheads. Moving code off the critical path realises\r\nperformance benefits of up to 1.57× compared to a state-of\u0002the-art queue.\r\nIn the following sections we first provide an overview of\r\nthe existing queue designs (section 2), then show the per\u0002formance bottleneck of the state-of-the-art (section 3). Sec\u0002tion 4 provides a detailed description of Lynx, which we",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/f25721a1-4fca-455c-905f-227a7b55dc9b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b95e7bb870797f3b346e625cfaf4afb0aace4eb0f21abd1c3b256504cd327183",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 742
      },
      {
        "segments": [
          {
            "segment_id": "f25721a1-4fca-455c-905f-227a7b55dc9b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Lynx: Using OS and Hardware Support for Fast\r\nFine-Grained Inter-Core Communication\r\nKonstantina Mitropoulou, Vasileios Porpodas1, Xiaochun Zhang and Timothy M. Jones\r\nComputer Laboratory\r\nUniversity of Cambridge, UK\r\nfirstname.lastname@cl.cam.ac.uk\r\nABSTRACT\r\nDesigning high-performance software queues for fast inter\u0002core communication is challenging, but critical for maximis\u0002ing software parallelism. State-of-the-art single-producer /\r\nsingle-consumer queues for streaming applications contain\r\nmultiple sections, requiring the producer and consumer to\r\noperate independently on different sections from each other.\r\nWhile these queues perform well for coarse-grained data\r\ntransfers, they perform poorly in the fine-grained case.\r\nThis paper proposes Lynx, a novel SP/SC queue, specif\u0002ically tuned for fine-grained communication. Lynx is built\r\nfrom the ground up, reducing the generated code on the\r\ncritical-path to just two operations per enqueue and de\u0002queue. To achieve this it relies on existing commodity pro\u0002cessor hardware and operating system exception handling\r\nsupport to deal with infrequent queue maintenance opera\u0002tions. Lynx outperforms the state-of-the art by up to 1.57×\r\nin total 64-bit throughput reaching a peak throughput of\r\n15.7GB/s on a common desktop system. Real applications\r\nusing Lynx get a performance improvement of up to 1.4×.\r\nCCS Concepts\r\n•Software and its engineering → Buffering;\r\nKeywords\r\nSingle-Producer / Single-Consumer Software Queue, Fine\u0002grained Communication, Hardware Exceptions\r\n1. INTRODUCTION\r\nHigh-performance parallel applications rely on fast inter\u0002core communication to share data between tasks. Existing\r\ncommodity processors implement cache coherence protocols\r\nto maintain a consistent shared memory for this purpose.\r\nSoftware then builds upon this memory model with data\r\nstructures that facilitate the transfer. For asynchronous\r\n1Currently at Intel, Santa Clara.\r\nPermission to make digital or hard copies of all or part of this work for personal or\r\nclassroom use is granted without fee provided that copies are not made or distributed\r\nfor profit or commercial advantage and that copies bear this notice and the full cita\u0002tion on the first page. Copyrights for components of this work owned by others than\r\nACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re\u0002publish, to post on servers or to redistribute to lists, requires prior specific permission\r\nand/or a fee. Request permissions from permissions@acm.org.\r\nICS ’16, June 01-03, 2016, Istanbul, Turkey\r\n\rc 2016 ACM. ISBN 978-1-4503-4361-9/16/05. . . $15.00\r\nDOI: http://dx.doi.org/10.1145/2925426.2926274\r\ncommunication of large volumes of data, software queues are\r\nthe most common programming abstraction, which provide\r\na first-in first-out (FIFO) buffer with simple enqueue and de\u0002queue operations. However, building high-performance soft\u0002ware queues has proven to be a major challenge and there\r\nhas been significant work on improving their efficiency [8,\r\n14, 15, 22, 24, 25].\r\nSingle-producer / single-consumer (SP/SC) queues are a\r\nsubset of the generic multiple-producer / multiple-consumer\r\n(MP/MC) model. This more specialised type of queue is\r\nwidely used to aid pipeline parallelism, where each stage of\r\nthe pipeline produces data for the next. The performance\r\nof the SP/SC queue is critical in determining the amount\r\nand granularity of parallelism that can be extracted. Even\r\nthe fastest state-of-the-art queues have a prohibitively high\r\noverhead for transferring small amounts of data. To amor\u0002tise this cost, programmers typically only use a queue for\r\ncoarse-grained communication, thus limiting the potential\r\nfor parallelism. However, there are application domains that\r\nrely on extremely fast SP/SC queues for fine-grained data\r\ntransfers, such as automatic parallelization [9, 20], software\u0002based error detection [24, 25, 28] and fast line-rate network\r\ntraffic monitoring [16].\r\nSP/SC queues require numerous major innovations to\r\nachieve high performance. These involved lock-free imple\u0002mentations [7, 14]; minimising or completely avoiding fre\u0002quent bidirectional inter-core communication (a.k.a. cache\r\nping-pong [11]) of the queue control variables [8, 15, 16,\r\n24, 25]; and avoiding cache thrashing using specialised non\u0002temporal memory instructions [11]. However, our analysis of\r\na state-of-the-art queue shows that there is still performance\r\nleft on the table.\r\nIn this work we propose a new SP/SC queue design that\r\nprovides extremely fast inter-core communication even at\r\na very fine granularity. We show that current queues dis\u0002play poor fine-grained performance due to the execution of\r\ninfrequently-required code for producer/consumer synchro\u0002nisation, and for reaching the end of the queue. We then\r\ndevelop Lynx, a novel architecture that makes use of ex\u0002isting processor hardware and operating system support for\r\nexception handling to minimise enqueue and dequeue oper\u0002ation overheads. Moving code off the critical path realises\r\nperformance benefits of up to 1.57× compared to a state-of\u0002the-art queue.\r\nIn the following sections we first provide an overview of\r\nthe existing queue designs (section 2), then show the per\u0002formance bottleneck of the state-of-the-art (section 3). Sec\u0002tion 4 provides a detailed description of Lynx, which we",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/f25721a1-4fca-455c-905f-227a7b55dc9b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b95e7bb870797f3b346e625cfaf4afb0aace4eb0f21abd1c3b256504cd327183",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 742
      },
      {
        "segments": [
          {
            "segment_id": "17ca8e06-de0b-4d7f-a046-95202eb038c7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "state2 state3 stateN\r\nPtr : Local Variable stateX\r\nShared Variable\r\n: Infrequently Accessed\r\nSection1\r\nQ\r\nSection2 Section3\r\ndeqPtr enqPtr\r\nSectionN\r\n...\r\n...\r\nstate1\r\nFigure 1: State-of-the-art lock-free multi-section queue.\r\nevaluate in section 5. We then present related work (sec\u0002tion 6) and section 7 concludes.\r\n2. MULTI-SECTION QUEUE FOR\r\nSTREAMING\r\nWe start with an overview of the current state-of-the-art\r\nqueue implementation to provide insights into the overheads.\r\nMulti-section lock-free SP/SC queues are designed for\r\nstreaming, specialised compared to generic SP/SC queues so\r\nthey are tuned for high throughput. The queue is divided\r\ninto sections and only one thread (producer or consumer) is\r\nallowed to access each section at any time. Synchronisation\r\noccurs only at section boundaries. The multi-section queue\r\nis lock-free by design: both producer and consumer access\r\nthe queue simultaneously without locking, provided that\r\nthey do not access the same section [16]. This multi-section\r\ndesign solves many generic SP/SC performance problems,\r\nsuch as cache ping-pong and false sharing [11, 15, 16, 24].\r\nFigure 1 shows an example and listing 1 gives the code.\r\nThe queue shown uses lazy synchronisation [24, 25] for syn\u0002chronising across sections (lines 5 to 10 and 15 to 20). Over\u0002all, an efficient implementation of a multi-section queue must\r\naddress a number of challenges.\r\nQueue Size.\r\nIn general, the larger the queue, the larger the sections and\r\nthe smaller the amount of synchronisation required. How\u0002ever, once the queue is larger than the last level cache, per\u0002formance drops because the data gets invalidated before be\u0002ing read and the dequeue thread must obtain it from main\r\nmemory instead. Figure 2 shows the throughput for increas\u0002ing queue sizes when using mov instructions to write data,\r\nand three other schemes (described in the following para\u0002graphs). Our Intel Core i5-4570 evaluation system (more\r\ndetails in section 5), contains a 32KB first level cache so,\r\ncounter-intuitively, there is a large performance boost once\r\nthe queue is too large for the L1. This is because the threads\r\nevict their own data from their L1, meaning the other thread\r\ngets the cache line from the private L2, which is lower la\u0002tency compared to another core’s L1. Further, the last level\r\ncache is 6MB, so performance drops off when the queue is\r\n8MB or larger because data is evicted to slow main memory.\r\nCache Thrashing.\r\nWhen an application’s working set fits into the cache, the\r\nqueue should avoid evicting it which prevents later cache\r\nmisses and performance loss. This can be achieved by re\u0002placing regular store instructions into the queue with non\u0002temporal stores, as in the Liberty queues [11]. These in\u0002structions write directly to memory, bypassing the caches\r\naltogether and removing the problem. In x86-64 (SSE ex\u00021 void enqueue (queue_t q, long data) {\r\n2 *q->enqPtr = data;\r\n3 q->enqPtr = (q->enqPtr + 8) & ROTATE_MASK;\r\n4 /* Synchronisation */\r\n5 if ((q->enqPtr & SECTION_MASK) == 0) {\r\n6 while (q->enqPtr == q->deqLocalPtr) {\r\n7 q->deqLocalPtr = q->deqSharedPtr;\r\n8 }\r\n9 q->enqSharedPtr = q->enqPtr;\r\n10 }\r\n11 }\r\n12\r\n13 long dequeue(queue_t q) {\r\n14 /* Synchronisation */\r\n15 if ((q->deqPtr & SECTION_MASK) == 0) {\r\n16 q->deqSharedPtr = q->deqPtr;\r\n17 while (q->deqPtr == q->enqLocalPtr) {\r\n18 q->enqLocalPtr = q->enqSharedPtr;\r\n19 }\r\n20 }\r\n21 long data = *((long *)q->deqPtr);\r\n22 q->deqPtr = (q->deqPtr + 8) & ROTATE_MASK);\r\n23 return data;\r\n24 }\r\nListing 1: State-of-the-art multi-section queue [24, 25].\r\n 0\r\n 1\r\n 2\r\n 3\r\n 4\r\n 5\r\n 6\r\n 7\r\n 8\r\n 9\r\n10\r\n16B32B64B128B256B512B1KB2KB4KB8KB16KB32KB64KB128KB256Kb512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nGBytes/s\r\nmov\r\nmovnti\r\nmov-pf-1K\r\nFits in L1\r\nmovnti-pf-1K Fits in L3\r\nFigure 2: Throughput exploration for a state-of-the-art 2-\r\nsection queue on an Intel Core i5-4570 using common mov\r\ninstructions and non-temporal movnti, both with and with\u0002out software prefetching.\r\ntensions) the non-temporal store instruction is the non\u0002sequentially-consistent movnti [4].\r\nThe impact of movnti instructions is shown in figure 2.\r\nIts performance is largely unaffected by the cache size once\r\nthe queue is too large for the L1 because the data to the de\u0002queue thread is fetched straight from main memory. On the\r\nother hand, its performance is limited by the memory band\u0002width available, and so is slower than a normal mov when\r\nthe bandwidth is saturated but the queue fits in the last level\r\ncache (i.e., from 128KB to 4MB in this experiment). This\r\ntrend is also shown in figure 9 (section 5.2). Overall it is up\r\nto the programmer to determine the correct instruction to\r\nuse given their program’s characteristics.\r\nPrefetching.\r\nLiberty queues [11] propose using software prefetch in\u0002structions in the dequeue function code. Figure 2 shows\r\nthe throughput of dequeue with a software prefetching dis\u0002tance of 1024 bytes, labelled mov-pf-1K and movnti-pf-1K.\r\nPrefetching leads to 10% lower peak performance compared\r\nto mov alone, but slightly better performance for queue sizes",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/17ca8e06-de0b-4d7f-a046-95202eb038c7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4acc7890f91aff777039bab4208b673abe17ff54dd0d8d3ebeef609ea8e48d08",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 778
      },
      {
        "segments": [
          {
            "segment_id": "17ca8e06-de0b-4d7f-a046-95202eb038c7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "state2 state3 stateN\r\nPtr : Local Variable stateX\r\nShared Variable\r\n: Infrequently Accessed\r\nSection1\r\nQ\r\nSection2 Section3\r\ndeqPtr enqPtr\r\nSectionN\r\n...\r\n...\r\nstate1\r\nFigure 1: State-of-the-art lock-free multi-section queue.\r\nevaluate in section 5. We then present related work (sec\u0002tion 6) and section 7 concludes.\r\n2. MULTI-SECTION QUEUE FOR\r\nSTREAMING\r\nWe start with an overview of the current state-of-the-art\r\nqueue implementation to provide insights into the overheads.\r\nMulti-section lock-free SP/SC queues are designed for\r\nstreaming, specialised compared to generic SP/SC queues so\r\nthey are tuned for high throughput. The queue is divided\r\ninto sections and only one thread (producer or consumer) is\r\nallowed to access each section at any time. Synchronisation\r\noccurs only at section boundaries. The multi-section queue\r\nis lock-free by design: both producer and consumer access\r\nthe queue simultaneously without locking, provided that\r\nthey do not access the same section [16]. This multi-section\r\ndesign solves many generic SP/SC performance problems,\r\nsuch as cache ping-pong and false sharing [11, 15, 16, 24].\r\nFigure 1 shows an example and listing 1 gives the code.\r\nThe queue shown uses lazy synchronisation [24, 25] for syn\u0002chronising across sections (lines 5 to 10 and 15 to 20). Over\u0002all, an efficient implementation of a multi-section queue must\r\naddress a number of challenges.\r\nQueue Size.\r\nIn general, the larger the queue, the larger the sections and\r\nthe smaller the amount of synchronisation required. How\u0002ever, once the queue is larger than the last level cache, per\u0002formance drops because the data gets invalidated before be\u0002ing read and the dequeue thread must obtain it from main\r\nmemory instead. Figure 2 shows the throughput for increas\u0002ing queue sizes when using mov instructions to write data,\r\nand three other schemes (described in the following para\u0002graphs). Our Intel Core i5-4570 evaluation system (more\r\ndetails in section 5), contains a 32KB first level cache so,\r\ncounter-intuitively, there is a large performance boost once\r\nthe queue is too large for the L1. This is because the threads\r\nevict their own data from their L1, meaning the other thread\r\ngets the cache line from the private L2, which is lower la\u0002tency compared to another core’s L1. Further, the last level\r\ncache is 6MB, so performance drops off when the queue is\r\n8MB or larger because data is evicted to slow main memory.\r\nCache Thrashing.\r\nWhen an application’s working set fits into the cache, the\r\nqueue should avoid evicting it which prevents later cache\r\nmisses and performance loss. This can be achieved by re\u0002placing regular store instructions into the queue with non\u0002temporal stores, as in the Liberty queues [11]. These in\u0002structions write directly to memory, bypassing the caches\r\naltogether and removing the problem. In x86-64 (SSE ex\u00021 void enqueue (queue_t q, long data) {\r\n2 *q->enqPtr = data;\r\n3 q->enqPtr = (q->enqPtr + 8) & ROTATE_MASK;\r\n4 /* Synchronisation */\r\n5 if ((q->enqPtr & SECTION_MASK) == 0) {\r\n6 while (q->enqPtr == q->deqLocalPtr) {\r\n7 q->deqLocalPtr = q->deqSharedPtr;\r\n8 }\r\n9 q->enqSharedPtr = q->enqPtr;\r\n10 }\r\n11 }\r\n12\r\n13 long dequeue(queue_t q) {\r\n14 /* Synchronisation */\r\n15 if ((q->deqPtr & SECTION_MASK) == 0) {\r\n16 q->deqSharedPtr = q->deqPtr;\r\n17 while (q->deqPtr == q->enqLocalPtr) {\r\n18 q->enqLocalPtr = q->enqSharedPtr;\r\n19 }\r\n20 }\r\n21 long data = *((long *)q->deqPtr);\r\n22 q->deqPtr = (q->deqPtr + 8) & ROTATE_MASK);\r\n23 return data;\r\n24 }\r\nListing 1: State-of-the-art multi-section queue [24, 25].\r\n 0\r\n 1\r\n 2\r\n 3\r\n 4\r\n 5\r\n 6\r\n 7\r\n 8\r\n 9\r\n10\r\n16B32B64B128B256B512B1KB2KB4KB8KB16KB32KB64KB128KB256Kb512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nGBytes/s\r\nmov\r\nmovnti\r\nmov-pf-1K\r\nFits in L1\r\nmovnti-pf-1K Fits in L3\r\nFigure 2: Throughput exploration for a state-of-the-art 2-\r\nsection queue on an Intel Core i5-4570 using common mov\r\ninstructions and non-temporal movnti, both with and with\u0002out software prefetching.\r\ntensions) the non-temporal store instruction is the non\u0002sequentially-consistent movnti [4].\r\nThe impact of movnti instructions is shown in figure 2.\r\nIts performance is largely unaffected by the cache size once\r\nthe queue is too large for the L1 because the data to the de\u0002queue thread is fetched straight from main memory. On the\r\nother hand, its performance is limited by the memory band\u0002width available, and so is slower than a normal mov when\r\nthe bandwidth is saturated but the queue fits in the last level\r\ncache (i.e., from 128KB to 4MB in this experiment). This\r\ntrend is also shown in figure 9 (section 5.2). Overall it is up\r\nto the programmer to determine the correct instruction to\r\nuse given their program’s characteristics.\r\nPrefetching.\r\nLiberty queues [11] propose using software prefetch in\u0002structions in the dequeue function code. Figure 2 shows\r\nthe throughput of dequeue with a software prefetching dis\u0002tance of 1024 bytes, labelled mov-pf-1K and movnti-pf-1K.\r\nPrefetching leads to 10% lower peak performance compared\r\nto mov alone, but slightly better performance for queue sizes",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/17ca8e06-de0b-4d7f-a046-95202eb038c7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4acc7890f91aff777039bab4208b673abe17ff54dd0d8d3ebeef609ea8e48d08",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 778
      },
      {
        "segments": [
          {
            "segment_id": "960c40cc-912c-46a8-955b-3bc3a312788f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": " 0\r\n 1\r\n 2\r\n 3\r\n 4\r\n 5\r\n 6\r\n 7\r\n 8\r\n 9\r\n10\r\n8B16B32B64B128B256B512B1KB2KB4KB8KB16KB32KB64KB128KB256KB512KB\r\nGBytes/sec\r\nmov\r\nmovnti\r\nFits in Cache Line\r\nFigure 3: Throughput exploration as we increase the section\r\nsize of a 1MB queue from 8B (128K sections) to 512KB (2\r\nsections) on an Intel Core i5-4570.\r\nlarger than 8MB. The throughput of movnti with prefetch\u0002ing is about 10% lower than using movnti alone because the\r\nprefetch instructions simply add to the already-saturated\r\noff-chip memory bandwidth. Again, the use of software\r\nprefetch is orthogonal to the queue design and left for the\r\nprogrammer to determine.\r\nSection Size.\r\nThe size of each section has an impact on the amount of\r\ndata each thread can push or pop until hitting the section\r\n“owned” by the other thread. Larger sections mean that\r\nsynchronisation is infrequent, but are less efficient at dealing\r\nwith bursty queue usage from either thread. With larger\r\nsections, there is less room between enqueue and dequeue to\r\nabsorb bursts.\r\nFigure 3 shows the throughput of a 1MB queue as we in\u0002crease the section size. For sections smaller than a cache-line\r\n(64 bytes in this case), performance is dominated by false\r\nsharing between enqueue and dequeue threads as they may\r\nboth access the same cache line, even though they are in dif\u0002ferent sections. For larger sections the queue’s performance\r\nis influenced by the overhead of synchronisation, but this be\u0002comes negligible for section sizes of 8KB and higher. When\r\nusing movnti, performance is dominated by the fence in\u0002structions required at the end of each section that maintain\r\ncorrectness (i.e., between the spin-loop and the instruction\r\nthat signals the other thread, in listing 1 line 9), so its per\u0002formance increases steadily with the section size.\r\nSoftware implementation.\r\nAn efficient implementation must ensure that global vari\u0002ables frequently modified by each individual thread, but\r\nrarely read by the other, end up in different cache lines,\r\nin order to avoid false sharing. Therefore the enqPtr and\r\ndeqPtr (listing 1) should be declared in the code with suf\u0002ficient padding, large enough to guarantee that they map to\r\ndifferent cache lines.\r\n3. ANALYSIS OF REMAINING\r\nOVERHEADS\r\nA multi-section lock-free queue with infrequent accesses to\r\nthe queue’s shared synchronisation control variables is the\r\nstate-of-the-art design. The performance bottleneck of this\r\nqueue is no longer false sharing or inter-core communica\u00021 lea rax, [rdx+8] ;Increment pointer\r\n2 mov QWORD PTR [rdx], rcx ;Store to queue\r\n3 mov rdx, rax ;Compiler’s copy\r\n4 and rdx, ROTATE_MASK ;Rotate pointer\r\n5 test eax, SECTION_MASK ;End of section\r\n6 jne .L2 ;Skip sync code\r\nListing 2: Multi-section critical path in x86-64 assembly.\r\ntion of the control variables for synchronisation, but rather\r\nthe boilerplate code for the enqueue and dequeue opera\u0002tions which is responsible for checking whether the thread\r\nhas reached the end of the current section and moving the\r\nthread’s pointer back to the beginning of the queue once it\r\nreaches the end. This boilerplate code becomes a signifi\u0002cant overhead in the context of frequent fine-grained queue\r\ntransfers. This section studies these overheads, motivating\r\nthe need for a new queue implementation.\r\n3.1 Critical Path Code\r\nListing 2 shows the most frequently-executed code for an\r\nenqueue in the multi-section queue (dequeue is very similar).\r\nThis code was generated by the GCC-4.8.2 compiler [1] and\r\nis x86-64 assembly (Intel’s dialect where the output is the\r\nleftmost operand). The rest of the code (not shown here)\r\nis 10 instructions long and has non-trivial control-flow (that\r\nincludes the spin-loop).\r\nThe first two instructions are fundamental to the opera\u0002tion of enqueue (mov and lea, lines 1 and 2). They incre\u0002ment the queue pointer to the next location and store the\r\ndata into the queue (into the old location—note the desti\u0002nation rax in line 1 but source rdx in line 2). The following\r\nand instruction (line 4) performs rotation of the index, once\r\nit reaches the end of the queue. This can be done in one\r\ninstruction with an AND mask because the queue size is a\r\npower of 2 and is aligned in memory at a multiple of the\r\nqueue size. Next, test (line 5) checks whether the thread\r\nis at the end of the section and sets the flag for the con\u0002ditional jump that follows. The end of section is critical\r\nfor the queue as it is the point where synchronisation hap\u0002pens. The jne (line 6) will fall through if the index is at\r\nthe end of the section to execute the synchronisation code\r\n(not shown). This happens rarely, given that the section\r\nis large, so synchronisation is not on the critical path. In\r\nthe common case, where the pointer is not at the end of\r\nthe section, the jne will skip synchronisation and continue\r\nexecuting the application code after the enqueue.\r\nThe instruction in line 3 (mov rdx, rax) is a copy for\r\nperformance optimisation (created by both GCC [1] and\r\nLLVM [3]). It allows the instructions in lines 1 and 2 to\r\nexecute in parallel, as well as those in lines 4 and 5. With\u0002out this copy there would be one fewer instruction, but\r\nonly lines 1 and 2 could execute in parallel, meaning less\r\ninstruction-level parallelism (and they would have to exe\u0002cute in a different order: mov, lea, test, jne, and).\r\n3.2 Performance Enhancements\r\nExamination of the enqueue instructions shows that the\r\nlast three (rotating the pointer, checking for the end of a\r\nsection, and skipping the synchronisation code) are there to\r\nperform infrequent actions. If we had alternative mecha\u0002nisms to perform these tasks only when needed, we would\r\nreduce the enqueue instructions down to the absolute mini-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/960c40cc-912c-46a8-955b-3bc3a312788f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=48fdb4bc24023a51155f67e6c4af013890af07f279fda3da617bea769054c66f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 921
      },
      {
        "segments": [
          {
            "segment_id": "960c40cc-912c-46a8-955b-3bc3a312788f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": " 0\r\n 1\r\n 2\r\n 3\r\n 4\r\n 5\r\n 6\r\n 7\r\n 8\r\n 9\r\n10\r\n8B16B32B64B128B256B512B1KB2KB4KB8KB16KB32KB64KB128KB256KB512KB\r\nGBytes/sec\r\nmov\r\nmovnti\r\nFits in Cache Line\r\nFigure 3: Throughput exploration as we increase the section\r\nsize of a 1MB queue from 8B (128K sections) to 512KB (2\r\nsections) on an Intel Core i5-4570.\r\nlarger than 8MB. The throughput of movnti with prefetch\u0002ing is about 10% lower than using movnti alone because the\r\nprefetch instructions simply add to the already-saturated\r\noff-chip memory bandwidth. Again, the use of software\r\nprefetch is orthogonal to the queue design and left for the\r\nprogrammer to determine.\r\nSection Size.\r\nThe size of each section has an impact on the amount of\r\ndata each thread can push or pop until hitting the section\r\n“owned” by the other thread. Larger sections mean that\r\nsynchronisation is infrequent, but are less efficient at dealing\r\nwith bursty queue usage from either thread. With larger\r\nsections, there is less room between enqueue and dequeue to\r\nabsorb bursts.\r\nFigure 3 shows the throughput of a 1MB queue as we in\u0002crease the section size. For sections smaller than a cache-line\r\n(64 bytes in this case), performance is dominated by false\r\nsharing between enqueue and dequeue threads as they may\r\nboth access the same cache line, even though they are in dif\u0002ferent sections. For larger sections the queue’s performance\r\nis influenced by the overhead of synchronisation, but this be\u0002comes negligible for section sizes of 8KB and higher. When\r\nusing movnti, performance is dominated by the fence in\u0002structions required at the end of each section that maintain\r\ncorrectness (i.e., between the spin-loop and the instruction\r\nthat signals the other thread, in listing 1 line 9), so its per\u0002formance increases steadily with the section size.\r\nSoftware implementation.\r\nAn efficient implementation must ensure that global vari\u0002ables frequently modified by each individual thread, but\r\nrarely read by the other, end up in different cache lines,\r\nin order to avoid false sharing. Therefore the enqPtr and\r\ndeqPtr (listing 1) should be declared in the code with suf\u0002ficient padding, large enough to guarantee that they map to\r\ndifferent cache lines.\r\n3. ANALYSIS OF REMAINING\r\nOVERHEADS\r\nA multi-section lock-free queue with infrequent accesses to\r\nthe queue’s shared synchronisation control variables is the\r\nstate-of-the-art design. The performance bottleneck of this\r\nqueue is no longer false sharing or inter-core communica\u00021 lea rax, [rdx+8] ;Increment pointer\r\n2 mov QWORD PTR [rdx], rcx ;Store to queue\r\n3 mov rdx, rax ;Compiler’s copy\r\n4 and rdx, ROTATE_MASK ;Rotate pointer\r\n5 test eax, SECTION_MASK ;End of section\r\n6 jne .L2 ;Skip sync code\r\nListing 2: Multi-section critical path in x86-64 assembly.\r\ntion of the control variables for synchronisation, but rather\r\nthe boilerplate code for the enqueue and dequeue opera\u0002tions which is responsible for checking whether the thread\r\nhas reached the end of the current section and moving the\r\nthread’s pointer back to the beginning of the queue once it\r\nreaches the end. This boilerplate code becomes a signifi\u0002cant overhead in the context of frequent fine-grained queue\r\ntransfers. This section studies these overheads, motivating\r\nthe need for a new queue implementation.\r\n3.1 Critical Path Code\r\nListing 2 shows the most frequently-executed code for an\r\nenqueue in the multi-section queue (dequeue is very similar).\r\nThis code was generated by the GCC-4.8.2 compiler [1] and\r\nis x86-64 assembly (Intel’s dialect where the output is the\r\nleftmost operand). The rest of the code (not shown here)\r\nis 10 instructions long and has non-trivial control-flow (that\r\nincludes the spin-loop).\r\nThe first two instructions are fundamental to the opera\u0002tion of enqueue (mov and lea, lines 1 and 2). They incre\u0002ment the queue pointer to the next location and store the\r\ndata into the queue (into the old location—note the desti\u0002nation rax in line 1 but source rdx in line 2). The following\r\nand instruction (line 4) performs rotation of the index, once\r\nit reaches the end of the queue. This can be done in one\r\ninstruction with an AND mask because the queue size is a\r\npower of 2 and is aligned in memory at a multiple of the\r\nqueue size. Next, test (line 5) checks whether the thread\r\nis at the end of the section and sets the flag for the con\u0002ditional jump that follows. The end of section is critical\r\nfor the queue as it is the point where synchronisation hap\u0002pens. The jne (line 6) will fall through if the index is at\r\nthe end of the section to execute the synchronisation code\r\n(not shown). This happens rarely, given that the section\r\nis large, so synchronisation is not on the critical path. In\r\nthe common case, where the pointer is not at the end of\r\nthe section, the jne will skip synchronisation and continue\r\nexecuting the application code after the enqueue.\r\nThe instruction in line 3 (mov rdx, rax) is a copy for\r\nperformance optimisation (created by both GCC [1] and\r\nLLVM [3]). It allows the instructions in lines 1 and 2 to\r\nexecute in parallel, as well as those in lines 4 and 5. With\u0002out this copy there would be one fewer instruction, but\r\nonly lines 1 and 2 could execute in parallel, meaning less\r\ninstruction-level parallelism (and they would have to exe\u0002cute in a different order: mov, lea, test, jne, and).\r\n3.2 Performance Enhancements\r\nExamination of the enqueue instructions shows that the\r\nlast three (rotating the pointer, checking for the end of a\r\nsection, and skipping the synchronisation code) are there to\r\nperform infrequent actions. If we had alternative mecha\u0002nisms to perform these tasks only when needed, we would\r\nreduce the enqueue instructions down to the absolute mini-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/960c40cc-912c-46a8-955b-3bc3a312788f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=48fdb4bc24023a51155f67e6c4af013890af07f279fda3da617bea769054c66f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 921
      },
      {
        "segments": [
          {
            "segment_id": "efd77e5b-11cd-41d1-85f1-311000dab554",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1.0\r\n1.2\r\n1.4\r\nMSQ NR NS NSNR\r\nExecution Speedup\r\nFigure 4: Removing index rotation (NR), synchronisation\r\n(NS) and both (NRNS) from the enqueue function in a\r\n256MB queue on an Intel Core i5-4570.\r\nmum: just the store and the increment of the index.\r\nTo quantify the benefits of removing these overheads we\r\nmeasured the performance of a hypothetical queue with these\r\ninfrequently-required instructions removed from the code1.\r\nWe measured the execution time of the baseline state-of-the\u0002art queue [24] (“MSQ”) and three overhead-removing opti\u0002misations. It is important to note that for these experiments\r\nwe simply removed the corresponding assembly instructions\r\nfrom the code. Therefore, they do not account for any opti\u0002misations that the compiler could perform on the code with\r\nthese enhancements.\r\nRemoving Pointer Rotation Overhead (NR).\r\nThe first optimisation is to remove the pointer rotation\r\n(listing 2 line 4). This is only required once for each traversal\r\nthrough the queue (that is in the order of once per hundreds\r\nof thousands of executions), once the pointer reaches the\r\nend. At all other times it has no effect on the pointer vari\u0002able. The NR bar in figure 4 shows the performance of en\u0002queue when removing this instruction, indicating that there\r\nis 7% performance improvement available over the original\r\nqueue.\r\nRemoving Synchronisation Overhead (NS).\r\nAvoiding the synchronisation overhead is critical for per\u0002formance. It not only means that we can remove two in\u0002structions from the critical path (listing 2 lines 5 and 6),\r\nbut it also means that we can remove the body of the syn\u0002chronisation code. This acts as an optimisation barrier for\r\nboth the compiler and the architecture as it comprises of\r\n9 assembly instructions in 4 basic blocks (including a spin\u0002loop). According to Jablin et al. [11], the compiler will not\r\nperform efficient code movement across the spin-loop as it\r\nhas no guarantee that the spin-loop will halt. Another ex\u0002ample of a simple compiler optimisation that is not applied\r\ndue to this code is loop unrolling. Figure 4 shows the per\u0002formance of removing this code from enqueue (the NS bar),\r\nwhich is approximately 17% faster than the full queue.\r\nRemoving Both Overheads (NSNR).\r\nRemoving both synchronisation and pointer rotation over\u0002heads leads to even better performance. The NSNR bar\r\n1For this experiment the queue size was set to 256MB, the\r\ndequeue function was disabled and the workload was a loop\r\npushing the 64-bit loop index into the queue until it gets\r\nfull.\r\n\r\n\r\n\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n\r\n\r\n\r\n\u0001\u0001\r\n\u0001\u0001\r\n\u0001\u0001\r\nfixed\r\nQueue\r\nEND\r\nof deqPtr enqPtr\r\nSection 1 SS Section 2 SS PR\r\n: Free space : Written data\r\nSS\r\nPR : Pointer Rotation Red−Zone. Fixed At End.\r\n: Section Synchronisation Red−Zone. Moves Left.\r\nFigure 5: Memory layout in Lynx.\r\nin figure 4 indicates that the performance improvements of\r\nboth optimisations aid each other, meaning there is 27%\r\nspeed-up available if we can remove these instructions.\r\n3.3 Summary\r\nAnalysis of the enqueue operation shows that there are 4\r\ninstructions that are only required infrequently, for rotating\r\nthe queue pointer and checking for the end of a section. The\r\nresults in figure 4 show that removing these infrequently\u0002required instructions can lead to speed-ups of 27%. The\r\nnext section shows how we can build a queue that does this.\r\n4. LYNX\r\nLynx is a radically different queue design that reduces the\r\noverheads of enqueue and dequeue actions to a minimum\r\nby removing instructions that perform infrequently-required\r\noperations. The novelty resides in a combination of hard\u0002ware and operating system support, using memory access\r\nviolations to deal with uncommon events in a specialised sig\u0002nal handler. We first explain Lynx’s memory layout, then\r\nshow the C code to access the queue, and finally describe\r\nhow signals are handled to synchronise the threads using\r\nthe queue and to move from the end of the queue back to\r\nthe beginning. Lynx is architecture and operating system\r\nindependent, but we evaluate it in section 5 on x86-64 sys\u0002tems running Linux.\r\n4.1 Memory Layout\r\nAn overview of the memory layout in Lynx is shown in\r\nfigure 5. The queue is aligned on a page boundary and\r\nis split into sections by red-zones (red-filled boxes). Each\r\nred-zone is one page-size long (usually 4KB) and is marked\r\nas non-readable and non-writable (e.g., using mprotect()\r\non Unix-like systems or VirtualProtect() on Windows).\r\nTherefore, whenever a thread attempts to access data in a\r\nred-zone, the processor triggers an interrupt and an operat\u0002ing system exception is raised.\r\nThere are two types of red-zone. Section synchronisa\u0002tion red-zones (SSRZs) deal with thread synchronisation for\r\neach section and a pointer rotation red-zone (PRRZ) en\u0002ables threads to move back to the start of the queue once\r\nthey reach the end. Both are explained in more detail in sub\u0002sequent sections. Figure 5 shows a queue with two sections,\r\nhence it has two SSRZs and one PRRZ.\r\nRed-zones are core components of Lynx. Their purpose is\r\nto allow the removal of non-essential code from the enqueue\r\nand dequeue operations, replacing instructions in the critical\r\npath of an application with those in a specialised signal han\u0002dler which is called infrequently. This means that code to",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/efd77e5b-11cd-41d1-85f1-311000dab554.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=053f516e15dae63b2263c37fc85a9da42593171601fb29124e18b37857ff3c88",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 846
      },
      {
        "segments": [
          {
            "segment_id": "efd77e5b-11cd-41d1-85f1-311000dab554",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1.0\r\n1.2\r\n1.4\r\nMSQ NR NS NSNR\r\nExecution Speedup\r\nFigure 4: Removing index rotation (NR), synchronisation\r\n(NS) and both (NRNS) from the enqueue function in a\r\n256MB queue on an Intel Core i5-4570.\r\nmum: just the store and the increment of the index.\r\nTo quantify the benefits of removing these overheads we\r\nmeasured the performance of a hypothetical queue with these\r\ninfrequently-required instructions removed from the code1.\r\nWe measured the execution time of the baseline state-of-the\u0002art queue [24] (“MSQ”) and three overhead-removing opti\u0002misations. It is important to note that for these experiments\r\nwe simply removed the corresponding assembly instructions\r\nfrom the code. Therefore, they do not account for any opti\u0002misations that the compiler could perform on the code with\r\nthese enhancements.\r\nRemoving Pointer Rotation Overhead (NR).\r\nThe first optimisation is to remove the pointer rotation\r\n(listing 2 line 4). This is only required once for each traversal\r\nthrough the queue (that is in the order of once per hundreds\r\nof thousands of executions), once the pointer reaches the\r\nend. At all other times it has no effect on the pointer vari\u0002able. The NR bar in figure 4 shows the performance of en\u0002queue when removing this instruction, indicating that there\r\nis 7% performance improvement available over the original\r\nqueue.\r\nRemoving Synchronisation Overhead (NS).\r\nAvoiding the synchronisation overhead is critical for per\u0002formance. It not only means that we can remove two in\u0002structions from the critical path (listing 2 lines 5 and 6),\r\nbut it also means that we can remove the body of the syn\u0002chronisation code. This acts as an optimisation barrier for\r\nboth the compiler and the architecture as it comprises of\r\n9 assembly instructions in 4 basic blocks (including a spin\u0002loop). According to Jablin et al. [11], the compiler will not\r\nperform efficient code movement across the spin-loop as it\r\nhas no guarantee that the spin-loop will halt. Another ex\u0002ample of a simple compiler optimisation that is not applied\r\ndue to this code is loop unrolling. Figure 4 shows the per\u0002formance of removing this code from enqueue (the NS bar),\r\nwhich is approximately 17% faster than the full queue.\r\nRemoving Both Overheads (NSNR).\r\nRemoving both synchronisation and pointer rotation over\u0002heads leads to even better performance. The NSNR bar\r\n1For this experiment the queue size was set to 256MB, the\r\ndequeue function was disabled and the workload was a loop\r\npushing the 64-bit loop index into the queue until it gets\r\nfull.\r\n\r\n\r\n\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n\r\n\r\n\r\n\u0001\u0001\r\n\u0001\u0001\r\n\u0001\u0001\r\nfixed\r\nQueue\r\nEND\r\nof deqPtr enqPtr\r\nSection 1 SS Section 2 SS PR\r\n: Free space : Written data\r\nSS\r\nPR : Pointer Rotation Red−Zone. Fixed At End.\r\n: Section Synchronisation Red−Zone. Moves Left.\r\nFigure 5: Memory layout in Lynx.\r\nin figure 4 indicates that the performance improvements of\r\nboth optimisations aid each other, meaning there is 27%\r\nspeed-up available if we can remove these instructions.\r\n3.3 Summary\r\nAnalysis of the enqueue operation shows that there are 4\r\ninstructions that are only required infrequently, for rotating\r\nthe queue pointer and checking for the end of a section. The\r\nresults in figure 4 show that removing these infrequently\u0002required instructions can lead to speed-ups of 27%. The\r\nnext section shows how we can build a queue that does this.\r\n4. LYNX\r\nLynx is a radically different queue design that reduces the\r\noverheads of enqueue and dequeue actions to a minimum\r\nby removing instructions that perform infrequently-required\r\noperations. The novelty resides in a combination of hard\u0002ware and operating system support, using memory access\r\nviolations to deal with uncommon events in a specialised sig\u0002nal handler. We first explain Lynx’s memory layout, then\r\nshow the C code to access the queue, and finally describe\r\nhow signals are handled to synchronise the threads using\r\nthe queue and to move from the end of the queue back to\r\nthe beginning. Lynx is architecture and operating system\r\nindependent, but we evaluate it in section 5 on x86-64 sys\u0002tems running Linux.\r\n4.1 Memory Layout\r\nAn overview of the memory layout in Lynx is shown in\r\nfigure 5. The queue is aligned on a page boundary and\r\nis split into sections by red-zones (red-filled boxes). Each\r\nred-zone is one page-size long (usually 4KB) and is marked\r\nas non-readable and non-writable (e.g., using mprotect()\r\non Unix-like systems or VirtualProtect() on Windows).\r\nTherefore, whenever a thread attempts to access data in a\r\nred-zone, the processor triggers an interrupt and an operat\u0002ing system exception is raised.\r\nThere are two types of red-zone. Section synchronisa\u0002tion red-zones (SSRZs) deal with thread synchronisation for\r\neach section and a pointer rotation red-zone (PRRZ) en\u0002ables threads to move back to the start of the queue once\r\nthey reach the end. Both are explained in more detail in sub\u0002sequent sections. Figure 5 shows a queue with two sections,\r\nhence it has two SSRZs and one PRRZ.\r\nRed-zones are core components of Lynx. Their purpose is\r\nto allow the removal of non-essential code from the enqueue\r\nand dequeue operations, replacing instructions in the critical\r\npath of an application with those in a specialised signal han\u0002dler which is called infrequently. This means that code to",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/efd77e5b-11cd-41d1-85f1-311000dab554.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=053f516e15dae63b2263c37fc85a9da42593171601fb29124e18b37857ff3c88",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 846
      },
      {
        "segments": [
          {
            "segment_id": "4506d11a-6697-4fd8-bbc0-474a45862d06",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "\u0001\u0001\r\n\u0001\u0001\u0001\u0001\r\nSS SS PR\r\nPtr1 Ptr2\r\nSection 1 Section 2 Sec1\r\nSS\r\n(a) Ptr1 hits SSRZ; Ptr2 in spin-loop before section1.\r\n\r\n\r\n\u0001\u0001\r\n\u0001\u0001\u0001\u0001\r\nSS SS SS PR\r\nPtr1 Ptr2 New SS\r\nSS\r\n(b) New SSRZ created.\r\n\r\n\r\n\u0001\u0001\r\nSS SS PR SS \u0001\u0001\r\nPtr1 signal Ptr2\r\n(c) At signal, Ptr2 deletes old SSRZ, enters section1.\r\n\r\n\r\n\u0001\u0001 SS PR SS \u0001\u0001\r\nPtr1 Ptr2\r\n(d) Ptr1 deletes old SSRZ and enters section2.\r\nFigure 6: Moving a Section Synch. Red-Zone (SSRZ).\r\nmove from the end of the queue to the start, to find the next\r\nqueue section, to wait until the next section becomes free,\r\nand to signal the other thread that the old section is avail\u0002able can all be moved out of enqueue and dequeue functions\r\nand placed in the signal handler.\r\n4.1.1 Section Synchronisation Red-Zones (SSRZs)\r\nThe first type of red-zone is used for synchronising threads\r\nbetween sections. These are to ensure that a thread only\r\nenters a queue section when the other thread has left, en\u0002suring that a maximum of one thread occupies any section\r\nat any time. The SSRZs are not fixed, but move through\r\nthe queue towards the front as the threads move between\r\nsections. Once they get to the beginning of the queue, they\r\nwrap around to the back and move towards the front again.\r\nMoving these red-zones means that instructions do not get\r\ntrapped in a red-zone and the signal handler can operate in\u0002dependently; once it is finished, the queue access instruction\r\nwill be re-executed (at the same address as before) and will\r\nsucceed so execution can continue.\r\nFigure 6 shows the details of how red-zone movement and\r\nsynchronisation are performed. In figure 6a a thread accesses\r\nthe queue (in this case to perform an enqueue operation) and\r\nuses an address in the SSRZ at the end of section 1 (Ptr1).\r\nThis causes a hardware interrupt from an access violation,\r\nsince the red-zone is non-readable and non-writable, and, in\r\nresponse, the OS kernel calls the queue signal handler. The\r\nfirst thing the signal handler does is to create a new red-zone\r\nimmediately to the left of the current one (figure 6b). The\r\nthread then signals to the other thread that it has left section\r\n1, meaning that the other thread (Ptr2) can safely access\r\nsection 1 (to read from the queue), shown in figure 6c. The\r\nfirst thread spin-waits for section 2 to become free, which\r\nin this example happens immediately, then it deletes the\r\noriginal red-zone and leaves the signal handler (figure 6d).\r\nThe original instruction is then re-executed and, since it is\r\nnow accessing an address inside section 2, rather than an\r\nSSRZ, it will succeed and the application can continue.\r\nThe SSRZs never meet each other because they always\r\nmove in the same direction, always move when they are hit,\r\n\r\n\r\n\r\n\u0001\u0001\r\n\u0001\u0001\r\n\u0001\u0001\r\nSS SS PR\r\nPtr\r\n(a) Ptr hits PRRZ.\r\n\r\n\r\n\u0001\u0001\r\nSS PR SS \u0001\u0001\r\nPtr\r\n(b) Ptr updated to point to the beginning of the queue.\r\nFigure 7: Rotating the pointer when the Pointer Rotation\r\nRed-Zone (PRRZ) is hit.\r\nand they move by the same amount when a thread reaches\r\nthem. When a red-zone gets moved towards its predecessor,\r\nits predecessor has already been moved by the same thread,\r\nmeaning that the SSRZs are always a fixed distance from\r\ntheir neighbours ± one page-size. Additionally, we size the\r\nqueue so that each section is at least two pages long (in\u0002cluding an SSRZ), so that there is always room to move the\r\nSSRZs without overlapping another red-zone.\r\nNote that pushed data is never over-written when moving\r\nthe SSRZ because the operation only alters the access per\u0002mission bits of the memory page. When moving a red-zone,\r\nthe enqueue thread will place the new one over the top of\r\nthe data it has just written into the queue. However, when\r\nthe dequeue thread hits that red-zone, it will move it again\r\nand, since the access address remains the same, once the\r\nsignal handler has finished it will be able to read the data\r\nout of the queue. The only difference is that this page of\r\nmemory it reads from will be in a later section of the queue\r\nto the one it was written into, but this has no effect on the\r\nqueue’s operation.\r\n4.1.2 Pointer Rotation Red-Zone (PRRZ)\r\nAs previously mentioned in section 3.2, pointer rotation\r\nis another performance overhead for enqueue and dequeue\r\nthat is on the critical path. Even in a highly optimised\r\nqueue with a power-of-two size and aligned start address, it\r\nis still one instruction in the critical path of execution that\r\ndoes not alter the pointer for the vast majority of accesses.\r\nLynx optimises this away through an additional red-zone\r\njust after the end of the queue. Unlike the SSRZs, this red\u0002zone is fixed and is unique, serving only to rotate the access\r\npointer back to the start of the queue when it reaches the\r\nend.\r\nFigure 7 shows how this occurs. In figure 7a the enqueue\r\nthread accesses the queue just past the end, hitting in the\r\nPRRZ. The hardware interrupt again results in the OS call\u0002ing the signal handler, which alters the address that the\r\ninstruction is trying to access back to the start of the queue.\r\nSection 4.3 describes how this is performed. The result is\r\nshown in figure 7b which is the state of the queue once\r\nthe signal handler finishes. The thread now proceeds to\r\nre-execute its access, and this time it will succeed because\r\nit will write at the start of the queue in the continuation of\r\nsection 1.\r\n4.2 User Code\r\nC code for the enqueue and dequeue functions is shown in\r\nlisting 3 using inline assembly for the x86-64 architecture.\r\nInline assembly code is required for reading and writing to\r\nthe queue so that we have control over the instruction and",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/4506d11a-6697-4fd8-bbc0-474a45862d06.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=95bff5fb2dff024a93d67bd520f5748a0c433ff4e3c3053783249104d04d340f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 971
      },
      {
        "segments": [
          {
            "segment_id": "4506d11a-6697-4fd8-bbc0-474a45862d06",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "\u0001\u0001\r\n\u0001\u0001\u0001\u0001\r\nSS SS PR\r\nPtr1 Ptr2\r\nSection 1 Section 2 Sec1\r\nSS\r\n(a) Ptr1 hits SSRZ; Ptr2 in spin-loop before section1.\r\n\r\n\r\n\u0001\u0001\r\n\u0001\u0001\u0001\u0001\r\nSS SS SS PR\r\nPtr1 Ptr2 New SS\r\nSS\r\n(b) New SSRZ created.\r\n\r\n\r\n\u0001\u0001\r\nSS SS PR SS \u0001\u0001\r\nPtr1 signal Ptr2\r\n(c) At signal, Ptr2 deletes old SSRZ, enters section1.\r\n\r\n\r\n\u0001\u0001 SS PR SS \u0001\u0001\r\nPtr1 Ptr2\r\n(d) Ptr1 deletes old SSRZ and enters section2.\r\nFigure 6: Moving a Section Synch. Red-Zone (SSRZ).\r\nmove from the end of the queue to the start, to find the next\r\nqueue section, to wait until the next section becomes free,\r\nand to signal the other thread that the old section is avail\u0002able can all be moved out of enqueue and dequeue functions\r\nand placed in the signal handler.\r\n4.1.1 Section Synchronisation Red-Zones (SSRZs)\r\nThe first type of red-zone is used for synchronising threads\r\nbetween sections. These are to ensure that a thread only\r\nenters a queue section when the other thread has left, en\u0002suring that a maximum of one thread occupies any section\r\nat any time. The SSRZs are not fixed, but move through\r\nthe queue towards the front as the threads move between\r\nsections. Once they get to the beginning of the queue, they\r\nwrap around to the back and move towards the front again.\r\nMoving these red-zones means that instructions do not get\r\ntrapped in a red-zone and the signal handler can operate in\u0002dependently; once it is finished, the queue access instruction\r\nwill be re-executed (at the same address as before) and will\r\nsucceed so execution can continue.\r\nFigure 6 shows the details of how red-zone movement and\r\nsynchronisation are performed. In figure 6a a thread accesses\r\nthe queue (in this case to perform an enqueue operation) and\r\nuses an address in the SSRZ at the end of section 1 (Ptr1).\r\nThis causes a hardware interrupt from an access violation,\r\nsince the red-zone is non-readable and non-writable, and, in\r\nresponse, the OS kernel calls the queue signal handler. The\r\nfirst thing the signal handler does is to create a new red-zone\r\nimmediately to the left of the current one (figure 6b). The\r\nthread then signals to the other thread that it has left section\r\n1, meaning that the other thread (Ptr2) can safely access\r\nsection 1 (to read from the queue), shown in figure 6c. The\r\nfirst thread spin-waits for section 2 to become free, which\r\nin this example happens immediately, then it deletes the\r\noriginal red-zone and leaves the signal handler (figure 6d).\r\nThe original instruction is then re-executed and, since it is\r\nnow accessing an address inside section 2, rather than an\r\nSSRZ, it will succeed and the application can continue.\r\nThe SSRZs never meet each other because they always\r\nmove in the same direction, always move when they are hit,\r\n\r\n\r\n\r\n\u0001\u0001\r\n\u0001\u0001\r\n\u0001\u0001\r\nSS SS PR\r\nPtr\r\n(a) Ptr hits PRRZ.\r\n\r\n\r\n\u0001\u0001\r\nSS PR SS \u0001\u0001\r\nPtr\r\n(b) Ptr updated to point to the beginning of the queue.\r\nFigure 7: Rotating the pointer when the Pointer Rotation\r\nRed-Zone (PRRZ) is hit.\r\nand they move by the same amount when a thread reaches\r\nthem. When a red-zone gets moved towards its predecessor,\r\nits predecessor has already been moved by the same thread,\r\nmeaning that the SSRZs are always a fixed distance from\r\ntheir neighbours ± one page-size. Additionally, we size the\r\nqueue so that each section is at least two pages long (in\u0002cluding an SSRZ), so that there is always room to move the\r\nSSRZs without overlapping another red-zone.\r\nNote that pushed data is never over-written when moving\r\nthe SSRZ because the operation only alters the access per\u0002mission bits of the memory page. When moving a red-zone,\r\nthe enqueue thread will place the new one over the top of\r\nthe data it has just written into the queue. However, when\r\nthe dequeue thread hits that red-zone, it will move it again\r\nand, since the access address remains the same, once the\r\nsignal handler has finished it will be able to read the data\r\nout of the queue. The only difference is that this page of\r\nmemory it reads from will be in a later section of the queue\r\nto the one it was written into, but this has no effect on the\r\nqueue’s operation.\r\n4.1.2 Pointer Rotation Red-Zone (PRRZ)\r\nAs previously mentioned in section 3.2, pointer rotation\r\nis another performance overhead for enqueue and dequeue\r\nthat is on the critical path. Even in a highly optimised\r\nqueue with a power-of-two size and aligned start address, it\r\nis still one instruction in the critical path of execution that\r\ndoes not alter the pointer for the vast majority of accesses.\r\nLynx optimises this away through an additional red-zone\r\njust after the end of the queue. Unlike the SSRZs, this red\u0002zone is fixed and is unique, serving only to rotate the access\r\npointer back to the start of the queue when it reaches the\r\nend.\r\nFigure 7 shows how this occurs. In figure 7a the enqueue\r\nthread accesses the queue just past the end, hitting in the\r\nPRRZ. The hardware interrupt again results in the OS call\u0002ing the signal handler, which alters the address that the\r\ninstruction is trying to access back to the start of the queue.\r\nSection 4.3 describes how this is performed. The result is\r\nshown in figure 7b which is the state of the queue once\r\nthe signal handler finishes. The thread now proceeds to\r\nre-execute its access, and this time it will succeed because\r\nit will write at the start of the queue in the continuation of\r\nsection 1.\r\n4.2 User Code\r\nC code for the enqueue and dequeue functions is shown in\r\nlisting 3 using inline assembly for the x86-64 architecture.\r\nInline assembly code is required for reading and writing to\r\nthe queue so that we have control over the instruction and",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/4506d11a-6697-4fd8-bbc0-474a45862d06.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=95bff5fb2dff024a93d67bd520f5748a0c433ff4e3c3053783249104d04d340f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 971
      },
      {
        "segments": [
          {
            "segment_id": "503d7eed-e562-4ecf-a72f-fd8f9697111a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "1 void enqueue (queue_t q, long data) {\r\n2 asm(\"movq %0, (%1, %2)\"\r\n3 : /* no output */\r\n4 : \"r\" (data), /* input %0 */\r\n5 \"r\" (q->enqBase), /* input %1 */\r\n6 \"r\" (q->enqIdx) /* input %2 */\r\n7 : \"1\" ); /* clobber */\r\n8 q->enqIdx += sizeof(long);\r\n9 }\r\n10\r\n11 long dequeue(queue_t q) {\r\n12 long data;\r\n13 asm(\"movq (%1, %2), %0\"\r\n14 : \"=&r\" (data) /* output */\r\n15 : \"r\" (q->deqBase), /* input %1 */\r\n16 \"r\" (q->deqIdx) /* input %2 */\r\n17 : \"1\" ); /* clobber */\r\n18 deqIdx += sizeof(long);\r\n19 return data;\r\n20 }\r\nListing 3: Implementation of Lynx.\r\n\r\n\r\n\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n2.\r\n1. Get Pointer That Triggered Exception \r\nGet Type Of Red−Zone\r\nSSRZ PRRZ\r\nCalculate New Index\r\nSet Register 4. 5. RaiseMove SSRZ Exception\r\nNot in RZ\r\n3.\r\nResume Execution\r\nLeave Handler 6.\r\nSynchronise\r\nFigure 8: Overview of the operation of Lynx’s handler.\r\noperands used, which is vital to enable the PRRZ to func\u0002tion correctly. Section 4.3.2 describes this in more detail.\r\nAn implementation of a subset of Lynx without the PRRZ\r\npointer rotation functionality would not require assembly\r\ncoding and could be implemented purely in C.\r\nOnce compiled, the final assembly code for Lynx’s enqueue\r\nand dequeue functions consists of just two x86-64 instruc\u0002tions each: a memory instruction (for storing or loading from\r\nthe queue) and an addition that increments the pointer. As\r\nwith state-of-the-art queues, the compiler fully inlines this\r\ncode. However, in contrast to the state-of-the-art, there is\r\nno complicated control flow (and no spinning loop) which\r\nallows the code to get heavily optimised by the compiler.\r\n4.3 Lynx’s Exception Handler\r\nThe Lynx exception handler is called whenever a thread\r\nattempts to access one of the queue’s red-zones. As ex\u0002plained in sections 4.1.1 and 4.1.2, the handler’s actions de\u0002pend on the type of red-zone being accessed. An overview\r\nof the handler is shown in figure 8.\r\nStep 1. The first task is to get the address which caused\r\nthe exception, so that it can take appropriate actions\r\nbased on the type of red-zone being accessed (if any).\r\nThis is available through the arguments to the signal\r\nhandler (specifically the si_addr field in the POSIX\r\nsiginfo_t structure).\r\nStep 2. Using the address, the type of red-zone can be\r\ndetermined and acted upon.\r\n1 enqSync() {\r\n2 newRedzone = getNewRedzoneLeft(currPtr);\r\n3 configRedZone (ON, newRedzone);\r\n4 redzone = newRedzone;\r\n5 prevSectionState = ENQ_DONE;\r\n6 /* Spin-loop */\r\n7 while (nextSectionState != DEQ_DONE) ;\r\n8 configRedZone (OFF, currPtr);\r\n9 *nextSectionState = ENQ_WRITES;\r\n10 }\r\n11\r\n12 deqSync() {\r\n13 newRedzone = getNewRedzoneLeft(currPtr);\r\n14 configRedZone (ON, newRedzone);\r\n15 redzone = newRedzone;\r\n16 if (prevSectionState != ENQ_EXITED)\r\n17 prevSectionState = DEQ_DONE;\r\n18 /* Spin-loop */\r\n19 while (nextSectionState != ENQ_DONE\r\n20 && nextSectionState != ENQ_EXITED) ;\r\n21 configRedZone (OFF, currPtr);\r\n22 nextSectionState = DEQ_READS;\r\n23 }\r\nListing 4: Lynx handler’s synchronisation code.\r\nStep 3. If the address is in the SSRZ then the red-zone\r\nmust be moved and the threads synchronised, as ex\u0002plained in section 4.3.1.\r\nStep 4. If the address is in the PRRZ then the thread’s\r\nstate needs to be altered so that it re-executes the ac\u0002cess at the beginning of the queue. This is described\r\nin section 4.3.2, and requires altering the instruction’s\r\nsource registers.\r\nStep 5. Otherwise the exception did not come from ac\u0002cess the queue, but is part of the actual program, so it\r\ngets re-raised.\r\nStep 6. Once the handler has dealt with an exception\r\nin a red-zone, the thread is free to leave and execution\r\ncontinues by replaying the instruction that caused the\r\nfault.\r\n4.3.1 SSRZ Movement and Synchronisation\r\nAs section 4.1.1 explained, accessing an SSRZ means that\r\nthe handler must move the red-zone towards the start of\r\nthe queue and synchronise the threads. Listing 4 shows the\r\nC code for these actions. The functions to enqueue and\r\ndequeue are almost the same, except the dequeue operation\r\nhas to deal with the enqueue thread exiting before reaching\r\na red-zone in line 16, so for brevity we only walk through\r\nthe enqSync() function.\r\nEach section has its own state variable, and these are used\r\nto synchronise the threads. The first step is to get the ad\u0002dress of a new red-zone on the left of the current one (list\u0002ing 4 line 2). The new red-zone is configured (access per\u0002mission bits set, line 3) then the previous section’s state is\r\nupdated (line 5). This allows the dequeue thread to enter\r\nthe previous section, if it is ready to, and so avoids dead\u0002lock. The enqueue thread then enters a spin-loop, waiting\r\nfor the next section to become available (line 7), which it\r\nwill when the other thread is no longer accessing it (state\r\nset to DEQ DONE). Keeping the thread spinning avoids re\u0002peatedly calling the signal handler while waiting which is\r\nimportant for performance; handling an exception has a sig\u0002nificant overhead because the operating system has to be",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/503d7eed-e562-4ecf-a72f-fd8f9697111a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5bebb9fbd68fe1e094fdc4afd12cb17aaaa48b19f915ded1e0e4f3435ddd93cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 821
      },
      {
        "segments": [
          {
            "segment_id": "503d7eed-e562-4ecf-a72f-fd8f9697111a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "1 void enqueue (queue_t q, long data) {\r\n2 asm(\"movq %0, (%1, %2)\"\r\n3 : /* no output */\r\n4 : \"r\" (data), /* input %0 */\r\n5 \"r\" (q->enqBase), /* input %1 */\r\n6 \"r\" (q->enqIdx) /* input %2 */\r\n7 : \"1\" ); /* clobber */\r\n8 q->enqIdx += sizeof(long);\r\n9 }\r\n10\r\n11 long dequeue(queue_t q) {\r\n12 long data;\r\n13 asm(\"movq (%1, %2), %0\"\r\n14 : \"=&r\" (data) /* output */\r\n15 : \"r\" (q->deqBase), /* input %1 */\r\n16 \"r\" (q->deqIdx) /* input %2 */\r\n17 : \"1\" ); /* clobber */\r\n18 deqIdx += sizeof(long);\r\n19 return data;\r\n20 }\r\nListing 3: Implementation of Lynx.\r\n\r\n\r\n\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n\u0001\u0001\u0001\r\n2.\r\n1. Get Pointer That Triggered Exception \r\nGet Type Of Red−Zone\r\nSSRZ PRRZ\r\nCalculate New Index\r\nSet Register 4. 5. RaiseMove SSRZ Exception\r\nNot in RZ\r\n3.\r\nResume Execution\r\nLeave Handler 6.\r\nSynchronise\r\nFigure 8: Overview of the operation of Lynx’s handler.\r\noperands used, which is vital to enable the PRRZ to func\u0002tion correctly. Section 4.3.2 describes this in more detail.\r\nAn implementation of a subset of Lynx without the PRRZ\r\npointer rotation functionality would not require assembly\r\ncoding and could be implemented purely in C.\r\nOnce compiled, the final assembly code for Lynx’s enqueue\r\nand dequeue functions consists of just two x86-64 instruc\u0002tions each: a memory instruction (for storing or loading from\r\nthe queue) and an addition that increments the pointer. As\r\nwith state-of-the-art queues, the compiler fully inlines this\r\ncode. However, in contrast to the state-of-the-art, there is\r\nno complicated control flow (and no spinning loop) which\r\nallows the code to get heavily optimised by the compiler.\r\n4.3 Lynx’s Exception Handler\r\nThe Lynx exception handler is called whenever a thread\r\nattempts to access one of the queue’s red-zones. As ex\u0002plained in sections 4.1.1 and 4.1.2, the handler’s actions de\u0002pend on the type of red-zone being accessed. An overview\r\nof the handler is shown in figure 8.\r\nStep 1. The first task is to get the address which caused\r\nthe exception, so that it can take appropriate actions\r\nbased on the type of red-zone being accessed (if any).\r\nThis is available through the arguments to the signal\r\nhandler (specifically the si_addr field in the POSIX\r\nsiginfo_t structure).\r\nStep 2. Using the address, the type of red-zone can be\r\ndetermined and acted upon.\r\n1 enqSync() {\r\n2 newRedzone = getNewRedzoneLeft(currPtr);\r\n3 configRedZone (ON, newRedzone);\r\n4 redzone = newRedzone;\r\n5 prevSectionState = ENQ_DONE;\r\n6 /* Spin-loop */\r\n7 while (nextSectionState != DEQ_DONE) ;\r\n8 configRedZone (OFF, currPtr);\r\n9 *nextSectionState = ENQ_WRITES;\r\n10 }\r\n11\r\n12 deqSync() {\r\n13 newRedzone = getNewRedzoneLeft(currPtr);\r\n14 configRedZone (ON, newRedzone);\r\n15 redzone = newRedzone;\r\n16 if (prevSectionState != ENQ_EXITED)\r\n17 prevSectionState = DEQ_DONE;\r\n18 /* Spin-loop */\r\n19 while (nextSectionState != ENQ_DONE\r\n20 && nextSectionState != ENQ_EXITED) ;\r\n21 configRedZone (OFF, currPtr);\r\n22 nextSectionState = DEQ_READS;\r\n23 }\r\nListing 4: Lynx handler’s synchronisation code.\r\nStep 3. If the address is in the SSRZ then the red-zone\r\nmust be moved and the threads synchronised, as ex\u0002plained in section 4.3.1.\r\nStep 4. If the address is in the PRRZ then the thread’s\r\nstate needs to be altered so that it re-executes the ac\u0002cess at the beginning of the queue. This is described\r\nin section 4.3.2, and requires altering the instruction’s\r\nsource registers.\r\nStep 5. Otherwise the exception did not come from ac\u0002cess the queue, but is part of the actual program, so it\r\ngets re-raised.\r\nStep 6. Once the handler has dealt with an exception\r\nin a red-zone, the thread is free to leave and execution\r\ncontinues by replaying the instruction that caused the\r\nfault.\r\n4.3.1 SSRZ Movement and Synchronisation\r\nAs section 4.1.1 explained, accessing an SSRZ means that\r\nthe handler must move the red-zone towards the start of\r\nthe queue and synchronise the threads. Listing 4 shows the\r\nC code for these actions. The functions to enqueue and\r\ndequeue are almost the same, except the dequeue operation\r\nhas to deal with the enqueue thread exiting before reaching\r\na red-zone in line 16, so for brevity we only walk through\r\nthe enqSync() function.\r\nEach section has its own state variable, and these are used\r\nto synchronise the threads. The first step is to get the ad\u0002dress of a new red-zone on the left of the current one (list\u0002ing 4 line 2). The new red-zone is configured (access per\u0002mission bits set, line 3) then the previous section’s state is\r\nupdated (line 5). This allows the dequeue thread to enter\r\nthe previous section, if it is ready to, and so avoids dead\u0002lock. The enqueue thread then enters a spin-loop, waiting\r\nfor the next section to become available (line 7), which it\r\nwill when the other thread is no longer accessing it (state\r\nset to DEQ DONE). Keeping the thread spinning avoids re\u0002peatedly calling the signal handler while waiting which is\r\nimportant for performance; handling an exception has a sig\u0002nificant overhead because the operating system has to be",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/503d7eed-e562-4ecf-a72f-fd8f9697111a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5bebb9fbd68fe1e094fdc4afd12cb17aaaa48b19f915ded1e0e4f3435ddd93cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 821
      },
      {
        "segments": [
          {
            "segment_id": "831430b8-4212-4814-b3d7-8eaa2e308473",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "involved. Finally, in lines 8 and 9, the current red-zone is\r\ndisabled and the next section’s state changed to indicate the\r\nenqueue thread accessing it.\r\nIn both enqueue and dequeue functions, deadlock is\r\navoided by unblocking the other thread before trying to\r\nmove into the next section. In addition, by creating the\r\nnew red-zone before setting the previous section’s state, we\r\nensure that the two threads can both be at the end of the\r\nsame section, but will be hitting different red-zones.\r\nTo guarantee correctness, the compiler should not re\u0002order the memory operations in the handler’s synchro\u0002nisation code. We do this by inserting compiler mem\u0002ory reordering barriers between the critical instructions:\r\nasm volatile(\" ::: “memory”) in GCC. Under the\r\nTotal Store Order (TSO) memory model of the x86 archi\u0002tectures [4], no memory barrier instructions are required in\r\neither the enqueue()/dequeue() or the handler. Even though\r\nmemory operations may execute out-of-order on the actual\r\nhardware, TSO guarantees that loads will see the values of\r\nearlier stores across cores. The handler requires fences for\r\nmovnti instructions (these are not TSO) and architectures\r\nwith relaxed consistency models. These fences guarantee\r\nthat the status variables get updated after all queue data\r\nhas been updated. Supporting targets with more relaxed\r\nmemory models does not introduce any performance over\u0002heads as all the additional barrier instructions are in the\r\nhandler, not in the critical path of execution.\r\n4.3.2 Pointer Rotation in PRRZ\r\nThe sole job of the PRRZ is to alter the thread’s state\r\nso that it accesses the start of the queue again, instead of\r\ncontinuing past the end. This requires the signal handler to\r\nidentify the registers used by the instruction to create the\r\nmemory address, determine the values they need to access\r\nthe start of the queue, and then update them.\r\nAs shown in listing 3, we use inline assembly to specify the\r\ninstructions that read and write to the queue. Using inline\r\nassembly means that we have control over the exact instruc\u0002tion that is used and the format of the memory access calcu\u0002lation. The compiler is allowed to choose the actual registers\r\nthat contain the operands so that it can perform register al\u0002location as usually. Using the POSIX sigaction API, the\r\nhandler can get a pointer to the instruction which it can\r\nthen parse to identify the source operands. Once the regis\u0002ters involved in the computation are identified, their values\r\nat the point of the exception can be retrieved through the\r\nucontext_t structure that is given as the third argument\r\nto the signal handler.\r\nAs a concrete example, an x86 memory instruction calcu\u0002lates its address using equation 1.\r\nAddr = SegReg + BaseReg + (IdxReg ∗ Scale) + Offset (1)\r\nOur inline assembly instruction uses only the BaseReg and\r\nIdxReg, which are linked to queue variables, setting Scale\r\nto 1 and Offset to 0. We allow the IdxReg to increment\r\nwhenever the queue is accessed, as shown in listing 3 line 8,\r\nand do not alter it in the signal handler. The BaseReg comes\r\nfrom another queue variable that we modify when accessing\r\nthe PRRZ to set the address in equation 1 back to the start\r\nof the queue. In practice, this means we use equation 2.\r\nVal = QueueStartAddr − IdxReg (2)\r\nThe result of this is that IdxReg can take any value, even\r\naddresses that are beyond the boundaries of the queue, but\r\nthe effective address calculation performed by the processor\r\nwill always create an address within the queue. This works\r\neven when the value of IdxReg overflows. We perform a sim\u0002ilar calculation and transformation for other architectures.\r\nUsing inline assembly is required for correctness for two\r\nreasons: 1) By using it we have a dedicated BaseReg for\r\nour own use, and we are free to update it within the han\u0002dler without modifying the semantics of the surrounding\r\ncode. If inline assembly is not used, then the compiler\r\nmay optimise the code to use one register for both the\r\nloop iteration variable and IdxReg(or BaseReg), meaning\r\nthat if we alter IdxReg (or BaseReg) within the signal han\u0002dler, we also change the semantics of the code. 2) The in\u0002line assembly acts as an instruction re-ordering barrier in\r\nGCC, prohibiting dangerous re-ordering like in this case:\r\nobj->elem = x; enqueue(obj);\r\nTo actually update the BaseReg from within the signal\r\nhandler, we cannot use a simple mov instruction because any\r\nchanges to registers are reverted once the handler finishes.\r\nInstead we alter the relevant entry in the ucontext_t struc\u0002ture which defines the values of the registers that will be\r\nrestored once the signal has been dealt with. We also up\u0002date the variables that BaseReg is linked to (q->enqBase\r\nand q->deqBase as listed in listing 1 lines 4 and 15) so that\r\nthe code will work even when compiled without optimisation\r\n(-O0), because in this case the value is read straight from\r\nmemory before being used, and is not kept in a register.\r\n4.4 Reporting Program Exceptions Correctly\r\nIt is crucial for the queue to be completely transparent\r\nfor all exceptions that are unrelated to the workings of the\r\nqueue. For example, if the program has a bug and de\u0002references a pointer to invalid memory, this should not be\r\nconfused with the exceptions triggered by Lynx’s enqueue or\r\ndequeue actions. We can effectively distinguish between the\r\ntwo by setting the handler to only catch segmentation faults\r\n(SIGSEGV) and by re-raising these faults if they are not re\u0002lated to queue operations (i.e., if the address that triggers\r\nthe exception is not in a red-zone).\r\n4.5 Summary\r\nWe have presented Lynx, a novel queue that uses hard\u0002ware virtual memory permission checks and OS signal sup\u0002port to deal with infrequently-occurring queue actions. We\r\naugment the queue with two types of red-zone and configure\r\nthem so that the threads are not allowed access. This allows\r\nus to read and write to the queue using only two machine\r\ninstructions, placing all other code in a signal handler that\r\nis executed whenever a red-zone is touched.\r\n5. RESULTS\r\nWe evaluate the throughput of Lynx for different data\r\ntypes and compare it to the state-of-the-art Multi-Section\r\nQueue (MSQ). We then show case studies for the use of\r\nLynx in real applications in section 5.6.\r\n5.1 Experimental Setup\r\nWe evaluated Lynx on a number of machines ranging from\r\narchitectures used in embedded systems (like the Intel Bay\u0002Trail-based J1900), up to those used in servers (like the Xeon\r\nand the Opteron). They are listed in table 1. Unless oth\u0002erwise stated, our analysis was performed on the Intel Core",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/831430b8-4212-4814-b3d7-8eaa2e308473.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=36886a5963a9666cae1a85f16d68f8782b75bfb574325ceafdae633691e4f232",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1084
      },
      {
        "segments": [
          {
            "segment_id": "831430b8-4212-4814-b3d7-8eaa2e308473",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "involved. Finally, in lines 8 and 9, the current red-zone is\r\ndisabled and the next section’s state changed to indicate the\r\nenqueue thread accessing it.\r\nIn both enqueue and dequeue functions, deadlock is\r\navoided by unblocking the other thread before trying to\r\nmove into the next section. In addition, by creating the\r\nnew red-zone before setting the previous section’s state, we\r\nensure that the two threads can both be at the end of the\r\nsame section, but will be hitting different red-zones.\r\nTo guarantee correctness, the compiler should not re\u0002order the memory operations in the handler’s synchro\u0002nisation code. We do this by inserting compiler mem\u0002ory reordering barriers between the critical instructions:\r\nasm volatile(\" ::: “memory”) in GCC. Under the\r\nTotal Store Order (TSO) memory model of the x86 archi\u0002tectures [4], no memory barrier instructions are required in\r\neither the enqueue()/dequeue() or the handler. Even though\r\nmemory operations may execute out-of-order on the actual\r\nhardware, TSO guarantees that loads will see the values of\r\nearlier stores across cores. The handler requires fences for\r\nmovnti instructions (these are not TSO) and architectures\r\nwith relaxed consistency models. These fences guarantee\r\nthat the status variables get updated after all queue data\r\nhas been updated. Supporting targets with more relaxed\r\nmemory models does not introduce any performance over\u0002heads as all the additional barrier instructions are in the\r\nhandler, not in the critical path of execution.\r\n4.3.2 Pointer Rotation in PRRZ\r\nThe sole job of the PRRZ is to alter the thread’s state\r\nso that it accesses the start of the queue again, instead of\r\ncontinuing past the end. This requires the signal handler to\r\nidentify the registers used by the instruction to create the\r\nmemory address, determine the values they need to access\r\nthe start of the queue, and then update them.\r\nAs shown in listing 3, we use inline assembly to specify the\r\ninstructions that read and write to the queue. Using inline\r\nassembly means that we have control over the exact instruc\u0002tion that is used and the format of the memory access calcu\u0002lation. The compiler is allowed to choose the actual registers\r\nthat contain the operands so that it can perform register al\u0002location as usually. Using the POSIX sigaction API, the\r\nhandler can get a pointer to the instruction which it can\r\nthen parse to identify the source operands. Once the regis\u0002ters involved in the computation are identified, their values\r\nat the point of the exception can be retrieved through the\r\nucontext_t structure that is given as the third argument\r\nto the signal handler.\r\nAs a concrete example, an x86 memory instruction calcu\u0002lates its address using equation 1.\r\nAddr = SegReg + BaseReg + (IdxReg ∗ Scale) + Offset (1)\r\nOur inline assembly instruction uses only the BaseReg and\r\nIdxReg, which are linked to queue variables, setting Scale\r\nto 1 and Offset to 0. We allow the IdxReg to increment\r\nwhenever the queue is accessed, as shown in listing 3 line 8,\r\nand do not alter it in the signal handler. The BaseReg comes\r\nfrom another queue variable that we modify when accessing\r\nthe PRRZ to set the address in equation 1 back to the start\r\nof the queue. In practice, this means we use equation 2.\r\nVal = QueueStartAddr − IdxReg (2)\r\nThe result of this is that IdxReg can take any value, even\r\naddresses that are beyond the boundaries of the queue, but\r\nthe effective address calculation performed by the processor\r\nwill always create an address within the queue. This works\r\neven when the value of IdxReg overflows. We perform a sim\u0002ilar calculation and transformation for other architectures.\r\nUsing inline assembly is required for correctness for two\r\nreasons: 1) By using it we have a dedicated BaseReg for\r\nour own use, and we are free to update it within the han\u0002dler without modifying the semantics of the surrounding\r\ncode. If inline assembly is not used, then the compiler\r\nmay optimise the code to use one register for both the\r\nloop iteration variable and IdxReg(or BaseReg), meaning\r\nthat if we alter IdxReg (or BaseReg) within the signal han\u0002dler, we also change the semantics of the code. 2) The in\u0002line assembly acts as an instruction re-ordering barrier in\r\nGCC, prohibiting dangerous re-ordering like in this case:\r\nobj->elem = x; enqueue(obj);\r\nTo actually update the BaseReg from within the signal\r\nhandler, we cannot use a simple mov instruction because any\r\nchanges to registers are reverted once the handler finishes.\r\nInstead we alter the relevant entry in the ucontext_t struc\u0002ture which defines the values of the registers that will be\r\nrestored once the signal has been dealt with. We also up\u0002date the variables that BaseReg is linked to (q->enqBase\r\nand q->deqBase as listed in listing 1 lines 4 and 15) so that\r\nthe code will work even when compiled without optimisation\r\n(-O0), because in this case the value is read straight from\r\nmemory before being used, and is not kept in a register.\r\n4.4 Reporting Program Exceptions Correctly\r\nIt is crucial for the queue to be completely transparent\r\nfor all exceptions that are unrelated to the workings of the\r\nqueue. For example, if the program has a bug and de\u0002references a pointer to invalid memory, this should not be\r\nconfused with the exceptions triggered by Lynx’s enqueue or\r\ndequeue actions. We can effectively distinguish between the\r\ntwo by setting the handler to only catch segmentation faults\r\n(SIGSEGV) and by re-raising these faults if they are not re\u0002lated to queue operations (i.e., if the address that triggers\r\nthe exception is not in a red-zone).\r\n4.5 Summary\r\nWe have presented Lynx, a novel queue that uses hard\u0002ware virtual memory permission checks and OS signal sup\u0002port to deal with infrequently-occurring queue actions. We\r\naugment the queue with two types of red-zone and configure\r\nthem so that the threads are not allowed access. This allows\r\nus to read and write to the queue using only two machine\r\ninstructions, placing all other code in a signal handler that\r\nis executed whenever a red-zone is touched.\r\n5. RESULTS\r\nWe evaluate the throughput of Lynx for different data\r\ntypes and compare it to the state-of-the-art Multi-Section\r\nQueue (MSQ). We then show case studies for the use of\r\nLynx in real applications in section 5.6.\r\n5.1 Experimental Setup\r\nWe evaluated Lynx on a number of machines ranging from\r\narchitectures used in embedded systems (like the Intel Bay\u0002Trail-based J1900), up to those used in servers (like the Xeon\r\nand the Opteron). They are listed in table 1. Unless oth\u0002erwise stated, our analysis was performed on the Intel Core",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/831430b8-4212-4814-b3d7-8eaa2e308473.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=36886a5963a9666cae1a85f16d68f8782b75bfb574325ceafdae633691e4f232",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1084
      },
      {
        "segments": [
          {
            "segment_id": "5797177d-dbb0-4bfd-a85d-d588350e1f0e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "0\r\n2\r\n4\r\n6\r\n8\r\n10\r\n12\r\n14\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(a) 64-Bit Integers (-O3)\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(b) 32-Bit Integers (-O3)\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov Lynx-mov\r\n(c) 16-Bit Integers (-O3)\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov Lynx-mov\r\n(d) 8-Bit Integers (-O3)\r\nFigure 9: Throughput GBytes/s (y axis) of Lynx and the\r\nstate-of-the-art Multi-Section Queue for different queue sizes\r\n(x axis) on an Intel Core i5-4570.\r\ni5-4570. We pinned the enqueue and dequeue threads to\r\ndistinct cores in all experiments (avoiding core sharing in\r\nprocessors supporting hyper-threading), choosing cores that\r\nshared the highest level of cache, i.e., an L2 cache if shared,\r\notherwise L3. All experiments used a 2-section queue (for\r\nboth Lynx and the state-of-the-art queue).\r\nThe throughput experiment’s code consists of two threads\r\nmoving 8GB of data through the queue. One thread pushes\r\nvalues into the queue and sums them, while the other thread\r\nremoves them from the queue and also sums them. We\r\nran the throughput experiments 3 times for warm-up and\r\nthen took the average over the following 10 runs. We\r\ncompiled with the system’s GCC (shown in table 1) with\r\n-O3. The binaries in section 5.5 were generated with -O3\r\n-funroll-loops.\r\n5.2 Throughput Tests\r\nWe measured the throughput of Lynx for various queue\r\nsizes and compared it against the state-of-the-art [11, 15,\r\n16, 24] (as in listing 1). We tested four data widths: 64-\r\nbit, 32-bit, 16-bit and 8-bit, shown in figure 9. We only\r\nshow the mov results for the latter two because there is no\r\nnon-temporal move instruction for these data widths2.\r\nOnce the queue reaches a certain size, Lynx outperforms\r\nthe state-of-the-art queue. The results show that Lynx\r\n(Lynx-mov) outperforms the state-of-the-art queue (MSQ\u0002mov) for a range of queue sizes, depending on the data\r\nwidth. For the 64-bit test, Lynx is better for any size larger\r\nthan 512KB. As the data width decreases, Lynx becomes\r\nbetter sooner, with the 32-bit and 16-bit cases starting at\r\n256KB, and the 16-bit case at 128KB. The reason is that the\r\nnarrower the data width, the more data of this type can fit\r\nin each queue section, therefore the larger the effective sec\u0002tion size. For example, for the 8-bit data type (figure 9d),\r\na 128KB queue size is effectively 8 times larger than that of\r\n2A movnti with a wider data type could be used, but ex\u0002ploring this is not in the scope of this paper.\r\n 0\r\n 10\r\n 20\r\n 30\r\n 40\r\n 50\r\n 60\r\n 70\r\n 80\r\n 90\r\n100\r\n64KB128KB256Kb512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\n% Execution Time\r\nreal kernel sync handler other\r\nFigure 10: Breakdown of Lynx’s cycles for the throughput\r\ntest on the Intel Core-i5 system and 64-bit integers.\r\nthe 64-bit case (equal to 1MB), and, as shown in figure 9a,\r\nLynx is better than the state-of-the-art for that size. The\r\nmaximum throughput speedup of Lynx versus MSQ is 1.44×\r\nfor 64-bit, 1.6× for 32-bit, and 1.4× for 16- and 8-bit data.\r\nThe throughput of the MSQ-movnti and Lynx-movnti for\r\n64-bit data (figure 9a) is lower than the peak MSQ-mov and\r\nLynx-mov by a large margin (8.5GB/s versus 14.6GB/s).\r\nThis is because the movnti experiments’ throughput is lim\u0002ited by the throughput of the system’s main DRAM memory,\r\nsince all stores bypass the caches completely. The evaluation\r\nsystem was equipped with dual-channel 1600MT/s DDR3\r\nmodules, with a maximum total interface throughput of ap\u0002proximately 2×12.8GB/s = 25.6GB/s. The 8.5GB/s queue\r\nthroughput involves both storing and loading from memory,\r\nthus causing a 2×8.5GB/s = 17GB/s write+read memory\r\nload, which is very close to the theoretical maximum. This\r\nalso explains the more intuitive results of the 32-bit tests\r\n(figure 9b where the maximum throughput with movnti is\r\nat 7.8GB/s versus 5GB/s for the standard mov.\r\nAccessing data through the cache (MSQ-mov and Lynx\u0002mov) has higher memory bandwidth compared to bypassing\r\nthe caches altogether. This is the reason why the throughput\r\nof Lynx-mov (figure 9a) is significantly higher compared to\r\nLynx-movnti or MSQ-movnti for the bandwidth-stagnated\r\n64-bit case.\r\n5.3 Breakdown of Lynx Overheads\r\nFigure 10 breaks down the execution time of Lynx for in\u0002creasing queue sizes, showing that the overheads decrease\r\nrapidly, becoming negligible for large queues. Data was\r\ncollected while running the throughput test for 64-bit in\u0002tegers using perf, which uses sampling to perform its mea\u0002surements. The kernel time spent servicing the red-zone\r\ninterrupts (kernel) is a significant fraction of the overall ex\u0002ecution time for small queues. It is approximately 60% for\r\na 64KB queue, but decreases as the queue gets larger at a\r\nrate of about 12% per queue size increment, until it gets\r\nto about 10% for a 2MB queue. This is expected as the\r\nkernel is involved in section synchronisation and index ro\u0002tation, which together occur at least three times per walk\r\nof the queue. For small sizes, traversing the queue is fast,\r\nso the kernel is entered frequently. For larger queues, there\r\nare more entries to push and pop within each section, so the\r\nSSRZ and PRRZ are not hit as often and the kernel is less\r\nregularly called.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/5797177d-dbb0-4bfd-a85d-d588350e1f0e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e89f3129428de1dd0040fb41bd900b164efcf190b9364c689799e7556fd8a175",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 849
      },
      {
        "segments": [
          {
            "segment_id": "5797177d-dbb0-4bfd-a85d-d588350e1f0e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "0\r\n2\r\n4\r\n6\r\n8\r\n10\r\n12\r\n14\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(a) 64-Bit Integers (-O3)\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(b) 32-Bit Integers (-O3)\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov Lynx-mov\r\n(c) 16-Bit Integers (-O3)\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n64KB128KB256KB512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\nMSQ-mov Lynx-mov\r\n(d) 8-Bit Integers (-O3)\r\nFigure 9: Throughput GBytes/s (y axis) of Lynx and the\r\nstate-of-the-art Multi-Section Queue for different queue sizes\r\n(x axis) on an Intel Core i5-4570.\r\ni5-4570. We pinned the enqueue and dequeue threads to\r\ndistinct cores in all experiments (avoiding core sharing in\r\nprocessors supporting hyper-threading), choosing cores that\r\nshared the highest level of cache, i.e., an L2 cache if shared,\r\notherwise L3. All experiments used a 2-section queue (for\r\nboth Lynx and the state-of-the-art queue).\r\nThe throughput experiment’s code consists of two threads\r\nmoving 8GB of data through the queue. One thread pushes\r\nvalues into the queue and sums them, while the other thread\r\nremoves them from the queue and also sums them. We\r\nran the throughput experiments 3 times for warm-up and\r\nthen took the average over the following 10 runs. We\r\ncompiled with the system’s GCC (shown in table 1) with\r\n-O3. The binaries in section 5.5 were generated with -O3\r\n-funroll-loops.\r\n5.2 Throughput Tests\r\nWe measured the throughput of Lynx for various queue\r\nsizes and compared it against the state-of-the-art [11, 15,\r\n16, 24] (as in listing 1). We tested four data widths: 64-\r\nbit, 32-bit, 16-bit and 8-bit, shown in figure 9. We only\r\nshow the mov results for the latter two because there is no\r\nnon-temporal move instruction for these data widths2.\r\nOnce the queue reaches a certain size, Lynx outperforms\r\nthe state-of-the-art queue. The results show that Lynx\r\n(Lynx-mov) outperforms the state-of-the-art queue (MSQ\u0002mov) for a range of queue sizes, depending on the data\r\nwidth. For the 64-bit test, Lynx is better for any size larger\r\nthan 512KB. As the data width decreases, Lynx becomes\r\nbetter sooner, with the 32-bit and 16-bit cases starting at\r\n256KB, and the 16-bit case at 128KB. The reason is that the\r\nnarrower the data width, the more data of this type can fit\r\nin each queue section, therefore the larger the effective sec\u0002tion size. For example, for the 8-bit data type (figure 9d),\r\na 128KB queue size is effectively 8 times larger than that of\r\n2A movnti with a wider data type could be used, but ex\u0002ploring this is not in the scope of this paper.\r\n 0\r\n 10\r\n 20\r\n 30\r\n 40\r\n 50\r\n 60\r\n 70\r\n 80\r\n 90\r\n100\r\n64KB128KB256Kb512KB1MB2MB4MB8MB16MB32MB64MB128MB256MB\r\n% Execution Time\r\nreal kernel sync handler other\r\nFigure 10: Breakdown of Lynx’s cycles for the throughput\r\ntest on the Intel Core-i5 system and 64-bit integers.\r\nthe 64-bit case (equal to 1MB), and, as shown in figure 9a,\r\nLynx is better than the state-of-the-art for that size. The\r\nmaximum throughput speedup of Lynx versus MSQ is 1.44×\r\nfor 64-bit, 1.6× for 32-bit, and 1.4× for 16- and 8-bit data.\r\nThe throughput of the MSQ-movnti and Lynx-movnti for\r\n64-bit data (figure 9a) is lower than the peak MSQ-mov and\r\nLynx-mov by a large margin (8.5GB/s versus 14.6GB/s).\r\nThis is because the movnti experiments’ throughput is lim\u0002ited by the throughput of the system’s main DRAM memory,\r\nsince all stores bypass the caches completely. The evaluation\r\nsystem was equipped with dual-channel 1600MT/s DDR3\r\nmodules, with a maximum total interface throughput of ap\u0002proximately 2×12.8GB/s = 25.6GB/s. The 8.5GB/s queue\r\nthroughput involves both storing and loading from memory,\r\nthus causing a 2×8.5GB/s = 17GB/s write+read memory\r\nload, which is very close to the theoretical maximum. This\r\nalso explains the more intuitive results of the 32-bit tests\r\n(figure 9b where the maximum throughput with movnti is\r\nat 7.8GB/s versus 5GB/s for the standard mov.\r\nAccessing data through the cache (MSQ-mov and Lynx\u0002mov) has higher memory bandwidth compared to bypassing\r\nthe caches altogether. This is the reason why the throughput\r\nof Lynx-mov (figure 9a) is significantly higher compared to\r\nLynx-movnti or MSQ-movnti for the bandwidth-stagnated\r\n64-bit case.\r\n5.3 Breakdown of Lynx Overheads\r\nFigure 10 breaks down the execution time of Lynx for in\u0002creasing queue sizes, showing that the overheads decrease\r\nrapidly, becoming negligible for large queues. Data was\r\ncollected while running the throughput test for 64-bit in\u0002tegers using perf, which uses sampling to perform its mea\u0002surements. The kernel time spent servicing the red-zone\r\ninterrupts (kernel) is a significant fraction of the overall ex\u0002ecution time for small queues. It is approximately 60% for\r\na 64KB queue, but decreases as the queue gets larger at a\r\nrate of about 12% per queue size increment, until it gets\r\nto about 10% for a 2MB queue. This is expected as the\r\nkernel is involved in section synchronisation and index ro\u0002tation, which together occur at least three times per walk\r\nof the queue. For small sizes, traversing the queue is fast,\r\nso the kernel is entered frequently. For larger queues, there\r\nare more entries to push and pop within each section, so the\r\nSSRZ and PRRZ are not hit as often and the kernel is less\r\nregularly called.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/5797177d-dbb0-4bfd-a85d-d588350e1f0e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e89f3129428de1dd0040fb41bd900b164efcf190b9364c689799e7556fd8a175",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 849
      },
      {
        "segments": [
          {
            "segment_id": "a8be62df-b50e-4fd4-a8a3-2dd02bdf615c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "Processor Name u-arch nm TDP GHz Cores L1 Cache L2 Cache L3 Cache Mem MT/s Linux GCC\r\nIntel Xeon E5-2667 v2 Sandy Bridge 32 130W 3.30 2×8 6×32KB 6×256KB 15MB (S) 4×1600 3.13.0 4.8.4\r\nAMD Opteron 6376 Piledriver 32 115W 2.3 2×16 16×16KB 8×2MB (S) 2×8MB (S) 4×1600 3.13.0 4.8.4\r\nIntel Core i5-4570 Haswell 22 84W 3.20 1×4 4×32KB 4×256KB 6MB (S) 2×1600 3.10.17 4.8.3\r\nIntel Core i3-2367M Sandy Bridge 32 17W 1.40 1×2 2×32KB 2×256KB 3MB (S) 1×1333 3.9.3 4.8.2\r\nIntel Celeron J1900 BayTrail-D 22 10W 2.42 1×4 4×24KB 2×1MB (S) - 1×1333 3.16.0 4.9.2\r\nTable 1: Description of systems that we evaluated. The shared cache is marked with an (S). TIME\r\nenqueue dequeue\r\nSection 1 Section 2\r\nSection 2\r\nSection 1\r\nInter−Core Latency\r\nSection 1\r\n: Real Execution\r\n: Synchronisation (in spin−lock)\r\n(a) Multi-Section Queue\r\n: Kernel Overhead\r\nSection 2\r\nSection 1\r\nSection 1\r\nSection 2\r\nSection 1\r\nenqueue dequeue\r\n(b) Lynx\r\nFigure 11: The synchronisation overhead of queues.\r\nThe kernel overhead is strongly correlated to the synchro\u0002nisation overhead (sync). This is because when one thread\r\nis in the process of servicing an interrupt (in kernel mode)\r\nthe other thread is likely in a spin-loop, waiting for the state\r\nvariable to change from the other thread, allowing it to pro\u0002ceed to the next section. This is illustrated in figure 11 where\r\nthe threads execute from a cold start (dequeue has nothing\r\nto read in the beginning). In Lynx (figure 11b), even though\r\nthe real execution is faster (shorter green boxes), there is\r\nthe additional kernel overhead (dark red) that leads to more\r\ntime spent in spin-loops (yellow). In MSQ (figure 11a), on\r\nthe other hand, synchronisation is direct and faster. The\r\ncycles spent in the spin-loops ranges from 15% for a queue\r\nsize of 64KB down to less than 2% for queue sizes 2MB or\r\nhigher.\r\nFinally there are miscellaneous remaining overheads. The\r\ncode of the handler itself (handler) is the code that parses the\r\ninstruction and calculates the value of the index. Its over\u0002head is negligible even for small queue sizes. The remaining\r\noverheads (other) refer to time spent in other boilerplate\r\ncode, e.g., libc and libpthread. Again, these overheads are\r\nnegligible for all queue sizes.\r\n5.4 Performance Impact of Compiler\r\nOptimisations\r\nLynx, due to its minimal instruction count and lack of\r\ncontrol flow in enqueue and dequeue operations, allows the\r\ncompiler to highly optimise it within its surrounding code.\r\nFigure 12 shows the throughput impact of increasing the\r\ncompiler optimisation level (using GCC-4.8.3), starting from\r\nno optimisation (-O0) all the way to -O3 and then forcing\r\nloop unrolling (-O3 -funroll-loops). At -O0, the compiler will\r\nnot cache values in registers. Instead, before each instruction\r\nall its inputs are loaded from memory and after its execution\r\nthe outputs are stored back into memory. However with\r\nhigher optimisation levels Lynx is significantly better.\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\n12\r\n14\r\n16\r\n-O0 -O1 -O2 -O3 -O3-unroll\r\nThroughput GBytes/s\r\n14.9 14.2 14.5 15.7\r\nMSQ-mov\r\nLynx-mov\r\nFigure 12: Throughput under several compiler optimisation\r\nlevels on Intel Core-i5 for 64-bit integers (4MB queue).\r\nThere is a mismatch between the two queue implementa\u0002tions at -O1, with Lynx seeing a dramatic rise in through\u0002put, whereas the MSQ achieves only a modest increase. The\r\nMSQ code when compiled with -O1 contains more instruc\u0002tions (some of them even access memory). Adding strict\r\naliasing and partial redundancy elimination to -O1 (by de\u0002fault only enabled in -O2) brings the performance of MSQ\r\nto the expected levels. On the other hand, the code of Lynx\r\nis only 2 instructions long, contains no control-flow instruc\u0002tions and therefore it can be optimised very efficiently even\r\nwith fewer optimisation passes.\r\nWhile MSQ achieves the same throughput from -O2 on\u0002wards, Lynx improves when compiled with the unrolling flag,\r\nreaching a peak throughput of 15.69 GB/s, 5% faster than\r\nthe previous best at -O1. The compiler’s unrolling pass suc\u0002cessfully unrolls Lynx’s code 8 times, but it fails to do so for\r\nthe MSQ due to its larger code size and complexity.\r\n5.5 Evaluation on Various Machines\r\nWe evaluated Lynx on several systems in a large range of\r\nthe power and performance spectrum. We evaluated low\u0002power architectures (Intel Celeron J1900 Bay Trail-D 10W),\r\nlow-power laptop processors (like the Intel Core i3 M se\u0002ries), common desktop processors (Intel Core i5) and pow\u0002erful server components (Intel Xeon and AMD Opteron). A\r\ndescription of the systems can be found in table 1. The 64bit\r\nthroughput for these machines is shown in figure 13 and the\r\n32bit throughput in figure 14.\r\nIn all systems Lynx-mov starts to outperform the state-of\u0002the-art queue (MSQ-mov) once the queue size is big enough\r\nto amortise the cost of frequent exception handling. This\r\npoint is at a queue size of either 512KB or 1MB for the\r\n64bit test and usually earlier (starting from 256KB for the\r\nCore-i3 and Celeron-J1900) for the 32-bit test. The results\r\nfor the movnti versions of the queues are similar but Lynx\u0002movnti usually overtakes MSQ-movnti for larger queue sizes\r\nin the 64-bit test, usually 4MB, and the performance differ\u0002ence is significantly smaller compared to the regular mov.\r\nFor the 32-bit results, just like in figure 9b for the Core\r\ni5, the queue’s performance is less constrained by the mem-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/a8be62df-b50e-4fd4-a8a3-2dd02bdf615c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e8eb14e7333e78344816d8a3ed2bb6f7ea1df41b8fc587bf07aeefbbcdbe1a5b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 871
      },
      {
        "segments": [
          {
            "segment_id": "a8be62df-b50e-4fd4-a8a3-2dd02bdf615c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "Processor Name u-arch nm TDP GHz Cores L1 Cache L2 Cache L3 Cache Mem MT/s Linux GCC\r\nIntel Xeon E5-2667 v2 Sandy Bridge 32 130W 3.30 2×8 6×32KB 6×256KB 15MB (S) 4×1600 3.13.0 4.8.4\r\nAMD Opteron 6376 Piledriver 32 115W 2.3 2×16 16×16KB 8×2MB (S) 2×8MB (S) 4×1600 3.13.0 4.8.4\r\nIntel Core i5-4570 Haswell 22 84W 3.20 1×4 4×32KB 4×256KB 6MB (S) 2×1600 3.10.17 4.8.3\r\nIntel Core i3-2367M Sandy Bridge 32 17W 1.40 1×2 2×32KB 2×256KB 3MB (S) 1×1333 3.9.3 4.8.2\r\nIntel Celeron J1900 BayTrail-D 22 10W 2.42 1×4 4×24KB 2×1MB (S) - 1×1333 3.16.0 4.9.2\r\nTable 1: Description of systems that we evaluated. The shared cache is marked with an (S). TIME\r\nenqueue dequeue\r\nSection 1 Section 2\r\nSection 2\r\nSection 1\r\nInter−Core Latency\r\nSection 1\r\n: Real Execution\r\n: Synchronisation (in spin−lock)\r\n(a) Multi-Section Queue\r\n: Kernel Overhead\r\nSection 2\r\nSection 1\r\nSection 1\r\nSection 2\r\nSection 1\r\nenqueue dequeue\r\n(b) Lynx\r\nFigure 11: The synchronisation overhead of queues.\r\nThe kernel overhead is strongly correlated to the synchro\u0002nisation overhead (sync). This is because when one thread\r\nis in the process of servicing an interrupt (in kernel mode)\r\nthe other thread is likely in a spin-loop, waiting for the state\r\nvariable to change from the other thread, allowing it to pro\u0002ceed to the next section. This is illustrated in figure 11 where\r\nthe threads execute from a cold start (dequeue has nothing\r\nto read in the beginning). In Lynx (figure 11b), even though\r\nthe real execution is faster (shorter green boxes), there is\r\nthe additional kernel overhead (dark red) that leads to more\r\ntime spent in spin-loops (yellow). In MSQ (figure 11a), on\r\nthe other hand, synchronisation is direct and faster. The\r\ncycles spent in the spin-loops ranges from 15% for a queue\r\nsize of 64KB down to less than 2% for queue sizes 2MB or\r\nhigher.\r\nFinally there are miscellaneous remaining overheads. The\r\ncode of the handler itself (handler) is the code that parses the\r\ninstruction and calculates the value of the index. Its over\u0002head is negligible even for small queue sizes. The remaining\r\noverheads (other) refer to time spent in other boilerplate\r\ncode, e.g., libc and libpthread. Again, these overheads are\r\nnegligible for all queue sizes.\r\n5.4 Performance Impact of Compiler\r\nOptimisations\r\nLynx, due to its minimal instruction count and lack of\r\ncontrol flow in enqueue and dequeue operations, allows the\r\ncompiler to highly optimise it within its surrounding code.\r\nFigure 12 shows the throughput impact of increasing the\r\ncompiler optimisation level (using GCC-4.8.3), starting from\r\nno optimisation (-O0) all the way to -O3 and then forcing\r\nloop unrolling (-O3 -funroll-loops). At -O0, the compiler will\r\nnot cache values in registers. Instead, before each instruction\r\nall its inputs are loaded from memory and after its execution\r\nthe outputs are stored back into memory. However with\r\nhigher optimisation levels Lynx is significantly better.\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\n12\r\n14\r\n16\r\n-O0 -O1 -O2 -O3 -O3-unroll\r\nThroughput GBytes/s\r\n14.9 14.2 14.5 15.7\r\nMSQ-mov\r\nLynx-mov\r\nFigure 12: Throughput under several compiler optimisation\r\nlevels on Intel Core-i5 for 64-bit integers (4MB queue).\r\nThere is a mismatch between the two queue implementa\u0002tions at -O1, with Lynx seeing a dramatic rise in through\u0002put, whereas the MSQ achieves only a modest increase. The\r\nMSQ code when compiled with -O1 contains more instruc\u0002tions (some of them even access memory). Adding strict\r\naliasing and partial redundancy elimination to -O1 (by de\u0002fault only enabled in -O2) brings the performance of MSQ\r\nto the expected levels. On the other hand, the code of Lynx\r\nis only 2 instructions long, contains no control-flow instruc\u0002tions and therefore it can be optimised very efficiently even\r\nwith fewer optimisation passes.\r\nWhile MSQ achieves the same throughput from -O2 on\u0002wards, Lynx improves when compiled with the unrolling flag,\r\nreaching a peak throughput of 15.69 GB/s, 5% faster than\r\nthe previous best at -O1. The compiler’s unrolling pass suc\u0002cessfully unrolls Lynx’s code 8 times, but it fails to do so for\r\nthe MSQ due to its larger code size and complexity.\r\n5.5 Evaluation on Various Machines\r\nWe evaluated Lynx on several systems in a large range of\r\nthe power and performance spectrum. We evaluated low\u0002power architectures (Intel Celeron J1900 Bay Trail-D 10W),\r\nlow-power laptop processors (like the Intel Core i3 M se\u0002ries), common desktop processors (Intel Core i5) and pow\u0002erful server components (Intel Xeon and AMD Opteron). A\r\ndescription of the systems can be found in table 1. The 64bit\r\nthroughput for these machines is shown in figure 13 and the\r\n32bit throughput in figure 14.\r\nIn all systems Lynx-mov starts to outperform the state-of\u0002the-art queue (MSQ-mov) once the queue size is big enough\r\nto amortise the cost of frequent exception handling. This\r\npoint is at a queue size of either 512KB or 1MB for the\r\n64bit test and usually earlier (starting from 256KB for the\r\nCore-i3 and Celeron-J1900) for the 32-bit test. The results\r\nfor the movnti versions of the queues are similar but Lynx\u0002movnti usually overtakes MSQ-movnti for larger queue sizes\r\nin the 64-bit test, usually 4MB, and the performance differ\u0002ence is significantly smaller compared to the regular mov.\r\nFor the 32-bit results, just like in figure 9b for the Core\r\ni5, the queue’s performance is less constrained by the mem-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/a8be62df-b50e-4fd4-a8a3-2dd02bdf615c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e8eb14e7333e78344816d8a3ed2bb6f7ea1df41b8fc587bf07aeefbbcdbe1a5b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 871
      },
      {
        "segments": [
          {
            "segment_id": "1a985dbd-87c2-4696-9d4b-a83b36f0f87a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "0\r\n2\r\n4\r\n6\r\n8\r\n10\r\n12\r\n14\r\n16\r\n18\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(a) Intel Xeon E5-2667 v2\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(b) AMD Opteron 6376\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(c) Intel Core-i3 2367M\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n4.0\r\n4.5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(d) Intel Celeron J1900\r\nFigure 13: Throughput GB/s (y axis) of 64-bit data for\r\ndifferent queue sizes (x axis) and machines.\r\nory throughput of either caches or DRAM. Lynx, with its\r\nlightweight enqueue and dequeue operations achieves signif\u0002icantly higher bandwidth for both mov and movnti versions\r\nwith speedups of 1.9× for Core-i3 mov and 2.0× for Xeon\r\nand Opteron with movnti.\r\n5.6 Case Studies\r\nThis section evaluates Lynx in actual applications to show\r\nits speedups over state-of-the-art transfer to real-world set\u0002tings.\r\nSRMT Fault Tolerance.\r\nSRMT [24, 25] is a technique for detecting transient errors\r\n(i.e., bit-flips) with software support, by running the appli\u0002cation on one thread (main thread) and a modified copy\r\nof the code on another thread (checker thread). The main\r\nthread sends all data it reads / writes from / to memory to\r\nthe checker thread via a software queue. The checker thread\r\ncompares this data against the values that are produced lo\u0002cally. If they differ, then a fault has occurred.\r\nWe implemented SRMT using both the state-of-the-art\r\nMSQ and Lynx and used benchmarks from the NAS NPB\u00022.3 [2] suite (BT, CG, EP, IS, LU, MG and SP) as inputs.\r\nFT is missing due to compilation error in SRMT’s imple\u0002mentation. We used the queue size that performed best on\r\naverage for each queue (256KB for MSQ and 2MB for Lynx),\r\nthe best performing instruction (mov for MSQ and movnti\r\nfor Lynx) and prefetch instructions for both. We measured\r\nthe execution time and standard deviation on the Core-i5\r\nsystem (table 1). Figure 16a shows that the performance\r\nwith Lynx is improved up to 1.4× at an average of 1.12×\r\n(geometric mean). For some benchmarks, the Lynx queue\r\nsize is not ideal. Choosing the best queue size per bench\u0002mark means Lynx always out-performs MSQ.\r\nSD3 Data-Dependence Profiling Tool.\r\nThis is a state-of-the-art tool for fast dynamic data\u0002dependence profiling [12]. At a high level view, the in\u00020\r\n2\r\n4\r\n6\r\n8\r\n10\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(a) Intel Xeon E5-2667 v2\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(b) AMD Opteron 6376\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n4.0\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(c) Intel Core-i3 2367M\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(d) Intel Celeron J1900\r\nFigure 14: Throughput GB/s (y axis) of 32-bit data for\r\ndifferent queue sizes (x axis) and machines.\r\nQueue\r\nThread\r\nMain \r\nThread\r\nChecker\r\n(a) SRMT [24]\r\n...\r\nQueue\r\nThread\r\nCode\r\nInstrum.\r\nDispatch\r\nThreads\r\nWorker\r\n...\r\n(b) SD3 [12]\r\nAnalysis\r\nDispatch\r\nParser\r\nPacket \r\nAnalysis\r\nPartial Main\r\n... ...\r\n(c) Network Monitoring[16]\r\nFigure 15: Software structure of case studies.\r\nstrumented code of a benchmark sends information about\r\ndata accesses to the worker threads, which perform the data\u0002dependence analysis. To speed up the execution we use an\r\nintermediate thread, the dispatcher thread, to minimise the\r\nburden on the instrumented thread. This enhancement im\u0002proves both queues equally.\r\nSince SD3 has a total of 10 parallel threads (instrumen\u0002tation, dispatch and 8 workers), we configured the queue\r\nsize at 1MB such that they all fit in L3 cache (best perfor\u0002mance). We pinned all threads on a single physical hyper\u0002threaded 8-core Xeon (table 1) and made sure that the in\u0002strumentation and dispatch threads ran on different cores.\r\nThe normalised performance and standard deviation of SD3\r\nfor the NAS benchmarks with mov instructions are shown\r\nin figure 16b. SD3 benefits from high-speed inter-core com\u0002munication. The performance improvement with Lynx is up\r\nto 1.16× with a geometric mean of 1.07×.\r\nNetwork Traffic Monitoring (NTM).\r\nThis tool is a fast parallel network traffic monitor to be\r\nused for line-rate network statistics on very high-speed net\u0002works. The design is based on the tool evaluated by Lee",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/1a985dbd-87c2-4696-9d4b-a83b36f0f87a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6b27822f7376322a5338fec2c45b16b1651fc1d72eab3f877e3ef23bbd3f36a8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 681
      },
      {
        "segments": [
          {
            "segment_id": "1a985dbd-87c2-4696-9d4b-a83b36f0f87a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "0\r\n2\r\n4\r\n6\r\n8\r\n10\r\n12\r\n14\r\n16\r\n18\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(a) Intel Xeon E5-2667 v2\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(b) AMD Opteron 6376\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(c) Intel Core-i3 2367M\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n4.0\r\n4.5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(d) Intel Celeron J1900\r\nFigure 13: Throughput GB/s (y axis) of 64-bit data for\r\ndifferent queue sizes (x axis) and machines.\r\nory throughput of either caches or DRAM. Lynx, with its\r\nlightweight enqueue and dequeue operations achieves signif\u0002icantly higher bandwidth for both mov and movnti versions\r\nwith speedups of 1.9× for Core-i3 mov and 2.0× for Xeon\r\nand Opteron with movnti.\r\n5.6 Case Studies\r\nThis section evaluates Lynx in actual applications to show\r\nits speedups over state-of-the-art transfer to real-world set\u0002tings.\r\nSRMT Fault Tolerance.\r\nSRMT [24, 25] is a technique for detecting transient errors\r\n(i.e., bit-flips) with software support, by running the appli\u0002cation on one thread (main thread) and a modified copy\r\nof the code on another thread (checker thread). The main\r\nthread sends all data it reads / writes from / to memory to\r\nthe checker thread via a software queue. The checker thread\r\ncompares this data against the values that are produced lo\u0002cally. If they differ, then a fault has occurred.\r\nWe implemented SRMT using both the state-of-the-art\r\nMSQ and Lynx and used benchmarks from the NAS NPB\u00022.3 [2] suite (BT, CG, EP, IS, LU, MG and SP) as inputs.\r\nFT is missing due to compilation error in SRMT’s imple\u0002mentation. We used the queue size that performed best on\r\naverage for each queue (256KB for MSQ and 2MB for Lynx),\r\nthe best performing instruction (mov for MSQ and movnti\r\nfor Lynx) and prefetch instructions for both. We measured\r\nthe execution time and standard deviation on the Core-i5\r\nsystem (table 1). Figure 16a shows that the performance\r\nwith Lynx is improved up to 1.4× at an average of 1.12×\r\n(geometric mean). For some benchmarks, the Lynx queue\r\nsize is not ideal. Choosing the best queue size per bench\u0002mark means Lynx always out-performs MSQ.\r\nSD3 Data-Dependence Profiling Tool.\r\nThis is a state-of-the-art tool for fast dynamic data\u0002dependence profiling [12]. At a high level view, the in\u00020\r\n2\r\n4\r\n6\r\n8\r\n10\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(a) Intel Xeon E5-2667 v2\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(b) AMD Opteron 6376\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n4.0\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(c) Intel Core-i3 2367M\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n3.5\r\n64K128K256K512K1M2M4M8M16M32M64M128M256M\r\nMSQ-mov\r\nMSQ-movnti\r\nLynx-mov\r\nLynx-movnti\r\n(d) Intel Celeron J1900\r\nFigure 14: Throughput GB/s (y axis) of 32-bit data for\r\ndifferent queue sizes (x axis) and machines.\r\nQueue\r\nThread\r\nMain \r\nThread\r\nChecker\r\n(a) SRMT [24]\r\n...\r\nQueue\r\nThread\r\nCode\r\nInstrum.\r\nDispatch\r\nThreads\r\nWorker\r\n...\r\n(b) SD3 [12]\r\nAnalysis\r\nDispatch\r\nParser\r\nPacket \r\nAnalysis\r\nPartial Main\r\n... ...\r\n(c) Network Monitoring[16]\r\nFigure 15: Software structure of case studies.\r\nstrumented code of a benchmark sends information about\r\ndata accesses to the worker threads, which perform the data\u0002dependence analysis. To speed up the execution we use an\r\nintermediate thread, the dispatcher thread, to minimise the\r\nburden on the instrumented thread. This enhancement im\u0002proves both queues equally.\r\nSince SD3 has a total of 10 parallel threads (instrumen\u0002tation, dispatch and 8 workers), we configured the queue\r\nsize at 1MB such that they all fit in L3 cache (best perfor\u0002mance). We pinned all threads on a single physical hyper\u0002threaded 8-core Xeon (table 1) and made sure that the in\u0002strumentation and dispatch threads ran on different cores.\r\nThe normalised performance and standard deviation of SD3\r\nfor the NAS benchmarks with mov instructions are shown\r\nin figure 16b. SD3 benefits from high-speed inter-core com\u0002munication. The performance improvement with Lynx is up\r\nto 1.16× with a geometric mean of 1.07×.\r\nNetwork Traffic Monitoring (NTM).\r\nThis tool is a fast parallel network traffic monitor to be\r\nused for line-rate network statistics on very high-speed net\u0002works. The design is based on the tool evaluated by Lee",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/1a985dbd-87c2-4696-9d4b-a83b36f0f87a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6b27822f7376322a5338fec2c45b16b1651fc1d72eab3f877e3ef23bbd3f36a8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 681
      },
      {
        "segments": [
          {
            "segment_id": "2c1e8f8d-f643-4523-9fc1-918056c047c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "0.80\r\n0.90\r\n1.00\r\n1.10\r\n1.20\r\n1.30\r\n1.40\r\nBT CG EP IS LU MG SPGeo\r\nMSQ Lynx\r\n(a) SRMT\r\n0.80\r\n0.85\r\n0.90\r\n0.95\r\n1.00\r\n1.05\r\n1.10\r\n1.15\r\n1.20\r\n1.25\r\nBT CG EP IS LU MG SPGeo\r\nMSQ Lynx\r\n(b) SD3\r\n0.96\r\n0.98\r\n1.00\r\n1.02\r\n1.04\r\n1.06\r\n1.08\r\n1.10\r\n1.12\r\n1.14\r\n1.16\r\n1.18\r\n1T 2T 3T 4T 5T 6T Geo\r\nMSQ Lynx\r\n(c) NTM Speedups\r\nFigure 16: Normalised Speedup using Lynx over MSQ.\r\nCLF SP/SC Queue Sections Drawbacks Performance Enhancements\r\nLamport’s [14, 7] 1 Cache ping-pong Lock-free queue\r\nFastForward [8] 1 Non-generic synchronisation Write/Read slip\r\nDBLS[24] > 1 Number of synchronisation instructions Lazy synchronisation\r\nMCRingBuffer[15, 16] > 1 Number of synchronisation instructions Lazy synchronisation\r\nLiberty[11] > 1 Number of synchronisation instructions Non-temporal SSE instructions (e.g. movnti), prefetching\r\nHAQu [17] 1 Hardware modification Extra Hardware\r\nLynx (proposed) > 1 Handler’s overhead on very small queue sizes Synchronisation overhead is moved off the critical path\r\nTable 2: Overview of features provided by CLF queues.\r\net al. [16] and is composed of several threads, as shown in\r\nfigure 15c. The main thread parses IPv4 network packet\r\nheaders, extracts information and dispatches them to sub\u0002analyser threads. These gather partial statistics and provide\r\nthe data to the main analyser which collects them together.\r\nWe evaluated NTM performance on the Xeon E5-2667\r\n(table 1) varying the number of sub-analyser threads from 1\r\nto 6. We used the queue size that leads to best performance\r\nfor each queue (128KB for MSQ, 2MB for Lynx), mov for\r\nboth. The speedups shown in figure 16c are normalised to\r\nMSQ. The geometric mean speedup of Lynx over MSQ is\r\n1.09x.\r\n6. RELATED WORK\r\nLamport proved that a concurrent lock-free (CLF) queue\r\ncan be implemented in a SP/SC scenario [7, 14]. However,\r\neven though it is lock-free, this queue has very poor per\u0002formance due to frequent cache ping-pong of queue control\r\nvariables.\r\nAn improvement over Lamport’s implementation is the\r\nFastForward CLF queue [8]. In this implementation the en\u0002queue and dequeue indices are private to the enqueue and\r\ndequeue functions and their values are never passed to the\r\nother. To avoid the threads overtaking each other, the de\u0002queue operation writes NULL values to the queue after read\u0002ing data from it; enqueue checks that the queue value at\r\nthe write index is NULL before overwriting it. False shar\u0002ing (reading and writing to different queue elements in the\r\nsame cache line) is avoided by enforcing a buffer between\r\nthe writes and reads (referred to as slip).\r\nMultiple sections were introduced by DBLS [24] and\r\nMCRingBuffer [15, 16] queues. Both queues keep enqueue\r\nand dequeue indices private and occasionally share these val\u0002ues, once for every section. Liberty queues [11] are similar to\r\nboth DBLS and MCRingBuffer in design, but they introduce\r\nseveral implementation-specific performance improvements,\r\nincluding non-symmetric producer and consumer, prefetch\u0002ing, streaming instructions. Finally, [26] studies the dead\u0002lock problem of multi-section queues and proposes a solution\r\nto it.\r\nIn HAQu [17], the authors recognise that the high instruc\u0002tion overhead of existing software queues becomes critical\r\nin fine-grained communication. They propose a hardware\u0002accelerated queue to decrease the number of instructions\r\nwithin enqueue/dequeue functions.\r\nAn overview of the attributes of the SP/SC CLF queues\r\nare shown in table 2. Several CLF queues have been stud\u0002ied since Lamport’s research [13, 18, 19, 21, 22, 23]. These\r\nworks, however, have focused on improving MP/MC queues,\r\nrather than improving the SP/SC case. These have higher\r\noverheads to avoid ABA problems [18] and to maintain lin\u0002earizability [10].\r\nExploiting the hardware/OS memory protection systems\r\nfor improving performance has been used in the past. A\r\nsurvey of algorithms that make use of memory protection\r\ntechniques is presented in [6]. Typical uses include garbage\r\ncollection, and overflow protections. A similar technique has\r\nbeen used under the fine-grained Mondrian memory protec\u0002tion [27] system to implement a zero-copy network stack.\r\nMore recently, the memory protection system has also been\r\nused in the context of software transactional memories to\r\nachieve strong atomicity [5].\r\n7. CONCLUSION\r\nHigh performance single-producer / single-consumer soft\u0002ware queues are fundamental building components of par\u0002allel software. Maximising their efficiency is crucial for ex\u0002tracting the maximum performance of parallel software. This\r\npaper has presented Lynx, a radically new software archi\u0002tecture for SP/SC queues, which reduces the critical path\r\noverhead of enqueue and dequeue operations down to a min\u0002imum. Evaluation of Lynx on various commodity hardware\r\nplatforms shows throughput improvements of over 1.57×\r\ncompared to the state-of-the-art and significant improve\u0002ments in actual applications of up to 1.4×.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/2c1e8f8d-f643-4523-9fc1-918056c047c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5ccd92bafabc39f3af60cf1772544c2fd3b4b49893ef15681efb5bc55798d615",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 739
      },
      {
        "segments": [
          {
            "segment_id": "2c1e8f8d-f643-4523-9fc1-918056c047c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "0.80\r\n0.90\r\n1.00\r\n1.10\r\n1.20\r\n1.30\r\n1.40\r\nBT CG EP IS LU MG SPGeo\r\nMSQ Lynx\r\n(a) SRMT\r\n0.80\r\n0.85\r\n0.90\r\n0.95\r\n1.00\r\n1.05\r\n1.10\r\n1.15\r\n1.20\r\n1.25\r\nBT CG EP IS LU MG SPGeo\r\nMSQ Lynx\r\n(b) SD3\r\n0.96\r\n0.98\r\n1.00\r\n1.02\r\n1.04\r\n1.06\r\n1.08\r\n1.10\r\n1.12\r\n1.14\r\n1.16\r\n1.18\r\n1T 2T 3T 4T 5T 6T Geo\r\nMSQ Lynx\r\n(c) NTM Speedups\r\nFigure 16: Normalised Speedup using Lynx over MSQ.\r\nCLF SP/SC Queue Sections Drawbacks Performance Enhancements\r\nLamport’s [14, 7] 1 Cache ping-pong Lock-free queue\r\nFastForward [8] 1 Non-generic synchronisation Write/Read slip\r\nDBLS[24] > 1 Number of synchronisation instructions Lazy synchronisation\r\nMCRingBuffer[15, 16] > 1 Number of synchronisation instructions Lazy synchronisation\r\nLiberty[11] > 1 Number of synchronisation instructions Non-temporal SSE instructions (e.g. movnti), prefetching\r\nHAQu [17] 1 Hardware modification Extra Hardware\r\nLynx (proposed) > 1 Handler’s overhead on very small queue sizes Synchronisation overhead is moved off the critical path\r\nTable 2: Overview of features provided by CLF queues.\r\net al. [16] and is composed of several threads, as shown in\r\nfigure 15c. The main thread parses IPv4 network packet\r\nheaders, extracts information and dispatches them to sub\u0002analyser threads. These gather partial statistics and provide\r\nthe data to the main analyser which collects them together.\r\nWe evaluated NTM performance on the Xeon E5-2667\r\n(table 1) varying the number of sub-analyser threads from 1\r\nto 6. We used the queue size that leads to best performance\r\nfor each queue (128KB for MSQ, 2MB for Lynx), mov for\r\nboth. The speedups shown in figure 16c are normalised to\r\nMSQ. The geometric mean speedup of Lynx over MSQ is\r\n1.09x.\r\n6. RELATED WORK\r\nLamport proved that a concurrent lock-free (CLF) queue\r\ncan be implemented in a SP/SC scenario [7, 14]. However,\r\neven though it is lock-free, this queue has very poor per\u0002formance due to frequent cache ping-pong of queue control\r\nvariables.\r\nAn improvement over Lamport’s implementation is the\r\nFastForward CLF queue [8]. In this implementation the en\u0002queue and dequeue indices are private to the enqueue and\r\ndequeue functions and their values are never passed to the\r\nother. To avoid the threads overtaking each other, the de\u0002queue operation writes NULL values to the queue after read\u0002ing data from it; enqueue checks that the queue value at\r\nthe write index is NULL before overwriting it. False shar\u0002ing (reading and writing to different queue elements in the\r\nsame cache line) is avoided by enforcing a buffer between\r\nthe writes and reads (referred to as slip).\r\nMultiple sections were introduced by DBLS [24] and\r\nMCRingBuffer [15, 16] queues. Both queues keep enqueue\r\nand dequeue indices private and occasionally share these val\u0002ues, once for every section. Liberty queues [11] are similar to\r\nboth DBLS and MCRingBuffer in design, but they introduce\r\nseveral implementation-specific performance improvements,\r\nincluding non-symmetric producer and consumer, prefetch\u0002ing, streaming instructions. Finally, [26] studies the dead\u0002lock problem of multi-section queues and proposes a solution\r\nto it.\r\nIn HAQu [17], the authors recognise that the high instruc\u0002tion overhead of existing software queues becomes critical\r\nin fine-grained communication. They propose a hardware\u0002accelerated queue to decrease the number of instructions\r\nwithin enqueue/dequeue functions.\r\nAn overview of the attributes of the SP/SC CLF queues\r\nare shown in table 2. Several CLF queues have been stud\u0002ied since Lamport’s research [13, 18, 19, 21, 22, 23]. These\r\nworks, however, have focused on improving MP/MC queues,\r\nrather than improving the SP/SC case. These have higher\r\noverheads to avoid ABA problems [18] and to maintain lin\u0002earizability [10].\r\nExploiting the hardware/OS memory protection systems\r\nfor improving performance has been used in the past. A\r\nsurvey of algorithms that make use of memory protection\r\ntechniques is presented in [6]. Typical uses include garbage\r\ncollection, and overflow protections. A similar technique has\r\nbeen used under the fine-grained Mondrian memory protec\u0002tion [27] system to implement a zero-copy network stack.\r\nMore recently, the memory protection system has also been\r\nused in the context of software transactional memories to\r\nachieve strong atomicity [5].\r\n7. CONCLUSION\r\nHigh performance single-producer / single-consumer soft\u0002ware queues are fundamental building components of par\u0002allel software. Maximising their efficiency is crucial for ex\u0002tracting the maximum performance of parallel software. This\r\npaper has presented Lynx, a radically new software archi\u0002tecture for SP/SC queues, which reduces the critical path\r\noverhead of enqueue and dequeue operations down to a min\u0002imum. Evaluation of Lynx on various commodity hardware\r\nplatforms shows throughput improvements of over 1.57×\r\ncompared to the state-of-the-art and significant improve\u0002ments in actual applications of up to 1.4×.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/2c1e8f8d-f643-4523-9fc1-918056c047c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5ccd92bafabc39f3af60cf1772544c2fd3b4b49893ef15681efb5bc55798d615",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 739
      },
      {
        "segments": [
          {
            "segment_id": "1e9960bd-1821-4e44-a524-d4c43bc61f8d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "Acknowledgements\r\nThis work was supported by the Engineering and Phys\u0002ical Sciences Research Council (EPSRC), through grant\r\nreference EP/K026399/1. Additional data related to\r\nthis publication is available in the data repository at\r\nhttps://www.repository.cam.ac.uk/handle/1810/254651.\r\n8. REFERENCES\r\n[1] GCC: Gnu Compiler Collection. http://gcc.gnu.org.\r\n[2] NAS Parallel Benchmarks.\r\nhttp://www.nas.nasa.gov/publications/npb.html.\r\n[3] The LLVM Compiler Infrastructure. http://llvm.org.\r\n[4] Intel 64 and IA-32 Architectures Software Developer’s\r\nManual. 2015.\r\n[5] M. Abadi, T. Harris, and M. Mehrara. Transactional\r\nMemory with Strong Atomicity Using Off-the-shelf\r\nMemory Protection Hardware. In Proceedings of the\r\n14th Symposium on Principles and Practice of Paral\u0002lel Programming (PPoPP), 2009.\r\n[6] A. W. Appel and K. Li. Virtual Memory Primitives for\r\nUser Programs. In Proceedings of the 4th International\r\nConference on Architectural Support for Programming\r\nLanguages and Operating Systems (ASPLOS IV), 1991.\r\n[7] K. Gharachorloo and P. B. Gibbons. Detecting Viola\u0002tions of Sequential Consistency. In Proceedings of the\r\n3rd Annual Symposium on Parallel Algorithms and Ar\u0002chitectures (SPAA), 1991.\r\n[8] J. Giacomoni, T. Moseley, and M. Vachharajani. Fast\u0002Forward for Efficient Pipeline Parallelism: A Cache\u0002Optimized Concurrent Lock-free Queue. In Proceedings\r\nof the 13th Symposium on Principles and Practice of\r\nParallel Programming (PPoPP), 2008.\r\n[9] M. Girkar and C. D. Polychronopoulos. Automatic Ex\u0002traction of Functional Parallelism from Ordinary Pro\u0002grams. Transactions on Parallel and Distributed Sys\u0002tems, 3(2), 1992.\r\n[10] M. P. Herlihy and J. M. Wing. Linearizability: A Cor\u0002rectness Condition for Concurrent Objects. Transac\u0002tions on Programming Languages and Systems, 12(3),\r\n1990.\r\n[11] T. B. Jablin, Y. Zhang, J. A. Jablin, J. Huang, H. Kim,\r\nand D. I. August. Liberty Queues for EPIC Architec\u0002tures. In Proceedings of EPIC Workshop, 2010.\r\n[12] M. Kim, H. Kim, and C.-K. Luk. SD3: A Scalable Ap\u0002proach to Dynamic Data-Dependence Profiling. In Pro\u0002ceedings of the 43rd Annual International Symposium\r\non Microarchitecture (MICRO 43), 2010.\r\n[13] E. Ladan-Mozes and N. Shavit. An Optimistic Ap\u0002proach to Lock-Free FIFO Queues. Distributed Com\u0002puting, 20(5), 2007.\r\n[14] L. Lamport. Specifying Concurrent Program Modules.\r\nTransactions on Programming Languages and Systems,\r\n5(2), 1983.\r\n[15] P. P. C. Lee, T. Bu, and G. Chandranmenon. A Lock\u0002Free, Cache-Efficient Shared Ring Buffer for Multi-Core\r\nArchitectures. In Proceedings of the 5th Symposium on\r\nArchitectures for Networking and Communications Sys\u0002tems (ANCS), 2009.\r\n[16] P. P. C. Lee, T. Bu, and G. Chandranmenon. A\r\nLock-Free, Cache-Efficient Multi-Core Synchronization\r\nMechanism for Line-Rate Network Traffic Monitoring.\r\nIn International Parallel and Distributed Processing\r\nSymposium (IPDPS), 2010.\r\n[17] S. Lee, D. Tiwari, Y. Solihin, and J. Tuck.\r\nHAQu: Hardware-Accelerated Queueing for Fine\u0002Grained Threading on a Chip Multiprocessor. In 17th\r\nInternational Symposium on High Performance Com\u0002puter Architecture (HPCA), 2011.\r\n[18] M. M. Michael and M. L. Scott. Nonblocking Al\u0002gorithms and Preemption-Safe Locking on Multipro\u0002grammed Shared Memory Multiprocessors. Journal of\r\nParallel and Distributed Computing, 51(1), 1998.\r\n[19] M. Moir, D. Nussbaum, O. Shalev, and N. Shavit. Us\u0002ing Elimination to Implement Scalable and Lock-Free\r\nFIFO Queues. In Proceedings of the 17th Annual Sym\u0002posium on Parallelism in Algorithms and Architectures\r\n(SPAA), 2005.\r\n[20] G. Ottoni, R. Rangan, A. Stoler, and D. I. August.\r\nAutomatic Thread Extraction with Decoupled Software\r\nPipelining. In Proceedings of the 38th Annual Interna\u0002tional Symposium on Microarchitecture (MICRO 38),\r\n2005.\r\n[21] S. Prakash, Y. H. Lee, and T. Johnson. A Nonblock\u0002ing Algorithm for Shared Queues Using Compare-and\u0002Swap. Transactions on Computers, 43(5), 1994.\r\n[22] W. N. Scherer, III, D. Lea, and M. L. Scott. Scalable\r\nSynchronous Queues. In Proceedings of the 11th Sym\u0002posium on Principles and Practice of Parallel Program\u0002ming (PPoPP), 2006.\r\n[23] P. Tsigas and Y. Zhang. A Simple, Fast and Scal\u0002able Non-blocking Concurrent FIFO Queue for Shared\r\nMemory Multiprocessor Systems. In Proceedings of the\r\n13th Annual Symposium on Parallel Algorithms and Ar\u0002chitectures (SPAA), 2001.\r\n[24] C. Wang, H. s. Kim, Y. Wu, and V. Ying. Compiler\u0002Managed Software-based Redundant Multi-Threading\r\nfor Transient Fault Detection. In International Sym\u0002posium on Code Generation and Optimization (CGO),\r\n2007.\r\n[25] C. C. Wang and Y. Wu. Apparatus and Method for\r\nRedundant Software Thread Computation. 2010. US\r\nPatent 7,818,744.\r\n[26] J. Wang, K. Zhang, X. Tang, and B. Hua. B-Queue:\r\nEfficient and Practical Queuing for Fast Core-to-Core\r\nCommunication. International Journal of Parallel Pro\u0002gramming, 41(1), 2012.\r\n[27] E. Witchel, J. Cates, and K. Asanovi´c. Mondrian mem\u0002ory protection. In Proceedings of the 10th International\r\nConference on Architectural Support for Programming\r\nLanguages and Operating Systems (ASPLOS X), 2002.\r\n[28] Y. Zhang, J. W. Lee, N. P. Johnson, and D. I. August.\r\nDAFT: Decoupled Acyclic Fault Tolerance. In Proceed\u0002ings of the 19th International Conference on Paral\u0002lel Architectures and Compilation Techniques (PACT),\r\n2010.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/1e9960bd-1821-4e44-a524-d4c43bc61f8d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3fac981d5bbb40e8ed69f647dc4b1b072ce19fd22005a9f627f61090f178e1af",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 728
      },
      {
        "segments": [
          {
            "segment_id": "1e9960bd-1821-4e44-a524-d4c43bc61f8d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "Acknowledgements\r\nThis work was supported by the Engineering and Phys\u0002ical Sciences Research Council (EPSRC), through grant\r\nreference EP/K026399/1. Additional data related to\r\nthis publication is available in the data repository at\r\nhttps://www.repository.cam.ac.uk/handle/1810/254651.\r\n8. REFERENCES\r\n[1] GCC: Gnu Compiler Collection. http://gcc.gnu.org.\r\n[2] NAS Parallel Benchmarks.\r\nhttp://www.nas.nasa.gov/publications/npb.html.\r\n[3] The LLVM Compiler Infrastructure. http://llvm.org.\r\n[4] Intel 64 and IA-32 Architectures Software Developer’s\r\nManual. 2015.\r\n[5] M. Abadi, T. Harris, and M. Mehrara. Transactional\r\nMemory with Strong Atomicity Using Off-the-shelf\r\nMemory Protection Hardware. In Proceedings of the\r\n14th Symposium on Principles and Practice of Paral\u0002lel Programming (PPoPP), 2009.\r\n[6] A. W. Appel and K. Li. Virtual Memory Primitives for\r\nUser Programs. In Proceedings of the 4th International\r\nConference on Architectural Support for Programming\r\nLanguages and Operating Systems (ASPLOS IV), 1991.\r\n[7] K. Gharachorloo and P. B. Gibbons. Detecting Viola\u0002tions of Sequential Consistency. In Proceedings of the\r\n3rd Annual Symposium on Parallel Algorithms and Ar\u0002chitectures (SPAA), 1991.\r\n[8] J. Giacomoni, T. Moseley, and M. Vachharajani. Fast\u0002Forward for Efficient Pipeline Parallelism: A Cache\u0002Optimized Concurrent Lock-free Queue. In Proceedings\r\nof the 13th Symposium on Principles and Practice of\r\nParallel Programming (PPoPP), 2008.\r\n[9] M. Girkar and C. D. Polychronopoulos. Automatic Ex\u0002traction of Functional Parallelism from Ordinary Pro\u0002grams. Transactions on Parallel and Distributed Sys\u0002tems, 3(2), 1992.\r\n[10] M. P. Herlihy and J. M. Wing. Linearizability: A Cor\u0002rectness Condition for Concurrent Objects. Transac\u0002tions on Programming Languages and Systems, 12(3),\r\n1990.\r\n[11] T. B. Jablin, Y. Zhang, J. A. Jablin, J. Huang, H. Kim,\r\nand D. I. August. Liberty Queues for EPIC Architec\u0002tures. In Proceedings of EPIC Workshop, 2010.\r\n[12] M. Kim, H. Kim, and C.-K. Luk. SD3: A Scalable Ap\u0002proach to Dynamic Data-Dependence Profiling. In Pro\u0002ceedings of the 43rd Annual International Symposium\r\non Microarchitecture (MICRO 43), 2010.\r\n[13] E. Ladan-Mozes and N. Shavit. An Optimistic Ap\u0002proach to Lock-Free FIFO Queues. Distributed Com\u0002puting, 20(5), 2007.\r\n[14] L. Lamport. Specifying Concurrent Program Modules.\r\nTransactions on Programming Languages and Systems,\r\n5(2), 1983.\r\n[15] P. P. C. Lee, T. Bu, and G. Chandranmenon. A Lock\u0002Free, Cache-Efficient Shared Ring Buffer for Multi-Core\r\nArchitectures. In Proceedings of the 5th Symposium on\r\nArchitectures for Networking and Communications Sys\u0002tems (ANCS), 2009.\r\n[16] P. P. C. Lee, T. Bu, and G. Chandranmenon. A\r\nLock-Free, Cache-Efficient Multi-Core Synchronization\r\nMechanism for Line-Rate Network Traffic Monitoring.\r\nIn International Parallel and Distributed Processing\r\nSymposium (IPDPS), 2010.\r\n[17] S. Lee, D. Tiwari, Y. Solihin, and J. Tuck.\r\nHAQu: Hardware-Accelerated Queueing for Fine\u0002Grained Threading on a Chip Multiprocessor. In 17th\r\nInternational Symposium on High Performance Com\u0002puter Architecture (HPCA), 2011.\r\n[18] M. M. Michael and M. L. Scott. Nonblocking Al\u0002gorithms and Preemption-Safe Locking on Multipro\u0002grammed Shared Memory Multiprocessors. Journal of\r\nParallel and Distributed Computing, 51(1), 1998.\r\n[19] M. Moir, D. Nussbaum, O. Shalev, and N. Shavit. Us\u0002ing Elimination to Implement Scalable and Lock-Free\r\nFIFO Queues. In Proceedings of the 17th Annual Sym\u0002posium on Parallelism in Algorithms and Architectures\r\n(SPAA), 2005.\r\n[20] G. Ottoni, R. Rangan, A. Stoler, and D. I. August.\r\nAutomatic Thread Extraction with Decoupled Software\r\nPipelining. In Proceedings of the 38th Annual Interna\u0002tional Symposium on Microarchitecture (MICRO 38),\r\n2005.\r\n[21] S. Prakash, Y. H. Lee, and T. Johnson. A Nonblock\u0002ing Algorithm for Shared Queues Using Compare-and\u0002Swap. Transactions on Computers, 43(5), 1994.\r\n[22] W. N. Scherer, III, D. Lea, and M. L. Scott. Scalable\r\nSynchronous Queues. In Proceedings of the 11th Sym\u0002posium on Principles and Practice of Parallel Program\u0002ming (PPoPP), 2006.\r\n[23] P. Tsigas and Y. Zhang. A Simple, Fast and Scal\u0002able Non-blocking Concurrent FIFO Queue for Shared\r\nMemory Multiprocessor Systems. In Proceedings of the\r\n13th Annual Symposium on Parallel Algorithms and Ar\u0002chitectures (SPAA), 2001.\r\n[24] C. Wang, H. s. Kim, Y. Wu, and V. Ying. Compiler\u0002Managed Software-based Redundant Multi-Threading\r\nfor Transient Fault Detection. In International Sym\u0002posium on Code Generation and Optimization (CGO),\r\n2007.\r\n[25] C. C. Wang and Y. Wu. Apparatus and Method for\r\nRedundant Software Thread Computation. 2010. US\r\nPatent 7,818,744.\r\n[26] J. Wang, K. Zhang, X. Tang, and B. Hua. B-Queue:\r\nEfficient and Practical Queuing for Fast Core-to-Core\r\nCommunication. International Journal of Parallel Pro\u0002gramming, 41(1), 2012.\r\n[27] E. Witchel, J. Cates, and K. Asanovi´c. Mondrian mem\u0002ory protection. In Proceedings of the 10th International\r\nConference on Architectural Support for Programming\r\nLanguages and Operating Systems (ASPLOS X), 2002.\r\n[28] Y. Zhang, J. W. Lee, N. P. Johnson, and D. I. August.\r\nDAFT: Decoupled Acyclic Fault Tolerance. In Proceed\u0002ings of the 19th International Conference on Paral\u0002lel Architectures and Compilation Techniques (PACT),\r\n2010.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/248c5e86-bf0b-4ea9-8819-9ce920ac645f/images/1e9960bd-1821-4e44-a524-d4c43bc61f8d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041541Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3fac981d5bbb40e8ed69f647dc4b1b072ce19fd22005a9f627f61090f178e1af",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 728
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "No response"
        }
      ]
    }
  }
}