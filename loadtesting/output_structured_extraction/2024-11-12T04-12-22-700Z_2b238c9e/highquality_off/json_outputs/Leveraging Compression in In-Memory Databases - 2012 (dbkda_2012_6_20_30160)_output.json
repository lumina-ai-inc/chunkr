{
  "file_name": "Leveraging Compression in In-Memory Databases - 2012 (dbkda_2012_6_20_30160).pdf",
  "task_id": "a2b66cf6-dd90-4103-96ae-d4ba6acaefb3",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "eccdba19-3603-4d0d-904a-41badda0a7ea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Leveraging Compression in In-Memory Databases\r\nJens Krueger, Johannes Wust, Martin Linkhorst, Hasso Plattner\r\nHasso Plattner Institute for Software Engineering\r\nUniversity of Potsdam\r\nPotsdam, Germany\r\nEmail: {jens.krueger@hpi.uni-potsdam.de, johannes.wust@hpi.uni-potsdam.de,\r\nmartin.linkhorst@hpi.uni-potsdam.de, hasso.plattner@hpi.uni-potsdam.de}\r\nAbstract—Recently, there has been a trend towards column\u0002oriented databases, which in most cases apply lightweight com\u0002pression techniques to improve read access. At the same time,\r\nin-memory databases become reality due to availability of huge\r\namounts of main memory. In-memory databases achieve their\r\noptimal performance by building up cache-aware algorithms\r\nbased on cost models for memory hierarchies. In this paper, we\r\nuse a generic cost model for main memory access and show how\r\nlightweight compression schemes improve the cache behavior,\r\nwhich directly correlates with the performance of in-memory\r\ndatabases.\r\nKeywords-in-memory databases; database compression; dictio\u0002nary compression.\r\nI. INTRODUCTION\r\nNowadays, most database management systems are hard\r\ndisk based and - since I/O-operations are expensive - therefore,\r\nlimited by both the throughput and latency of those hard\r\ndisks. Increasing capacities of main memory that reach up\r\nto several terabytes today offer the opportunity to store an\r\nentire database completely in main memory. Besides, the much\r\nhigher throughput of main memory compared to disk access\r\nsignificant performance improvements are also achieved by the\r\nmuch faster random access capability of main memory and at\r\nthe same time much lower latency. A database management\r\nsystem that stores all of its data completely in main memory -\r\nusing hard disks only for persistency and recovery – is called\r\nan in-memory database (IMDB).\r\nIn earlier work, we have shown that in-memory databases\r\nperform especially well in enterprise application scenar\u0002ios [12], [14]. As shown in [12], enterprise workloads are\r\nmostly reads rather than data modification operations; this has\r\nlead to the conclusion to leverage read-optimized databases\r\nwith a differential buffer for this workloads [11]. Furthermore,\r\nenterprise data is typically sparse data with a well known\r\nvalue domain and a relatively low number of distinct values.\r\nTherefore, enterprise data qualifies particularly well for data\r\ncompression as these techniques exploit redundancy within\r\ndata and knowledge about the data domain for optimal results.\r\nWe apply compression for two reasons:\r\n• Reducing the overall size of the database to fit the entire\r\ndatabase into main memory, and\r\n• Increasing database performance by reducing the amount\r\nof data transferred from and to main memory.\r\nIn this paper, we focus on the second aspect. We analyze\r\ndifferent lightweight compression schemes regarding cache\r\nbehavior, based on a cost model that estimates expected cache\r\nmisses.\r\nA. The Memory Bottleneck\r\nDuring the last two decades, processor speed increased\r\nfaster than memory speed did [6]. The effect of this de\u0002velopment is that processors nowadays have to wait more\r\ncycles to get a response from memory than they needed to\r\n20 years ago. Since processors need to access data from\r\nmemory for any computation, performance improvements are\r\nlimited by memory latency time. As seen from a processor’s\r\nperspective, main memory access becomes more and more\r\nexpensive compared to earlier days – the Memory Gap widens.\r\nNevertheless, it would be possible to manufacture memory that\r\nis as fast as a processor is but there is a direct trade-off between\r\nmemory size and latency. The more capacity memory has, the\r\nlonger is its latency time or - important as well - the faster\r\nmemory is, the more expensive it gets. Since manufacturers\r\nconcentrated on increasing capacity of main memory there\r\nwasn’t much focus on improving latency times.\r\nA solution to the problem found in modern processors is\r\nthe use of a cache hierarchy to hide the latency of the main\r\nmemory. Between the processors registers and main memory,\r\na faster but smaller memory layer is placed that holds copies\r\nof a subset of data found in main memory. When a processor\r\nfinds the needed data in the cache it will copy it from there\r\nwaiting less processor cycles. The whole cache is usually much\r\nsmaller and much faster than main memory. Since the Memory\r\nGap widens with every new processor generation one layer of\r\ncache is not enough to fulfill both capacity and latency time\r\ndemands. Therefore, modern CPUs have up to three layers of\r\ncache, each of which with more capacity but worse latency\r\ntimes than the one closer to the processor [8].\r\nSince programs usually do not need to access the whole\r\naddress space of main memory randomly there is the concept\r\nof locality. When a processor fetches a processor word from\r\nmemory, it is very likely that it needs to fetch another\r\nword close by, so-called data locality. Leveraging that fact,\r\nprocessors do not only copy the requested data to its registers\r\nbut also copy subsequent bytes to the cache. The amount of\r\nbytes that are copied at once to the cache is called a cache\r\nline or a cache block and usually is about four to 16 processor\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 147\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/eccdba19-3603-4d0d-904a-41badda0a7ea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fb4a7d84db3dbdd831cc23cf0f0c30d7286c736d270477265c6f948e1ffb3f09",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 820
      },
      {
        "segments": [
          {
            "segment_id": "eccdba19-3603-4d0d-904a-41badda0a7ea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Leveraging Compression in In-Memory Databases\r\nJens Krueger, Johannes Wust, Martin Linkhorst, Hasso Plattner\r\nHasso Plattner Institute for Software Engineering\r\nUniversity of Potsdam\r\nPotsdam, Germany\r\nEmail: {jens.krueger@hpi.uni-potsdam.de, johannes.wust@hpi.uni-potsdam.de,\r\nmartin.linkhorst@hpi.uni-potsdam.de, hasso.plattner@hpi.uni-potsdam.de}\r\nAbstract—Recently, there has been a trend towards column\u0002oriented databases, which in most cases apply lightweight com\u0002pression techniques to improve read access. At the same time,\r\nin-memory databases become reality due to availability of huge\r\namounts of main memory. In-memory databases achieve their\r\noptimal performance by building up cache-aware algorithms\r\nbased on cost models for memory hierarchies. In this paper, we\r\nuse a generic cost model for main memory access and show how\r\nlightweight compression schemes improve the cache behavior,\r\nwhich directly correlates with the performance of in-memory\r\ndatabases.\r\nKeywords-in-memory databases; database compression; dictio\u0002nary compression.\r\nI. INTRODUCTION\r\nNowadays, most database management systems are hard\r\ndisk based and - since I/O-operations are expensive - therefore,\r\nlimited by both the throughput and latency of those hard\r\ndisks. Increasing capacities of main memory that reach up\r\nto several terabytes today offer the opportunity to store an\r\nentire database completely in main memory. Besides, the much\r\nhigher throughput of main memory compared to disk access\r\nsignificant performance improvements are also achieved by the\r\nmuch faster random access capability of main memory and at\r\nthe same time much lower latency. A database management\r\nsystem that stores all of its data completely in main memory -\r\nusing hard disks only for persistency and recovery – is called\r\nan in-memory database (IMDB).\r\nIn earlier work, we have shown that in-memory databases\r\nperform especially well in enterprise application scenar\u0002ios [12], [14]. As shown in [12], enterprise workloads are\r\nmostly reads rather than data modification operations; this has\r\nlead to the conclusion to leverage read-optimized databases\r\nwith a differential buffer for this workloads [11]. Furthermore,\r\nenterprise data is typically sparse data with a well known\r\nvalue domain and a relatively low number of distinct values.\r\nTherefore, enterprise data qualifies particularly well for data\r\ncompression as these techniques exploit redundancy within\r\ndata and knowledge about the data domain for optimal results.\r\nWe apply compression for two reasons:\r\n• Reducing the overall size of the database to fit the entire\r\ndatabase into main memory, and\r\n• Increasing database performance by reducing the amount\r\nof data transferred from and to main memory.\r\nIn this paper, we focus on the second aspect. We analyze\r\ndifferent lightweight compression schemes regarding cache\r\nbehavior, based on a cost model that estimates expected cache\r\nmisses.\r\nA. The Memory Bottleneck\r\nDuring the last two decades, processor speed increased\r\nfaster than memory speed did [6]. The effect of this de\u0002velopment is that processors nowadays have to wait more\r\ncycles to get a response from memory than they needed to\r\n20 years ago. Since processors need to access data from\r\nmemory for any computation, performance improvements are\r\nlimited by memory latency time. As seen from a processor’s\r\nperspective, main memory access becomes more and more\r\nexpensive compared to earlier days – the Memory Gap widens.\r\nNevertheless, it would be possible to manufacture memory that\r\nis as fast as a processor is but there is a direct trade-off between\r\nmemory size and latency. The more capacity memory has, the\r\nlonger is its latency time or - important as well - the faster\r\nmemory is, the more expensive it gets. Since manufacturers\r\nconcentrated on increasing capacity of main memory there\r\nwasn’t much focus on improving latency times.\r\nA solution to the problem found in modern processors is\r\nthe use of a cache hierarchy to hide the latency of the main\r\nmemory. Between the processors registers and main memory,\r\na faster but smaller memory layer is placed that holds copies\r\nof a subset of data found in main memory. When a processor\r\nfinds the needed data in the cache it will copy it from there\r\nwaiting less processor cycles. The whole cache is usually much\r\nsmaller and much faster than main memory. Since the Memory\r\nGap widens with every new processor generation one layer of\r\ncache is not enough to fulfill both capacity and latency time\r\ndemands. Therefore, modern CPUs have up to three layers of\r\ncache, each of which with more capacity but worse latency\r\ntimes than the one closer to the processor [8].\r\nSince programs usually do not need to access the whole\r\naddress space of main memory randomly there is the concept\r\nof locality. When a processor fetches a processor word from\r\nmemory, it is very likely that it needs to fetch another\r\nword close by, so-called data locality. Leveraging that fact,\r\nprocessors do not only copy the requested data to its registers\r\nbut also copy subsequent bytes to the cache. The amount of\r\nbytes that are copied at once to the cache is called a cache\r\nline or a cache block and usually is about four to 16 processor\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 147\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/eccdba19-3603-4d0d-904a-41badda0a7ea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fb4a7d84db3dbdd831cc23cf0f0c30d7286c736d270477265c6f948e1ffb3f09",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 820
      },
      {
        "segments": [
          {
            "segment_id": "855d2482-8b40-4aa5-ad64-3e01abcb1190",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "words long depending on the specific CPU architecture. On a\r\ncurrent 64 bit machine, it is between 32 and 128 bytes.\r\nConsequently, memory access is not truly random since\r\nalways a complete cache line is fetched regardless the actual\r\nrequested value. In the worst case, only one value out of the\r\ncache line are needed while the rest of the transferred date\r\nis polluting both the limited memory bandwidth and limited\r\ncapacity on each cache. Data that are not found in the cache\r\nneeds to be fetched from main memory, a so-called cache\r\nmiss. There is a direct dependency between the performance\r\nof in-memory database algorithms and the number of issued\r\ncache misses as for instance described in [4], [15]. To gain\r\nsignificant performance improvements or to avoid performance\r\nloss, algorithms have to be cache conscious, which means that\r\nthey have to efficiently use the cache and cache lines issuing\r\nas few cache misses as possible. This means data should be\r\nread sequentially from main memory instead of randomly.\r\nMulticore processors that are supposed to work in parallel\r\nhave to wait for other cores to finish their shared memory ac\u0002cess before starting its own. Additionally, the physical distance\r\nbetween a processor and its cache also influences the latency\r\ntime. Multicore processors’ shared cache is normally placed in\r\nequal distance to each core resulting in less performance than\r\npossible on a single core chip. Intel has a solution called Non\u0002Uniform Memory Access (NUMA), where the shared memory\r\nis logically the same but physically splitted on the chip. For\r\nexample the first half of the address space is local to core\r\none and the second half is local to core two resulting in better\r\nperformance for core one when accessing addresses in the first\r\nhalf but worse performance for the other addresses. When core\r\none requests data from main memory it will be fetched into an\r\naddress in the first half of the address space if possible [16].\r\nB. In-memory databases in Enterprise Application scenarios\r\nToday’s disk-based database management systems are ei\u0002ther optimized for transactional record-oriented or analytical\r\nattribute-oriented workload, also called Online Transaction\r\nProcessing (OLTP) and Online Analytical Processing (OLAP).\r\nThe distinguishment arises from enterprises that have trans\u0002actional systems to support their daily business and need\r\nto answer analytical queries on top of that data. OLAP\r\nstyle queries are typically slow on OLTP system; therefore,\r\nenterprises usually have a separate OLAP system, e.g., a data\r\nwarehouse, that stores the same information in a different way\r\nand precomputes certain values up-front to improve query\r\nperformance of analytical queries. The main reason for the\r\nperformance loss is that OLAP queries are attribute-focused\r\nrather than entity-focused, usually reading only a few attributes\r\nbut more records, e.g., read a whole column or apply a\r\npredicate on a complete column. Most OLTP systems store\r\ntheir data row-oriented: a record is stored sequentially on\r\ndisk and then another record follows maintained by a page\r\nlayout. Since OLAP queries read only a part of many records,\r\ne.g., one attribute of each record, the needed data is not\r\nstored sequentially on disk resulting in less read performance.\r\nFurthermore, the page layout determines the access pattern\r\nthat read complete pages from disk as this is the finest gran\u0002ularity to read a record. Due to this fact lots of unnecessary\r\ndata is transferred in case a few attributes of a relation are\r\nrequested. Therefore, modern OLAP systems organize the data\r\ncolumn-oriented to improve performance of accessing whole\r\nattributes [21].\r\nWith up to several terabytes of main memory available to\r\napplications as well as the increase of computing power with\r\nnew multi core hardware architectures holding entire databases\r\nin main memory becomes feasible [17]; the application of\r\nthese in-memory databases is especially promising in the field\r\nof enterprise applications.\r\nIn [14], we could show that Enterprise Applications typ\u0002ically reveal a mix of OLAP and OLTP characteristics. In\r\norder to combine both requirements for mixed workload\r\nscenarios, the introduction of a write optimized differential\r\nbuffer together with a read-optimized main storage has been\r\nproposed [7], [11], [21]. The differential buffer stores all\r\nwrite operations in an uncompressed manner to allow fast\r\nappends. At regular intervals, the differential buffer is merged\r\nwith the main database to maintain compression and query\r\nperformance. During this process the buffered values are\r\nmerge into the read-optimized store as described in [11].\r\nThe merge process essentially does two things: it merges\r\nthe main dictionary with the delta dictionary and keeps track\r\nof value ids that may have changed along with their new value.\r\nThen, it merges the main attribute vector of the compressed\r\nread-optimized store and the attribute vector of the differential\r\nbuffer while applying the old-value-id/new-value-id mapping\r\nfrom the step before. That second step is not needed if value\r\nids cannot change like in the basic dictionary or hash map\r\napproach. However, in an order-preserving dictionary approach\r\nthat mapping needs to be applied taking a significant amount of\r\nclock cycles of the overall merge process. The same happens\r\nif the value id are bit compressed and a new dictionary entry\r\nmake an additional bit necessary in order to represent the\r\nvalues.\r\nII. COMPRESSION\r\nA. Motivation\r\nAs described in the previous section, main memory latency\r\nis a bottleneck for the execution time of computations: proces\u0002sors are wasting cycles while waiting for data to arrive. This is\r\nespecially true for databases as described in [4]. While cache\r\nconscious algorithms are one way to improve performance\r\nsignificantly [3], [19], [20] another option is to reduce the\r\namount of data transferred from and to main memory, which\r\ncan be achieved by compressing data [22]. On the one hand,\r\ncompression reduces I/O-operations between main memory\r\nand processor registers, on the other hand it leverages the\r\ncache hierarchy more effectively, because more data fits in\r\neach cache line.\r\nThe needed processor cycles to compress and decompress\r\ndata and the less wasted cycles while waiting for memory\r\nresult in increased processor utilization. This increases overall\r\nperformance as long as memory access time is the bottleneck.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 148\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/855d2482-8b40-4aa5-ad64-3e01abcb1190.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e7ea47b1f1036a354bade3d283437b5de52c91c431434370318ba5b142719ea3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1022
      },
      {
        "segments": [
          {
            "segment_id": "855d2482-8b40-4aa5-ad64-3e01abcb1190",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "words long depending on the specific CPU architecture. On a\r\ncurrent 64 bit machine, it is between 32 and 128 bytes.\r\nConsequently, memory access is not truly random since\r\nalways a complete cache line is fetched regardless the actual\r\nrequested value. In the worst case, only one value out of the\r\ncache line are needed while the rest of the transferred date\r\nis polluting both the limited memory bandwidth and limited\r\ncapacity on each cache. Data that are not found in the cache\r\nneeds to be fetched from main memory, a so-called cache\r\nmiss. There is a direct dependency between the performance\r\nof in-memory database algorithms and the number of issued\r\ncache misses as for instance described in [4], [15]. To gain\r\nsignificant performance improvements or to avoid performance\r\nloss, algorithms have to be cache conscious, which means that\r\nthey have to efficiently use the cache and cache lines issuing\r\nas few cache misses as possible. This means data should be\r\nread sequentially from main memory instead of randomly.\r\nMulticore processors that are supposed to work in parallel\r\nhave to wait for other cores to finish their shared memory ac\u0002cess before starting its own. Additionally, the physical distance\r\nbetween a processor and its cache also influences the latency\r\ntime. Multicore processors’ shared cache is normally placed in\r\nequal distance to each core resulting in less performance than\r\npossible on a single core chip. Intel has a solution called Non\u0002Uniform Memory Access (NUMA), where the shared memory\r\nis logically the same but physically splitted on the chip. For\r\nexample the first half of the address space is local to core\r\none and the second half is local to core two resulting in better\r\nperformance for core one when accessing addresses in the first\r\nhalf but worse performance for the other addresses. When core\r\none requests data from main memory it will be fetched into an\r\naddress in the first half of the address space if possible [16].\r\nB. In-memory databases in Enterprise Application scenarios\r\nToday’s disk-based database management systems are ei\u0002ther optimized for transactional record-oriented or analytical\r\nattribute-oriented workload, also called Online Transaction\r\nProcessing (OLTP) and Online Analytical Processing (OLAP).\r\nThe distinguishment arises from enterprises that have trans\u0002actional systems to support their daily business and need\r\nto answer analytical queries on top of that data. OLAP\r\nstyle queries are typically slow on OLTP system; therefore,\r\nenterprises usually have a separate OLAP system, e.g., a data\r\nwarehouse, that stores the same information in a different way\r\nand precomputes certain values up-front to improve query\r\nperformance of analytical queries. The main reason for the\r\nperformance loss is that OLAP queries are attribute-focused\r\nrather than entity-focused, usually reading only a few attributes\r\nbut more records, e.g., read a whole column or apply a\r\npredicate on a complete column. Most OLTP systems store\r\ntheir data row-oriented: a record is stored sequentially on\r\ndisk and then another record follows maintained by a page\r\nlayout. Since OLAP queries read only a part of many records,\r\ne.g., one attribute of each record, the needed data is not\r\nstored sequentially on disk resulting in less read performance.\r\nFurthermore, the page layout determines the access pattern\r\nthat read complete pages from disk as this is the finest gran\u0002ularity to read a record. Due to this fact lots of unnecessary\r\ndata is transferred in case a few attributes of a relation are\r\nrequested. Therefore, modern OLAP systems organize the data\r\ncolumn-oriented to improve performance of accessing whole\r\nattributes [21].\r\nWith up to several terabytes of main memory available to\r\napplications as well as the increase of computing power with\r\nnew multi core hardware architectures holding entire databases\r\nin main memory becomes feasible [17]; the application of\r\nthese in-memory databases is especially promising in the field\r\nof enterprise applications.\r\nIn [14], we could show that Enterprise Applications typ\u0002ically reveal a mix of OLAP and OLTP characteristics. In\r\norder to combine both requirements for mixed workload\r\nscenarios, the introduction of a write optimized differential\r\nbuffer together with a read-optimized main storage has been\r\nproposed [7], [11], [21]. The differential buffer stores all\r\nwrite operations in an uncompressed manner to allow fast\r\nappends. At regular intervals, the differential buffer is merged\r\nwith the main database to maintain compression and query\r\nperformance. During this process the buffered values are\r\nmerge into the read-optimized store as described in [11].\r\nThe merge process essentially does two things: it merges\r\nthe main dictionary with the delta dictionary and keeps track\r\nof value ids that may have changed along with their new value.\r\nThen, it merges the main attribute vector of the compressed\r\nread-optimized store and the attribute vector of the differential\r\nbuffer while applying the old-value-id/new-value-id mapping\r\nfrom the step before. That second step is not needed if value\r\nids cannot change like in the basic dictionary or hash map\r\napproach. However, in an order-preserving dictionary approach\r\nthat mapping needs to be applied taking a significant amount of\r\nclock cycles of the overall merge process. The same happens\r\nif the value id are bit compressed and a new dictionary entry\r\nmake an additional bit necessary in order to represent the\r\nvalues.\r\nII. COMPRESSION\r\nA. Motivation\r\nAs described in the previous section, main memory latency\r\nis a bottleneck for the execution time of computations: proces\u0002sors are wasting cycles while waiting for data to arrive. This is\r\nespecially true for databases as described in [4]. While cache\r\nconscious algorithms are one way to improve performance\r\nsignificantly [3], [19], [20] another option is to reduce the\r\namount of data transferred from and to main memory, which\r\ncan be achieved by compressing data [22]. On the one hand,\r\ncompression reduces I/O-operations between main memory\r\nand processor registers, on the other hand it leverages the\r\ncache hierarchy more effectively, because more data fits in\r\neach cache line.\r\nThe needed processor cycles to compress and decompress\r\ndata and the less wasted cycles while waiting for memory\r\nresult in increased processor utilization. This increases overall\r\nperformance as long as memory access time is the bottleneck.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 148\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/855d2482-8b40-4aa5-ad64-3e01abcb1190.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e7ea47b1f1036a354bade3d283437b5de52c91c431434370318ba5b142719ea3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1022
      },
      {
        "segments": [
          {
            "segment_id": "82cccd94-e9dd-4983-be6d-631e5ce25f8d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Once compression and decompression become so processor\u0002intensive that the processor is limiting the performance instead\r\nof the memory, compression has a negative effect on the\r\noverall execution time. Therefore, most in-memory databases\r\nuse light-weight compression techniques that have low CPU\r\noverhead [1].\r\nIn addition, some operators can operate directly on com\u0002pressed data - saving decompression time. Abadi et al. illus\u0002trate [2] this concept of late materialization to further improve\r\nexecution speed of queries.\r\nIn order to estimate the performance improvements achieved\r\nby different compression techniques, the cost model to esti\u0002mate cache misses presented in [15] is extended by taking\r\ncompression into account. The basic formula of the model for\r\nan uncompressed column scan is:\r\nM(s trav(R)) = \u0018\r\nR.w · R.n\r\nB\r\n\u0019\r\n(1)\r\nwhere M(s trav(R)) are the estimated cache misses on\r\none cache level while traversing a region R in memory\r\nsequentially for the first time. The parameters are R.w being\r\nthe width of one data item in bytes, R.n which is the number\r\nof data items to traverse, as well as B which is the number\r\nof data items that fit into the cache. In case of an in-memory\r\ndatabase R.w is the width of a tuple while R.n is the number\r\nof tuples.\r\nAs in [15], an inclusive Level 1 cache is assumed meaning\r\nthat all data that is present in the Level 1 cache is also present\r\nin the Level 2 cache. This condition may only be violated\r\ntemporarily when data in the L1 is changed and marked as\r\ndirty before being written back to L2. This assumption holds\r\nfor Intel CPUs but not for AMD CPUs which have an exclusive\r\ncache, meaning that data can be either in L1 or in L2 bot not\r\nin both. Modeling exclusive caches is left for future work.\r\nIn the following, we provide the cost model for various\r\nlight-weight compression techniques and compare their per\u0002formance for a typical analytical query size. Other atomic data\r\npatterns as the conditional traversal read [12] can be extended\r\nthe same way.\r\nB. Run-Length Encoding\r\nWhen using run-length encoding (RLE), subsequent\r\nequal values in memory are stored as a RLE-tuple\r\nof (value, runLength), thus encoding a sequence of\r\n(1, 1, 3, 3, 3, 4, 4, 4) as ((1, 2),(3, 3),(4, 3)) reducing the size\r\nin memory the larger runs in the data exist. Whether a good\r\namount of runs exist depends on two parameters: First, equal\r\nvalues need to be stored subsequently - this is usually the case\r\nif the column is stored in sort order of the values. Second,\r\nif the column is ordered, the number of distinct values in\r\nthe data defines the number of tuples needed to be stored.\r\nHowever, having sorted data is much more important because\r\na randomly ordered column with a few distinct values can\r\ncontain no runs in worst case if the data is distributed equally.\r\nOn the other hand, if the number of unique values is close to\r\nthe number of data items, sorting the items has limited impact\r\non compression, as there are only few runs in this case.\r\nIn a sorted run-length encoded column the main indicator\r\nof the size of the column is the cardinality of distinct values.\r\nHence, the performance on aggregate operations in an in\u0002memory database is mainly based on the amount of distinct\r\nvalues. Assuming a column’s values are in sorted order and\r\nthe number of distinct values of that column is given by\r\n|D|, the column can be encoded with |D| RLE-tuples, each\r\nholding the value and the run-length. A defensive approach\r\nto determine the space needed for saving the run-length is to\r\ntake the maximum run-length one value can span. Then, the\r\nmaximum run-length is R.n, and therefore, can be encoded\r\nwith dlog2 R.ne bits (for simplicity reasons. Actually, it is\r\nrunLengthmax = R.n − |D| + 1), while bit-compressing the\r\nrun-length value.\r\nThe basic cost model can then be extended to take a run\u0002length encoded column into account:\r\nM(s trav(R)) = \u0018\r\n(R.w + dlog2 R.ne) · |D|\r\nB\r\n\u0019\r\n(2)\r\nSince each tuple has the overhead of storing the run-length,\r\nrun-length encoding becomes less effective as |D| comes close\r\nto R.n. Hence, the number of distinct values is important. The\r\nbreak even point can be estimated with:\r\n|D| =\r\n\u0018\r\nR.w · R.n\r\nR.w + dlog2 R.ne\r\n\u0019\r\n(3)\r\nA generalized formula for unsorted columns encoded with\r\nrun-length encoding depends on the average run-length of\r\nvalues in the collection which can be answered by examining\r\nthe topology of the data. For example, imagine a customer\r\ntable with a column of the customer’s address’ city name that\r\nis ordered in the sort order of another column with zip codes.\r\nClearly the city names aren’t in their sort order but they will\r\ncontain a good amount of runs since equal and similar zip\u0002codes map to the same city name. Given that average run\u0002length |r|, one can estimate the cache misses with:\r\nM(s trav(R)) =\r\n\r\n\r\n\r\n\r\n(R.w + dlog2 R.ne) ·\r\nl\r\nR.n\r\n|r|\r\nm\r\nB\r\n\r\n\r\n\r\n\r\n(4)\r\nThe break even point, i.e., the minimal number of the\r\naverage run-length can be computed with:\r\nr =\r\nR.w + dlog2 R.ne\r\nR.w\r\n(5)\r\nC. Bit-Vector Encoding\r\nBit-vector encoding stores a bitmap for each distinct value.\r\nEach bitmap has the length of the number of data items to\r\nencode in bits. The value 1 in a bitmap for a distinct value\r\nindicates that the data item with the same index has this\r\nparticular value. As each data item can only have one value\r\nassigned, only one bitmap has the value 1 at a given index.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 149\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/82cccd94-e9dd-4983-be6d-631e5ce25f8d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=86e973d48bc2b612a792f9a5dbd50782300e14e3784c48079c3d9aaebe7ac49a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 963
      },
      {
        "segments": [
          {
            "segment_id": "82cccd94-e9dd-4983-be6d-631e5ce25f8d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Once compression and decompression become so processor\u0002intensive that the processor is limiting the performance instead\r\nof the memory, compression has a negative effect on the\r\noverall execution time. Therefore, most in-memory databases\r\nuse light-weight compression techniques that have low CPU\r\noverhead [1].\r\nIn addition, some operators can operate directly on com\u0002pressed data - saving decompression time. Abadi et al. illus\u0002trate [2] this concept of late materialization to further improve\r\nexecution speed of queries.\r\nIn order to estimate the performance improvements achieved\r\nby different compression techniques, the cost model to esti\u0002mate cache misses presented in [15] is extended by taking\r\ncompression into account. The basic formula of the model for\r\nan uncompressed column scan is:\r\nM(s trav(R)) = \u0018\r\nR.w · R.n\r\nB\r\n\u0019\r\n(1)\r\nwhere M(s trav(R)) are the estimated cache misses on\r\none cache level while traversing a region R in memory\r\nsequentially for the first time. The parameters are R.w being\r\nthe width of one data item in bytes, R.n which is the number\r\nof data items to traverse, as well as B which is the number\r\nof data items that fit into the cache. In case of an in-memory\r\ndatabase R.w is the width of a tuple while R.n is the number\r\nof tuples.\r\nAs in [15], an inclusive Level 1 cache is assumed meaning\r\nthat all data that is present in the Level 1 cache is also present\r\nin the Level 2 cache. This condition may only be violated\r\ntemporarily when data in the L1 is changed and marked as\r\ndirty before being written back to L2. This assumption holds\r\nfor Intel CPUs but not for AMD CPUs which have an exclusive\r\ncache, meaning that data can be either in L1 or in L2 bot not\r\nin both. Modeling exclusive caches is left for future work.\r\nIn the following, we provide the cost model for various\r\nlight-weight compression techniques and compare their per\u0002formance for a typical analytical query size. Other atomic data\r\npatterns as the conditional traversal read [12] can be extended\r\nthe same way.\r\nB. Run-Length Encoding\r\nWhen using run-length encoding (RLE), subsequent\r\nequal values in memory are stored as a RLE-tuple\r\nof (value, runLength), thus encoding a sequence of\r\n(1, 1, 3, 3, 3, 4, 4, 4) as ((1, 2),(3, 3),(4, 3)) reducing the size\r\nin memory the larger runs in the data exist. Whether a good\r\namount of runs exist depends on two parameters: First, equal\r\nvalues need to be stored subsequently - this is usually the case\r\nif the column is stored in sort order of the values. Second,\r\nif the column is ordered, the number of distinct values in\r\nthe data defines the number of tuples needed to be stored.\r\nHowever, having sorted data is much more important because\r\na randomly ordered column with a few distinct values can\r\ncontain no runs in worst case if the data is distributed equally.\r\nOn the other hand, if the number of unique values is close to\r\nthe number of data items, sorting the items has limited impact\r\non compression, as there are only few runs in this case.\r\nIn a sorted run-length encoded column the main indicator\r\nof the size of the column is the cardinality of distinct values.\r\nHence, the performance on aggregate operations in an in\u0002memory database is mainly based on the amount of distinct\r\nvalues. Assuming a column’s values are in sorted order and\r\nthe number of distinct values of that column is given by\r\n|D|, the column can be encoded with |D| RLE-tuples, each\r\nholding the value and the run-length. A defensive approach\r\nto determine the space needed for saving the run-length is to\r\ntake the maximum run-length one value can span. Then, the\r\nmaximum run-length is R.n, and therefore, can be encoded\r\nwith dlog2 R.ne bits (for simplicity reasons. Actually, it is\r\nrunLengthmax = R.n − |D| + 1), while bit-compressing the\r\nrun-length value.\r\nThe basic cost model can then be extended to take a run\u0002length encoded column into account:\r\nM(s trav(R)) = \u0018\r\n(R.w + dlog2 R.ne) · |D|\r\nB\r\n\u0019\r\n(2)\r\nSince each tuple has the overhead of storing the run-length,\r\nrun-length encoding becomes less effective as |D| comes close\r\nto R.n. Hence, the number of distinct values is important. The\r\nbreak even point can be estimated with:\r\n|D| =\r\n\u0018\r\nR.w · R.n\r\nR.w + dlog2 R.ne\r\n\u0019\r\n(3)\r\nA generalized formula for unsorted columns encoded with\r\nrun-length encoding depends on the average run-length of\r\nvalues in the collection which can be answered by examining\r\nthe topology of the data. For example, imagine a customer\r\ntable with a column of the customer’s address’ city name that\r\nis ordered in the sort order of another column with zip codes.\r\nClearly the city names aren’t in their sort order but they will\r\ncontain a good amount of runs since equal and similar zip\u0002codes map to the same city name. Given that average run\u0002length |r|, one can estimate the cache misses with:\r\nM(s trav(R)) =\r\n\r\n\r\n\r\n\r\n(R.w + dlog2 R.ne) ·\r\nl\r\nR.n\r\n|r|\r\nm\r\nB\r\n\r\n\r\n\r\n\r\n(4)\r\nThe break even point, i.e., the minimal number of the\r\naverage run-length can be computed with:\r\nr =\r\nR.w + dlog2 R.ne\r\nR.w\r\n(5)\r\nC. Bit-Vector Encoding\r\nBit-vector encoding stores a bitmap for each distinct value.\r\nEach bitmap has the length of the number of data items to\r\nencode in bits. The value 1 in a bitmap for a distinct value\r\nindicates that the data item with the same index has this\r\nparticular value. As each data item can only have one value\r\nassigned, only one bitmap has the value 1 at a given index.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 149\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/82cccd94-e9dd-4983-be6d-631e5ce25f8d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=86e973d48bc2b612a792f9a5dbd50782300e14e3784c48079c3d9aaebe7ac49a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 963
      },
      {
        "segments": [
          {
            "segment_id": "6a174282-21cb-44d2-8b53-396bdcfb756c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "The compression size is therefore dependent on the number\r\nof data items and the number of distinct values to encode.\r\nM(s trav(R)) = \u0018\r\n(R.w + R.n) · |D|\r\nB\r\n\u0019\r\n(6)\r\nFor each distinct value, the value itself needs to be stored\r\nonce plus a bitmap of the number of tuples in bits. The break\r\neven point can be estimated with:\r\n|D| =\r\n\u0018\r\nR.w · R.n\r\nR.w + R.n\u0019\r\n(7)\r\nD. Null Suppression\r\nNull Suppression stores data by omitting leading 0s of each\r\nvalue. The main indicator of how good the compression will\r\nbe is the average number of 0s that can be suppressed. Think\r\nof an integer column that stores the number of products sold\r\nper month. Since only the less significant bits would equal to\r\n1 and no negative values would appear one could get a good\r\ncompression ratio with Null Suppression. Since each value has\r\na variable length Null Suppression needs to store the length\r\nof each value. A good way to do that is to suppress only\r\nbyte-wise, so a value can be stored with one to four bytes. To\r\nencode that length one needs two bits so the length-metadata\r\nfor four values fits on one byte. Given an average number of\r\n0s to suppress |z| and the suppressable bits |zb| = 8 ·\r\nj\r\n|z|\r\n8\r\nk\r\nan\r\nestimation of the cache misses is possible:\r\nM(s trav(R)) = \u0018\r\nR.n · (R.w − |zb|) + 2 · R.n\r\nB\r\n\u0019\r\n(8)\r\nE. Dictionary Encoding\r\nDictionary Encoding is a widely used compression tech\u0002nique in column-store environments. A dictionary is created\r\nby associating each distinct value with a generated unique\r\nkey - a value id - and replacing the original values in the\r\nattribute vector with their value id replacements. By combining\r\nthe attribute vector with the dictionary entries the original\r\nvalues can be reconstructed. Each distinct value is stored only\r\nonce while the smaller value ids are used as their references\r\nwhich saves space in memory as well as allowing compatible\r\noperators to directly work on the dictionary only, e.g find all\r\ndistinct values, or vice versa operate on the attribute vector\r\nwithout accessing the dictionary. Usually, storing the value ids\r\ninstead of the actual values take much less space in memory\r\nand their length is fixed allowing variable length values to be\r\ntreated as fixed length values in the document vector, which\r\nleads to increased performance [9]. Given the dictionary fits\r\ninto the cache the cache misses for a single column scan can\r\nbe estimated with the following formula:\r\nM(s trav(R)) = \u0018\r\nR.id · R.n\r\nB\r\n\u0019\r\n+\r\n\u0018\r\n|D| · R.w\r\nB\r\n\u0019\r\n(9)\r\nThe more distinct values the dictionary needs to hold the\r\nmore likely it is that a lookup leads to a cache miss. Since\r\nthe access is random previously unloaded cache lines need to\r\nbe fetched again. Hence, the size of the dictionary matters.\r\nThe cache misses can be estimated with the formula for a\r\nrepetitive random access pattern rr acc presented in [15],\r\nsince the accessed position in the dictionary is random and\r\ncan be the same multiple times.\r\nM(s trav(R)) = \u0018\r\nR.id · R.n\r\nB\r\n\u0019\r\n+ M(rr acc(|D| · R.w))\r\n(10)\r\nwith:\r\nM(rr acc(R)) = \u0018\r\nC + ( r\r\nI\r\n− 1) · (C −\r\n#\r\nC\r\n· #)\u0019(11)\r\nand C =\r\n\u0006\r\nI·R.w\r\nB\r\n\u0007\r\nwhere I is an approximation of the\r\nnumber of accessed tuples. Since the whole data is read, each\r\nvalue in the dictionary is read at least once and I = R.n. r is\r\nthe number of access operations which is equal to R.n, too.\r\n# is the number of slots in the cache and therefore equals to\r\ncacheSize/B in a fully associative cache.\r\nF. Comparison\r\nWe compare the expected cash misses for a table with a size\r\ntypical in Enterprise Data: Given one million 48 byte string\r\nvalues of which 50,000 are distinct an uncompressed column\r\nscan would issue l\r\n48·106\r\n64 m\r\n= 750, 000 cache misses with a 64\r\nbyte cache line. We calculate the expected cash misses for each\r\nalgorithm and provide a sensitivity analysis on the number of\r\ndistinct values.\r\n1) Run-Length Encoding: A run-length encoded\r\ncolumn when stored in sort order would issue only l\r\n(8·48+log2 106)·50,000\r\n8·64 m\r\n= 39, 454 cache misses.\r\nThe break even point would be at l\r\n8·48·106\r\n8·48+log2 106\r\nm\r\n= 950, 496\r\ndistinct values.\r\nIf the column was stored unsorted the number of expected\r\ncash misses would be in the worst case l\r\n(8·48+log2 106)·106\r\n8·64 m\r\n=\r\n788, 929. This worst case would occur in the scenario of an\r\naverage run-length of 1. The average run-length for each value\r\nhas to be at least l\r\n8·48+log2 106\r\n8·48 m\r\n= 1, 05 to issue less cache\r\nmisses compared to no compression.\r\n2) Bit-Vector Encoding: For the example above a full scan\r\nwould issue l\r\n(8·48+106)·50,000\r\n8·64 m\r\n= 97, 693, 750 cache misses.\r\nBit-vector encoding clearly is not suitable for lots of distinct\r\nvalues or a small amount of bytes to compress. Since each\r\ncolumn has a good number of 0-runs, run-length encoding on\r\ntop of bit-vector encoding might help. The break even point\r\nis at l\r\n8·48·106\r\n8·48+106\r\nm\r\n= 384 distinct values.\r\nOn the other hand, the formulas show that the amount of\r\nbytes per value play a little role in the overall amount of cache\r\nmisses. Thus, bit-vector encoding should be used when the\r\nvalues to compress have a certain size.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 150\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/6a174282-21cb-44d2-8b53-396bdcfb756c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d113f3628cf830431e2029880f4bd964d95b5c28de1985b191368c664a3d5aca",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 942
      },
      {
        "segments": [
          {
            "segment_id": "6a174282-21cb-44d2-8b53-396bdcfb756c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "The compression size is therefore dependent on the number\r\nof data items and the number of distinct values to encode.\r\nM(s trav(R)) = \u0018\r\n(R.w + R.n) · |D|\r\nB\r\n\u0019\r\n(6)\r\nFor each distinct value, the value itself needs to be stored\r\nonce plus a bitmap of the number of tuples in bits. The break\r\neven point can be estimated with:\r\n|D| =\r\n\u0018\r\nR.w · R.n\r\nR.w + R.n\u0019\r\n(7)\r\nD. Null Suppression\r\nNull Suppression stores data by omitting leading 0s of each\r\nvalue. The main indicator of how good the compression will\r\nbe is the average number of 0s that can be suppressed. Think\r\nof an integer column that stores the number of products sold\r\nper month. Since only the less significant bits would equal to\r\n1 and no negative values would appear one could get a good\r\ncompression ratio with Null Suppression. Since each value has\r\na variable length Null Suppression needs to store the length\r\nof each value. A good way to do that is to suppress only\r\nbyte-wise, so a value can be stored with one to four bytes. To\r\nencode that length one needs two bits so the length-metadata\r\nfor four values fits on one byte. Given an average number of\r\n0s to suppress |z| and the suppressable bits |zb| = 8 ·\r\nj\r\n|z|\r\n8\r\nk\r\nan\r\nestimation of the cache misses is possible:\r\nM(s trav(R)) = \u0018\r\nR.n · (R.w − |zb|) + 2 · R.n\r\nB\r\n\u0019\r\n(8)\r\nE. Dictionary Encoding\r\nDictionary Encoding is a widely used compression tech\u0002nique in column-store environments. A dictionary is created\r\nby associating each distinct value with a generated unique\r\nkey - a value id - and replacing the original values in the\r\nattribute vector with their value id replacements. By combining\r\nthe attribute vector with the dictionary entries the original\r\nvalues can be reconstructed. Each distinct value is stored only\r\nonce while the smaller value ids are used as their references\r\nwhich saves space in memory as well as allowing compatible\r\noperators to directly work on the dictionary only, e.g find all\r\ndistinct values, or vice versa operate on the attribute vector\r\nwithout accessing the dictionary. Usually, storing the value ids\r\ninstead of the actual values take much less space in memory\r\nand their length is fixed allowing variable length values to be\r\ntreated as fixed length values in the document vector, which\r\nleads to increased performance [9]. Given the dictionary fits\r\ninto the cache the cache misses for a single column scan can\r\nbe estimated with the following formula:\r\nM(s trav(R)) = \u0018\r\nR.id · R.n\r\nB\r\n\u0019\r\n+\r\n\u0018\r\n|D| · R.w\r\nB\r\n\u0019\r\n(9)\r\nThe more distinct values the dictionary needs to hold the\r\nmore likely it is that a lookup leads to a cache miss. Since\r\nthe access is random previously unloaded cache lines need to\r\nbe fetched again. Hence, the size of the dictionary matters.\r\nThe cache misses can be estimated with the formula for a\r\nrepetitive random access pattern rr acc presented in [15],\r\nsince the accessed position in the dictionary is random and\r\ncan be the same multiple times.\r\nM(s trav(R)) = \u0018\r\nR.id · R.n\r\nB\r\n\u0019\r\n+ M(rr acc(|D| · R.w))\r\n(10)\r\nwith:\r\nM(rr acc(R)) = \u0018\r\nC + ( r\r\nI\r\n− 1) · (C −\r\n#\r\nC\r\n· #)\u0019(11)\r\nand C =\r\n\u0006\r\nI·R.w\r\nB\r\n\u0007\r\nwhere I is an approximation of the\r\nnumber of accessed tuples. Since the whole data is read, each\r\nvalue in the dictionary is read at least once and I = R.n. r is\r\nthe number of access operations which is equal to R.n, too.\r\n# is the number of slots in the cache and therefore equals to\r\ncacheSize/B in a fully associative cache.\r\nF. Comparison\r\nWe compare the expected cash misses for a table with a size\r\ntypical in Enterprise Data: Given one million 48 byte string\r\nvalues of which 50,000 are distinct an uncompressed column\r\nscan would issue l\r\n48·106\r\n64 m\r\n= 750, 000 cache misses with a 64\r\nbyte cache line. We calculate the expected cash misses for each\r\nalgorithm and provide a sensitivity analysis on the number of\r\ndistinct values.\r\n1) Run-Length Encoding: A run-length encoded\r\ncolumn when stored in sort order would issue only l\r\n(8·48+log2 106)·50,000\r\n8·64 m\r\n= 39, 454 cache misses.\r\nThe break even point would be at l\r\n8·48·106\r\n8·48+log2 106\r\nm\r\n= 950, 496\r\ndistinct values.\r\nIf the column was stored unsorted the number of expected\r\ncash misses would be in the worst case l\r\n(8·48+log2 106)·106\r\n8·64 m\r\n=\r\n788, 929. This worst case would occur in the scenario of an\r\naverage run-length of 1. The average run-length for each value\r\nhas to be at least l\r\n8·48+log2 106\r\n8·48 m\r\n= 1, 05 to issue less cache\r\nmisses compared to no compression.\r\n2) Bit-Vector Encoding: For the example above a full scan\r\nwould issue l\r\n(8·48+106)·50,000\r\n8·64 m\r\n= 97, 693, 750 cache misses.\r\nBit-vector encoding clearly is not suitable for lots of distinct\r\nvalues or a small amount of bytes to compress. Since each\r\ncolumn has a good number of 0-runs, run-length encoding on\r\ntop of bit-vector encoding might help. The break even point\r\nis at l\r\n8·48·106\r\n8·48+106\r\nm\r\n= 384 distinct values.\r\nOn the other hand, the formulas show that the amount of\r\nbytes per value play a little role in the overall amount of cache\r\nmisses. Thus, bit-vector encoding should be used when the\r\nvalues to compress have a certain size.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 150\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/6a174282-21cb-44d2-8b53-396bdcfb756c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d113f3628cf830431e2029880f4bd964d95b5c28de1985b191368c664a3d5aca",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 942
      },
      {
        "segments": [
          {
            "segment_id": "60129ad2-e42c-4656-aa46-5f3318cba021",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "3) Null Suppression: Given that the average amount of\r\nbytes to suppress is l3 then the estimated cache misses are\r\n106·(8·48−8·3)+2·106\r\n8·64 m\r\n= 707, 032.\r\n4) Dictionary Encoding: Given a 4 byte value id the num\u0002ber of cache misses for a single column scan in a dictionary\r\nencoded column are l\r\n4·106\r\n64 m\r\n+\r\n\u0006\r\n50,000·48\r\n64 \u0007\r\n= 100, 000\r\nThe following table shows the estimated number of cache\r\nmisses for the example above with the 5% distinct cardinality.\r\nCache Misses\r\nNo compression 750,000\r\nRLE (sorted) 39,454\r\nRLE (unsorted, worst case) 788,929\r\nBit-Vector Encoding 97,693,750\r\nNull Suppression 707,032\r\nDictionary Encoding 100,000\r\nFigure 1 shows a sensitivity analysis with regards to the\r\nnumber of distinct values in order to investigate the influence\r\nof a changing cardinality of those.\r\nG. Evaluation\r\nThe usefulness of compression algorithms depends on the\r\ndata profile of a column. The following table describes the\r\napplicability of each compression technique for several data\r\nprofiles.\r\nfew distinct many distinct\r\nNo compression - - - -\r\nRLE (sorted) + + +\r\nRLE (unsorted) - - - -\r\nBit-Vector Encoding + - -\r\nNull Suppression - -\r\nDictionary Encoding + +\r\nOur goal is to find a compression technique that performs\r\nbest under OLTP as well as OLAP workloads in an enter\u0002prise environment. Based on our findings of enterprise data\r\ncharacteristics in [12], we focus on a sparse data set with\r\na vast amount of columns but most of them storing a small\r\nnumber of distinct values. We use a column-oriented store\r\nsince it performs better under an OLAP workload than a row\r\nstore does [21]. However, in an OLTP scenario most queries\r\nfetch only few complete records. The column store finds each\r\nrespective entry in all columns separately and then reconstructs\r\nthe record. Since there are lots of columns finding those entries\r\nhas to be fast.\r\nRun-length encoding on a sorted column issues by far the\r\nfewest cache misses of all presented compression techniques.\r\nHowever, applying run-length encoding requires sorting each\r\ncolumn before storing it. In order to reconstruct records\r\ncorrectly we would have to store the original row number\r\nas well, called the surrogate id. When reconstructing records\r\neach column needs to be searched for that id resulting in a\r\ncomplexity of O(R.n) per column. As Enterprise Applications\r\ntypically operate on tables with up to millions records we\r\ncannot use surrogate ids and prefer direct or implicit offsetting\r\ninstead (O(1)).\r\nBasic dictionary encoding allows for direct offsetting into\r\neach column and also benefits from a sparse data set as\r\nenterprises have it. In addition the compaction process’ perfor\u0002mance increases when using dictionary encoding compared to\r\nrun-length encoding. Therefore, dictionary encoding fits our\r\nneeds best and is our compression technique of choice for\r\na mixed workload. Furthermore, it still can be optimized as\r\ndescribed in the following section.\r\nIII. DICTIONARY COMPRESSION TECHNIQUES\r\nIn this section, we discuss various optimization of the basic\r\ndictionary approach introduced in section II-E. We evaluate\r\ntheir applicability for different data access profiles.\r\nA. Order-Indifferent Dictionary\r\nThe basic dictionary approach described in the last chapter\r\ndid not care about how the values in the dictionary are ordered.\r\nThat makes finding a value in the dictionary, e.g., for an\r\ninsert - one needs to find out whether the value is already in\r\nthe dictionary - an expensive operation (O(|D|)). A possible\r\nsolution is to store the values based on their hash value. Given\r\na good hash function one can find a value in the dictionary\r\nin O(1) as well as finding a value for a specific value id in\r\nO(1). This clearly depends on a good hash function and may\r\nincrease compression size.\r\nB. Order-Preserving Dictionary\r\nIn comparison to the basic dictionary a hash value supported\r\ndictionary approach could speed up finding a value in the\r\ndictionary but still does not enforce ordered data items in\r\nmemory. That becomes a disadvantage when executing range\r\nqueries like finding all records that begin with the letter\r\nK, e.g., in a column that stores names. An order-indifferent\r\ndictionary needs to traverse the whole dictionary filtering all\r\nvalues that begin with K and returning their associated value\r\nids. This has a complexity of O(|D|). In a sorted dictionary,\r\none could find the first occurrence of a value starting with\r\nK and L with binary search and then return the lower and\r\nupper bound for all value ids that are associated with values\r\nbeginning with K without actually checking their values. The\r\ncomplexity is O(log2|D|). The downside of that approach is\r\nthat if new values need to be added to the dictionary they can\r\ndestroy the sort order invalidating possibly all value-id/value\r\nassociations and resulting in a complete rewrite of the attribute\r\nvector O(R.n).\r\nC. Bit-Compressed Attribute Vector\r\nThe size of the attribute vector, hence the read performance,\r\nis also affected by the compression ratio between the original\r\nvalues and the value id replacements. However, the number\r\nof distinct values in the uncompressed values collection is\r\nimportant as well. Firstly, because the entries in the dictionary\r\nincrease with every unique value - for every value-id/value\r\ncompression ratio, there is a number of distinct values hat\r\ndictionary encoding becomes useless - secondly, the more\r\nunique values need to be encoded, the more unique value\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 151\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/60129ad2-e42c-4656-aa46-5f3318cba021.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3ee575eefb71c514449f7227176c1516389a07468fdd7f9a600a440b6587afc7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 896
      },
      {
        "segments": [
          {
            "segment_id": "60129ad2-e42c-4656-aa46-5f3318cba021",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "3) Null Suppression: Given that the average amount of\r\nbytes to suppress is l3 then the estimated cache misses are\r\n106·(8·48−8·3)+2·106\r\n8·64 m\r\n= 707, 032.\r\n4) Dictionary Encoding: Given a 4 byte value id the num\u0002ber of cache misses for a single column scan in a dictionary\r\nencoded column are l\r\n4·106\r\n64 m\r\n+\r\n\u0006\r\n50,000·48\r\n64 \u0007\r\n= 100, 000\r\nThe following table shows the estimated number of cache\r\nmisses for the example above with the 5% distinct cardinality.\r\nCache Misses\r\nNo compression 750,000\r\nRLE (sorted) 39,454\r\nRLE (unsorted, worst case) 788,929\r\nBit-Vector Encoding 97,693,750\r\nNull Suppression 707,032\r\nDictionary Encoding 100,000\r\nFigure 1 shows a sensitivity analysis with regards to the\r\nnumber of distinct values in order to investigate the influence\r\nof a changing cardinality of those.\r\nG. Evaluation\r\nThe usefulness of compression algorithms depends on the\r\ndata profile of a column. The following table describes the\r\napplicability of each compression technique for several data\r\nprofiles.\r\nfew distinct many distinct\r\nNo compression - - - -\r\nRLE (sorted) + + +\r\nRLE (unsorted) - - - -\r\nBit-Vector Encoding + - -\r\nNull Suppression - -\r\nDictionary Encoding + +\r\nOur goal is to find a compression technique that performs\r\nbest under OLTP as well as OLAP workloads in an enter\u0002prise environment. Based on our findings of enterprise data\r\ncharacteristics in [12], we focus on a sparse data set with\r\na vast amount of columns but most of them storing a small\r\nnumber of distinct values. We use a column-oriented store\r\nsince it performs better under an OLAP workload than a row\r\nstore does [21]. However, in an OLTP scenario most queries\r\nfetch only few complete records. The column store finds each\r\nrespective entry in all columns separately and then reconstructs\r\nthe record. Since there are lots of columns finding those entries\r\nhas to be fast.\r\nRun-length encoding on a sorted column issues by far the\r\nfewest cache misses of all presented compression techniques.\r\nHowever, applying run-length encoding requires sorting each\r\ncolumn before storing it. In order to reconstruct records\r\ncorrectly we would have to store the original row number\r\nas well, called the surrogate id. When reconstructing records\r\neach column needs to be searched for that id resulting in a\r\ncomplexity of O(R.n) per column. As Enterprise Applications\r\ntypically operate on tables with up to millions records we\r\ncannot use surrogate ids and prefer direct or implicit offsetting\r\ninstead (O(1)).\r\nBasic dictionary encoding allows for direct offsetting into\r\neach column and also benefits from a sparse data set as\r\nenterprises have it. In addition the compaction process’ perfor\u0002mance increases when using dictionary encoding compared to\r\nrun-length encoding. Therefore, dictionary encoding fits our\r\nneeds best and is our compression technique of choice for\r\na mixed workload. Furthermore, it still can be optimized as\r\ndescribed in the following section.\r\nIII. DICTIONARY COMPRESSION TECHNIQUES\r\nIn this section, we discuss various optimization of the basic\r\ndictionary approach introduced in section II-E. We evaluate\r\ntheir applicability for different data access profiles.\r\nA. Order-Indifferent Dictionary\r\nThe basic dictionary approach described in the last chapter\r\ndid not care about how the values in the dictionary are ordered.\r\nThat makes finding a value in the dictionary, e.g., for an\r\ninsert - one needs to find out whether the value is already in\r\nthe dictionary - an expensive operation (O(|D|)). A possible\r\nsolution is to store the values based on their hash value. Given\r\na good hash function one can find a value in the dictionary\r\nin O(1) as well as finding a value for a specific value id in\r\nO(1). This clearly depends on a good hash function and may\r\nincrease compression size.\r\nB. Order-Preserving Dictionary\r\nIn comparison to the basic dictionary a hash value supported\r\ndictionary approach could speed up finding a value in the\r\ndictionary but still does not enforce ordered data items in\r\nmemory. That becomes a disadvantage when executing range\r\nqueries like finding all records that begin with the letter\r\nK, e.g., in a column that stores names. An order-indifferent\r\ndictionary needs to traverse the whole dictionary filtering all\r\nvalues that begin with K and returning their associated value\r\nids. This has a complexity of O(|D|). In a sorted dictionary,\r\none could find the first occurrence of a value starting with\r\nK and L with binary search and then return the lower and\r\nupper bound for all value ids that are associated with values\r\nbeginning with K without actually checking their values. The\r\ncomplexity is O(log2|D|). The downside of that approach is\r\nthat if new values need to be added to the dictionary they can\r\ndestroy the sort order invalidating possibly all value-id/value\r\nassociations and resulting in a complete rewrite of the attribute\r\nvector O(R.n).\r\nC. Bit-Compressed Attribute Vector\r\nThe size of the attribute vector, hence the read performance,\r\nis also affected by the compression ratio between the original\r\nvalues and the value id replacements. However, the number\r\nof distinct values in the uncompressed values collection is\r\nimportant as well. Firstly, because the entries in the dictionary\r\nincrease with every unique value - for every value-id/value\r\ncompression ratio, there is a number of distinct values hat\r\ndictionary encoding becomes useless - secondly, the more\r\nunique values need to be encoded, the more unique value\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 151\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/60129ad2-e42c-4656-aa46-5f3318cba021.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3ee575eefb71c514449f7227176c1516389a07468fdd7f9a600a440b6587afc7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 896
      },
      {
        "segments": [
          {
            "segment_id": "d15002e2-4ae2-456d-a423-08688b9476e1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "0\"\r\n100000\"\r\n200000\"\r\n300000\"\r\n400000\"\r\n500000\"\r\n600000\"\r\n700000\"\r\n800000\"\r\n900000\"\r\n1000000\"\r\n0,01%\" 0,10%\" 1,00%\" 5,00%\" 10,00%\" 20,00%\" 30,00%\" 40,00%\" 50,00%\" 60,00%\" 70,00%\" 80,00%\" 90,00%\"100,00%\"\r\nCache&misses&\r\nPercentage&of&dis2nct&values&\r\nno\"compression\"\r\nRLE\"sorted\"\r\nRLE\"unsorted\"\r\nBVE\"\r\nNull\"Suppr.\"\r\nDict.\"Enc.\"\r\nBit\"compressed\"dict\"\r\nFig. 1. Compression techniques with regards to cache misses and distinct values.\r\nids are needed. In the basic dictionary example above a lot\r\nof compression opportunities are wasted by reserving 32 bits\r\nto define the value id space. Dictionary encoding with bit\u0002compressed value ids varies the length of the value ids and\r\nreserves only the needed number of bits to encode all distinct\r\nvalues but still guarantees fixed-length value ids. Given 200\r\nvalues in the dictionary, the attribute vector that needs to\r\nbe compressed needs only one byte to store each value id,\r\nallowing 64 value ids to fit on a 64 byte cache line. Similar\r\nto the order-preserving dictionary the disadvantage of this\r\napproach is that the bit-length of the value ids needs to be\r\nincreased and all values in the attribute vector need to be\r\nrewritten when the number of distinct values in the dictionary\r\nexceeds the amount of values that can be encoded with the\r\ncurrent number of bits - O(R.n).\r\nThe cost model can then be extended with:\r\nM(s trav(R)) = \u0018\r\ndlog2|D|e · R.n\r\nB\r\n\u0019\r\n+\r\n\u0018\r\n|D| · R.w\r\nB\r\n\u0019\r\n(12)\r\nTaking the same parameters from the previous section the\r\nissued cache misses then are l\r\ndlog2 50,000e·106\r\n8·64 m\r\n+\r\n\u0006\r\n50,000·48\r\n64 \u0007\r\n=\r\n68, 750 which is almost half the amount of the basic dictionary.\r\nD. Bit-Compressed Order-Preserving Dictionary Encoding\r\nThe last two described dictionary compression techniques\r\nhave the same problem of rewriting the whole attribute vector\r\nfor different reasons. However, the two problems are con\u0002nected. A reordering of the dictionary can only happen if new\r\ndistinct values are added to the collection or when deleting\r\nvalues. Furthermore, an extension of the value id space can\r\nonly be a result of adding new distinct values to the dictionary.\r\nUsing both approaches together can lower the cost of inserts\r\nand updates. When new values are added to the dictionary\r\nand the amount of values exceeds the value id space then the\r\nrewriting of the attribute vector can do both, updating to new\r\nvalue ids with the new bit-length, in one step.\r\nE. Comparison\r\nThe following table shows the different dictionary encoding\r\nvariants under different workloads.\r\nBasic Hash\r\nMap\r\nBit\u0002Compr.Order\u0002Pres.\r\nfew inserts, many\r\nequal queries\r\n+ + + + -\r\nfew inserts, many\r\nrange queries\r\n- - - + + +\r\nmany inserts,\r\nmany equal\r\nqueries\r\n+ + + - -\r\nmany inserts,\r\nmany range\r\nqueries\r\n- - - - +\r\nThe following table lists the advantages and disadvantages\r\nof the different dictionary encoding variants.\r\nADVANTAGES DISADVANTAGES\r\nBasic Dict. fixed length, com\u0002pression timecompression size\r\nHash Map compression time execution time\r\nBit-Compr. compression size compression time\r\nOrder-Pres. execution time compression time\r\nOrder-Pres. &\r\nBit-Compr.\r\nexecution time,\r\ncompression size\r\ncompression time\r\nIV. RELATED WORK\r\nIn the area of database management systems, compression is\r\nalso used to improve query speed as described in [22] as work\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 152\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/d15002e2-4ae2-456d-a423-08688b9476e1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6e40274d89725a259661f83a8b5a8f55ec1b01efbea461d99e7fa106cfe57a7b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 537
      },
      {
        "segments": [
          {
            "segment_id": "d15002e2-4ae2-456d-a423-08688b9476e1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "0\"\r\n100000\"\r\n200000\"\r\n300000\"\r\n400000\"\r\n500000\"\r\n600000\"\r\n700000\"\r\n800000\"\r\n900000\"\r\n1000000\"\r\n0,01%\" 0,10%\" 1,00%\" 5,00%\" 10,00%\" 20,00%\" 30,00%\" 40,00%\" 50,00%\" 60,00%\" 70,00%\" 80,00%\" 90,00%\"100,00%\"\r\nCache&misses&\r\nPercentage&of&dis2nct&values&\r\nno\"compression\"\r\nRLE\"sorted\"\r\nRLE\"unsorted\"\r\nBVE\"\r\nNull\"Suppr.\"\r\nDict.\"Enc.\"\r\nBit\"compressed\"dict\"\r\nFig. 1. Compression techniques with regards to cache misses and distinct values.\r\nids are needed. In the basic dictionary example above a lot\r\nof compression opportunities are wasted by reserving 32 bits\r\nto define the value id space. Dictionary encoding with bit\u0002compressed value ids varies the length of the value ids and\r\nreserves only the needed number of bits to encode all distinct\r\nvalues but still guarantees fixed-length value ids. Given 200\r\nvalues in the dictionary, the attribute vector that needs to\r\nbe compressed needs only one byte to store each value id,\r\nallowing 64 value ids to fit on a 64 byte cache line. Similar\r\nto the order-preserving dictionary the disadvantage of this\r\napproach is that the bit-length of the value ids needs to be\r\nincreased and all values in the attribute vector need to be\r\nrewritten when the number of distinct values in the dictionary\r\nexceeds the amount of values that can be encoded with the\r\ncurrent number of bits - O(R.n).\r\nThe cost model can then be extended with:\r\nM(s trav(R)) = \u0018\r\ndlog2|D|e · R.n\r\nB\r\n\u0019\r\n+\r\n\u0018\r\n|D| · R.w\r\nB\r\n\u0019\r\n(12)\r\nTaking the same parameters from the previous section the\r\nissued cache misses then are l\r\ndlog2 50,000e·106\r\n8·64 m\r\n+\r\n\u0006\r\n50,000·48\r\n64 \u0007\r\n=\r\n68, 750 which is almost half the amount of the basic dictionary.\r\nD. Bit-Compressed Order-Preserving Dictionary Encoding\r\nThe last two described dictionary compression techniques\r\nhave the same problem of rewriting the whole attribute vector\r\nfor different reasons. However, the two problems are con\u0002nected. A reordering of the dictionary can only happen if new\r\ndistinct values are added to the collection or when deleting\r\nvalues. Furthermore, an extension of the value id space can\r\nonly be a result of adding new distinct values to the dictionary.\r\nUsing both approaches together can lower the cost of inserts\r\nand updates. When new values are added to the dictionary\r\nand the amount of values exceeds the value id space then the\r\nrewriting of the attribute vector can do both, updating to new\r\nvalue ids with the new bit-length, in one step.\r\nE. Comparison\r\nThe following table shows the different dictionary encoding\r\nvariants under different workloads.\r\nBasic Hash\r\nMap\r\nBit\u0002Compr.Order\u0002Pres.\r\nfew inserts, many\r\nequal queries\r\n+ + + + -\r\nfew inserts, many\r\nrange queries\r\n- - - + + +\r\nmany inserts,\r\nmany equal\r\nqueries\r\n+ + + - -\r\nmany inserts,\r\nmany range\r\nqueries\r\n- - - - +\r\nThe following table lists the advantages and disadvantages\r\nof the different dictionary encoding variants.\r\nADVANTAGES DISADVANTAGES\r\nBasic Dict. fixed length, com\u0002pression timecompression size\r\nHash Map compression time execution time\r\nBit-Compr. compression size compression time\r\nOrder-Pres. execution time compression time\r\nOrder-Pres. &\r\nBit-Compr.\r\nexecution time,\r\ncompression size\r\ncompression time\r\nIV. RELATED WORK\r\nIn the area of database management systems, compression is\r\nalso used to improve query speed as described in [22] as work\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 152\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/d15002e2-4ae2-456d-a423-08688b9476e1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6e40274d89725a259661f83a8b5a8f55ec1b01efbea461d99e7fa106cfe57a7b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 537
      },
      {
        "segments": [
          {
            "segment_id": "c1a5b75f-811d-413d-b3b0-21054e4cf57c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "focused on reducing the amount of data only. That becomes\r\nespecially useful when data is stored column-wise such as in\r\nC-Store, a disk-based column-oriented database, see [21]. The\r\nwork presented in [1] describes how compression can be inte\u0002grated into C-Store and shows the impact on read performance.\r\nIn a real world scenario one has to consider the negative impact\r\non write performance when using compression. [10] comes\r\nto the conclusion that column stores perform better than row\r\nstores in most cases.\r\nHowever, data compression can limit the applicability to\r\nscenarios with frequent updates leading to dedicated delta\r\nstructures to improve the performance of inserts, updates and\r\ndeletes. The authors of [7] and [18] describe a concept of\r\ntreating compressed fragments as immutable objects, using\r\na separate list for deleted tuples and uncompressed delta\r\ncolumns for appended data while using a combination of both\r\nfor updates. In contrast, the work of [11] maintains all data\r\nmodification of a table in one differential buffer that is write\u0002optimized and keeps track of invalidation with a valid bit\u0002vector. Later work of the same authors shows how to enable\r\nfast updates on read-optimized databases by leveraging multi\u0002core CPUs [13].\r\nThe work of [5] depicts a technique of maintaining a dic\u0002tionary in a order-preserving way while still allowing inserts\r\nin sort order without rebuilding the attribute vector due to\r\nchanged value id’s.\r\nIn the area of in-memory databases with the focus on OLTP\r\nand real-time OLAP, the customer study presented in [12]\r\nshow a very high amount of read queries compared to write\r\nqueries supporting the fact that a compressed read-optimized\r\nstore is useful.\r\nV. CONCLUSION\r\nIn this paper, we showed and explained the positive im\u0002pact on read performance for an in-memory database when\r\nusing data compression. In order to compare different kinds\r\nof lightweight compression techniques under different data\r\ndistributions we extended the generic cost model to take com\u0002pression into account. We also presented several lightweight\r\ncompression techniques and different optimizations regarding\r\ndictionary compression as well as trade-offs that have to be\r\nmade in favor of late materialization and write performance.\r\nThe paper also described why focussing on read performance\r\nis necessary and how a sufficient write performance can be\r\nachieved as well. It concludes that under most circumstances\r\n- especially for column stores - dictionary compression is\r\nthe best choice when it comes to optimizing read and write\r\nperformance under a mixed workload.\r\nREFERENCES\r\n[1] D. J. Abadi, S. Madden, and M. Ferreira. Integrating compression\r\nand execution in column-oriented database systems. In Proceedings of\r\nthe ACM SIGMOD International Conference on Management of Data,\r\nChicago, Illinois, USA, June 27-29, 2006, pages 671–682, 2006.\r\n[2] D. J. Abadi, D. S. Myers, D. J. DeWitt, and S. Madden. Materialization\r\nStrategies in a Column-Oriented DBMS. In Proceedings of the 23rd In\u0002ternational Conference on Data Engineering, ICDE 2007, The Marmara\r\nHotel, Istanbul, Turkey, April 15-20, 2007, pages 466–475, 2007.\r\n[3] A. Ailamaki, D. J. DeWitt, and M. D. Hill. Data page layouts for\r\nrelational databases on deep memory hierarchies. VLDB J., 11(3):198–\r\n215, 2002.\r\n[4] A. Ailamaki, D. J. DeWitt, M. D. Hill, and D. A. Wood. DBMSs on\r\na Modern Processor: Where Does Time Go? In VLDB’99, Proceedings\r\nof 25th International Conference on Very Large Data Bases, September\r\n7-10, 1999, Edinburgh, Scotland, UK, pages 266–277, 1999.\r\n[5] C. Binnig, S. Hildenbrand, and F. Farber. Dictionary-based order- ¨\r\npreserving string compression for main memory column stores. In\r\nProceedings of the ACM SIGMOD International Conference on Man\u0002agement of Data, SIGMOD 2009, Providence, Rhode Island, USA, June\r\n29 - July 2, 2009, pages 283–296, 2009.\r\n[6] P. A. Boncz, S. Manegold, and M. L. Kersten. Database Architecture\r\nOptimized for the New Bottleneck: Memory Access. In VLDB’99,\r\nProceedings of 25th International Conference on Very Large Data Bases,\r\nSeptember 7-10, 1999, Edinburgh, Scotland, UK, pages 54–65, 1999.\r\n[7] P. A. Boncz, M. Zukowski, and N. Nes. MonetDB/X100: Hyper\u0002Pipelining Query Execution. In CIDR, pages 225–237, 2005.\r\n[8] U. Drepper. What every programmer should know about memory, 2007.\r\n[9] S. Harizopoulos, V. Liang, D. J. Abadi, and S. Madden. Performance\r\ntradeoffs in read-optimized databases. In VLDB ’06: Proceedings of the\r\n32nd international conference on Very large data bases, pages 487–498.\r\nVLDB Endowment, 2006.\r\n[10] S. Harizopoulos, V. Liang, D. J. Abadi, and S. Madden. Performance\r\nTradeoffs in Read-Optimized Databases. In Proceedings of the 32nd\r\nInternational Conference on Very Large Data Bases, Seoul, Korea,\r\nSeptember 12-15, 2006, pages 487–498, 2006.\r\n[11] J. Kruger, M. Grund, C. Tinnefeld, H. Plattner, A. Zeier, and F. Faerber. ¨\r\nOptimizing Write Performance for Read Optimized Databases. In\r\nDatabase Systems for Advanced Applications, 15th International Con\u0002ference, DASFAA 2010, Tsukuba, Japan, April 1-4, 2010, Proceedings,\r\nPart II, pages 291–305, 2010.\r\n[12] J. Kruger, M. Grund, A. Zeier, and H. Plattner. Enterprise Application- ¨\r\nSpecific Data Management. In Proceedings of the 14th IEEE Inter\u0002national Enterprise Distributed Object Computing Conference, EDOC\r\n2010, Vitoria, Brazil, 25-29 October 2010.\r\n[13] J. Kruger, C. Kim, M. Grund, N. Satish, D. Schwalb, J. Chhugani, ¨\r\nH. Plattner, P. Dubey, and A. Zeier. Fast Updates on Read-Optimized\r\nDatabases Using Multi-Core CPUs. PVLDB, 5(1):61–72, 2011.\r\n[14] J. Kruger, C. Tinnefeld, M. Grund, A. Zeier, and H. Plattner. A case ¨\r\nfor online mixed workload processing. In Proceedings of the Third\r\nInternational Workshop on Testing Database Systems, DBTest 2010,\r\nIndianapolis, Indiana, USA, June 7, 2010, 2010.\r\n[15] S. Manegold, P. A. Boncz, and M. L. Kersten. Generic Database Cost\r\nModels for Hierarchical Memory Systems. In VLDB 2002, Proceedings\r\nof 28th International Conference on Very Large Data Bases, August\r\n20-23, 2002, Hong Kong, China, pages 191–202, 2002.\r\n[16] D. E. Ott. Optimizing software applications for numa.\r\n[17] H. Plattner. A common database approach for OLTP and OLAP using\r\nan in-memory column database. In Proceedings of the ACM SIGMOD\r\nInternational Conference on Management of Data, SIGMOD 2009,\r\nProvidence, Rhode Island, USA, June 29 - July 2, 2009, pages 1–2,\r\n2009.\r\n[18] R. Ramamurthy, D. J. DeWitt, and Q. Su. A case for fractured mirrors.\r\nVLDB J., 12(2):89–101, 2003.\r\n[19] J. Rao and K. A. Ross. Cache Conscious Indexing for Decision-Support\r\nin Main Memory. In VLDB’99, Proceedings of 25th International Con\u0002ference on Very Large Data Bases, September 7-10, 1999, Edinburgh,\r\nScotland, UK, pages 78–89, 1999.\r\n[20] J. Rao and K. A. Ross. Making B+-Trees Cache Conscious in Main\r\nMemory. In Proceedings of the 2000 ACM SIGMOD International\r\nConference on Management of Data, May 16-18, 2000, Dallas, Texas,\r\nUSA, pages 475–486, 2000.\r\n[21] M. Stonebraker, D. J. Abadi, A. Batkin, X. Chen, M. Cherniack,\r\nM. Ferreira, E. Lau, A. Lin, S. Madden, E. J. O’Neil, P. E. O’Neil,\r\nA. Rasin, N. Tran, and S. B. Zdonik. C-Store: A Column-oriented\r\nDBMS. In Proceedings of the 31st International Conference on Very\r\nLarge Data Bases, Trondheim, Norway, August 30 - September 2, 2005,\r\npages 553–564, 2005.\r\n[22] T. Westmann, D. Kossmann, S. Helmer, and G. Moerkotte. The\r\nImplementation and Performance of Compressed Databases. SIGMOD\r\nRecord, 29(3):55–67, 2000.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 153\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/c1a5b75f-811d-413d-b3b0-21054e4cf57c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6f5942fd86799f5636fa3773f729b8c5aa2132b810e0b15e47fd4f3ca30f2f35",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1171
      },
      {
        "segments": [
          {
            "segment_id": "c1a5b75f-811d-413d-b3b0-21054e4cf57c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "focused on reducing the amount of data only. That becomes\r\nespecially useful when data is stored column-wise such as in\r\nC-Store, a disk-based column-oriented database, see [21]. The\r\nwork presented in [1] describes how compression can be inte\u0002grated into C-Store and shows the impact on read performance.\r\nIn a real world scenario one has to consider the negative impact\r\non write performance when using compression. [10] comes\r\nto the conclusion that column stores perform better than row\r\nstores in most cases.\r\nHowever, data compression can limit the applicability to\r\nscenarios with frequent updates leading to dedicated delta\r\nstructures to improve the performance of inserts, updates and\r\ndeletes. The authors of [7] and [18] describe a concept of\r\ntreating compressed fragments as immutable objects, using\r\na separate list for deleted tuples and uncompressed delta\r\ncolumns for appended data while using a combination of both\r\nfor updates. In contrast, the work of [11] maintains all data\r\nmodification of a table in one differential buffer that is write\u0002optimized and keeps track of invalidation with a valid bit\u0002vector. Later work of the same authors shows how to enable\r\nfast updates on read-optimized databases by leveraging multi\u0002core CPUs [13].\r\nThe work of [5] depicts a technique of maintaining a dic\u0002tionary in a order-preserving way while still allowing inserts\r\nin sort order without rebuilding the attribute vector due to\r\nchanged value id’s.\r\nIn the area of in-memory databases with the focus on OLTP\r\nand real-time OLAP, the customer study presented in [12]\r\nshow a very high amount of read queries compared to write\r\nqueries supporting the fact that a compressed read-optimized\r\nstore is useful.\r\nV. CONCLUSION\r\nIn this paper, we showed and explained the positive im\u0002pact on read performance for an in-memory database when\r\nusing data compression. In order to compare different kinds\r\nof lightweight compression techniques under different data\r\ndistributions we extended the generic cost model to take com\u0002pression into account. We also presented several lightweight\r\ncompression techniques and different optimizations regarding\r\ndictionary compression as well as trade-offs that have to be\r\nmade in favor of late materialization and write performance.\r\nThe paper also described why focussing on read performance\r\nis necessary and how a sufficient write performance can be\r\nachieved as well. It concludes that under most circumstances\r\n- especially for column stores - dictionary compression is\r\nthe best choice when it comes to optimizing read and write\r\nperformance under a mixed workload.\r\nREFERENCES\r\n[1] D. J. Abadi, S. Madden, and M. Ferreira. Integrating compression\r\nand execution in column-oriented database systems. In Proceedings of\r\nthe ACM SIGMOD International Conference on Management of Data,\r\nChicago, Illinois, USA, June 27-29, 2006, pages 671–682, 2006.\r\n[2] D. J. Abadi, D. S. Myers, D. J. DeWitt, and S. Madden. Materialization\r\nStrategies in a Column-Oriented DBMS. In Proceedings of the 23rd In\u0002ternational Conference on Data Engineering, ICDE 2007, The Marmara\r\nHotel, Istanbul, Turkey, April 15-20, 2007, pages 466–475, 2007.\r\n[3] A. Ailamaki, D. J. DeWitt, and M. D. Hill. Data page layouts for\r\nrelational databases on deep memory hierarchies. VLDB J., 11(3):198–\r\n215, 2002.\r\n[4] A. Ailamaki, D. J. DeWitt, M. D. Hill, and D. A. Wood. DBMSs on\r\na Modern Processor: Where Does Time Go? In VLDB’99, Proceedings\r\nof 25th International Conference on Very Large Data Bases, September\r\n7-10, 1999, Edinburgh, Scotland, UK, pages 266–277, 1999.\r\n[5] C. Binnig, S. Hildenbrand, and F. Farber. Dictionary-based order- ¨\r\npreserving string compression for main memory column stores. In\r\nProceedings of the ACM SIGMOD International Conference on Man\u0002agement of Data, SIGMOD 2009, Providence, Rhode Island, USA, June\r\n29 - July 2, 2009, pages 283–296, 2009.\r\n[6] P. A. Boncz, S. Manegold, and M. L. Kersten. Database Architecture\r\nOptimized for the New Bottleneck: Memory Access. In VLDB’99,\r\nProceedings of 25th International Conference on Very Large Data Bases,\r\nSeptember 7-10, 1999, Edinburgh, Scotland, UK, pages 54–65, 1999.\r\n[7] P. A. Boncz, M. Zukowski, and N. Nes. MonetDB/X100: Hyper\u0002Pipelining Query Execution. In CIDR, pages 225–237, 2005.\r\n[8] U. Drepper. What every programmer should know about memory, 2007.\r\n[9] S. Harizopoulos, V. Liang, D. J. Abadi, and S. Madden. Performance\r\ntradeoffs in read-optimized databases. In VLDB ’06: Proceedings of the\r\n32nd international conference on Very large data bases, pages 487–498.\r\nVLDB Endowment, 2006.\r\n[10] S. Harizopoulos, V. Liang, D. J. Abadi, and S. Madden. Performance\r\nTradeoffs in Read-Optimized Databases. In Proceedings of the 32nd\r\nInternational Conference on Very Large Data Bases, Seoul, Korea,\r\nSeptember 12-15, 2006, pages 487–498, 2006.\r\n[11] J. Kruger, M. Grund, C. Tinnefeld, H. Plattner, A. Zeier, and F. Faerber. ¨\r\nOptimizing Write Performance for Read Optimized Databases. In\r\nDatabase Systems for Advanced Applications, 15th International Con\u0002ference, DASFAA 2010, Tsukuba, Japan, April 1-4, 2010, Proceedings,\r\nPart II, pages 291–305, 2010.\r\n[12] J. Kruger, M. Grund, A. Zeier, and H. Plattner. Enterprise Application- ¨\r\nSpecific Data Management. In Proceedings of the 14th IEEE Inter\u0002national Enterprise Distributed Object Computing Conference, EDOC\r\n2010, Vitoria, Brazil, 25-29 October 2010.\r\n[13] J. Kruger, C. Kim, M. Grund, N. Satish, D. Schwalb, J. Chhugani, ¨\r\nH. Plattner, P. Dubey, and A. Zeier. Fast Updates on Read-Optimized\r\nDatabases Using Multi-Core CPUs. PVLDB, 5(1):61–72, 2011.\r\n[14] J. Kruger, C. Tinnefeld, M. Grund, A. Zeier, and H. Plattner. A case ¨\r\nfor online mixed workload processing. In Proceedings of the Third\r\nInternational Workshop on Testing Database Systems, DBTest 2010,\r\nIndianapolis, Indiana, USA, June 7, 2010, 2010.\r\n[15] S. Manegold, P. A. Boncz, and M. L. Kersten. Generic Database Cost\r\nModels for Hierarchical Memory Systems. In VLDB 2002, Proceedings\r\nof 28th International Conference on Very Large Data Bases, August\r\n20-23, 2002, Hong Kong, China, pages 191–202, 2002.\r\n[16] D. E. Ott. Optimizing software applications for numa.\r\n[17] H. Plattner. A common database approach for OLTP and OLAP using\r\nan in-memory column database. In Proceedings of the ACM SIGMOD\r\nInternational Conference on Management of Data, SIGMOD 2009,\r\nProvidence, Rhode Island, USA, June 29 - July 2, 2009, pages 1–2,\r\n2009.\r\n[18] R. Ramamurthy, D. J. DeWitt, and Q. Su. A case for fractured mirrors.\r\nVLDB J., 12(2):89–101, 2003.\r\n[19] J. Rao and K. A. Ross. Cache Conscious Indexing for Decision-Support\r\nin Main Memory. In VLDB’99, Proceedings of 25th International Con\u0002ference on Very Large Data Bases, September 7-10, 1999, Edinburgh,\r\nScotland, UK, pages 78–89, 1999.\r\n[20] J. Rao and K. A. Ross. Making B+-Trees Cache Conscious in Main\r\nMemory. In Proceedings of the 2000 ACM SIGMOD International\r\nConference on Management of Data, May 16-18, 2000, Dallas, Texas,\r\nUSA, pages 475–486, 2000.\r\n[21] M. Stonebraker, D. J. Abadi, A. Batkin, X. Chen, M. Cherniack,\r\nM. Ferreira, E. Lau, A. Lin, S. Madden, E. J. O’Neil, P. E. O’Neil,\r\nA. Rasin, N. Tran, and S. B. Zdonik. C-Store: A Column-oriented\r\nDBMS. In Proceedings of the 31st International Conference on Very\r\nLarge Data Bases, Trondheim, Norway, August 30 - September 2, 2005,\r\npages 553–564, 2005.\r\n[22] T. Westmann, D. Kossmann, S. Helmer, and G. Moerkotte. The\r\nImplementation and Performance of Compressed Databases. SIGMOD\r\nRecord, 29(3):55–67, 2000.\r\nCopyright (c) IARIA, 2012. ISBN: 978-1-61208-185-4 153\r\nDBKDA 2012 : The Fourth International Conference on Advances in Databases, Knowledge, and Data Applications",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/a2b66cf6-dd90-4103-96ae-d4ba6acaefb3/images/c1a5b75f-811d-413d-b3b0-21054e4cf57c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041921Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6f5942fd86799f5636fa3773f729b8c5aa2132b810e0b15e47fd4f3ca30f2f35",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1171
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\"title\": \"Leveraging Compression in In-Memory Databases\"}\n```"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "```json\n{\"author\": \"Jens Krueger, Johannes Wust, Martin Linkhorst, Hasso Plattner\"}\n```"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "```json\n{\n \"date_published\": \"2012\"\n}\n```"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "Germany\n"
        }
      ]
    }
  }
}