{
  "file_name": "Longest Common Extension with Recompression - 16th Nov 2016 (1611.05359).pdf",
  "task_id": "b91e711f-a476-4ea3-ac4d-4a509851aa69",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "2c084cbf-309c-4c0b-ab83-ac93aca4d336",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 1,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Longest Common Extension with Recompression\r\nTomohiro I\r\nKyushu Institute of Technology, Japan\r\ntomohiro@ai.kyutech.ac.jp\r\nAbstract\r\nGiven two positions i and j in a string T of length N, a longest common extension (LCE) query\r\nasks for the length of the longest common prefix between suffixes beginning at i and j. A compressed\r\nLCE data structure is a data structure that stores T in a compressed form while supporting fast LCE\r\nqueries. In this article we show that the recompression technique is a powerful tool for compressed\r\nLCE data structures. We present a new compressed LCE data structure of size O(z lg(N/z)) that\r\nsupports LCE queries in O(lg N) time, where z is the size of Lempel-Ziv 77 factorization without\r\nself-reference of T. Given T as an uncompressed form, we show how to build our data structure in\r\nO(N) time and space. Given T as a grammar compressed form, i.e., an straight-line program of size\r\nn generating T, we show how to build our data structure in O(n lg(N/n)) time and O(n + z lg(N/z))\r\nspace. Our algorithms are deterministic and always return correct answers.\r\n1 Introduction\r\nGiven two positions i and j in a text T of length N, a longest common extension (LCE) query LCE(i, j)\r\nasks for the length of the longest common prefix between suffixes beginning at i and j. Since LCE queries\r\nplay a central role in many string processing algorithms (see text book [6] for example), efficient LCE\r\ndata structures have been extensively studied. If we are allowed to use O(N) space, optimal O(1) query\r\ntime can be achieved by, e.g., lowest common ancestor queries [1] on the suffix tree of T. However, O(N)\r\nspace can be too expensive nowadays as the size of strings to be processed is quite large. Thus, recent\r\nstudies focus on more space efficient solutions.\r\nRoughly there are three scenarios: Several authors have studied tradeoffs among query time, construc\u0002tion time and data structure size [17, 5, 4, 19]; In [16], Prezza presented in-place LCE data structures\r\nshowing that the memory space for storing T can be replaced with an LCE data structure while retaining\r\noptimal substring extraction time; LCE data structures working on grammar compressed representation\r\nof T were studied in [7, 2, 3, 15].\r\nIn this article we pursue the third scenario, which is advantageous when T is highly compressible. In\r\ngrammar compression, T is represented by a Context Free Grammar (CFG) that generates T and only T.\r\nIn particular CFGs in Chomsky normal form, called Straight Line Programs (SLPs), are often considered\r\nas any CFG can be easily transformed into an SLP without changing the order of grammar size. Let S be\r\nan arbitrary SLP of size n generating T. Bille et al. [3] showed a Monte Carlo randomized data structure\r\nof O(n) space that supports LCE queries in O(lg N + lg2`) time, where ` is the answer to the LCE query.\r\nBecause their algorithm is based on Karp-Rabin fingerprints, the answer is correct w.h.p (with high\r\nprobability). If we always expect correct answers, we have to verify fingerprints in preprocessing phase,\r\nspending either O(N lg N) time (w.h.p.) and O(N) space or O(\r\nN2\r\nn\r\nlg N) time (w.h.p.) and O(n) space.\r\nFor a deterministic solution, I et al. [7] proposed an O(n\r\n2\r\n)-space data structure, which can be built in\r\nO(n\r\n2h) time and O(n2\r\n) space from S, and supports LCE queries in O(h lg N) time, where h is the height\r\nof S. As will be stated in Theorem 2, we outstrip this result.\r\nOur work is most similar to that presented in [15]. They showed that the signature encoding [13]\r\nof T, a special kind of CFGs that can be stored in O(z lg N lg∗ N) space, can support LCE queries in\r\nO(lg N + lg ` lg∗ N) time, where z is the size of LZ77 factorization of T.\r\n1 The signature encoding is based\r\non localy consistent parsing technique, which determines the parsing of a string by local surrounding.\r\nA key property of the signature encoding is that any occurrence of the same substring of length ` in\r\nT is guaranteed to be compressed in almost same way leaving only O(lg ` lg∗ N) discrepancies in its\r\n1Note that there are several variants of LZ77 factorization. We refer to LZ77 factorization as the one that is known as\r\nthe f-factorization without self-reference unless otherwise noted.\r\n1\r\narXiv:1611.05359v1 [cs.DS] 16 Nov 2016",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/2c084cbf-309c-4c0b-ab83-ac93aca4d336.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4b959a9266c882b92530a15c5abc48c3fe2c31b54cfa62ff25aeddb623176814",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 742
      },
      {
        "segments": [
          {
            "segment_id": "2c084cbf-309c-4c0b-ab83-ac93aca4d336",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 1,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Longest Common Extension with Recompression\r\nTomohiro I\r\nKyushu Institute of Technology, Japan\r\ntomohiro@ai.kyutech.ac.jp\r\nAbstract\r\nGiven two positions i and j in a string T of length N, a longest common extension (LCE) query\r\nasks for the length of the longest common prefix between suffixes beginning at i and j. A compressed\r\nLCE data structure is a data structure that stores T in a compressed form while supporting fast LCE\r\nqueries. In this article we show that the recompression technique is a powerful tool for compressed\r\nLCE data structures. We present a new compressed LCE data structure of size O(z lg(N/z)) that\r\nsupports LCE queries in O(lg N) time, where z is the size of Lempel-Ziv 77 factorization without\r\nself-reference of T. Given T as an uncompressed form, we show how to build our data structure in\r\nO(N) time and space. Given T as a grammar compressed form, i.e., an straight-line program of size\r\nn generating T, we show how to build our data structure in O(n lg(N/n)) time and O(n + z lg(N/z))\r\nspace. Our algorithms are deterministic and always return correct answers.\r\n1 Introduction\r\nGiven two positions i and j in a text T of length N, a longest common extension (LCE) query LCE(i, j)\r\nasks for the length of the longest common prefix between suffixes beginning at i and j. Since LCE queries\r\nplay a central role in many string processing algorithms (see text book [6] for example), efficient LCE\r\ndata structures have been extensively studied. If we are allowed to use O(N) space, optimal O(1) query\r\ntime can be achieved by, e.g., lowest common ancestor queries [1] on the suffix tree of T. However, O(N)\r\nspace can be too expensive nowadays as the size of strings to be processed is quite large. Thus, recent\r\nstudies focus on more space efficient solutions.\r\nRoughly there are three scenarios: Several authors have studied tradeoffs among query time, construc\u0002tion time and data structure size [17, 5, 4, 19]; In [16], Prezza presented in-place LCE data structures\r\nshowing that the memory space for storing T can be replaced with an LCE data structure while retaining\r\noptimal substring extraction time; LCE data structures working on grammar compressed representation\r\nof T were studied in [7, 2, 3, 15].\r\nIn this article we pursue the third scenario, which is advantageous when T is highly compressible. In\r\ngrammar compression, T is represented by a Context Free Grammar (CFG) that generates T and only T.\r\nIn particular CFGs in Chomsky normal form, called Straight Line Programs (SLPs), are often considered\r\nas any CFG can be easily transformed into an SLP without changing the order of grammar size. Let S be\r\nan arbitrary SLP of size n generating T. Bille et al. [3] showed a Monte Carlo randomized data structure\r\nof O(n) space that supports LCE queries in O(lg N + lg2`) time, where ` is the answer to the LCE query.\r\nBecause their algorithm is based on Karp-Rabin fingerprints, the answer is correct w.h.p (with high\r\nprobability). If we always expect correct answers, we have to verify fingerprints in preprocessing phase,\r\nspending either O(N lg N) time (w.h.p.) and O(N) space or O(\r\nN2\r\nn\r\nlg N) time (w.h.p.) and O(n) space.\r\nFor a deterministic solution, I et al. [7] proposed an O(n\r\n2\r\n)-space data structure, which can be built in\r\nO(n\r\n2h) time and O(n2\r\n) space from S, and supports LCE queries in O(h lg N) time, where h is the height\r\nof S. As will be stated in Theorem 2, we outstrip this result.\r\nOur work is most similar to that presented in [15]. They showed that the signature encoding [13]\r\nof T, a special kind of CFGs that can be stored in O(z lg N lg∗ N) space, can support LCE queries in\r\nO(lg N + lg ` lg∗ N) time, where z is the size of LZ77 factorization of T.\r\n1 The signature encoding is based\r\non localy consistent parsing technique, which determines the parsing of a string by local surrounding.\r\nA key property of the signature encoding is that any occurrence of the same substring of length ` in\r\nT is guaranteed to be compressed in almost same way leaving only O(lg ` lg∗ N) discrepancies in its\r\n1Note that there are several variants of LZ77 factorization. We refer to LZ77 factorization as the one that is known as\r\nthe f-factorization without self-reference unless otherwise noted.\r\n1\r\narXiv:1611.05359v1 [cs.DS] 16 Nov 2016",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/2c084cbf-309c-4c0b-ab83-ac93aca4d336.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4b959a9266c882b92530a15c5abc48c3fe2c31b54cfa62ff25aeddb623176814",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 742
      },
      {
        "segments": [
          {
            "segment_id": "fc544eeb-49f7-4805-a8e8-13c9f9fea1c4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 2,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Input Time Space Reference\r\nT NfA z lg N lg∗ N Theorem 3 (1a) of [15]\r\nT N N Theorem 3 (1b) of [15]\r\nS nfA lg N lg∗ N n + z lg N lg∗ N Theorem 3 (3a) of [15]\r\nS n lg lg n lg N lg∗ N n lg∗ N + z lg N lg∗ N Theorem 3 (3b) of [15]\r\nLZ77 zfA lg N lg∗ M z lg N lg∗ N Theorem 3 (2) of [15]\r\nT N N this work, Theorem 1\r\nS n lg(N/n) n + z lg(N/z) this work, Theorem 2\r\nLZ77 z lg2(N/z) z lg(N/z) this work, Corollary 3\r\nTable 1: Comparison of construction time and space between ours and [15], where N is the length of T,\r\nS is an SLP of size n generating T, z is the size of LZ77 factorization of T, and fA is the time needed for\r\npredecessor queries on a set of z lg N lg∗ N integers from an N-element universe.\r\nsurrounding. As a result, an LCE query can be answered by tracing the O(lg ` lg∗ N) surroundings\r\ncreated over two occurrences of the longest common extension. The algorithm is quite simple as we simply\r\nsimulate the traversal of the derivation tree on the CFG while matching substrings by appearances of the\r\ncommon variables. Note that another cost O(lg N) is needed to traverse the derivation tree of height\r\nO(lg N) from the root.\r\nIn this article we show that CFGs created by the recompression technique exhibit a similar property\r\nthat can be used to answer LCE queries in O(lg N) time. In recent years recompression has been proved\r\nto be a powerful tool in problems related to grammar compression [8, 9] and word equations [10, 11].\r\nThe main component of recompression is to replace some pairs in a string with variables of the CFG.\r\nAlthough we use global information (like the frequencies of pairs in the string) to determine which pairs\r\nto be replaced, the pairing itself is done very locally, i.e., all occurrences of the pairs are replaced. Then\r\nwe can show that any occurrence of the same substring in T is guaranteed to be compressed in almost\r\nsame way leaving only O(lg N) discrepancies in its surrounding. This leads to an O(lg N)-time algorithm\r\nto answer LCE queries, improving the O(lg N + lg ` lg∗ N)-time algorithm of [15]. We also improve the\r\ndata structure size from O(z lg N lg∗ N) to O(z lg(N/z)).2\r\nIn [15], the authors proposed efficient algorithms to construct the signature encoding from various\r\nkinds of input as summarized in Table 1. We achieve better and cleaner complexities of construction\r\nfrom SLPs, which is important to plug them in compressed string processing in which we are to solve\r\nproblems on SLPs without decompressing the string explicitly. However it should be noted that the data\r\nstructures in [15] also support efficient text edit operations. We are not sure if our data structures can be\r\nefficiently dynamized.\r\nTheorems 1 and 2 show our main results. Note that our data structure is a simple CFG of height\r\nO(lg N) on which we can simulate the traversal of the derivation tree to a target position in constant\r\ntime per move. Thus, it naturally supports Extract(i, `) queries, which asks for retrieving the substring\r\nT[i..i + ` − 1], in O(lg N + `) time.\r\nTheorem 1. Given a string T of length N, we can compute in O(N) time and space a compressed\r\nrepresentation of T of size O(z lg(N/z)) that supports Extract(i, `) in O(lg N + `) time and LCE queries\r\nin O(lg N) time.\r\nTheorem 2. Given an SLP of size n generating a string T of length N, we can compute in O(n lg(N/n))\r\ntime and O(n + z lg(N/z)) space a compressed representation of T of size O(z lg(N/z)) that supports\r\nExtract(i, `) in O(lg N + `) time and LCE queries in O(lg N) time.\r\nSuppose that we are given the LZ77-compression of size z of T as an input. Since we can convert\r\nthe input into an SLP of size O(z lg(N/z)) [18], we can apply Theorem 2 to the SLP and get the next\r\ncorollary.\r\nCorollary 3. Given the LZ77-compression of size z of a string T of length N, we can compute in\r\nO(z lg2(N/z)) time and O(z lg(N/z)) space a compressed representation of T of size O(z lg(N/z)) that\r\nsupports Extract(i, `) in O(lg N + `) time and LCE queries in O(lg N) time.\r\n2We believe that the space complexities of [15] can be improved to O(z lg(N/z) lg∗ N) by using the same trick we use in\r\nLemma 14.\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/fc544eeb-49f7-4805-a8e8-13c9f9fea1c4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cbad2118d404518f6932673565bc6995ebbb635f0858906a31862e8297828e6f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 785
      },
      {
        "segments": [
          {
            "segment_id": "fc544eeb-49f7-4805-a8e8-13c9f9fea1c4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 2,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Input Time Space Reference\r\nT NfA z lg N lg∗ N Theorem 3 (1a) of [15]\r\nT N N Theorem 3 (1b) of [15]\r\nS nfA lg N lg∗ N n + z lg N lg∗ N Theorem 3 (3a) of [15]\r\nS n lg lg n lg N lg∗ N n lg∗ N + z lg N lg∗ N Theorem 3 (3b) of [15]\r\nLZ77 zfA lg N lg∗ M z lg N lg∗ N Theorem 3 (2) of [15]\r\nT N N this work, Theorem 1\r\nS n lg(N/n) n + z lg(N/z) this work, Theorem 2\r\nLZ77 z lg2(N/z) z lg(N/z) this work, Corollary 3\r\nTable 1: Comparison of construction time and space between ours and [15], where N is the length of T,\r\nS is an SLP of size n generating T, z is the size of LZ77 factorization of T, and fA is the time needed for\r\npredecessor queries on a set of z lg N lg∗ N integers from an N-element universe.\r\nsurrounding. As a result, an LCE query can be answered by tracing the O(lg ` lg∗ N) surroundings\r\ncreated over two occurrences of the longest common extension. The algorithm is quite simple as we simply\r\nsimulate the traversal of the derivation tree on the CFG while matching substrings by appearances of the\r\ncommon variables. Note that another cost O(lg N) is needed to traverse the derivation tree of height\r\nO(lg N) from the root.\r\nIn this article we show that CFGs created by the recompression technique exhibit a similar property\r\nthat can be used to answer LCE queries in O(lg N) time. In recent years recompression has been proved\r\nto be a powerful tool in problems related to grammar compression [8, 9] and word equations [10, 11].\r\nThe main component of recompression is to replace some pairs in a string with variables of the CFG.\r\nAlthough we use global information (like the frequencies of pairs in the string) to determine which pairs\r\nto be replaced, the pairing itself is done very locally, i.e., all occurrences of the pairs are replaced. Then\r\nwe can show that any occurrence of the same substring in T is guaranteed to be compressed in almost\r\nsame way leaving only O(lg N) discrepancies in its surrounding. This leads to an O(lg N)-time algorithm\r\nto answer LCE queries, improving the O(lg N + lg ` lg∗ N)-time algorithm of [15]. We also improve the\r\ndata structure size from O(z lg N lg∗ N) to O(z lg(N/z)).2\r\nIn [15], the authors proposed efficient algorithms to construct the signature encoding from various\r\nkinds of input as summarized in Table 1. We achieve better and cleaner complexities of construction\r\nfrom SLPs, which is important to plug them in compressed string processing in which we are to solve\r\nproblems on SLPs without decompressing the string explicitly. However it should be noted that the data\r\nstructures in [15] also support efficient text edit operations. We are not sure if our data structures can be\r\nefficiently dynamized.\r\nTheorems 1 and 2 show our main results. Note that our data structure is a simple CFG of height\r\nO(lg N) on which we can simulate the traversal of the derivation tree to a target position in constant\r\ntime per move. Thus, it naturally supports Extract(i, `) queries, which asks for retrieving the substring\r\nT[i..i + ` − 1], in O(lg N + `) time.\r\nTheorem 1. Given a string T of length N, we can compute in O(N) time and space a compressed\r\nrepresentation of T of size O(z lg(N/z)) that supports Extract(i, `) in O(lg N + `) time and LCE queries\r\nin O(lg N) time.\r\nTheorem 2. Given an SLP of size n generating a string T of length N, we can compute in O(n lg(N/n))\r\ntime and O(n + z lg(N/z)) space a compressed representation of T of size O(z lg(N/z)) that supports\r\nExtract(i, `) in O(lg N + `) time and LCE queries in O(lg N) time.\r\nSuppose that we are given the LZ77-compression of size z of T as an input. Since we can convert\r\nthe input into an SLP of size O(z lg(N/z)) [18], we can apply Theorem 2 to the SLP and get the next\r\ncorollary.\r\nCorollary 3. Given the LZ77-compression of size z of a string T of length N, we can compute in\r\nO(z lg2(N/z)) time and O(z lg(N/z)) space a compressed representation of T of size O(z lg(N/z)) that\r\nsupports Extract(i, `) in O(lg N + `) time and LCE queries in O(lg N) time.\r\n2We believe that the space complexities of [15] can be improved to O(z lg(N/z) lg∗ N) by using the same trick we use in\r\nLemma 14.\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/fc544eeb-49f7-4805-a8e8-13c9f9fea1c4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cbad2118d404518f6932673565bc6995ebbb635f0858906a31862e8297828e6f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 785
      },
      {
        "segments": [
          {
            "segment_id": "cfdd60bc-510f-4cec-86a8-ba521ea4065a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 3,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Technically, this work owes very much to two papers [9, 8]. For instance, our construction algorithm\r\nof Theorem 1 is essentially the same as the grammar compression algorithm based on recompression\r\npresented in [9]. Our contribution is in discovering the above mentioned property that can be used for\r\nfast LCE queries. Also, we use the property to upper bound the size of our data structure in terms of\r\nz rather than the smallest grammar size g\r\n∗\r\n. Since it is known that z ≤ g\r\n∗ holds, an upper bound in\r\nterms of z is preferable. Our construction algorithm of Theorem 2 owes to [8], in which the recompression\r\ntechnique solves the fully-compressed pattern matching problems. Basically our results can be obtained\r\nby applying the technique. However, we make some contributions on top of it: We give a new observation\r\nthat simplifies the implementation and analysis of a component of recompression called BComp (see\r\nSection 4.1.2). Also, we show that we can improve the time complexity from O(n lg N) to O(n lg(N/n)).\r\n2 Preliminaries\r\nAn alphabet Σ is a set of characters. A string over Σ is an element in Σ∗. For any string w ∈ Σ\r\n∗\r\n,\r\n|w| denotes the length of w. Let ε be the empty string, i.e., |ε| = 0. Let Σ+ = Σ∗ \\ {ε}. For any\r\n1 ≤ i ≤ |w|, w[i] denotes the i-th character of w. For any 1 ≤ i ≤ j ≤ |w|, w[i..j] denotes the substring of\r\nw beginning at i and ending at j. For convenience, let w[i..j] = ε if i > j. For any 0 ≤ i ≤ |w|, w[1..i]\r\n(resp. w[|w| − i + 1..|w|]) is called the prefix (resp. suffix) of w of length i. We say taht a string x occurs\r\nat position i in w iff w[i..|x| − 1] = x. A substring w[i..j] = c\r\nd\r\n(c ∈ Σ, d ≥ 1) of w is called a block iff it is\r\na maximal run of a single character, i.e., w[i − 1] 6= c and w[j + 1] 6= c.\r\nThe text on which LCE queries are performed is denoted by T ∈ Σ\r\n∗ with N = |T| throughout this\r\npaper. We assume that Σ is an integer alphabet [1..N O(1)] and the standard word RAM model with word\r\nsize Ω(lg N).\r\nThe size of our compressed LCE data structure is bounded by O(z lg(N/z)), where z is the size of the\r\nLZ77 factorization of T defined as follows:\r\nDefinition 4 (LZ77 factorization). The factorization T = f1f2 · · · fz is the LZ77 factorization of T iff\r\nthe following condition holds: For any 1 ≤ i ≤ z, let pi = |f1f2 · · · fi−1| + 1, then fi = T[pi] if T[pi] does\r\nnot appear in T[1..pi − 1], otherwise fi is the longest prefix of T[pi..N] that occurs in T[1..pi − 1].\r\nExample 5. The LZ77 factorization of abaabaabb is a · b · a · aba · ab · b and z = 6.\r\nIn this article, we deal with grammar compressed strings, in which a string is represented by a Context\r\nFree Grammar (CFG) generating the string only. In particular, we consider Straight-Line Programs\r\n(SLPs) that are CFGs in Chomsky normal form. Formally, an SLP that generates a string T is a triple\r\nS = (Σ, V, D), where Σ is the set of characters (terminals), V is the set of variables (non-terminals), D\r\nis the set of deterministic production rules whose righthand sides are in V\r\n2 ∪ Σ, and the last variable\r\nderives T.\r\n3 Let n = |V|. We treat variables as integers in [1..n] (which should be distinguishable from Σ\r\nby having extra one bit), and D as an injective function that maps a variable to its righthand side. We\r\nassume that given any variable X we can access in O(1) time to the data space storing the information of\r\nX, e.g., D(X). We refer to n as the size of S since S can be encoded in O(n) space. Note that N can be\r\nas large as 2n−1, and so, SLPs have a potential to achieve exponential compression.\r\nWe extend SLPs by allowing run-length encoded rules whose righthand sides are of the form Xd with\r\nX ∈ V and d ≥ 2, and call such CFGs run-length SLPs (RLSLPs). Since a run-length encoded rule can\r\nbe stored in O(1) space, we still define the size of an RLSLP by the number of variables.\r\nLet us consider the derivation tree T of an RLSLP S that generates a string T, where we delete all\r\nthe nodes labeled with terminals for simplicity. That is, every node in T is labeled with a variable. The\r\nheight of S is the height of T . We say that a sequence C = v1 · · · vm of nodes is a chain iff the nodes are\r\nall adjacent in this order, i.e., the beginning position of vi+1 is the ending position of vi plus one for any\r\n1 ≤ i < m. C is labeled with the sequence of labels of v1 · · · vm.\r\nFor any sequence p ∈ V∗ of variables, let val S (p) denote the string obtained by concatenating the\r\nstrings derived from all variables in the sequence. We omit S when it is clear from context. We say that\r\np generates val(p). Also, we say that p occurs at position i iff there is a chain that is labeled with p and\r\nbegins at i.\r\nThe next lemma, which is somewhat standard for SLPs, also holds for RLSLPs.\r\nLemma 6. For any RSLP S of height h generating T, by storing |val(X)| for every variable X, we can\r\nsupport Extract(i, `) in O(h + `) time.\r\n3We treat the last variable as the starting variable.\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/cfdd60bc-510f-4cec-86a8-ba521ea4065a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9f019ee5e6748c7756f1d75f5fe330ab9f1c3527ad5dad8fee81e099cbd9cb7e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 983
      },
      {
        "segments": [
          {
            "segment_id": "cfdd60bc-510f-4cec-86a8-ba521ea4065a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 3,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Technically, this work owes very much to two papers [9, 8]. For instance, our construction algorithm\r\nof Theorem 1 is essentially the same as the grammar compression algorithm based on recompression\r\npresented in [9]. Our contribution is in discovering the above mentioned property that can be used for\r\nfast LCE queries. Also, we use the property to upper bound the size of our data structure in terms of\r\nz rather than the smallest grammar size g\r\n∗\r\n. Since it is known that z ≤ g\r\n∗ holds, an upper bound in\r\nterms of z is preferable. Our construction algorithm of Theorem 2 owes to [8], in which the recompression\r\ntechnique solves the fully-compressed pattern matching problems. Basically our results can be obtained\r\nby applying the technique. However, we make some contributions on top of it: We give a new observation\r\nthat simplifies the implementation and analysis of a component of recompression called BComp (see\r\nSection 4.1.2). Also, we show that we can improve the time complexity from O(n lg N) to O(n lg(N/n)).\r\n2 Preliminaries\r\nAn alphabet Σ is a set of characters. A string over Σ is an element in Σ∗. For any string w ∈ Σ\r\n∗\r\n,\r\n|w| denotes the length of w. Let ε be the empty string, i.e., |ε| = 0. Let Σ+ = Σ∗ \\ {ε}. For any\r\n1 ≤ i ≤ |w|, w[i] denotes the i-th character of w. For any 1 ≤ i ≤ j ≤ |w|, w[i..j] denotes the substring of\r\nw beginning at i and ending at j. For convenience, let w[i..j] = ε if i > j. For any 0 ≤ i ≤ |w|, w[1..i]\r\n(resp. w[|w| − i + 1..|w|]) is called the prefix (resp. suffix) of w of length i. We say taht a string x occurs\r\nat position i in w iff w[i..|x| − 1] = x. A substring w[i..j] = c\r\nd\r\n(c ∈ Σ, d ≥ 1) of w is called a block iff it is\r\na maximal run of a single character, i.e., w[i − 1] 6= c and w[j + 1] 6= c.\r\nThe text on which LCE queries are performed is denoted by T ∈ Σ\r\n∗ with N = |T| throughout this\r\npaper. We assume that Σ is an integer alphabet [1..N O(1)] and the standard word RAM model with word\r\nsize Ω(lg N).\r\nThe size of our compressed LCE data structure is bounded by O(z lg(N/z)), where z is the size of the\r\nLZ77 factorization of T defined as follows:\r\nDefinition 4 (LZ77 factorization). The factorization T = f1f2 · · · fz is the LZ77 factorization of T iff\r\nthe following condition holds: For any 1 ≤ i ≤ z, let pi = |f1f2 · · · fi−1| + 1, then fi = T[pi] if T[pi] does\r\nnot appear in T[1..pi − 1], otherwise fi is the longest prefix of T[pi..N] that occurs in T[1..pi − 1].\r\nExample 5. The LZ77 factorization of abaabaabb is a · b · a · aba · ab · b and z = 6.\r\nIn this article, we deal with grammar compressed strings, in which a string is represented by a Context\r\nFree Grammar (CFG) generating the string only. In particular, we consider Straight-Line Programs\r\n(SLPs) that are CFGs in Chomsky normal form. Formally, an SLP that generates a string T is a triple\r\nS = (Σ, V, D), where Σ is the set of characters (terminals), V is the set of variables (non-terminals), D\r\nis the set of deterministic production rules whose righthand sides are in V\r\n2 ∪ Σ, and the last variable\r\nderives T.\r\n3 Let n = |V|. We treat variables as integers in [1..n] (which should be distinguishable from Σ\r\nby having extra one bit), and D as an injective function that maps a variable to its righthand side. We\r\nassume that given any variable X we can access in O(1) time to the data space storing the information of\r\nX, e.g., D(X). We refer to n as the size of S since S can be encoded in O(n) space. Note that N can be\r\nas large as 2n−1, and so, SLPs have a potential to achieve exponential compression.\r\nWe extend SLPs by allowing run-length encoded rules whose righthand sides are of the form Xd with\r\nX ∈ V and d ≥ 2, and call such CFGs run-length SLPs (RLSLPs). Since a run-length encoded rule can\r\nbe stored in O(1) space, we still define the size of an RLSLP by the number of variables.\r\nLet us consider the derivation tree T of an RLSLP S that generates a string T, where we delete all\r\nthe nodes labeled with terminals for simplicity. That is, every node in T is labeled with a variable. The\r\nheight of S is the height of T . We say that a sequence C = v1 · · · vm of nodes is a chain iff the nodes are\r\nall adjacent in this order, i.e., the beginning position of vi+1 is the ending position of vi plus one for any\r\n1 ≤ i < m. C is labeled with the sequence of labels of v1 · · · vm.\r\nFor any sequence p ∈ V∗ of variables, let val S (p) denote the string obtained by concatenating the\r\nstrings derived from all variables in the sequence. We omit S when it is clear from context. We say that\r\np generates val(p). Also, we say that p occurs at position i iff there is a chain that is labeled with p and\r\nbegins at i.\r\nThe next lemma, which is somewhat standard for SLPs, also holds for RLSLPs.\r\nLemma 6. For any RSLP S of height h generating T, by storing |val(X)| for every variable X, we can\r\nsupport Extract(i, `) in O(h + `) time.\r\n3We treat the last variable as the starting variable.\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/cfdd60bc-510f-4cec-86a8-ba521ea4065a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9f019ee5e6748c7756f1d75f5fe330ab9f1c3527ad5dad8fee81e099cbd9cb7e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 983
      },
      {
        "segments": [
          {
            "segment_id": "eb4dec58-89df-4114-81a5-ee2dc6bde1a3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 4,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "3 LCE data structure built from uncompressed texts\r\nIn this section, we prove Theorem 1. We basically show that the RLSLP obtained by grammar compression\r\nalgorithm based on recompression [8] can be used for fast LCE queries. In Subsection 3.1 we first review\r\nthe recompression and introduce notation we use. In Subsection 3.2 we present a new characterization of\r\nrecompression, which is a key to our contributions.\r\n3.1 TtoG: Grammar compression based on recompression\r\nIn [8] Je˙z proposed an algorithm TtoG to compute an RLSLP of T in O(N) time based on the recompression\r\ntechnique.4 Let TtoG(T) denote the RLSLP of T produced by TtoG. We use the term letters for variables\r\nintroduced by TtoG. Also, we use c (rather than X) to represent a letter.\r\nTtoG consists of two different types of compression BComp and PComp, which stand for Block\r\nCompression and Pair Compression, respectively.\r\n• BComp: Given a string w over Σ = [1..|w|], BComp compresses w by replacing all blocks of length\r\n≥ 2 with fresh letters. Note that BComp eliminates all blocks of length ≥ 2 in w. We can conduct\r\nBComp in O(|w|) time and space (see Lemma 7).\r\n• PComp: Given an string w over Σ = [1..|w|] that contains no block of length ≥ 2, PComp compresses\r\nw by replacing all pairs from Σ´ Σ` with fresh letters, where (Σ´ , Σ` ) is a partition of Σ, i.e., Σ = Σ´ ∪ Σ`\r\nand Σ´ ∩ Σ` = ∅. We can deterministically compute in O(|w|) time and space a partition of Σ by\r\nwhich at least (|w| − 1)/4 pairs are replaced (see Lemma 8), and conduct PComp in O(|w|) time\r\nand space (see Lemma 9).\r\nLet T0 be a sequence of letters obtained by replacing every character c of T with a letter generating c.\r\nThen TtoG compresses T0 by applying BComp and PComp by turns until the string gets shrunk into a\r\nsingle letter. Since PComp compresses a given string by a constant factor 3/4, the height of TtoG(T) is\r\nO(lg N), and the total running time can be bounded by O(N) (see Lemma 10).\r\nIn order to give a formal description we introduce some notation below. TtoG transforms level by\r\nlevel T0 into strings, T1, T2, . . . , Thˆ where |Thˆ | = 1. For any 0 ≤ h ≤ hˆ, we say that h is the level of Th.\r\nIf h is even, the transformation from Th to Th+1 is performed by BComp, and production rules of the\r\nform c → c¨\r\nd are introduced. If h is odd, the transformation from Th to Th+1 is performed by PComp,\r\nand production rules of the form c → c´c` are introduced. Let Σh be the set of letters appearing in Th. For\r\nany even h (0 ≤ h < hˆ), let Σ¨\r\nh denote the set of letters with which there is a block of length ≥ 2 in Th.\r\nFor any odd h (0 ≤ h < hˆ), let (Σ´\r\nh, Σ`h) denote the partition of Σh used in PComp of level h.\r\nThe following four lemmas show how to conduct BComp, PComp, and therefore TtoG, efficiently,\r\nwhich are essentially the same as respectively Lemma 2, Lemma 5, Lemma 6, and Theorem 1, stated\r\nin [8]. We give the proofs for the sake of completeness.\r\nLemma 7. Given a string w over Σ = [1..|w|], we can conduct BComp in O(|w|) time and space.\r\nProof. We first scan w in O(|w|) time and list all the blocks of length ≥ 2. Each block c\r\nd\r\n(c ∈ Σ, d ≥ 2)\r\nat position i is listed by a triple (c, d, i) of integers in Σ. Next we sort the list according to the pair of\r\nintegers (c, d), which can be done in O(|w|) time and space by radix sort. Finally, we replace each block\r\nc\r\nd by a fresh letter based on the rank of (c, d).\r\nFor any string w ∈ Σ\r\n∗\r\nthat contains no block of length ≥ 2, let Freqw(c, c, ˜ 0) (resp. Freqw(c, c, ˜ 1))\r\nwith c > c˜ ∈ Σ denote the number of occurrences of cc˜ (resp. cc˜ ) in w. We refer to the list of non-zero\r\nFreqw(c, c, ˜ ·) sorted in increasing order of c as the adjacency list of w. Note that it is the representation of\r\nthe weighted directed graph in which there are exactly Freqw(c, c, ˜ 0) (resp. Freqw(c, c, ˜ 1)) edges from c to\r\nc˜ (resp. from c˜ to c). Each occurrence of a pair in w is counted exactly once in the adjacency list. Then\r\nthe problem of computing a good partition (Σ´ , Σ` ) of Σ reduces to maximum directed cut problem on the\r\ngraph. Algorithm 1 is based on a simple greedy 1/4-approximation algorithm of maximum directed cut\r\nproblem.\r\nLemma 8. Given the adjacency list of size m of a string w ∈ Σ\r\n∗\r\n, Algorithm 1 computes in O(m) time a\r\npartition (Σ´ , Σ` ) of Σ such that the number of occurrences of pairs from Σ´ Σ` in w is at least (|w| − 1)/4.\r\n4\r\nIndeed, the paper shows how to compute an SLP of size O(g\r\n∗ lg(N/g∗)), where g∗ is the smallest SLP size to generate\r\nT.\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/eb4dec58-89df-4114-81a5-ee2dc6bde1a3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8b7b3e7f58590a81dbca6484d322ff29acb87755fea407532d84d2aaaf696fde",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 893
      },
      {
        "segments": [
          {
            "segment_id": "eb4dec58-89df-4114-81a5-ee2dc6bde1a3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 4,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "3 LCE data structure built from uncompressed texts\r\nIn this section, we prove Theorem 1. We basically show that the RLSLP obtained by grammar compression\r\nalgorithm based on recompression [8] can be used for fast LCE queries. In Subsection 3.1 we first review\r\nthe recompression and introduce notation we use. In Subsection 3.2 we present a new characterization of\r\nrecompression, which is a key to our contributions.\r\n3.1 TtoG: Grammar compression based on recompression\r\nIn [8] Je˙z proposed an algorithm TtoG to compute an RLSLP of T in O(N) time based on the recompression\r\ntechnique.4 Let TtoG(T) denote the RLSLP of T produced by TtoG. We use the term letters for variables\r\nintroduced by TtoG. Also, we use c (rather than X) to represent a letter.\r\nTtoG consists of two different types of compression BComp and PComp, which stand for Block\r\nCompression and Pair Compression, respectively.\r\n• BComp: Given a string w over Σ = [1..|w|], BComp compresses w by replacing all blocks of length\r\n≥ 2 with fresh letters. Note that BComp eliminates all blocks of length ≥ 2 in w. We can conduct\r\nBComp in O(|w|) time and space (see Lemma 7).\r\n• PComp: Given an string w over Σ = [1..|w|] that contains no block of length ≥ 2, PComp compresses\r\nw by replacing all pairs from Σ´ Σ` with fresh letters, where (Σ´ , Σ` ) is a partition of Σ, i.e., Σ = Σ´ ∪ Σ`\r\nand Σ´ ∩ Σ` = ∅. We can deterministically compute in O(|w|) time and space a partition of Σ by\r\nwhich at least (|w| − 1)/4 pairs are replaced (see Lemma 8), and conduct PComp in O(|w|) time\r\nand space (see Lemma 9).\r\nLet T0 be a sequence of letters obtained by replacing every character c of T with a letter generating c.\r\nThen TtoG compresses T0 by applying BComp and PComp by turns until the string gets shrunk into a\r\nsingle letter. Since PComp compresses a given string by a constant factor 3/4, the height of TtoG(T) is\r\nO(lg N), and the total running time can be bounded by O(N) (see Lemma 10).\r\nIn order to give a formal description we introduce some notation below. TtoG transforms level by\r\nlevel T0 into strings, T1, T2, . . . , Thˆ where |Thˆ | = 1. For any 0 ≤ h ≤ hˆ, we say that h is the level of Th.\r\nIf h is even, the transformation from Th to Th+1 is performed by BComp, and production rules of the\r\nform c → c¨\r\nd are introduced. If h is odd, the transformation from Th to Th+1 is performed by PComp,\r\nand production rules of the form c → c´c` are introduced. Let Σh be the set of letters appearing in Th. For\r\nany even h (0 ≤ h < hˆ), let Σ¨\r\nh denote the set of letters with which there is a block of length ≥ 2 in Th.\r\nFor any odd h (0 ≤ h < hˆ), let (Σ´\r\nh, Σ`h) denote the partition of Σh used in PComp of level h.\r\nThe following four lemmas show how to conduct BComp, PComp, and therefore TtoG, efficiently,\r\nwhich are essentially the same as respectively Lemma 2, Lemma 5, Lemma 6, and Theorem 1, stated\r\nin [8]. We give the proofs for the sake of completeness.\r\nLemma 7. Given a string w over Σ = [1..|w|], we can conduct BComp in O(|w|) time and space.\r\nProof. We first scan w in O(|w|) time and list all the blocks of length ≥ 2. Each block c\r\nd\r\n(c ∈ Σ, d ≥ 2)\r\nat position i is listed by a triple (c, d, i) of integers in Σ. Next we sort the list according to the pair of\r\nintegers (c, d), which can be done in O(|w|) time and space by radix sort. Finally, we replace each block\r\nc\r\nd by a fresh letter based on the rank of (c, d).\r\nFor any string w ∈ Σ\r\n∗\r\nthat contains no block of length ≥ 2, let Freqw(c, c, ˜ 0) (resp. Freqw(c, c, ˜ 1))\r\nwith c > c˜ ∈ Σ denote the number of occurrences of cc˜ (resp. cc˜ ) in w. We refer to the list of non-zero\r\nFreqw(c, c, ˜ ·) sorted in increasing order of c as the adjacency list of w. Note that it is the representation of\r\nthe weighted directed graph in which there are exactly Freqw(c, c, ˜ 0) (resp. Freqw(c, c, ˜ 1)) edges from c to\r\nc˜ (resp. from c˜ to c). Each occurrence of a pair in w is counted exactly once in the adjacency list. Then\r\nthe problem of computing a good partition (Σ´ , Σ` ) of Σ reduces to maximum directed cut problem on the\r\ngraph. Algorithm 1 is based on a simple greedy 1/4-approximation algorithm of maximum directed cut\r\nproblem.\r\nLemma 8. Given the adjacency list of size m of a string w ∈ Σ\r\n∗\r\n, Algorithm 1 computes in O(m) time a\r\npartition (Σ´ , Σ` ) of Σ such that the number of occurrences of pairs from Σ´ Σ` in w is at least (|w| − 1)/4.\r\n4\r\nIndeed, the paper shows how to compute an SLP of size O(g\r\n∗ lg(N/g∗)), where g∗ is the smallest SLP size to generate\r\nT.\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/eb4dec58-89df-4114-81a5-ee2dc6bde1a3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8b7b3e7f58590a81dbca6484d322ff29acb87755fea407532d84d2aaaf696fde",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 893
      },
      {
        "segments": [
          {
            "segment_id": "d4845a25-5b29-4e4b-a37f-eaa8fc1578e6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 5,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Proof. In the foreach loop, we first run a 1/2-approximation algorithm of maximum “undirected” cut\r\nproblem on the adjacency list, i.e., we ignore the direction of the edges here. For each c in increasing order,\r\nwe greedily determine whether c is added to Σ´ or to Σ` depending on P\r\nc˜∈Σ` Freq(c, c, ˜ ·) ≥\r\nP\r\nc˜∈Σ´ Freq(c, c, ˜ ·).\r\nNote that P\r\nc˜∈Σ` Freq(c, c˜) (resp. Pc˜∈Σ´ Freq(c, c˜)) represents the number of edges between c and a\r\ncharacter in Σ` (resp. Σ´ ). By greedy choice, at least half of the edges in subject become the ones\r\nconnecting two characters each from Σ´ and Σ` . Hence, in the end, |E| becomes at least (|w| − 1)/2, where\r\nlet E denote the set of edges between characters from Σ and ´ Σ (recalling that there are exactly ` |w| − 1\r\nedges). Since each edge in E corresponds to an occurrence of a pair from Σ´ Σ` ∪ Σ` Σ´ in w, at least one\r\nof the two partitions (Σ´ , Σ` ) and (Σ` , Σ´ ) covers more than half of E. Hence we achieve our final bound\r\n|E|/2 = (|w| − 1)/4 by choosing an appropriate partition at Line 7.\r\nIn order to see that Algorithm 1 runs in O(m) time, we only have to care about Line 3 and Line 7.\r\nWe can compute P\r\nc˜∈Σ` Freq(c, c, ˜ ·) and Pc˜∈Σ´ Freq(c, c, ˜ ·) by going through all Freq(c, ·, ·) for fixed c in\r\nthe adjacency list, which are consecutive in the sorted list. Since each element of the list is used only\r\nonce, the cost for Line 3 is O(m) in total. Similarly the computation at Line 7 can be done by going\r\nthrough the adjacency list again. Thus the algorithm runs in O(m) time.\r\nAlgorithm 1: How to compute a partition of Σ for PComp to compress w by 3/4.\r\nInput: Adjacency list of w ∈ Σ\r\n∗\r\n.\r\nOutput: (Σ´ , Σ ) s.t. # occurrences of pairs from ` Σ´ Σ in ` w is at least (|w| − 1)/4.\r\n/* The information whether c ∈ Σ is in Σ´ or Σ` is written in the data space for c,\r\nwhich can be accessed in O(1) time. */\r\n1 Σ´ ← Σ` ← ∅;\r\n2 foreach c ∈ Σ in increasing order do\r\n3 if P\r\nc˜∈Σ` Freqw(c, c, ˜ ·) ≥\r\nP\r\nc˜∈Σ´ Freqw(c, c, ˜ ·) then\r\n4 add c to Σ; ´\r\n5 else\r\n6 add c to Σ; `\r\n7 if # occurrences of pairs from Σ´ Σ` < # occurrences of pairs from Σ` Σ´ then\r\n8 switch Σ and ´ Σ; `\r\n9 return (Σ´ , Σ ); `\r\nLemma 9. Given a string w over Σ = [1..|w|] that contains no block of length ≥ 2, we can conduct\r\nPComp in O(|w|) time and space.\r\nProof. We first compute the adjacency list of w. This can be easily done in O(|w|) time and space by\r\nsorting the |w| − 1 size multiset {(w[i], w[i + 1], 0) | 1 ≤ i < |w|, w[i] > w[i + 1]} ∪ {(w[i], w[i + 1], 1) | 1 ≤\r\ni < |w|, w[i] < w[i + 1]} by radix sort. Then by Lemma 8 we compute a partition (Σ´ , Σ` ) in linear time in\r\nthe size of the adjacency list, which is O(|w|). Next we scan w in O(|w|) time and list all the occurrences\r\nof pairs to be compressed. Each pair c´c` ∈ Σ´ Σ` at position i is listed by a triple (c, ´ c, i ` ) of integers in Σ.\r\nThen we sort the list according to the pair of integers (c, ´ c`), which can be done in O(|w|) time and space\r\nby radix sort. Finally, we replace each pair with a fresh letter based on the rank of (´c, c`).\r\nLemma 10. Given a string T over Σ = [1..N O(1)], we can compute TtoG(T) in O(N) time and space.\r\nProof. We first compute T0 in O(N) by sorting the characters used in T and replacing them with ranks\r\nof characters. Then we compress T0 by applying BComp and PComp by turns and get T1, T2 . . . Thˆ . One\r\ntechnical problem is that characters used in an input string w of BComp and PComp should be in [1..|w|],\r\nwhich is crucial to conduct radix sort efficiently in O(|w|) time (see Lemmas 7 and 9). However letters in\r\nTh do not necessarily hold this property. To overcome this problem, during computation we maintain\r\nranks of letters among those used in the current Th, which should be in [1..|Th|], and use the ranks\r\ninstead of letters for radix sort. If we have such ranks in each level, we can easily maintain them by\r\nradix sort for the next level. Now, in every level h (0 ≤ h < hˆ) the compression from Th to Th+1 can be\r\nconducted in O(|Th|) time and space. Since PComp compresses a given string by a constant factor, the\r\ntotal running time can be bounded by O(N) time.\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/d4845a25-5b29-4e4b-a37f-eaa8fc1578e6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d6e67f172a718f622182c3fd26d10159d97bcc6efd87ba57b35ea1a9a851e814",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 863
      },
      {
        "segments": [
          {
            "segment_id": "d4845a25-5b29-4e4b-a37f-eaa8fc1578e6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 5,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Proof. In the foreach loop, we first run a 1/2-approximation algorithm of maximum “undirected” cut\r\nproblem on the adjacency list, i.e., we ignore the direction of the edges here. For each c in increasing order,\r\nwe greedily determine whether c is added to Σ´ or to Σ` depending on P\r\nc˜∈Σ` Freq(c, c, ˜ ·) ≥\r\nP\r\nc˜∈Σ´ Freq(c, c, ˜ ·).\r\nNote that P\r\nc˜∈Σ` Freq(c, c˜) (resp. Pc˜∈Σ´ Freq(c, c˜)) represents the number of edges between c and a\r\ncharacter in Σ` (resp. Σ´ ). By greedy choice, at least half of the edges in subject become the ones\r\nconnecting two characters each from Σ´ and Σ` . Hence, in the end, |E| becomes at least (|w| − 1)/2, where\r\nlet E denote the set of edges between characters from Σ and ´ Σ (recalling that there are exactly ` |w| − 1\r\nedges). Since each edge in E corresponds to an occurrence of a pair from Σ´ Σ` ∪ Σ` Σ´ in w, at least one\r\nof the two partitions (Σ´ , Σ` ) and (Σ` , Σ´ ) covers more than half of E. Hence we achieve our final bound\r\n|E|/2 = (|w| − 1)/4 by choosing an appropriate partition at Line 7.\r\nIn order to see that Algorithm 1 runs in O(m) time, we only have to care about Line 3 and Line 7.\r\nWe can compute P\r\nc˜∈Σ` Freq(c, c, ˜ ·) and Pc˜∈Σ´ Freq(c, c, ˜ ·) by going through all Freq(c, ·, ·) for fixed c in\r\nthe adjacency list, which are consecutive in the sorted list. Since each element of the list is used only\r\nonce, the cost for Line 3 is O(m) in total. Similarly the computation at Line 7 can be done by going\r\nthrough the adjacency list again. Thus the algorithm runs in O(m) time.\r\nAlgorithm 1: How to compute a partition of Σ for PComp to compress w by 3/4.\r\nInput: Adjacency list of w ∈ Σ\r\n∗\r\n.\r\nOutput: (Σ´ , Σ ) s.t. # occurrences of pairs from ` Σ´ Σ in ` w is at least (|w| − 1)/4.\r\n/* The information whether c ∈ Σ is in Σ´ or Σ` is written in the data space for c,\r\nwhich can be accessed in O(1) time. */\r\n1 Σ´ ← Σ` ← ∅;\r\n2 foreach c ∈ Σ in increasing order do\r\n3 if P\r\nc˜∈Σ` Freqw(c, c, ˜ ·) ≥\r\nP\r\nc˜∈Σ´ Freqw(c, c, ˜ ·) then\r\n4 add c to Σ; ´\r\n5 else\r\n6 add c to Σ; `\r\n7 if # occurrences of pairs from Σ´ Σ` < # occurrences of pairs from Σ` Σ´ then\r\n8 switch Σ and ´ Σ; `\r\n9 return (Σ´ , Σ ); `\r\nLemma 9. Given a string w over Σ = [1..|w|] that contains no block of length ≥ 2, we can conduct\r\nPComp in O(|w|) time and space.\r\nProof. We first compute the adjacency list of w. This can be easily done in O(|w|) time and space by\r\nsorting the |w| − 1 size multiset {(w[i], w[i + 1], 0) | 1 ≤ i < |w|, w[i] > w[i + 1]} ∪ {(w[i], w[i + 1], 1) | 1 ≤\r\ni < |w|, w[i] < w[i + 1]} by radix sort. Then by Lemma 8 we compute a partition (Σ´ , Σ` ) in linear time in\r\nthe size of the adjacency list, which is O(|w|). Next we scan w in O(|w|) time and list all the occurrences\r\nof pairs to be compressed. Each pair c´c` ∈ Σ´ Σ` at position i is listed by a triple (c, ´ c, i ` ) of integers in Σ.\r\nThen we sort the list according to the pair of integers (c, ´ c`), which can be done in O(|w|) time and space\r\nby radix sort. Finally, we replace each pair with a fresh letter based on the rank of (´c, c`).\r\nLemma 10. Given a string T over Σ = [1..N O(1)], we can compute TtoG(T) in O(N) time and space.\r\nProof. We first compute T0 in O(N) by sorting the characters used in T and replacing them with ranks\r\nof characters. Then we compress T0 by applying BComp and PComp by turns and get T1, T2 . . . Thˆ . One\r\ntechnical problem is that characters used in an input string w of BComp and PComp should be in [1..|w|],\r\nwhich is crucial to conduct radix sort efficiently in O(|w|) time (see Lemmas 7 and 9). However letters in\r\nTh do not necessarily hold this property. To overcome this problem, during computation we maintain\r\nranks of letters among those used in the current Th, which should be in [1..|Th|], and use the ranks\r\ninstead of letters for radix sort. If we have such ranks in each level, we can easily maintain them by\r\nradix sort for the next level. Now, in every level h (0 ≤ h < hˆ) the compression from Th to Th+1 can be\r\nconducted in O(|Th|) time and space. Since PComp compresses a given string by a constant factor, the\r\ntotal running time can be bounded by O(N) time.\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/d4845a25-5b29-4e4b-a37f-eaa8fc1578e6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d6e67f172a718f622182c3fd26d10159d97bcc6efd87ba57b35ea1a9a851e814",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 863
      },
      {
        "segments": [
          {
            "segment_id": "ddd9fb29-e824-4c38-980c-d0ddd16b682a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 6,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "w0 c1 c1\r\nc2\r\nc3\r\nc4\r\nc5 c5\r\nc6\r\nc7\r\nw1\r\nw2\r\nw3\r\nw4\r\nT0 c1 c1\r\nc2\r\nc3\r\nc4\r\nc5 c5\r\nc6\r\nc7\r\nT1\r\nT2\r\nT3\r\nT4\r\nFigure 1: Suppose that c1, c7 ∈ Σ¨\r\n0\r\n, c2 ∈ Σ`\r\n1\r\n, c6 ∈ Σ´\r\n1\r\n, c5 ∈ Σ¨\r\n2\r\n, c3 ∈ Σ`\r\n3 and c4 ∈ Σ¨4\r\n. Then PSeq(w) =\r\nc1c1c2c3c4c5c5c6c7. PSeq(w) occurs everywhere w occurs.\r\n3.2 Popped sequences\r\nWe give a new characterization of recompression, which is a key to fast LCE queries as well as the upper\r\nbound O(z lg(N/z)) for the size of TtoG(T). For any substring w of T, we define the Popped Sequence\r\n(PSeq) of w, denoted by PSeq(w). PSeq(w) is a sequence of letters such that val(PSeq(w)) = w and\r\nconsists of O(lg N) blocks of letters. It is not surprising that any substring can be represented by O(lg N)\r\nblocks of letters because the height of TtoG(T) is O(lg N). The significant property of PSeq(w) is that\r\nit occurs at “every” occurrence of w (even if the occurrences overlap). A similar property has been\r\nobserved in CFGs produced by locally consistent parsing and utilized for compressed indexes [12, 14]\r\nand a dynamic compressed LCE data structure [15]. For example, in [14, 15] the sequence having such a\r\nproperty is called the common sequence of w but its representation size is O(lg |w| lg∗ N) rather than\r\nO(lg N).\r\nPSeq(w) is the sequence of letters characterized by the following procedure. Let w0 be the substring\r\nof T0 that generates w. We consider applying BComp and PComp to w0 exactly as we did to T but in\r\neach level we pop some letters out if the letters can be coupled with letters outside the scope. Formally,\r\nin increasing order of h ≥ 0, we get wh+1 from wh as follows:\r\n• If h is even. We first pop out the leftmost and rightmost blocks of wh if they are blocks of letter\r\nc ∈ Σ¨\r\nh. Then we get wh+1 by applying BComp to the remaining string.\r\n• If h is odd. We first pop out the leftmost letter and rightmost letter of wh if they are letters in Σ`\r\nh\r\nand Σ´\r\nh, respectively. Then we get wh+1 by applying PComp to the remaining string.\r\nWe iterate this until the string disappears. PSeq(w) is the sequence obtained by concatenating the\r\npopped-out letters/blocks in an appropriate order. Note that for any occurrence of w the letters inside\r\nthe PSeq(w) are compressed in the same way. Hence wh is created for every occurrence of w and the\r\noccurrence of PSeq(w) is guaranteed (see also Figure 1).\r\nThe next lemma formalizes the above discussion.\r\nLemma 11. For any substring w of T, PSeq(w) consists of O(lg N) blocks of letters. In addition, w\r\noccurs at position i iff PSeq(w) occurs at i.\r\nLemma 12. For any chain C whose label consists of m blocks of letters, the number of ancestor nodes\r\nof C is O(m).\r\nProof. Since a block is compressed into one letter, the number of parent nodes of C is at most m. As\r\nevery internal node has two or more children, it is easy to see that there are O(m) ancestor nodes of the\r\nparent nodes of C.\r\nCorollary 13. For any chain C corresponding to PSeq(T[b..e]) for some interval [b..e], the number of\r\nancestor nodes of C is O(lg N).\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/ddd9fb29-e824-4c38-980c-d0ddd16b682a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=70f757e22790c241e27324907573b129e701648a9d3443aa687633e57b2ef086",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 573
      },
      {
        "segments": [
          {
            "segment_id": "ddd9fb29-e824-4c38-980c-d0ddd16b682a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 6,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "w0 c1 c1\r\nc2\r\nc3\r\nc4\r\nc5 c5\r\nc6\r\nc7\r\nw1\r\nw2\r\nw3\r\nw4\r\nT0 c1 c1\r\nc2\r\nc3\r\nc4\r\nc5 c5\r\nc6\r\nc7\r\nT1\r\nT2\r\nT3\r\nT4\r\nFigure 1: Suppose that c1, c7 ∈ Σ¨\r\n0\r\n, c2 ∈ Σ`\r\n1\r\n, c6 ∈ Σ´\r\n1\r\n, c5 ∈ Σ¨\r\n2\r\n, c3 ∈ Σ`\r\n3 and c4 ∈ Σ¨4\r\n. Then PSeq(w) =\r\nc1c1c2c3c4c5c5c6c7. PSeq(w) occurs everywhere w occurs.\r\n3.2 Popped sequences\r\nWe give a new characterization of recompression, which is a key to fast LCE queries as well as the upper\r\nbound O(z lg(N/z)) for the size of TtoG(T). For any substring w of T, we define the Popped Sequence\r\n(PSeq) of w, denoted by PSeq(w). PSeq(w) is a sequence of letters such that val(PSeq(w)) = w and\r\nconsists of O(lg N) blocks of letters. It is not surprising that any substring can be represented by O(lg N)\r\nblocks of letters because the height of TtoG(T) is O(lg N). The significant property of PSeq(w) is that\r\nit occurs at “every” occurrence of w (even if the occurrences overlap). A similar property has been\r\nobserved in CFGs produced by locally consistent parsing and utilized for compressed indexes [12, 14]\r\nand a dynamic compressed LCE data structure [15]. For example, in [14, 15] the sequence having such a\r\nproperty is called the common sequence of w but its representation size is O(lg |w| lg∗ N) rather than\r\nO(lg N).\r\nPSeq(w) is the sequence of letters characterized by the following procedure. Let w0 be the substring\r\nof T0 that generates w. We consider applying BComp and PComp to w0 exactly as we did to T but in\r\neach level we pop some letters out if the letters can be coupled with letters outside the scope. Formally,\r\nin increasing order of h ≥ 0, we get wh+1 from wh as follows:\r\n• If h is even. We first pop out the leftmost and rightmost blocks of wh if they are blocks of letter\r\nc ∈ Σ¨\r\nh. Then we get wh+1 by applying BComp to the remaining string.\r\n• If h is odd. We first pop out the leftmost letter and rightmost letter of wh if they are letters in Σ`\r\nh\r\nand Σ´\r\nh, respectively. Then we get wh+1 by applying PComp to the remaining string.\r\nWe iterate this until the string disappears. PSeq(w) is the sequence obtained by concatenating the\r\npopped-out letters/blocks in an appropriate order. Note that for any occurrence of w the letters inside\r\nthe PSeq(w) are compressed in the same way. Hence wh is created for every occurrence of w and the\r\noccurrence of PSeq(w) is guaranteed (see also Figure 1).\r\nThe next lemma formalizes the above discussion.\r\nLemma 11. For any substring w of T, PSeq(w) consists of O(lg N) blocks of letters. In addition, w\r\noccurs at position i iff PSeq(w) occurs at i.\r\nLemma 12. For any chain C whose label consists of m blocks of letters, the number of ancestor nodes\r\nof C is O(m).\r\nProof. Since a block is compressed into one letter, the number of parent nodes of C is at most m. As\r\nevery internal node has two or more children, it is easy to see that there are O(m) ancestor nodes of the\r\nparent nodes of C.\r\nCorollary 13. For any chain C corresponding to PSeq(T[b..e]) for some interval [b..e], the number of\r\nancestor nodes of C is O(lg N).\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/ddd9fb29-e824-4c38-980c-d0ddd16b682a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=70f757e22790c241e27324907573b129e701648a9d3443aa687633e57b2ef086",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 573
      },
      {
        "segments": [
          {
            "segment_id": "f90fc00c-124a-4145-984b-8714db6488cc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 7,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Lemma 14. The size of TtoG(T) is O(z lg(N/z)), where z is the size of the LZ77 factorization of T.\r\nProof. We first show the bound O(z lg N) and improve the analysis to O(z lg(N/z)) later.\r\nLet f1 . . . fz be the LZ77 factorization of T. For any 1 ≤ i ≤ z, let Li be the set of letters used in\r\nthe ancestor nodes of leaves corresponding to f1f2 . . . fi. Clearly |L1| = O(lg N). For any 1 < i ≤ z, we\r\nestimate |Li \\ Li−1|. Since fi occurs in f1 . . . fi−1, we can see that the letters of PSeq(fi) are in Li−1\r\nthanks to Lemma 11. Let Ci be the chain corresponding to the occurrence |f1 . . . fi−1 + 1| of PSeq(fi).\r\nThen, the letters in Li \\ Li−1 are only in the labels of ancestor nodes of Ci. Since PSeq(fi) consists of\r\nO(lg N) blocks of letters, |Li \\ Li−1| is bounded by O(lg N) due to Lemma 12. Therefore the size of\r\nTtoG(T) is O(z lg N).\r\nIn order to improve the bound to O(z lg(N/z)), we employ the same trick that has been used in\r\nthe literature. Let h = 2 lg4/3(N/z) = 2 lg3/4(z/N). Recall that PComp compresses a given string by a\r\nconstant factor 3/4. Since PComp has been applied h/2 times until the level h, |Th| ≤ N(3/4)h/2 = z,\r\nand hence, the number of letters introduced in level ≥ h is bounded by O(z). Then, we can ignore all the\r\nletters introduced in level ≥ h in the analysis of the previous paragraph, and by doing so, the bound\r\nO(lg N) of |Li \\ Li−1| is improved to O(h) = O(lg(N/z)). This yields the bound O(z lg(N/z)) for the\r\nsize of TtoG(T).\r\nLemma 15. Given TtoG(T), we can answer LCE(i, j) in O(lg N) time.\r\nProof. Let w be the longest common prefix of two suffixes beginning at i and j. In the light of Lemma 11,\r\nPSeq(w), which consists of O(lg N) blocks of letters, occurs at both i and j. Let Ci (resp. Cj ) be the\r\nchain that is labeled with PSeq(w) and begins at i (resp. j). We can compute |w| by traversing the\r\nancestor nodes of Ci and Cj simultaneously and matching PSeq(w) written in the labels of Ci and Cj .\r\nNote that we do matching from left to right and we do not have to know |w| in advance. Also, matching\r\na block of letters in PSeq(w) can be done in O(1) time on run-length encoded rules. By Corollary 13, the\r\nnumber of ancestor nodes we have to visit is bounded by O(lg N). Thus, we get the lemma.\r\n3.3 Proof of Theorem 1\r\nProof of Theorem 1. By Lemma 10 we can compute TtoG(T) in O(N) time and space. Since the height\r\nof TtoG(T) is O(lg N), we can support Extract queries in O(lg N + L) time due to Lemma 6. LCE queries\r\ncan be supported in O(lg N) time by Lemma 15.\r\n4 LCE data structure built from SLPs\r\nIn this section, we prove Theorem 2. Input is now an arbitrary SLP S = {Σ, V, D} of size n generating T.\r\nBasically what we consider is to simulate TtoG on S, namely, compute TtoG(T) without decompressing\r\nS explicitly. The recompression technique is celebrated for doing this kind of tasks (actually this is where\r\n“recompression” is named after). In Section 4.1, we present an algorithm SimTtoG that simulates TtoG in\r\nO(n lg2(N/n)) time and O(n + z lg(N/z)) space. In Section 4.2, we present how to modify SimTtoG to\r\nobtain an O(n lg(N/n))-time and O(n + z lg(N/z))-space construction of our LCE data structure.\r\n4.1 SimTtoG: Simulating TtoG on CFGs\r\nWe present an algorithm SimTtoG to simulate TtoG on S. To begin with, we compute the CFG\r\nS0 = {Σ0, V, D0} obtained by replacing, for all variables X ∈ V with D(X) ∈ Σ, every occurrence of\r\nX in the righthand sides of D with the letter generating D(X). Note that Σ0 is the set of terminals\r\nof S0, and S0 generates T0. SimTtoG transforms level by level S0 into CFGs, S1 = {Σ1, V, D1}, S2 =\r\n{Σ2, V, D2}, . . . , Shˆ = {Σhˆ , V, Dhˆ }, where each Sh generates Th. Namely, compression from Th to Th+1\r\nis simulated on Sh. We can correctly compute the letters in Σˆ\r\nh+1 while modifying Sh into Sh+1, and\r\nhence, we get all the letters of TtoG(T) in the end. We note that new variables are never introduced and\r\nthe modification is done by rewriting righthand sides of the original variables.\r\nHere we introduce the special formation of the CFGs Sh (it is a generalization of SLPs in a different\r\nsense from RLSLPs): For any X ∈ V, Dh(X) consists of an “arbitrary number” of letters and at most\r\n“two” variables. More precisely, the following condition holds:\r\nFor any variable X ∈ V with D(X) = X´X`, Dh(X) is either w1Xw´\r\n2Xw`3, w1Xw´2, w2Xw`3 or w2\r\nwith w1, w2, w3 ∈ Σ\r\n∗\r\nh\r\n, where w1 = w3 = ε if X is not the starting variable.\r\n7",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/f90fc00c-124a-4145-984b-8714db6488cc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d221725e06a155826d11f704024c7a0cba04b59e40080d93d6a1983d2895223c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 874
      },
      {
        "segments": [
          {
            "segment_id": "f90fc00c-124a-4145-984b-8714db6488cc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 7,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Lemma 14. The size of TtoG(T) is O(z lg(N/z)), where z is the size of the LZ77 factorization of T.\r\nProof. We first show the bound O(z lg N) and improve the analysis to O(z lg(N/z)) later.\r\nLet f1 . . . fz be the LZ77 factorization of T. For any 1 ≤ i ≤ z, let Li be the set of letters used in\r\nthe ancestor nodes of leaves corresponding to f1f2 . . . fi. Clearly |L1| = O(lg N). For any 1 < i ≤ z, we\r\nestimate |Li \\ Li−1|. Since fi occurs in f1 . . . fi−1, we can see that the letters of PSeq(fi) are in Li−1\r\nthanks to Lemma 11. Let Ci be the chain corresponding to the occurrence |f1 . . . fi−1 + 1| of PSeq(fi).\r\nThen, the letters in Li \\ Li−1 are only in the labels of ancestor nodes of Ci. Since PSeq(fi) consists of\r\nO(lg N) blocks of letters, |Li \\ Li−1| is bounded by O(lg N) due to Lemma 12. Therefore the size of\r\nTtoG(T) is O(z lg N).\r\nIn order to improve the bound to O(z lg(N/z)), we employ the same trick that has been used in\r\nthe literature. Let h = 2 lg4/3(N/z) = 2 lg3/4(z/N). Recall that PComp compresses a given string by a\r\nconstant factor 3/4. Since PComp has been applied h/2 times until the level h, |Th| ≤ N(3/4)h/2 = z,\r\nand hence, the number of letters introduced in level ≥ h is bounded by O(z). Then, we can ignore all the\r\nletters introduced in level ≥ h in the analysis of the previous paragraph, and by doing so, the bound\r\nO(lg N) of |Li \\ Li−1| is improved to O(h) = O(lg(N/z)). This yields the bound O(z lg(N/z)) for the\r\nsize of TtoG(T).\r\nLemma 15. Given TtoG(T), we can answer LCE(i, j) in O(lg N) time.\r\nProof. Let w be the longest common prefix of two suffixes beginning at i and j. In the light of Lemma 11,\r\nPSeq(w), which consists of O(lg N) blocks of letters, occurs at both i and j. Let Ci (resp. Cj ) be the\r\nchain that is labeled with PSeq(w) and begins at i (resp. j). We can compute |w| by traversing the\r\nancestor nodes of Ci and Cj simultaneously and matching PSeq(w) written in the labels of Ci and Cj .\r\nNote that we do matching from left to right and we do not have to know |w| in advance. Also, matching\r\na block of letters in PSeq(w) can be done in O(1) time on run-length encoded rules. By Corollary 13, the\r\nnumber of ancestor nodes we have to visit is bounded by O(lg N). Thus, we get the lemma.\r\n3.3 Proof of Theorem 1\r\nProof of Theorem 1. By Lemma 10 we can compute TtoG(T) in O(N) time and space. Since the height\r\nof TtoG(T) is O(lg N), we can support Extract queries in O(lg N + L) time due to Lemma 6. LCE queries\r\ncan be supported in O(lg N) time by Lemma 15.\r\n4 LCE data structure built from SLPs\r\nIn this section, we prove Theorem 2. Input is now an arbitrary SLP S = {Σ, V, D} of size n generating T.\r\nBasically what we consider is to simulate TtoG on S, namely, compute TtoG(T) without decompressing\r\nS explicitly. The recompression technique is celebrated for doing this kind of tasks (actually this is where\r\n“recompression” is named after). In Section 4.1, we present an algorithm SimTtoG that simulates TtoG in\r\nO(n lg2(N/n)) time and O(n + z lg(N/z)) space. In Section 4.2, we present how to modify SimTtoG to\r\nobtain an O(n lg(N/n))-time and O(n + z lg(N/z))-space construction of our LCE data structure.\r\n4.1 SimTtoG: Simulating TtoG on CFGs\r\nWe present an algorithm SimTtoG to simulate TtoG on S. To begin with, we compute the CFG\r\nS0 = {Σ0, V, D0} obtained by replacing, for all variables X ∈ V with D(X) ∈ Σ, every occurrence of\r\nX in the righthand sides of D with the letter generating D(X). Note that Σ0 is the set of terminals\r\nof S0, and S0 generates T0. SimTtoG transforms level by level S0 into CFGs, S1 = {Σ1, V, D1}, S2 =\r\n{Σ2, V, D2}, . . . , Shˆ = {Σhˆ , V, Dhˆ }, where each Sh generates Th. Namely, compression from Th to Th+1\r\nis simulated on Sh. We can correctly compute the letters in Σˆ\r\nh+1 while modifying Sh into Sh+1, and\r\nhence, we get all the letters of TtoG(T) in the end. We note that new variables are never introduced and\r\nthe modification is done by rewriting righthand sides of the original variables.\r\nHere we introduce the special formation of the CFGs Sh (it is a generalization of SLPs in a different\r\nsense from RLSLPs): For any X ∈ V, Dh(X) consists of an “arbitrary number” of letters and at most\r\n“two” variables. More precisely, the following condition holds:\r\nFor any variable X ∈ V with D(X) = X´X`, Dh(X) is either w1Xw´\r\n2Xw`3, w1Xw´2, w2Xw`3 or w2\r\nwith w1, w2, w3 ∈ Σ\r\n∗\r\nh\r\n, where w1 = w3 = ε if X is not the starting variable.\r\n7",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/f90fc00c-124a-4145-984b-8714db6488cc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d221725e06a155826d11f704024c7a0cba04b59e40080d93d6a1983d2895223c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 874
      },
      {
        "segments": [
          {
            "segment_id": "95fe2a43-fb53-4d9f-8869-9057a747e1bb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 8,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "As opposed to SLPs and RLSLPs, we define the size of Sh by the total lengths of righthand sides and\r\ndenote it by |Sh|.\r\n4.1.1 PComp on CFGs\r\nWe firstly show that the adjacency list of Th can be computed efficiently.\r\nLemma 16 (Lemma 6.1 of [9]). For any odd h (0 ≤ h < hˆ), the adjacency list of Th, whose size is\r\nO(|Sh|), can be computed in O(|Sh| + n) time and space.\r\nProof. For any variable X ∈ V, let VOcc(X) denote the number of occurrences of the nodes labeled\r\nwith X in the derivation tree of S. It is well known that VOcc(X) for all variables can be computed in\r\nO(n) time and space on the DAG representation of the tree.5 Also, for any variable X ∈ V, let LML(X)\r\nand RML(X) denote the leftmost letter and respectively rightmost letter of val Sh(X). We can compute\r\nLML(X) for all variables in O(|Sh|) time by a bottom up computation, i.e., LML(X) = LML(Y ) if Dh(X)\r\nstarts with a variable Y , and LML(X) = w[1] if Dh(X) starts with a non-empty string w. In a completely\r\nsymmetric way RML(X) can be computed in O(|Sh|) time.\r\nNow observe that any occurrence i of a pair c´c` in Th can be uniquely associated with a variable X\r\nthat is the label of the lowest node covering the interval [i..i + 1] in the derivation tree of Sh (recall that\r\nSh generates Th). We intend to count all the occurrences of pairs associated with X in Dh(X). For\r\nexample, let Dh(X) = Xw´\r\n2X` with w2 ∈ Σ\r\n∗\r\nh\r\n. Then c´c` appears explicitly in w2 or crosses the boundaries\r\nof X´ and/or X`. If c´c` crosses the boundary of X´, RML(X´) is c´ and c` follows, i.e., w2[1] = c` or w2 = ε and\r\nLML(X`) = c`. Using RML(X´) and LML(X`), we can list in O(|Dh(X)|) time and space all the explicit and\r\ncrossing pairs in Dh(X). Using RML(X´) and LML(X`), we can compute in O(|Dh(X)|) time and space a\r\n|Dh(X)| − 1 size multiset that lists all the explicit and crossing pairs in Dh(X). Each pair c´c` with c >´ c`\r\n(resp. c <´ c`) is listed by a quadruple (c, ´ c, ` 0, VOcc(X)}) (resp. (c, ` c, ´ 1, VOcc(X)}). VOcc(X) means that\r\nthe pair has a weight VOcc(X) because the pair appears every time a node labeled with X appears in\r\nthe derivation tree.\r\nWe compute such a multiset for every variable, which takes O(|Sh|) time and space in total. Next\r\nwe sort the obtained list in increasing order of the first three integers in a quadruple. Note that the\r\nmaximum value of letters is O(z lg(N/z)) due to Lemma 14, and O(z lg(N/z)) = O(n\r\n2\r\n) since z ≤ n and\r\nlg N ≤ n hold. Thus the sorting can be done in O(n) time and space by radix sort. Finally we can get the\r\nadjacency list of Th by summing up weights of the same pair. The size of the list is clearly O(|Sh|).\r\nThe next lemma shows how to implement PComp on CFGs:\r\nLemma 17. For any odd h (0 ≤ h < hˆ), we can compute Sh+1 from Sh in O(|Sh| + n) time and space.\r\nIn addition, |Sh+1| ≤ |Sh| + 2n.\r\nProof. We first compute the partition (Σ´\r\nh, Σ`h) of Σh, which can be done in O(|Sh| + n) time and space\r\nby Lemmas 16 and 8.\r\nGiven (Σ´\r\nh, Σ`h), we can detect all the positions of the pairs from Σ´hΣ`h in the righthands of Dh, which\r\nshould be compressed. Some of the appearances of the pairs are explicit and the others are crossing.\r\nWhile explicit pairs can be compressed easily, crossing pairs need an additional treatment. In order to\r\ndeal with crossing pairs, we first uncross them by popping out LML(Y ) (resp. RML(Y )) from val Sh(Y ) iff\r\nLML(Y ) ∈ Σ`\r\nh (resp. RML(Y ) ∈ Σ´h) for every variable Y other than the starting variable. More precisely,\r\nwe do the following:\r\nPopInLet For any variable X, if Dh(X)[i] = Y ∈ V with i > 1 (i ≥ 1 if X is the starting variable) and\r\nLML(Y ) ∈ Σ`\r\nh, replace the occurrence of Y with LML(Y )Y ; if Dh(X)[i] = Y ∈ V with i < |Dh(X)|\r\n(i ≤ |Dh(X)| if X is the starting variable) and RML(Y ) ∈ Σ´\r\nh, replace the occurrence of Y with\r\nY RML(Y ).\r\nPopOutLet For any variable X other than the starting variable, if Dh(X)[1] ∈ Σ`\r\nh, remove the first letter\r\nof Dh(X); and if Dh(X)[|Dh(X)|] ∈ Σ´\r\nh, remove the last letter of Dh(X).6\r\nPopOutLet removes LML(Y ) and RML(Y ) from val Sh\r\n(Y ) if they can be a part of a crossing pair and\r\nPopInLet introduces the removed letters into appropriate positions in Dh so that the modified Sh keeps to\r\ngenerate Th. Notice that for each variable X the positions where letters popped-in is at most two (four if\r\n5\r\nIt is enough to compute VOcc(X) once at the very beginning of SimTtoG.\r\n6\r\nIf X becomes empty, we will clear all the appearance of X in Dh.\r\n8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/95fe2a43-fb53-4d9f-8869-9057a747e1bb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8cef8846410e14919ddf6259db63ea232a36f10c9d170d626d3699d55d91fe5c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 879
      },
      {
        "segments": [
          {
            "segment_id": "95fe2a43-fb53-4d9f-8869-9057a747e1bb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 8,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "As opposed to SLPs and RLSLPs, we define the size of Sh by the total lengths of righthand sides and\r\ndenote it by |Sh|.\r\n4.1.1 PComp on CFGs\r\nWe firstly show that the adjacency list of Th can be computed efficiently.\r\nLemma 16 (Lemma 6.1 of [9]). For any odd h (0 ≤ h < hˆ), the adjacency list of Th, whose size is\r\nO(|Sh|), can be computed in O(|Sh| + n) time and space.\r\nProof. For any variable X ∈ V, let VOcc(X) denote the number of occurrences of the nodes labeled\r\nwith X in the derivation tree of S. It is well known that VOcc(X) for all variables can be computed in\r\nO(n) time and space on the DAG representation of the tree.5 Also, for any variable X ∈ V, let LML(X)\r\nand RML(X) denote the leftmost letter and respectively rightmost letter of val Sh(X). We can compute\r\nLML(X) for all variables in O(|Sh|) time by a bottom up computation, i.e., LML(X) = LML(Y ) if Dh(X)\r\nstarts with a variable Y , and LML(X) = w[1] if Dh(X) starts with a non-empty string w. In a completely\r\nsymmetric way RML(X) can be computed in O(|Sh|) time.\r\nNow observe that any occurrence i of a pair c´c` in Th can be uniquely associated with a variable X\r\nthat is the label of the lowest node covering the interval [i..i + 1] in the derivation tree of Sh (recall that\r\nSh generates Th). We intend to count all the occurrences of pairs associated with X in Dh(X). For\r\nexample, let Dh(X) = Xw´\r\n2X` with w2 ∈ Σ\r\n∗\r\nh\r\n. Then c´c` appears explicitly in w2 or crosses the boundaries\r\nof X´ and/or X`. If c´c` crosses the boundary of X´, RML(X´) is c´ and c` follows, i.e., w2[1] = c` or w2 = ε and\r\nLML(X`) = c`. Using RML(X´) and LML(X`), we can list in O(|Dh(X)|) time and space all the explicit and\r\ncrossing pairs in Dh(X). Using RML(X´) and LML(X`), we can compute in O(|Dh(X)|) time and space a\r\n|Dh(X)| − 1 size multiset that lists all the explicit and crossing pairs in Dh(X). Each pair c´c` with c >´ c`\r\n(resp. c <´ c`) is listed by a quadruple (c, ´ c, ` 0, VOcc(X)}) (resp. (c, ` c, ´ 1, VOcc(X)}). VOcc(X) means that\r\nthe pair has a weight VOcc(X) because the pair appears every time a node labeled with X appears in\r\nthe derivation tree.\r\nWe compute such a multiset for every variable, which takes O(|Sh|) time and space in total. Next\r\nwe sort the obtained list in increasing order of the first three integers in a quadruple. Note that the\r\nmaximum value of letters is O(z lg(N/z)) due to Lemma 14, and O(z lg(N/z)) = O(n\r\n2\r\n) since z ≤ n and\r\nlg N ≤ n hold. Thus the sorting can be done in O(n) time and space by radix sort. Finally we can get the\r\nadjacency list of Th by summing up weights of the same pair. The size of the list is clearly O(|Sh|).\r\nThe next lemma shows how to implement PComp on CFGs:\r\nLemma 17. For any odd h (0 ≤ h < hˆ), we can compute Sh+1 from Sh in O(|Sh| + n) time and space.\r\nIn addition, |Sh+1| ≤ |Sh| + 2n.\r\nProof. We first compute the partition (Σ´\r\nh, Σ`h) of Σh, which can be done in O(|Sh| + n) time and space\r\nby Lemmas 16 and 8.\r\nGiven (Σ´\r\nh, Σ`h), we can detect all the positions of the pairs from Σ´hΣ`h in the righthands of Dh, which\r\nshould be compressed. Some of the appearances of the pairs are explicit and the others are crossing.\r\nWhile explicit pairs can be compressed easily, crossing pairs need an additional treatment. In order to\r\ndeal with crossing pairs, we first uncross them by popping out LML(Y ) (resp. RML(Y )) from val Sh(Y ) iff\r\nLML(Y ) ∈ Σ`\r\nh (resp. RML(Y ) ∈ Σ´h) for every variable Y other than the starting variable. More precisely,\r\nwe do the following:\r\nPopInLet For any variable X, if Dh(X)[i] = Y ∈ V with i > 1 (i ≥ 1 if X is the starting variable) and\r\nLML(Y ) ∈ Σ`\r\nh, replace the occurrence of Y with LML(Y )Y ; if Dh(X)[i] = Y ∈ V with i < |Dh(X)|\r\n(i ≤ |Dh(X)| if X is the starting variable) and RML(Y ) ∈ Σ´\r\nh, replace the occurrence of Y with\r\nY RML(Y ).\r\nPopOutLet For any variable X other than the starting variable, if Dh(X)[1] ∈ Σ`\r\nh, remove the first letter\r\nof Dh(X); and if Dh(X)[|Dh(X)|] ∈ Σ´\r\nh, remove the last letter of Dh(X).6\r\nPopOutLet removes LML(Y ) and RML(Y ) from val Sh\r\n(Y ) if they can be a part of a crossing pair and\r\nPopInLet introduces the removed letters into appropriate positions in Dh so that the modified Sh keeps to\r\ngenerate Th. Notice that for each variable X the positions where letters popped-in is at most two (four if\r\n5\r\nIt is enough to compute VOcc(X) once at the very beginning of SimTtoG.\r\n6\r\nIf X becomes empty, we will clear all the appearance of X in Dh.\r\n8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/95fe2a43-fb53-4d9f-8869-9057a747e1bb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8cef8846410e14919ddf6259db63ea232a36f10c9d170d626d3699d55d91fe5c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 879
      },
      {
        "segments": [
          {
            "segment_id": "2b3fbd41-aed7-460d-ac83-58985065d7c8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 9,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "X is the starting variable) and there is at least one variable that has no variables below, and hence, the\r\nsize of Sh increases at most 2n. The uncrossing can be conducted in O(|Sh| + n) time.\r\nSince all the pairs to be compressed become explicit now, we can easily conduct BComp in O(|Sh| + n)\r\ntime. We scan righthand sides in O(|Sh|) time and list all the occurrences of pairs to be compressed.\r\nEach occurrence of pair c´c` ∈ Σ´ Σ` is listed by a triple (c, ´ c, p ` ), where p is the pointer to the occurrence.\r\nThen we sort the list according to the pair of integers (c, ´ c`), which can be done in O(|Sh| + n) time and\r\nspace by radix sort because c´ and c` are O(n\r\n2\r\n). Finally, we replace each pair at position p with a fresh\r\nletter based on the rank of (´c, c`).\r\n4.1.2 BComp on CFGs\r\nFor any even h (0 ≤ h < hˆ), BComp can be implemented in a similar way to PComp of Lemma 17. A\r\nblock Th[b..e] of length ≥ 2 is uniquely associated with a variable X that is the label of the lowest node\r\ncovering the interval [b − 1..e + 1] in the derivation tree of Sh (if b = 0 or e = |Th|, the block is associated\r\nwith the starting variable). Note that we take [b − 1..e + 1] rather than [b..e] to be sure that the block\r\ncannot extend outside the variable. Some blocks are explicitly written in Dh(X) and some others are\r\ncrossing the boundaries of variables in Dh(X). The numbers of explicit blocks and crossing blocks in Dh\r\nis at most |Sh| and 2n, respectively. The crossing blocks can be uncrossed in a similar way to uncrossing\r\npairs. Then BComp can be done by replacing all the blocks with fresh letters on righthand sides of Dh.\r\nHowever here we have a problem. Recall that in order to give a unique letter to a block c\r\nd\r\n, we have\r\nto sort the pairs of integers (c, d) (see Lemma 7). Since d might be exponentially larger than |Sh| + n,\r\nradix sort cannot be executed in O(|Sh| + n) time and space. In Section 6.2 of [9], Je˙z showed how to\r\nsolve this problem by tweaking the representation of lengths of long blocks, but its implementation and\r\nanalysis are involved.7\r\nWe show in Lemma 18 our new observation, which leads to a simpler implementation and analysis\r\nof BComp. We say that a block c\r\nd\r\nis short if d = O(|Sh| + n) and long otherwise. Also, we say that a\r\nvariable is unary iff its righthand side consists of a single block.\r\nLemma 18. For any even h (0 ≤ h < hˆ), a block Th[b..e] = c\r\nd\r\nis short if it does not include a substring\r\ngenerated from a unary variable.\r\nProof. Consider the derivation tree of Sh and the shortest path from Th[b] to Th[e]. Let X1X2 · · · Xm0 · · · Xm\r\nbe the sequence of labels of internal nodes on the path, where Xm0 corresponds to the lowest common\r\nancestor of Th[b] and Th[e]. Since SLPs have no loops in the derivation tree, X1, . . . , Xm0 are all distinct.\r\nSimilarly Xm0+1, . . . , Xm are all distinct. Since a unary variable is not involved to generate the block, it\r\nis easy to see that d ≤\r\nPm\r\ni=1 |Dh(Xi)| ≤ 2|Sh| holds.\r\nLemma 18 implies that most of blocks we find during the compression are short, which can be sorted\r\nefficiently by radix sort. If there is a long block in Dh, an occurrence of a unary variable X must be\r\ninvolved to generate the block. Since BComp at level h pops out all the letters from X and removes the\r\noccurrences of X in Dh, there are at most 2n long blocks in total. The number of long blocks can also be\r\nupper bounded by 2N/n with a different analysis based on the following fact:\r\nFact 19. If a substring of original text T generated from a long block overlaps with that generated from\r\nanother long block, one substring must include the other, and moreover, the shorter block is completely\r\nincluded in “one” letter of the longer block. Hence the length of the substring of the longer block is at\r\nleast n times longer than that of the shorter block.\r\nLet us consider the long blocks that generate substrings whose lengths are [n\r\ni\r\n..ni+1) for a fixed integer\r\ni ≥ 1. By Fact 19, the substrings cannot overlap, and hence, the number of such long blocks is at most\r\nN/ni. Therefore, the total number of long blocks is at most P\r\ni≥1 N/ni ≤ 2N/n. Thus we get the\r\nfollowing lemma.\r\nLemma 20. There are at most O(min(n, N/n)) long blocks found during SimTtoG.\r\nBy Lemma 20, we can employ a standard comparison-base sorting algorithm to sort all long blocks\r\nin O(n lg(min(n, N/n))) time in total. In particular, BComp of one level can be implemented in the\r\nfollowing complexities:\r\n7Note that Section 6.2 of [9] also takes care of the case where the word size is Θ(lg n) rather than Θ(lg N). We do not\r\nconsider the Θ(lg n)-bits model in this paper because using Θ(lg N) bits to store the length of string generated by every\r\nletter is crucial for extract and LCE queries. However, we believe that our new observation stated in Lemma 18 will simplify\r\nthe analysis for the Θ(lg n)-bits model, too.\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/2b3fbd41-aed7-460d-ac83-58985065d7c8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=00d91cb62ed65155963ea2c5deda47044f30326a843213c48d392f6a1344b3ee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 940
      },
      {
        "segments": [
          {
            "segment_id": "2b3fbd41-aed7-460d-ac83-58985065d7c8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 9,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "X is the starting variable) and there is at least one variable that has no variables below, and hence, the\r\nsize of Sh increases at most 2n. The uncrossing can be conducted in O(|Sh| + n) time.\r\nSince all the pairs to be compressed become explicit now, we can easily conduct BComp in O(|Sh| + n)\r\ntime. We scan righthand sides in O(|Sh|) time and list all the occurrences of pairs to be compressed.\r\nEach occurrence of pair c´c` ∈ Σ´ Σ` is listed by a triple (c, ´ c, p ` ), where p is the pointer to the occurrence.\r\nThen we sort the list according to the pair of integers (c, ´ c`), which can be done in O(|Sh| + n) time and\r\nspace by radix sort because c´ and c` are O(n\r\n2\r\n). Finally, we replace each pair at position p with a fresh\r\nletter based on the rank of (´c, c`).\r\n4.1.2 BComp on CFGs\r\nFor any even h (0 ≤ h < hˆ), BComp can be implemented in a similar way to PComp of Lemma 17. A\r\nblock Th[b..e] of length ≥ 2 is uniquely associated with a variable X that is the label of the lowest node\r\ncovering the interval [b − 1..e + 1] in the derivation tree of Sh (if b = 0 or e = |Th|, the block is associated\r\nwith the starting variable). Note that we take [b − 1..e + 1] rather than [b..e] to be sure that the block\r\ncannot extend outside the variable. Some blocks are explicitly written in Dh(X) and some others are\r\ncrossing the boundaries of variables in Dh(X). The numbers of explicit blocks and crossing blocks in Dh\r\nis at most |Sh| and 2n, respectively. The crossing blocks can be uncrossed in a similar way to uncrossing\r\npairs. Then BComp can be done by replacing all the blocks with fresh letters on righthand sides of Dh.\r\nHowever here we have a problem. Recall that in order to give a unique letter to a block c\r\nd\r\n, we have\r\nto sort the pairs of integers (c, d) (see Lemma 7). Since d might be exponentially larger than |Sh| + n,\r\nradix sort cannot be executed in O(|Sh| + n) time and space. In Section 6.2 of [9], Je˙z showed how to\r\nsolve this problem by tweaking the representation of lengths of long blocks, but its implementation and\r\nanalysis are involved.7\r\nWe show in Lemma 18 our new observation, which leads to a simpler implementation and analysis\r\nof BComp. We say that a block c\r\nd\r\nis short if d = O(|Sh| + n) and long otherwise. Also, we say that a\r\nvariable is unary iff its righthand side consists of a single block.\r\nLemma 18. For any even h (0 ≤ h < hˆ), a block Th[b..e] = c\r\nd\r\nis short if it does not include a substring\r\ngenerated from a unary variable.\r\nProof. Consider the derivation tree of Sh and the shortest path from Th[b] to Th[e]. Let X1X2 · · · Xm0 · · · Xm\r\nbe the sequence of labels of internal nodes on the path, where Xm0 corresponds to the lowest common\r\nancestor of Th[b] and Th[e]. Since SLPs have no loops in the derivation tree, X1, . . . , Xm0 are all distinct.\r\nSimilarly Xm0+1, . . . , Xm are all distinct. Since a unary variable is not involved to generate the block, it\r\nis easy to see that d ≤\r\nPm\r\ni=1 |Dh(Xi)| ≤ 2|Sh| holds.\r\nLemma 18 implies that most of blocks we find during the compression are short, which can be sorted\r\nefficiently by radix sort. If there is a long block in Dh, an occurrence of a unary variable X must be\r\ninvolved to generate the block. Since BComp at level h pops out all the letters from X and removes the\r\noccurrences of X in Dh, there are at most 2n long blocks in total. The number of long blocks can also be\r\nupper bounded by 2N/n with a different analysis based on the following fact:\r\nFact 19. If a substring of original text T generated from a long block overlaps with that generated from\r\nanother long block, one substring must include the other, and moreover, the shorter block is completely\r\nincluded in “one” letter of the longer block. Hence the length of the substring of the longer block is at\r\nleast n times longer than that of the shorter block.\r\nLet us consider the long blocks that generate substrings whose lengths are [n\r\ni\r\n..ni+1) for a fixed integer\r\ni ≥ 1. By Fact 19, the substrings cannot overlap, and hence, the number of such long blocks is at most\r\nN/ni. Therefore, the total number of long blocks is at most P\r\ni≥1 N/ni ≤ 2N/n. Thus we get the\r\nfollowing lemma.\r\nLemma 20. There are at most O(min(n, N/n)) long blocks found during SimTtoG.\r\nBy Lemma 20, we can employ a standard comparison-base sorting algorithm to sort all long blocks\r\nin O(n lg(min(n, N/n))) time in total. In particular, BComp of one level can be implemented in the\r\nfollowing complexities:\r\n7Note that Section 6.2 of [9] also takes care of the case where the word size is Θ(lg n) rather than Θ(lg N). We do not\r\nconsider the Θ(lg n)-bits model in this paper because using Θ(lg N) bits to store the length of string generated by every\r\nletter is crucial for extract and LCE queries. However, we believe that our new observation stated in Lemma 18 will simplify\r\nthe analysis for the Θ(lg n)-bits model, too.\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/2b3fbd41-aed7-460d-ac83-58985065d7c8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=00d91cb62ed65155963ea2c5deda47044f30326a843213c48d392f6a1344b3ee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 940
      },
      {
        "segments": [
          {
            "segment_id": "bae486e0-044d-4754-87c9-0bef0d00dba6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 10,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Lemma 21. For any even h (0 ≤ h < hˆ), we can compute Sh+1 from Sh in O(|Sh| + n + m lg m)) time\r\nand O(|Sh| + n) space, where m is the number of long blocks in Dh. In addition, |Sh+1| ≤ |Sh| + 2n.\r\n4.1.3 The complexities of SimTtoG\r\nTheorem 22. SimTtoG runs in O(n lg2(N/n)) time and O(n lg(N/n)) space.\r\nProof. Using PComp and BComp implemented on CFGs (see Lemma 17 and 21), SimTtoG transforms\r\nlevel by level S0 into S1, S2, . . . , Shˆ . In each level, the size of CFGs can increase at most 2n by the\r\nprocedure of uncrossing. Since |Sh| = O(n lg N) for any h (0 ≤ h < hˆ), we get the time complexity\r\nO(n lg2 N) by simply applying Lemmas 17 and 21.\r\nWe can improve it to O(n lg2(N/n)) by a similar trick used in the proof of Lemma 14. At some\r\nlevel h\r\n0 where |Th0 | becomes less than n, we decompress Sh0 and switch to TtoG, which transforms Th0\r\ninto Thˆ in O(n) time by Lemma 10. We apply Lemmas 17 and 21 only for h with 0 ≤ h < h0. Since\r\nh\r\n0 = O(lg(N/n)), |Sh| = O(n lg(N/n)) for any h (0 ≤ h < h0\r\n). Therefore, we get the time complexity\r\nO(n lg2(N/n)). The space complexity is bounded by the maximum size of CFGs S0, S1, . . . , Sh0 , which is\r\nO(n lg(N/n)).\r\n4.2 GtoG: O(n lg(N/n))-time recompression\r\nWe modify SimTtoG slightly to run in O(n lg(N/n)) time and O(n + z lg(N/z)) space. The idea is the\r\nsame as what has been presented in Section 6.1 of [9]. The problem of SimTtoG is that the sizes of\r\nintermediate CFGs Sh can grow up to O(n lg(N/n)). If we can keep their sizes to O(n), everything\r\ngoes fine. This can be achieved by using two different types of partitions of Σh for PComp: One is for\r\ncompressing Th by a constant factor, and the other for compressing |Sh| by a constant factor (unless |Sh|\r\nis too small to compress). Recall that the former partition has been used in TtoG and SimTtoG, and the\r\npartition is computed from the adjacency list of Th by Algorithm 1. Algorithm 1 can be extended to work\r\non a set of strings by just inputting the adjacency list from a set of strings. Then, we can compute the\r\npartition for compressing |Sh| by a constant factor by considering the adjacency list from a set of strings\r\nin the righthand sides of Dh. The adjacency list can be easily computed in O(|Sh| + n) time and space by\r\nmodifying the algorithm described in the proof of Lemma 16: We just ignore the weight VOcc(X), i.e.,\r\nuse a unit weight 1 for every listed pair. Using the two types of partitions alternately, we can compress\r\nstrings by a constant factor while keeping the intermediate CFGs to O(n).\r\nWe denote the modified algorithm by GtoG and the resulting RLSLP by GtoG(S). Note that GtoG(S)\r\nis not identical to TtoG(T) in general because the partitions used in GtoG change depending on the input\r\nS. Still the height of GtoG(S) is O(lg N) and the properties of PSeqs hold. Hence we can support LCE\r\nqueries on GtoG(S) as we did on TtoG(T) by Lemma 15.\r\n4.3 Proof of Theorem 2\r\nProof of Theorem 2. Let S be an input SLP of size n generating T. We compute GtoG(S) in O(n lg(N/n))\r\ntime and O(n + z lg(N/z)) space as described in Section 4.2. Since the height of TtoG(T) is O(lg N), we\r\ncan support Extract(i, `) queries in O(lg N + `) time due to Lemma 6. GtoG(S) supports LCE queries in\r\nO(lg N) time in the same way as what was described in Lemma 15.\r\nAcknowledgements. The author was supported by JSPS KAKENHI Grant Number 16K16009.\r\n10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/bae486e0-044d-4754-87c9-0bef0d00dba6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0137e1be36984a3f696cc542c96484ab1587bab982f666cc2f9de2f1a5369cdd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 656
      },
      {
        "segments": [
          {
            "segment_id": "bae486e0-044d-4754-87c9-0bef0d00dba6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 10,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "Lemma 21. For any even h (0 ≤ h < hˆ), we can compute Sh+1 from Sh in O(|Sh| + n + m lg m)) time\r\nand O(|Sh| + n) space, where m is the number of long blocks in Dh. In addition, |Sh+1| ≤ |Sh| + 2n.\r\n4.1.3 The complexities of SimTtoG\r\nTheorem 22. SimTtoG runs in O(n lg2(N/n)) time and O(n lg(N/n)) space.\r\nProof. Using PComp and BComp implemented on CFGs (see Lemma 17 and 21), SimTtoG transforms\r\nlevel by level S0 into S1, S2, . . . , Shˆ . In each level, the size of CFGs can increase at most 2n by the\r\nprocedure of uncrossing. Since |Sh| = O(n lg N) for any h (0 ≤ h < hˆ), we get the time complexity\r\nO(n lg2 N) by simply applying Lemmas 17 and 21.\r\nWe can improve it to O(n lg2(N/n)) by a similar trick used in the proof of Lemma 14. At some\r\nlevel h\r\n0 where |Th0 | becomes less than n, we decompress Sh0 and switch to TtoG, which transforms Th0\r\ninto Thˆ in O(n) time by Lemma 10. We apply Lemmas 17 and 21 only for h with 0 ≤ h < h0. Since\r\nh\r\n0 = O(lg(N/n)), |Sh| = O(n lg(N/n)) for any h (0 ≤ h < h0\r\n). Therefore, we get the time complexity\r\nO(n lg2(N/n)). The space complexity is bounded by the maximum size of CFGs S0, S1, . . . , Sh0 , which is\r\nO(n lg(N/n)).\r\n4.2 GtoG: O(n lg(N/n))-time recompression\r\nWe modify SimTtoG slightly to run in O(n lg(N/n)) time and O(n + z lg(N/z)) space. The idea is the\r\nsame as what has been presented in Section 6.1 of [9]. The problem of SimTtoG is that the sizes of\r\nintermediate CFGs Sh can grow up to O(n lg(N/n)). If we can keep their sizes to O(n), everything\r\ngoes fine. This can be achieved by using two different types of partitions of Σh for PComp: One is for\r\ncompressing Th by a constant factor, and the other for compressing |Sh| by a constant factor (unless |Sh|\r\nis too small to compress). Recall that the former partition has been used in TtoG and SimTtoG, and the\r\npartition is computed from the adjacency list of Th by Algorithm 1. Algorithm 1 can be extended to work\r\non a set of strings by just inputting the adjacency list from a set of strings. Then, we can compute the\r\npartition for compressing |Sh| by a constant factor by considering the adjacency list from a set of strings\r\nin the righthand sides of Dh. The adjacency list can be easily computed in O(|Sh| + n) time and space by\r\nmodifying the algorithm described in the proof of Lemma 16: We just ignore the weight VOcc(X), i.e.,\r\nuse a unit weight 1 for every listed pair. Using the two types of partitions alternately, we can compress\r\nstrings by a constant factor while keeping the intermediate CFGs to O(n).\r\nWe denote the modified algorithm by GtoG and the resulting RLSLP by GtoG(S). Note that GtoG(S)\r\nis not identical to TtoG(T) in general because the partitions used in GtoG change depending on the input\r\nS. Still the height of GtoG(S) is O(lg N) and the properties of PSeqs hold. Hence we can support LCE\r\nqueries on GtoG(S) as we did on TtoG(T) by Lemma 15.\r\n4.3 Proof of Theorem 2\r\nProof of Theorem 2. Let S be an input SLP of size n generating T. We compute GtoG(S) in O(n lg(N/n))\r\ntime and O(n + z lg(N/z)) space as described in Section 4.2. Since the height of TtoG(T) is O(lg N), we\r\ncan support Extract(i, `) queries in O(lg N + `) time due to Lemma 6. GtoG(S) supports LCE queries in\r\nO(lg N) time in the same way as what was described in Lemma 15.\r\nAcknowledgements. The author was supported by JSPS KAKENHI Grant Number 16K16009.\r\n10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/bae486e0-044d-4754-87c9-0bef0d00dba6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0137e1be36984a3f696cc542c96484ab1587bab982f666cc2f9de2f1a5369cdd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 656
      },
      {
        "segments": [
          {
            "segment_id": "0da7b584-8dff-4dd3-b5c1-64d8f7b4a78f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595.276,
              "height": 841.89
            },
            "page_number": 11,
            "page_width": 595.276,
            "page_height": 841.89,
            "content": "References\r\n[1] Michael A. Bender, Martin Farach-Colton, Giridhar Pemmasani, Steven Skiena, and Pavel Sumazin.\r\nLowest common ancestors in trees and directed acyclic graphs. J. Algorithms, 57(2):75–94, 2005.\r\n[2] P. Bille, P. H. Cording, I. L. Gørtz, B. Sach, H. W. Vildhøj, and Søren Vind. Fingerprints in\r\ncompressed strings. In Proc. WADS 2013, pages 146–157, 2013.\r\n[3] Philip Bille, Anders Roy Christiansen, Patrick Hagge Cording, and Inge Li Gørtz. Finger search in\r\ngrammar-compressed strings. arXiv:1507.02853v3.\r\n[4] Philip Bille, Inge Li Gørtz, Mathias Bæk Tejs Knudsen, Moshe Lewenstein, and Hjalte Wedel Vildhøj.\r\nLongest common extensions in sublinear space. In Proc. CPM 2015, pages 65–76, 2015.\r\n[5] Philip Bille, Inge Li Gørtz, Benjamin Sach, and Hjalte Wedel Vildhøj. Time-space trade-offs for\r\nlongest common extensions. J. Discrete Algorithms, 25:42–50, 2014.\r\n[6] Dan Gusfield. Algorithms on Strings, Trees, and Sequences. Cambridge University Press, 1997.\r\n[7] Tomohiro I, Wataru Matsubara, Kouji Shimohira, Shunsuke Inenaga, Hideo Bannai, Masayuki\r\nTakeda, Kazuyuki Narisawa, and Ayumi Shinohara. Detecting regularities on grammar-compressed\r\nstrings. Inf. Comput., 240:74–89, 2015.\r\n[8] Artur Je˙z. Approximation of grammar-based compression via recompression. Theor. Comput. Sci.,\r\n592:115–134, 2015.\r\n[9] Artur Je˙z. Faster fully compressed pattern matching by recompression. ACM Transactions on\r\nAlgorithms, 11(3):20:1–20:43, 2015.\r\n[10] Artur Je˙z. One-variable word equations in linear time. Algorithmica, 74(1):1–48, 2016.\r\n[11] Artur Je˙z. Recompression: A simple and powerful technique for word equations. J. ACM, 63(1):4,\r\n2016.\r\n[12] Shirou Maruyama, Masaya Nakahara, Naoya Kishiue, and Hiroshi Sakamoto. Esp-index: A com\u0002pressed index based on edit-sensitive parsing. J. Discrete Algorithms, 18:100–112, 2013.\r\n[13] Kurt Mehlhorn, R. Sundar, and Christian Uhrig. Maintaining dynamic sequences under equality\r\ntests in polylogarithmic time. Algorithmica, 17(2):183–198, 1997.\r\n[14] Takaaki Nishimoto, Tomohiro I, Shunsuke Inenaga, Hideo Bannai, and Masayuki Takeda. Dynamic\r\nindex and LZ factorization in compressed space. In Proceedings of the Prague Stringology Conference\r\n2016, Prague, Czech Republic, August 29-31, 2015, pages 158–170, 2016.\r\n[15] Takaaki Nishimoto, Tomohiro I, Shunsuke Inenaga, Hideo Bannai, and Masayuki Takeda. Fully\r\ndynamic data structure for LCE queries in compressed space. In 41st International Symposium on\r\nMathematical Foundations of Computer Science, MFCS 2016, August 22-26, 2016 - Krak´ow, Poland,\r\npages 72:1–72:15, 2016.\r\n[16] Nicola Prezza. In-place longest common extensions. arXiv:1608.05100v6, 2016.\r\n[17] Simon J. Puglisi and Andrew Turpin. Space-time tradeoffs for longest-common-prefix array com\u0002putation. In Proc. ISAAC ’08, volume 5369 of Lecture Notes in Computer Science, pages 124–135.\r\nSpringer, 2008.\r\n[18] Wojciech Rytter. Application of Lempel-Ziv factorization to the approximation of grammar-based\r\ncompression. Theoretical Computer Science, 302(1–3):211–222, 2003.\r\n[19] Yuka Tanimura, Tomohiro I, Hideo Bannai, Shunsuke Inenaga, Simon J. Puglisi, and Masayuki\r\nTakeda. Deterministic sub-linear space LCE data structures with efficient construction. In 27th\r\nAnnual Symposium on Combinatorial Pattern Matching, CPM 2016, June 27-29, 2016, Tel Aviv,\r\nIsrael, pages 1:1–1:10, 2016.\r\n11",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/b91e711f-a476-4ea3-ac4d-4a509851aa69/images/0da7b584-8dff-4dd3-b5c1-64d8f7b4a78f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041503Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c62fdcf314d74cf5cfcea649912fc5e9a21f01144ca225fb5d985a9c1c120b8f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 447
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\"title\": \"Longest Common Extension with Recompression\"}\n```"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "```json\n{\"author\": \"Tomohiro I\"}\n```"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "2016\n"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "```json\n{\"location\": \"Figure 1\"}\n```"
        }
      ]
    }
  }
}