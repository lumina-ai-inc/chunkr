{
  "file_name": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description (1411.4389v3).pdf",
  "task_id": "1569b1e7-9963-43a0-a405-cc0324c4271f",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "1d390ebf-5bdc-4856-9bf2-20f0c87a74d2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Long-term Recurrent Convolutional Networks for\r\nVisual Recognition and Description\r\nJeff Donahue? Lisa Anne Hendricks? Sergio Guadarrama? Marcus Rohrbach?∗\r\nSubhashini Venugopalan†\r\n†UT Austin\r\nAustin, TX\r\nvsub@cs.utexas.edu\r\nKate Saenko‡\r\n‡UMass Lowell\r\nLowell, MA\r\nsaenko@cs.uml.edu\r\nTrevor Darrell?∗\r\n?UC Berkeley, ∗\r\nICSI\r\nBerkeley, CA\r\n{jdonahue, lisa anne, sguada,\r\nrohrbach, trevor}@eecs.berkeley.edu\r\nAbstract\r\nModels based on deep convolutional networks have dom\u0002inated recent image interpretation tasks; we investigate\r\nwhether models which are also recurrent, or “temporally\r\ndeep”, are effective for tasks involving sequences, visual\r\nand otherwise. We develop a novel recurrent convolutional\r\narchitecture suitable for large-scale visual learning which\r\nis end-to-end trainable, and demonstrate the value of these\r\nmodels on benchmark video recognition tasks, image de\u0002scription and retrieval problems, and video narration chal\u0002lenges. In contrast to current models which assume a fixed\r\nspatio-temporal receptive field or simple temporal averag\u0002ing for sequential processing, recurrent convolutional mod\u0002els are “doubly deep” in that they can be compositional\r\nin spatial and temporal “layers”. Such models may have\r\nadvantages when target concepts are complex and/or train\u0002ing data are limited. Learning long-term dependencies is\r\npossible when nonlinearities are incorporated into the net\u0002work state updates. Long-term RNN models are appealing\r\nin that they directly can map variable-length inputs (e.g.,\r\nvideo frames) to variable length outputs (e.g., natural lan\u0002guage text) and can model complex temporal dynamics; yet\r\nthey can be optimized with backpropagation. Our recurrent\r\nlong-term models are directly connected to modern visual\r\nconvnet models and can be jointly trained to simultaneously\r\nlearn temporal dynamics and convolutional perceptual rep\u0002resentations. Our results show such models have distinct\r\nadvantages over state-of-the-art models for recognition or\r\ngeneration which are separately defined and/or optimized.\r\n1. Introduction\r\nRecognition and description of images and videos is\r\na fundamental challenge of computer vision. Dramatic\r\nCNN\r\nCNN\r\nCNN\r\nCNN\r\nCNN\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nW1\r\nW2\r\nW3\r\nW4\r\nWT\r\nVisual Input Visual Features Sequence Learning Predictions\r\nFigure 1: We propose Long-term Recurrent Convolutional Net\u0002works (LRCNs), a class of architectures leveraging the strengths\r\nof rapid progress in CNNs for visual recognition problem, and the\r\ngrowing desire to apply such models to time-varying inputs and\r\noutputs. LRCN processes the (possibly) variable-length visual in\u0002put (left) with a CNN (middle-left), whose outputs are fed into a\r\nstack of recurrent sequence models (LSTMs, middle-right), which\r\nfinally produce a variable-length prediction (right).\r\nprogress has been achieved by supervised convolutional\r\nmodels on image recognition tasks, and a number of exten\u0002sions to process video have been recently proposed. Ideally,\r\na video model should allow processing of variable length\r\ninput sequences, and also provide for variable length out\u0002puts, including generation of full-length sentence descrip\u0002tions that go beyond conventional one-versus-all prediction\r\ntasks. In this paper we propose long-term recurrent convo\u0002lutional networks (LRCNs), a novel architecture for visual\r\nrecognition and description which combines convolutional\r\nlayers and long-range temporal recursion and is end-to-end\r\ntrainable (see Figure 1). We instantiate our architecture for\r\nspecific video activity recognition, image caption genera\u00021\r\narXiv:1411.4389v3 [cs.CV] 17 Feb 2015",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/1d390ebf-5bdc-4856-9bf2-20f0c87a74d2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e6e580f16179158fe73779c616f748ef8bc10649ceca51ac269761c5cb833f8b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 485
      },
      {
        "segments": [
          {
            "segment_id": "cc6124b3-6966-4334-bd0d-bd15b3bd7ed1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "tion, and video description tasks as described below.\r\nTo date, CNN models for video processing have success\u0002fully considered learning of 3-D spatio-temporal filters over\r\nraw sequence data [13, 2], and learning of frame-to-frame\r\nrepresentations which incorporate instantaneous optic flow\r\nor trajectory-based models aggregated over fixed windows\r\nor video shot segments [16, 33]. Such models explore two\r\nextrema of perceptual time-series representation learning:\r\neither learn a fully general time-varying weighting, or apply\r\nsimple temporal pooling. Following the same inspiration\r\nthat motivates current deep convolutional models, we advo\u0002cate for video recognition and description models which are\r\nalso deep over temporal dimensions; i.e., have temporal re\u0002currence of latent variables. RNN models are well known\r\nto be “deep in time”; e.g., explicitly so when unrolled, and\r\nform implicit compositional representations in the time do\u0002main. Such “deep” models predated deep spatial convolu\u0002tion models in the literature [31, 44].\r\nRecurrent Neural Networks have long been explored in\r\nperceptual applications for many decades, with varying re\u0002sults. A significant limitation of simple RNN models which\r\nstrictly integrate state information over time is known as the\r\n“vanishing gradient” effect: the ability to backpropogate an\r\nerror signal through a long-range temporal interval becomes\r\nincreasingly impossible in practice. A class of models\r\nwhich enable long-range learning was first proposed in [12],\r\nand augments hidden state with nonlinear mechanisms to\r\ncause state to propagate without modification, be updated,\r\nor be reset, using simple memory-cell like neural gates.\r\nWhile this model proved useful for several tasks, its util\u0002ity became apparent in recent results reporting large-scale\r\nlearning of speech recognition [10] and language transla\u0002tion models [38, 5].\r\nWe show here that long-term recurrent convolutional\r\nmodels are generally applicable to visual time-series mod\u0002eling; we argue that in visual tasks where static or flat tem\u0002poral models have previously been employed, long-term\r\nRNNs can provide significant improvement when ample\r\ntraining data are available to learn or refine the representa\u0002tion. Specifically, we show LSTM-type models provide for\r\nimproved recognition on conventional video activity chal\u0002lenges and enable a novel end-to-end optimizable mapping\r\nfrom image pixels to sentence-level natural language de\u0002scriptions. We also show that these models improve gen\u0002eration of descriptions from intermediate visual representa\u0002tions derived from conventional visual models.\r\nWe instantiate our proposed architecture in three experi\u0002mental settings (see Figure 3). First, we show that directly\r\nconnecting a visual convolutional model to deep LSTM net\u0002works, we are able to train video recognition models that\r\ncapture complex temporal state dependencies (Figure 3 left;\r\nSection 4). While existing labeled video activity datasets\r\nmay not have actions or activities with extremely complex\r\ntime dynamics, we nonetheless see improvements on the or\u0002der of 4% on conventional benchmarks.\r\nSecond, we explore direct end-to-end trainable image to\r\nsentence mappings. Strong results for machine translation\r\ntasks have recently been reported [38, 5]; such models are\r\nencoder/decoder pairs based on LSTM networks. We pro\u0002pose a multimodal analog of this model, and describe an\r\narchitecture which uses a visual convnet to encode a deep\r\nstate vector, and an LSTM to decode the vector into an natu\u0002ral language string (Figure 3 middle; Section 5). The result\u0002ing model can be trained end-to-end on large-scale image\r\nand text datasets, and even with modest training provides\r\ncompetitive generation results compared to existing meth\u0002ods.\r\nFinally, we show that LSTM decoders can be driven di\u0002rectly from conventional computer vision methods which\r\npredict higher-level discriminative labels, such as the se\u0002mantic video role tuple predictors in [30] (Figure 3 right;\r\nSection 6). While not end-to-end trainable, such models\r\noffer architectural and performance advantages over previ\u0002ous statistical machine translation-based approaches, as re\u0002ported below.\r\nWe have realized a generalized “LSTM”-style RNN\r\nmodel in the widely-adopted open source deep learning\r\nframework Caffe [14], incorporating the specific LSTM\r\nunits of [46, 38, 5].\r\n2. Background: Recurrent Neural Networks\r\n(RNNs)\r\nTraditional RNNs (Figure 2, left) can learn complex tem\u0002poral dynamics by mapping input sequences to a sequence\r\nof hidden states, and hidden states to outputs via the follow\u0002ing recurrence equations (Figure 2, left):\r\nht = g(Wxhxt + Whhht−1 + bh)\r\nzt = g(Whzht + bz)\r\nwhere g is an element-wise non-linearity, such as a sigmoid\r\nor hyperbolic tangent, xt is the input, ht ∈ R\r\nN is the hidden\r\nstate with N hidden units, and yt is the output at time t.\r\nFor a length T input sequence hx1, x2, ..., xT i, the updates\r\nabove are computed sequentially as h1 (letting h0 = 0), y1,\r\nh2, y2, ..., hT , yT .\r\nThough RNNs have proven successful on tasks such as\r\nspeech recognition [42] and text generation [37], it can be\r\ndifficult to train them to learn long-term dynamics, likely\r\ndue in part to the vanishing and exploding gradients prob\u0002lem [12] that can result from propagating the gradients\r\ndown through the many layers of the recurrent network,\r\neach corresponding to a particular timestep. LSTMs pro\u0002vide a solution by incorporating memory units that allow\r\nthe network to learn when to forget previous hidden states\r\nand when to update hidden states given new information.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/cc6124b3-6966-4334-bd0d-bd15b3bd7ed1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7f604e52d6cb73748ccb1e2cda5256d2f24aaf8ce06f059e10e3a845513377c4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 822
      },
      {
        "segments": [
          {
            "segment_id": "cc6124b3-6966-4334-bd0d-bd15b3bd7ed1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "tion, and video description tasks as described below.\r\nTo date, CNN models for video processing have success\u0002fully considered learning of 3-D spatio-temporal filters over\r\nraw sequence data [13, 2], and learning of frame-to-frame\r\nrepresentations which incorporate instantaneous optic flow\r\nor trajectory-based models aggregated over fixed windows\r\nor video shot segments [16, 33]. Such models explore two\r\nextrema of perceptual time-series representation learning:\r\neither learn a fully general time-varying weighting, or apply\r\nsimple temporal pooling. Following the same inspiration\r\nthat motivates current deep convolutional models, we advo\u0002cate for video recognition and description models which are\r\nalso deep over temporal dimensions; i.e., have temporal re\u0002currence of latent variables. RNN models are well known\r\nto be “deep in time”; e.g., explicitly so when unrolled, and\r\nform implicit compositional representations in the time do\u0002main. Such “deep” models predated deep spatial convolu\u0002tion models in the literature [31, 44].\r\nRecurrent Neural Networks have long been explored in\r\nperceptual applications for many decades, with varying re\u0002sults. A significant limitation of simple RNN models which\r\nstrictly integrate state information over time is known as the\r\n“vanishing gradient” effect: the ability to backpropogate an\r\nerror signal through a long-range temporal interval becomes\r\nincreasingly impossible in practice. A class of models\r\nwhich enable long-range learning was first proposed in [12],\r\nand augments hidden state with nonlinear mechanisms to\r\ncause state to propagate without modification, be updated,\r\nor be reset, using simple memory-cell like neural gates.\r\nWhile this model proved useful for several tasks, its util\u0002ity became apparent in recent results reporting large-scale\r\nlearning of speech recognition [10] and language transla\u0002tion models [38, 5].\r\nWe show here that long-term recurrent convolutional\r\nmodels are generally applicable to visual time-series mod\u0002eling; we argue that in visual tasks where static or flat tem\u0002poral models have previously been employed, long-term\r\nRNNs can provide significant improvement when ample\r\ntraining data are available to learn or refine the representa\u0002tion. Specifically, we show LSTM-type models provide for\r\nimproved recognition on conventional video activity chal\u0002lenges and enable a novel end-to-end optimizable mapping\r\nfrom image pixels to sentence-level natural language de\u0002scriptions. We also show that these models improve gen\u0002eration of descriptions from intermediate visual representa\u0002tions derived from conventional visual models.\r\nWe instantiate our proposed architecture in three experi\u0002mental settings (see Figure 3). First, we show that directly\r\nconnecting a visual convolutional model to deep LSTM net\u0002works, we are able to train video recognition models that\r\ncapture complex temporal state dependencies (Figure 3 left;\r\nSection 4). While existing labeled video activity datasets\r\nmay not have actions or activities with extremely complex\r\ntime dynamics, we nonetheless see improvements on the or\u0002der of 4% on conventional benchmarks.\r\nSecond, we explore direct end-to-end trainable image to\r\nsentence mappings. Strong results for machine translation\r\ntasks have recently been reported [38, 5]; such models are\r\nencoder/decoder pairs based on LSTM networks. We pro\u0002pose a multimodal analog of this model, and describe an\r\narchitecture which uses a visual convnet to encode a deep\r\nstate vector, and an LSTM to decode the vector into an natu\u0002ral language string (Figure 3 middle; Section 5). The result\u0002ing model can be trained end-to-end on large-scale image\r\nand text datasets, and even with modest training provides\r\ncompetitive generation results compared to existing meth\u0002ods.\r\nFinally, we show that LSTM decoders can be driven di\u0002rectly from conventional computer vision methods which\r\npredict higher-level discriminative labels, such as the se\u0002mantic video role tuple predictors in [30] (Figure 3 right;\r\nSection 6). While not end-to-end trainable, such models\r\noffer architectural and performance advantages over previ\u0002ous statistical machine translation-based approaches, as re\u0002ported below.\r\nWe have realized a generalized “LSTM”-style RNN\r\nmodel in the widely-adopted open source deep learning\r\nframework Caffe [14], incorporating the specific LSTM\r\nunits of [46, 38, 5].\r\n2. Background: Recurrent Neural Networks\r\n(RNNs)\r\nTraditional RNNs (Figure 2, left) can learn complex tem\u0002poral dynamics by mapping input sequences to a sequence\r\nof hidden states, and hidden states to outputs via the follow\u0002ing recurrence equations (Figure 2, left):\r\nht = g(Wxhxt + Whhht−1 + bh)\r\nzt = g(Whzht + bz)\r\nwhere g is an element-wise non-linearity, such as a sigmoid\r\nor hyperbolic tangent, xt is the input, ht ∈ R\r\nN is the hidden\r\nstate with N hidden units, and yt is the output at time t.\r\nFor a length T input sequence hx1, x2, ..., xT i, the updates\r\nabove are computed sequentially as h1 (letting h0 = 0), y1,\r\nh2, y2, ..., hT , yT .\r\nThough RNNs have proven successful on tasks such as\r\nspeech recognition [42] and text generation [37], it can be\r\ndifficult to train them to learn long-term dynamics, likely\r\ndue in part to the vanishing and exploding gradients prob\u0002lem [12] that can result from propagating the gradients\r\ndown through the many layers of the recurrent network,\r\neach corresponding to a particular timestep. LSTMs pro\u0002vide a solution by incorporating memory units that allow\r\nthe network to learn when to forget previous hidden states\r\nand when to update hidden states given new information.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/cc6124b3-6966-4334-bd0d-bd15b3bd7ed1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7f604e52d6cb73748ccb1e2cda5256d2f24aaf8ce06f059e10e3a845513377c4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 822
      },
      {
        "segments": [
          {
            "segment_id": "e404279e-9e83-437b-afa9-06e4e2df3102",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "As research on LSTMs has progressed, hidden units with\r\nvarying connections within the memory unit have been pro\u0002posed. We use the LSTM unit as described in [45] (Figure 2,\r\nright), which is a slight simplification of the one described\r\nin [10]. Letting σ(x) = (1 + e\r\n−x\r\n)\r\n−1\r\nbe the sigmoid non\u0002linearity which squashes real-valued inputs to a [0, 1] range,\r\nand letting φ(x) = e\r\nx−e−x\r\ne\r\nx+e−x = 2σ(2x) − 1 be the hyper\u0002bolic tangent nonlinearity, similarly squashing its inputs to\r\na [−1, 1] range, the LSTM updates for timestep t given in\u0002puts xt, ht−1, and ct−1 are:\r\nit = σ(Wxixt + Whiht−1 + bi)\r\nft = σ(Wxfxt + Whfht−1 + bf )\r\not = σ(Wxoxt + Whoht−1 + bo)\r\ngt = φ(Wxcxt + Whcht−1 + bc)\r\nct = ft \f ct−1 + it \f gt\r\nht = ot \f φ(ct)\r\nxt\r\nht-1\r\nht\r\nOutput\r\nCell\r\nzt\r\nRNN Unit φ\r\nσ\r\nσ σ\r\nxt\r\nht-1\r\nht = zt\r\nCell\r\nOutput \r\nGate\r\nInput \r\nGate\r\nForget \r\nGate\r\nInput \r\nModulation LSTM Unit Gate\r\nσ\r\nφ\r\nσ\r\nFigure 2: A diagram of a basic RNN cell (left) and an LSTM mem\u0002ory cell (right) used in this paper (from [45], a slight simplification\r\nof the architecture described in [9], which was derived from the\r\nLSTM initially proposed in [12]).\r\nIn addition to a hidden unit ht ∈ R\r\nN , the LSTM in\u0002cludes an input gate it ∈ R\r\nN , forget gate ft ∈ RN , output\r\ngate ot ∈ R\r\nN , input modulation gate gt ∈ RN , and mem\u0002ory cell ct ∈ R\r\nN . The memory cell unit ct is a summation\r\nof two things: the previous memory cell unit ct−1 which\r\nis modulated by ft, and gt, a function of the current input\r\nand previous hidden state, modulated by the input gate it.\r\nBecause it and ft are sigmoidal, their values lie within the\r\nrange [0, 1], and it and ft can be thought of as knobs that\r\nthe LSTM learns to selectively forget its previous memory\r\nor consider its current input. Likewise, the output gate ot\r\nlearns how much of the memory cell to transfer to the hid\u0002den state. These additional cells enable the LSTM to learn\r\nextremely complex and long-term temporal dynamics the\r\nRNN is not capable of learning. Additional depth can be\r\nadded to LSTMs by stacking them on top of each other, us\u0002ing the hidden state of the LSTM in layer l − 1 as the input\r\nto the LSTM in layer l.\r\nRecently, LSTMs have achieved impressive results on\r\nlanguage tasks such as speech recognition [10] and ma\u0002chine translation [38, 5]. Analogous to CNNs, LSTMs are\r\nattractive because they allow end-to-end fine-tuning. For\r\nexample, [10] eliminates the need for complex multi-step\r\npipelines in speech recognition by training a deep bidirec\u0002tional LSTM which maps spectrogram inputs to text. Even\r\nwith no language model or pronunciation dictionary, the\r\nmodel produces convincing text translations. [38] and [5]\r\ntranslate sentences from English to French with a multi\u0002layer LSTM encoder and decoder. Sentences in the source\r\nlanguage are mapped to a hidden state using an encoding\r\nLSTM, and then a decoding LSTM maps the hidden state\r\nto a sequence in the target language. Such an encoder de\u0002coder scheme allows sequences of different lengths to be\r\nmapped to each other. Like [10] the sequence-to-sequence\r\narchitecture for machine translation circumvents the need\r\nfor language models.\r\nThe advantages of LSTMs for modeling sequential data\r\nin vision problems are twofold. First, when integrated with\r\ncurrent vision systems, LSTM models are straightforward\r\nto fine-tune end-to-end. Second, LSTMs are not confined\r\nto fixed length inputs or outputs allowing simple modeling\r\nfor sequential data of varying lengths, such as text or video.\r\nWe next describe a unified framework to combine LSTMs\r\nwith deep convolutional networks to create a model which\r\nis both spatially and temporally deep.\r\n3. Long-term Recurrent Convolutional Net\u0002work (LRCN) model\r\nThis work proposes a Long-term Recurrent Convolu\u0002tional Network (LRCN) model combinining a deep hier\u0002archical visual feature extractor (such as a CNN) with a\r\nmodel that can learn to recognize and synthesize temporal\r\ndynamics for tasks involving sequential data (inputs or out\u0002puts), visual, linsguistical or otherwise. Figure 1 depicts the\r\ncore of our approach. Our LRCN model works by pass\u0002ing each visual input vt (an image in isolation, or a frame\r\nfrom a video) through a feature transformation φV (vt)\r\nparametrized by V to produce a fixed-length vector rep\u0002resentation φt ∈ R\r\nd\r\n. Having computed the feature-space\r\nrepresentation of the visual input sequence hφ1, φ2, ..., φT i,\r\nthe sequence model then takes over.\r\nIn its most general form, a sequence model parametrized\r\nby W maps an input xt and a previous timestep hidden state\r\nht−1 to an output zt and updated hidden state ht. There\u0002fore, inference must be run sequentially (i.e., from top to\r\nbottom, in the Sequence Learning box of Figure 1), by\r\ncomputing in order: h1 = fW (x1, h0) = fW (x1, 0), then\r\nh2 = fW (x2, h1), etc., up to hT . Some of our models stack\r\nmultiple LSTMs atop one another as described in Section 2.\r\nThe final step in predicting a distribution P(yt) at\r\ntimestep t is to take a softmax over the outputs zt of the\r\nsequential model, producing a distribution over the (in our\r\ncase, finite and discrete) space C of possible per-timestep",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/e404279e-9e83-437b-afa9-06e4e2df3102.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0e5c78049ed94457944854d0d5796924141ee779a4355b7f21798379a6a6a555",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 891
      },
      {
        "segments": [
          {
            "segment_id": "e404279e-9e83-437b-afa9-06e4e2df3102",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "As research on LSTMs has progressed, hidden units with\r\nvarying connections within the memory unit have been pro\u0002posed. We use the LSTM unit as described in [45] (Figure 2,\r\nright), which is a slight simplification of the one described\r\nin [10]. Letting σ(x) = (1 + e\r\n−x\r\n)\r\n−1\r\nbe the sigmoid non\u0002linearity which squashes real-valued inputs to a [0, 1] range,\r\nand letting φ(x) = e\r\nx−e−x\r\ne\r\nx+e−x = 2σ(2x) − 1 be the hyper\u0002bolic tangent nonlinearity, similarly squashing its inputs to\r\na [−1, 1] range, the LSTM updates for timestep t given in\u0002puts xt, ht−1, and ct−1 are:\r\nit = σ(Wxixt + Whiht−1 + bi)\r\nft = σ(Wxfxt + Whfht−1 + bf )\r\not = σ(Wxoxt + Whoht−1 + bo)\r\ngt = φ(Wxcxt + Whcht−1 + bc)\r\nct = ft \f ct−1 + it \f gt\r\nht = ot \f φ(ct)\r\nxt\r\nht-1\r\nht\r\nOutput\r\nCell\r\nzt\r\nRNN Unit φ\r\nσ\r\nσ σ\r\nxt\r\nht-1\r\nht = zt\r\nCell\r\nOutput \r\nGate\r\nInput \r\nGate\r\nForget \r\nGate\r\nInput \r\nModulation LSTM Unit Gate\r\nσ\r\nφ\r\nσ\r\nFigure 2: A diagram of a basic RNN cell (left) and an LSTM mem\u0002ory cell (right) used in this paper (from [45], a slight simplification\r\nof the architecture described in [9], which was derived from the\r\nLSTM initially proposed in [12]).\r\nIn addition to a hidden unit ht ∈ R\r\nN , the LSTM in\u0002cludes an input gate it ∈ R\r\nN , forget gate ft ∈ RN , output\r\ngate ot ∈ R\r\nN , input modulation gate gt ∈ RN , and mem\u0002ory cell ct ∈ R\r\nN . The memory cell unit ct is a summation\r\nof two things: the previous memory cell unit ct−1 which\r\nis modulated by ft, and gt, a function of the current input\r\nand previous hidden state, modulated by the input gate it.\r\nBecause it and ft are sigmoidal, their values lie within the\r\nrange [0, 1], and it and ft can be thought of as knobs that\r\nthe LSTM learns to selectively forget its previous memory\r\nor consider its current input. Likewise, the output gate ot\r\nlearns how much of the memory cell to transfer to the hid\u0002den state. These additional cells enable the LSTM to learn\r\nextremely complex and long-term temporal dynamics the\r\nRNN is not capable of learning. Additional depth can be\r\nadded to LSTMs by stacking them on top of each other, us\u0002ing the hidden state of the LSTM in layer l − 1 as the input\r\nto the LSTM in layer l.\r\nRecently, LSTMs have achieved impressive results on\r\nlanguage tasks such as speech recognition [10] and ma\u0002chine translation [38, 5]. Analogous to CNNs, LSTMs are\r\nattractive because they allow end-to-end fine-tuning. For\r\nexample, [10] eliminates the need for complex multi-step\r\npipelines in speech recognition by training a deep bidirec\u0002tional LSTM which maps spectrogram inputs to text. Even\r\nwith no language model or pronunciation dictionary, the\r\nmodel produces convincing text translations. [38] and [5]\r\ntranslate sentences from English to French with a multi\u0002layer LSTM encoder and decoder. Sentences in the source\r\nlanguage are mapped to a hidden state using an encoding\r\nLSTM, and then a decoding LSTM maps the hidden state\r\nto a sequence in the target language. Such an encoder de\u0002coder scheme allows sequences of different lengths to be\r\nmapped to each other. Like [10] the sequence-to-sequence\r\narchitecture for machine translation circumvents the need\r\nfor language models.\r\nThe advantages of LSTMs for modeling sequential data\r\nin vision problems are twofold. First, when integrated with\r\ncurrent vision systems, LSTM models are straightforward\r\nto fine-tune end-to-end. Second, LSTMs are not confined\r\nto fixed length inputs or outputs allowing simple modeling\r\nfor sequential data of varying lengths, such as text or video.\r\nWe next describe a unified framework to combine LSTMs\r\nwith deep convolutional networks to create a model which\r\nis both spatially and temporally deep.\r\n3. Long-term Recurrent Convolutional Net\u0002work (LRCN) model\r\nThis work proposes a Long-term Recurrent Convolu\u0002tional Network (LRCN) model combinining a deep hier\u0002archical visual feature extractor (such as a CNN) with a\r\nmodel that can learn to recognize and synthesize temporal\r\ndynamics for tasks involving sequential data (inputs or out\u0002puts), visual, linsguistical or otherwise. Figure 1 depicts the\r\ncore of our approach. Our LRCN model works by pass\u0002ing each visual input vt (an image in isolation, or a frame\r\nfrom a video) through a feature transformation φV (vt)\r\nparametrized by V to produce a fixed-length vector rep\u0002resentation φt ∈ R\r\nd\r\n. Having computed the feature-space\r\nrepresentation of the visual input sequence hφ1, φ2, ..., φT i,\r\nthe sequence model then takes over.\r\nIn its most general form, a sequence model parametrized\r\nby W maps an input xt and a previous timestep hidden state\r\nht−1 to an output zt and updated hidden state ht. There\u0002fore, inference must be run sequentially (i.e., from top to\r\nbottom, in the Sequence Learning box of Figure 1), by\r\ncomputing in order: h1 = fW (x1, h0) = fW (x1, 0), then\r\nh2 = fW (x2, h1), etc., up to hT . Some of our models stack\r\nmultiple LSTMs atop one another as described in Section 2.\r\nThe final step in predicting a distribution P(yt) at\r\ntimestep t is to take a softmax over the outputs zt of the\r\nsequential model, producing a distribution over the (in our\r\ncase, finite and discrete) space C of possible per-timestep",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/e404279e-9e83-437b-afa9-06e4e2df3102.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0e5c78049ed94457944854d0d5796924141ee779a4355b7f21798379a6a6a555",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 891
      },
      {
        "segments": [
          {
            "segment_id": "b42d73d9-c9f9-45f0-84ce-8ddd25ff7d0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "outputs:\r\nP(yt = c) = exp(Wzczt,c + bc)\r\nP\r\nc\r\n0∈C\r\nexp(Wzczt,c0 + bc)\r\nThe success of recent very deep models for object recog\u0002nition [22, 34, 39] suggests that strategically composing\r\nmany “layers” of non-linear functions can result in very\r\npowerful models for perceptual problems. For large T,\r\nthe above recurrence indicates that the last few predictions\r\nfrom a recurrent network with T timesteps are computed by\r\na very “deep” (T-layered) non-linear function, suggesting\r\nthat the resulting recurrent model may have similar repre\u0002sentational power to a T-layer neural network. Critically,\r\nhowever, the sequential model’s weights W are reused at\r\nevery timestep, forcing the model to learn generic timestep\u0002to-timestep dynamics (as opposed to dynamics directly con\u0002ditioned on t, the sequence index) and preventing the pa\u0002rameter size from growing in proportion to the maximum\r\nnumber of timesteps.\r\nIn most of our experiments, the visual feature transfor\u0002mation φ corresponds to the activations in some layer of\r\na large CNN. Using a visual transformation φV (.) which\r\nis time-invariant and independent at each timestep has the\r\nimportant advantage of making the expensive convolutional\r\ninference and training parallelizable over all timesteps of\r\nthe input, facilitating the use of fast contemporary CNN im\u0002plementations whose efficiency relies on independent batch\r\nprocessing, and end-to-end optimization of the visual and\r\nsequential model parameters V and W.\r\nWe consider three vision problems (activity recognition,\r\nimage description and video description) which instantiate\r\none of the following broad classes of sequential learning\r\ntasks:\r\n1. Sequential inputs, fixed outputs (Figure 3, left):\r\nhx1, x2, ..., xT i 7→ y. The visual activity recognition\r\nproblem can fall under this umbrella, with videos of\r\narbitrary length T as input, but with the goal of pre\u0002dicting a single label like running or jumping drawn\r\nfrom a fixed vocabulary.\r\n2. Fixed inputs, sequential outputs (Figure 3, middle):\r\nx 7→ hy1, y2, ..., yT i. The image description problem\r\nfits in this category, with a non-time-varying image as\r\ninput, but a much larger and richer label space consist\u0002ing of sentences of any length.\r\n3. Sequential inputs and outputs (Figure 3, right):\r\nhx1, x2, ..., xT i 7→ hy1, y2, ..., yT0 i. Finally, it’s easy\r\nto imagine tasks for which both the visual input and\r\noutput are time-varying, and in general the number of\r\ninput and output timesteps may differ (i.e., we may\r\nhave T 6= T\r\n0\r\n). In the video description task, for exam\u0002ple, the input and output are both sequential, and the\r\nnumber of frames in the video should not constrain the\r\nlength of (number of words in) the natural-language\r\ndescription.\r\nIn the previously described formulation, each instance\r\nhas T inputs hx1, x2, ..., xT i and T outputs hy1, y2, ..., yT i.\r\nWe describe how we adapt this formulation in our hybrid\r\nmodel to tackle each of the above three problem settings.\r\nWith sequential inputs and scalar outputs, we take a late\r\nfusion approach to merging the per-timestep predictions\r\nhy1, y2, ..., yT i into a single prediction y for the full se\u0002quence. With fixed-size inputs and sequential outputs, we\r\nsimply duplicate the input x at all T timesteps xt := x (not\u0002ing this can be done cheaply due to the time-invariant vi\u0002sual feature extractor). Finally, for a sequence-to-sequence\r\nproblem with (in general) different input and output lengths,\r\nwe take an “encoder-decoder” approach inspired by [46]. In\r\nthis approach, one sequence model, the encoder, is used to\r\nmap the input sequence to a fixed-length vector, then an\u0002other sequence model, the decoder, is used to unroll this\r\nvector to sequential outputs of arbitrary length. Under this\r\nmodel, the system as a whole may be thought of as having\r\nT + T\r\n0\r\ntimesteps of input and output, wherein the input is\r\nprocessed and the decoder outputs are ignored for the first\r\nT timesteps, and the predictions are made and “dummy”\r\ninputs are ignored for the latter T\r\n0\r\ntimesteps.\r\nUnder the proposed system, the weights (V, W) of the\r\nmodel’s visual and sequential components can be learned\r\njointly by maximizing the likelihood of the ground truth\r\noutputs yt conditioned on the input data and labels up to that\r\npoint (x1:t, y1:t−1) In particular, we minimize the negative\r\nlog likelihood L(V, W) = − log PV,W (yt|x1:t, y1:t−1) of\r\nthe training data (x, y).\r\nOne of the most appealing aspects of the described sys\u0002tem is the ability to learn the parameters “end-to-end,” such\r\nthat the parameters V of the visual feature extractor learn\r\nto pick out the aspects of the visual input that are rele\u0002vant to the sequential classification problem. We train our\r\nLRCN models using stochastic gradient descent with mo\u0002mentum, with backpropagation used to compute the gradi\u0002ent ∇L(V, W) of the objective L with respect to all param\u0002eters (V, W).\r\nWe next demonstrate the power of models which are both\r\ndeep in space and deep in time by exploring three appli\u0002cations: activity recognition, image description, and video\r\ndescription.\r\n4. Activity recognition\r\nActivity recognition is an example of the first sequen\u0002tial learning task described above; T individual frames are\r\ninputs into T convolutional networks which are then con\u0002nected to a single-layer LSTM with 256 hidden units. A\r\nlarge body of recent work has proposed deep architectures",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/b42d73d9-c9f9-45f0-84ce-8ddd25ff7d0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1eec99e96956814305cbdee3592c2a4eac4b435eaf0580d55916255b3980b133",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 861
      },
      {
        "segments": [
          {
            "segment_id": "b42d73d9-c9f9-45f0-84ce-8ddd25ff7d0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "outputs:\r\nP(yt = c) = exp(Wzczt,c + bc)\r\nP\r\nc\r\n0∈C\r\nexp(Wzczt,c0 + bc)\r\nThe success of recent very deep models for object recog\u0002nition [22, 34, 39] suggests that strategically composing\r\nmany “layers” of non-linear functions can result in very\r\npowerful models for perceptual problems. For large T,\r\nthe above recurrence indicates that the last few predictions\r\nfrom a recurrent network with T timesteps are computed by\r\na very “deep” (T-layered) non-linear function, suggesting\r\nthat the resulting recurrent model may have similar repre\u0002sentational power to a T-layer neural network. Critically,\r\nhowever, the sequential model’s weights W are reused at\r\nevery timestep, forcing the model to learn generic timestep\u0002to-timestep dynamics (as opposed to dynamics directly con\u0002ditioned on t, the sequence index) and preventing the pa\u0002rameter size from growing in proportion to the maximum\r\nnumber of timesteps.\r\nIn most of our experiments, the visual feature transfor\u0002mation φ corresponds to the activations in some layer of\r\na large CNN. Using a visual transformation φV (.) which\r\nis time-invariant and independent at each timestep has the\r\nimportant advantage of making the expensive convolutional\r\ninference and training parallelizable over all timesteps of\r\nthe input, facilitating the use of fast contemporary CNN im\u0002plementations whose efficiency relies on independent batch\r\nprocessing, and end-to-end optimization of the visual and\r\nsequential model parameters V and W.\r\nWe consider three vision problems (activity recognition,\r\nimage description and video description) which instantiate\r\none of the following broad classes of sequential learning\r\ntasks:\r\n1. Sequential inputs, fixed outputs (Figure 3, left):\r\nhx1, x2, ..., xT i 7→ y. The visual activity recognition\r\nproblem can fall under this umbrella, with videos of\r\narbitrary length T as input, but with the goal of pre\u0002dicting a single label like running or jumping drawn\r\nfrom a fixed vocabulary.\r\n2. Fixed inputs, sequential outputs (Figure 3, middle):\r\nx 7→ hy1, y2, ..., yT i. The image description problem\r\nfits in this category, with a non-time-varying image as\r\ninput, but a much larger and richer label space consist\u0002ing of sentences of any length.\r\n3. Sequential inputs and outputs (Figure 3, right):\r\nhx1, x2, ..., xT i 7→ hy1, y2, ..., yT0 i. Finally, it’s easy\r\nto imagine tasks for which both the visual input and\r\noutput are time-varying, and in general the number of\r\ninput and output timesteps may differ (i.e., we may\r\nhave T 6= T\r\n0\r\n). In the video description task, for exam\u0002ple, the input and output are both sequential, and the\r\nnumber of frames in the video should not constrain the\r\nlength of (number of words in) the natural-language\r\ndescription.\r\nIn the previously described formulation, each instance\r\nhas T inputs hx1, x2, ..., xT i and T outputs hy1, y2, ..., yT i.\r\nWe describe how we adapt this formulation in our hybrid\r\nmodel to tackle each of the above three problem settings.\r\nWith sequential inputs and scalar outputs, we take a late\r\nfusion approach to merging the per-timestep predictions\r\nhy1, y2, ..., yT i into a single prediction y for the full se\u0002quence. With fixed-size inputs and sequential outputs, we\r\nsimply duplicate the input x at all T timesteps xt := x (not\u0002ing this can be done cheaply due to the time-invariant vi\u0002sual feature extractor). Finally, for a sequence-to-sequence\r\nproblem with (in general) different input and output lengths,\r\nwe take an “encoder-decoder” approach inspired by [46]. In\r\nthis approach, one sequence model, the encoder, is used to\r\nmap the input sequence to a fixed-length vector, then an\u0002other sequence model, the decoder, is used to unroll this\r\nvector to sequential outputs of arbitrary length. Under this\r\nmodel, the system as a whole may be thought of as having\r\nT + T\r\n0\r\ntimesteps of input and output, wherein the input is\r\nprocessed and the decoder outputs are ignored for the first\r\nT timesteps, and the predictions are made and “dummy”\r\ninputs are ignored for the latter T\r\n0\r\ntimesteps.\r\nUnder the proposed system, the weights (V, W) of the\r\nmodel’s visual and sequential components can be learned\r\njointly by maximizing the likelihood of the ground truth\r\noutputs yt conditioned on the input data and labels up to that\r\npoint (x1:t, y1:t−1) In particular, we minimize the negative\r\nlog likelihood L(V, W) = − log PV,W (yt|x1:t, y1:t−1) of\r\nthe training data (x, y).\r\nOne of the most appealing aspects of the described sys\u0002tem is the ability to learn the parameters “end-to-end,” such\r\nthat the parameters V of the visual feature extractor learn\r\nto pick out the aspects of the visual input that are rele\u0002vant to the sequential classification problem. We train our\r\nLRCN models using stochastic gradient descent with mo\u0002mentum, with backpropagation used to compute the gradi\u0002ent ∇L(V, W) of the objective L with respect to all param\u0002eters (V, W).\r\nWe next demonstrate the power of models which are both\r\ndeep in space and deep in time by exploring three appli\u0002cations: activity recognition, image description, and video\r\ndescription.\r\n4. Activity recognition\r\nActivity recognition is an example of the first sequen\u0002tial learning task described above; T individual frames are\r\ninputs into T convolutional networks which are then con\u0002nected to a single-layer LSTM with 256 hidden units. A\r\nlarge body of recent work has proposed deep architectures",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/b42d73d9-c9f9-45f0-84ce-8ddd25ff7d0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1eec99e96956814305cbdee3592c2a4eac4b435eaf0580d55916255b3980b133",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 861
      },
      {
        "segments": [
          {
            "segment_id": "f1313d4a-9150-4a8b-92f4-e42755905057",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Input:\r\nVideo\r\nCRF\r\nLSTM\r\nA man juiced the orange\r\nInput:\r\nImage\r\nLSTM\r\nC\r\nN\r\nN\r\nInput:\r\nSequence \r\nof Frames\r\nOutput:\r\nLabel\r\nLSTM\r\nC\r\nN\r\nN\r\nC\r\nN\r\nN\r\nActivity Recognition Image Description Video Description\r\nA large building with a\r\nclock on the front of it\r\nOutput:\r\nSentence \r\nOutput:\r\nSentence\r\nFigure 3: Task-specific instantiations of our LRCN model for activity recognition, image description, and video description.\r\nfor activity recognition ([16, 33, 13, 2, 1]). [33, 16] both\r\npropose convolutional networks which learn filters based on\r\na stack of N input frames. Though we analyze clips of 16\r\nframes in this work, we note that the LRCN system is more\r\nflexible than [33, 16] since it is not constrained to analyz\u0002ing fixed length inputs and could potentially learn to rec\u0002ognize complex video sequences (e.g., cooking sequences\r\nas presented in 6). [1, 2] use recurrent neural networks to\r\nlearn temporal dynamics of either traditional vision features\r\n([1]) or deep features ([2]), but do not train their models\r\nend-to-end and do not pre-train on larger object recognition\r\ndatabases for important performance gains.\r\nWe explore two variants of the LRCN architecture: one\r\nin which the LSTM is placed after the first fully connected\r\nlayer of the CNN (LRCN-fc6) and another in which the\r\nLSTM is placed after the second fully connected layer of\r\nthe CNN (LRCN-fc7). We train the LRCN networks with\r\nvideo clips of 16 frames. The LRCN predicts the video class\r\nat each time step and we average these predictions for final\r\nclassification. At test time, we extract 16 frame clips with a\r\nstride of 8 frames from each video and average across clips.\r\nWe also consider both RGB and flow inputs. Flow is\r\ncomputed with [4] and transformed into a “flow image”\r\nby centering x and y flow values around 128 and mul\u0002tiplying by a scalar such that flow values fall between 0\r\nand 255. A third channel for the flow image is created\r\nby calculating the flow magnitude. The CNN base of the\r\nLRCN is a hybrid of the Caffe [14] reference model, a mi\u0002nor variant of AlexNet [22], and the network used by Zeiler\r\n& Fergus [47]. The net is pre-trained on the 1.2M image\r\nILSVRC-2012 [32] classification training subset of the Im\u0002ageNet [7] dataset, giving the network a strong initialization\r\nto facilitate faster training and prevent over-fitting to the rel\u0002atively small video datasets. When classifying center crops,\r\nthe top-1 classification accuracy is 60.2% and 57.4% for\r\nthe hybrid and Caffe reference models, respectively. In our\r\nbaseline model, T video frames are individually classified\r\nby a CNN. As in the LSTM model, whole video classifica\u0002tion is done by averaging scores across all video frames.\r\n4.1. Evaluation\r\nWe evaluate our architecture on the UCF-101 dataset\r\n[36] which consists of over 12,000 videos categorized into\r\n101 human action classes. The dataset is split into three\r\nsplits, with a little under 8,000 videos in the training set for\r\neach split. We report accuracy for split-1.\r\nFigure 1, columns 2-3, compare video classification of\r\nour proposed models (LRCN-fc6, LRCN-fc7) against the\r\nbaseline architecture for both RGB and flow inputs. Each\r\nLRCN network is trained end-to-end. To determine if end\u0002to-end training is necessary, we also train a LRCN-fc6\r\nnetwork in which only the LSTM parameters are learned.\r\nThe fully fine-tuned network increases performance from\r\n70.47% to 71.12%, demonstrating that end-to-end fine\u0002tuning is indeed beneficial. The LRCN-fc6 network yields\r\nthe best results for both RGB and flow and improves upon\r\nthe baseline network by 2.12 % and 4.75% respectively.\r\nRGB and flow networks can be combined by comput\u0002ing a weighted average of network scores as proposed in\r\n[33]. Like [33], we report two weighted averages of the\r\npredictions from the RGB and flow networks in Table 1\r\n(right). Since the flow network outperforms the RGB net\u0002work, weighting the flow network higher unsurprisingly\r\nleads to better accuracy. In this case, LRCN outperforms\r\nthe baseline single-frame model by 3.88%.\r\nThe LRCN shows clear improvement over the baseline\r\nsingle-frame system and approaches the accuracy achieved\r\nby other deep models. [33] report the results on UCF-101",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/f1313d4a-9150-4a8b-92f4-e42755905057.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7a948e3148853408e659660b0e9d1e5098d8ff5a0d615d9b06ab7db809ca2586",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 676
      },
      {
        "segments": [
          {
            "segment_id": "f1313d4a-9150-4a8b-92f4-e42755905057",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Input:\r\nVideo\r\nCRF\r\nLSTM\r\nA man juiced the orange\r\nInput:\r\nImage\r\nLSTM\r\nC\r\nN\r\nN\r\nInput:\r\nSequence \r\nof Frames\r\nOutput:\r\nLabel\r\nLSTM\r\nC\r\nN\r\nN\r\nC\r\nN\r\nN\r\nActivity Recognition Image Description Video Description\r\nA large building with a\r\nclock on the front of it\r\nOutput:\r\nSentence \r\nOutput:\r\nSentence\r\nFigure 3: Task-specific instantiations of our LRCN model for activity recognition, image description, and video description.\r\nfor activity recognition ([16, 33, 13, 2, 1]). [33, 16] both\r\npropose convolutional networks which learn filters based on\r\na stack of N input frames. Though we analyze clips of 16\r\nframes in this work, we note that the LRCN system is more\r\nflexible than [33, 16] since it is not constrained to analyz\u0002ing fixed length inputs and could potentially learn to rec\u0002ognize complex video sequences (e.g., cooking sequences\r\nas presented in 6). [1, 2] use recurrent neural networks to\r\nlearn temporal dynamics of either traditional vision features\r\n([1]) or deep features ([2]), but do not train their models\r\nend-to-end and do not pre-train on larger object recognition\r\ndatabases for important performance gains.\r\nWe explore two variants of the LRCN architecture: one\r\nin which the LSTM is placed after the first fully connected\r\nlayer of the CNN (LRCN-fc6) and another in which the\r\nLSTM is placed after the second fully connected layer of\r\nthe CNN (LRCN-fc7). We train the LRCN networks with\r\nvideo clips of 16 frames. The LRCN predicts the video class\r\nat each time step and we average these predictions for final\r\nclassification. At test time, we extract 16 frame clips with a\r\nstride of 8 frames from each video and average across clips.\r\nWe also consider both RGB and flow inputs. Flow is\r\ncomputed with [4] and transformed into a “flow image”\r\nby centering x and y flow values around 128 and mul\u0002tiplying by a scalar such that flow values fall between 0\r\nand 255. A third channel for the flow image is created\r\nby calculating the flow magnitude. The CNN base of the\r\nLRCN is a hybrid of the Caffe [14] reference model, a mi\u0002nor variant of AlexNet [22], and the network used by Zeiler\r\n& Fergus [47]. The net is pre-trained on the 1.2M image\r\nILSVRC-2012 [32] classification training subset of the Im\u0002ageNet [7] dataset, giving the network a strong initialization\r\nto facilitate faster training and prevent over-fitting to the rel\u0002atively small video datasets. When classifying center crops,\r\nthe top-1 classification accuracy is 60.2% and 57.4% for\r\nthe hybrid and Caffe reference models, respectively. In our\r\nbaseline model, T video frames are individually classified\r\nby a CNN. As in the LSTM model, whole video classifica\u0002tion is done by averaging scores across all video frames.\r\n4.1. Evaluation\r\nWe evaluate our architecture on the UCF-101 dataset\r\n[36] which consists of over 12,000 videos categorized into\r\n101 human action classes. The dataset is split into three\r\nsplits, with a little under 8,000 videos in the training set for\r\neach split. We report accuracy for split-1.\r\nFigure 1, columns 2-3, compare video classification of\r\nour proposed models (LRCN-fc6, LRCN-fc7) against the\r\nbaseline architecture for both RGB and flow inputs. Each\r\nLRCN network is trained end-to-end. To determine if end\u0002to-end training is necessary, we also train a LRCN-fc6\r\nnetwork in which only the LSTM parameters are learned.\r\nThe fully fine-tuned network increases performance from\r\n70.47% to 71.12%, demonstrating that end-to-end fine\u0002tuning is indeed beneficial. The LRCN-fc6 network yields\r\nthe best results for both RGB and flow and improves upon\r\nthe baseline network by 2.12 % and 4.75% respectively.\r\nRGB and flow networks can be combined by comput\u0002ing a weighted average of network scores as proposed in\r\n[33]. Like [33], we report two weighted averages of the\r\npredictions from the RGB and flow networks in Table 1\r\n(right). Since the flow network outperforms the RGB net\u0002work, weighting the flow network higher unsurprisingly\r\nleads to better accuracy. In this case, LRCN outperforms\r\nthe baseline single-frame model by 3.88%.\r\nThe LRCN shows clear improvement over the baseline\r\nsingle-frame system and approaches the accuracy achieved\r\nby other deep models. [33] report the results on UCF-101",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/f1313d4a-9150-4a8b-92f4-e42755905057.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7a948e3148853408e659660b0e9d1e5098d8ff5a0d615d9b06ab7db809ca2586",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 676
      },
      {
        "segments": [
          {
            "segment_id": "ddc72660-1145-4175-9201-928ec073a127",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "Input Type Weighted Average\r\nModel RGB Flow 1/2,\r\n1/21/3,2/3\r\nSingle frame 65.40 53.20 – –\r\nSingle frame (ave.) 69.00 72.20 75.71 79.04\r\nLRCN-fc6 71.12 76.95 81.97 82.92\r\nLRCN-fc7 70.68 69.36 – –\r\nTable 1: Activity recognition: Comparing single frame models\r\nto LRCN networks for activity recognition in the UCF-101 [36]\r\ndataset, with both RGB and flow inputs. Our LRCN model con\u0002sistently and strongly outperforms a model based on predictions\r\nfrom the underlying convolutional network architecture alone.\r\nby computing a weighted average between flow and RGB\r\nnetworks (86.4% for split 1 and 87.6% averaging over all\r\nsplits). Though [16] does not report numbers on the sepa\u0002rate splits of UCF-101, the average split accuracy is 65.4%\r\nwhich is substantially lower than our LRCN model.\r\n5. Image description\r\nIn contrast to activity recognition, the static image de\u0002scription task only requires a single convolutional network\r\nsince the input consists of a single image. A variety of deep\r\nand multi-modal models [8, 35, 19, 20, 15, 25, 20, 18] have\r\nbeen proposed for image description; in particular, [20, 18]\r\ncombine deep temporal models with convolutional repre\u0002sentations. [20], utilizes a “vanilla” RNN as described\r\nin Section 2, potentially making learning long-term tempo\u0002ral dependencies difficult. Contemporaneous with and most\r\nsimilar to our work is [18], which proposes a different ar\u0002chitecture that uses the hidden state of an LSTM encoder\r\nat time T as the encoded representation of the length T in\u0002put sequence. It then maps this sequence representation,\r\ncombined with the visual representation from a convnet,\r\ninto a joint space from which a separate decoder predicts\r\nwords. This is distinct from our arguably simpler architec\u0002ture, which takes as per-timestep input a copy of the static\r\ninput image, along with the previous word. We present\r\nempirical results showing that our integrated LRCN archi\u0002tecture outperforms these prior approaches, none of which\r\ncomprise an end-to-end optimizable system over a hierar\u0002chy of visual and temporal parameters.\r\nWe now describe our instantiation of the LRCN architec\u0002ture for the image description task. At each timestep, both\r\nthe image features and the previous word are provided as in\u0002puts to the sequential model, in this case a stack of LSTMs\r\n(each with 1000 hidden units), which is used to learn the\r\ndynamics of the time-varying output sequence, natural lan\u0002guage. At timestep t, the input to the bottom-most LSTM is\r\nthe embedded ground truth word from the previous timestep\r\nwt−1. For sentence generation, the input becomes a sample\r\nwˆt−1 from the model’s predicted distribution at the previous\r\ntimestep. The second LSTM in the stack fuses the outputs\r\nof the bottom-most LSTM with the image representation\r\nφV (x) to produce a joint representation of the visual and\r\nlanguage inputs up to time t. (The visual model φV (x) used\r\nin this experiment is the base Caffe [14] reference model,\r\nvery similar to the well-known AlexNet [22], pre-trained on\r\nILSVRC-2012 [32] as in Section 4.) Any further LSTMs\r\nin the stack transform the outputs of the LSTM below, and\r\nthe fourth LSTM’s outputs are inputs to the softmax which\r\nproduces a distribution over words p(wt|w1:t−1).\r\nFollowing [19], we refer to the use of the bottom-most\r\nLSTM to exclusively process the language input (with no\r\nvisual input) as the factored version of the model, and study\r\nthe importance of this by comparing it to an unfactored vari\u0002ant. See Figure 4 for details on the variants we study.\r\nWithout any explicit language modeling or defined syn\u0002tax structure, the described LRCN system learns mappings\r\nfrom pixel intensity values to natural language descriptions\r\nthat are often semantically descriptive and grammatically\r\ncorrect.\r\n5.1. Evaluation\r\nWe evaluate our image description model on both image\r\nretrieval and image annotation generation. We first show\r\nthe effectiveness of our model by quantitatively evaluating\r\nit on the image retrieval task proposed by [26] and seen in\r\n[25, 15, 35, 8, 18]. Our model is trained on the combined\r\ntraining sets of the Flickr30k [28] (28,000 training images)\r\nand COCO2014 [24] dataset (80,000 training images). We\r\nreport results on Flickr30k [28], with 30,000 images and\r\nfive sentence annotations per image. We use 1000 images\r\neach for test and validation and the remaining 28,000 for\r\ntraining.\r\nImage retrieval results are recorded in Table 2 and re\u0002port median rank, Medr, of the first retrieved ground truth\r\nimage and Recall@K, the number of sentences for which\r\nthe correct image is retrieved in the top-K. Our model\r\nconsistently outperforms the strong baselines from recent\r\nwork [18, 25, 15, 35, 8] as can be seen in Table 2. Here,\r\nwe make a note that the new OxfordNet model in [18] out\u0002performs our model on the retrieval task. However, Ox\u0002fordNet [18] utilizes a better-performing convolutional net\u0002work to get the additional edge over the base ConvNet [18].\r\nThe strength of our temporal model (and integration of the\r\ntemporal and visual models) can be more directly measured\r\nagainst the ConvNet [18] result, which uses the same base\r\nCNN architecture [22] pretrained on the same data.\r\nIn Table 3, we report image-to-caption retrieval results\r\nfor each of the architectural variants in Figure 4, as well as a\r\nfour-layer version (LRCN4f ) of the factored model. Based\r\non the facts that LRCN2f outperforms the LRCN4f model,\r\nand LRCN1u outperforms LRCN2u, there seems to be little\r\nto be gained from naively stacking additional LSTM layers\r\natop an existing network. On the other hand, a compari-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/ddc72660-1145-4175-9201-928ec073a127.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=02f62b96c5c473e656875979fc70ce0f88fccd120471795306385031edf15e54",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 885
      },
      {
        "segments": [
          {
            "segment_id": "ddc72660-1145-4175-9201-928ec073a127",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "Input Type Weighted Average\r\nModel RGB Flow 1/2,\r\n1/21/3,2/3\r\nSingle frame 65.40 53.20 – –\r\nSingle frame (ave.) 69.00 72.20 75.71 79.04\r\nLRCN-fc6 71.12 76.95 81.97 82.92\r\nLRCN-fc7 70.68 69.36 – –\r\nTable 1: Activity recognition: Comparing single frame models\r\nto LRCN networks for activity recognition in the UCF-101 [36]\r\ndataset, with both RGB and flow inputs. Our LRCN model con\u0002sistently and strongly outperforms a model based on predictions\r\nfrom the underlying convolutional network architecture alone.\r\nby computing a weighted average between flow and RGB\r\nnetworks (86.4% for split 1 and 87.6% averaging over all\r\nsplits). Though [16] does not report numbers on the sepa\u0002rate splits of UCF-101, the average split accuracy is 65.4%\r\nwhich is substantially lower than our LRCN model.\r\n5. Image description\r\nIn contrast to activity recognition, the static image de\u0002scription task only requires a single convolutional network\r\nsince the input consists of a single image. A variety of deep\r\nand multi-modal models [8, 35, 19, 20, 15, 25, 20, 18] have\r\nbeen proposed for image description; in particular, [20, 18]\r\ncombine deep temporal models with convolutional repre\u0002sentations. [20], utilizes a “vanilla” RNN as described\r\nin Section 2, potentially making learning long-term tempo\u0002ral dependencies difficult. Contemporaneous with and most\r\nsimilar to our work is [18], which proposes a different ar\u0002chitecture that uses the hidden state of an LSTM encoder\r\nat time T as the encoded representation of the length T in\u0002put sequence. It then maps this sequence representation,\r\ncombined with the visual representation from a convnet,\r\ninto a joint space from which a separate decoder predicts\r\nwords. This is distinct from our arguably simpler architec\u0002ture, which takes as per-timestep input a copy of the static\r\ninput image, along with the previous word. We present\r\nempirical results showing that our integrated LRCN archi\u0002tecture outperforms these prior approaches, none of which\r\ncomprise an end-to-end optimizable system over a hierar\u0002chy of visual and temporal parameters.\r\nWe now describe our instantiation of the LRCN architec\u0002ture for the image description task. At each timestep, both\r\nthe image features and the previous word are provided as in\u0002puts to the sequential model, in this case a stack of LSTMs\r\n(each with 1000 hidden units), which is used to learn the\r\ndynamics of the time-varying output sequence, natural lan\u0002guage. At timestep t, the input to the bottom-most LSTM is\r\nthe embedded ground truth word from the previous timestep\r\nwt−1. For sentence generation, the input becomes a sample\r\nwˆt−1 from the model’s predicted distribution at the previous\r\ntimestep. The second LSTM in the stack fuses the outputs\r\nof the bottom-most LSTM with the image representation\r\nφV (x) to produce a joint representation of the visual and\r\nlanguage inputs up to time t. (The visual model φV (x) used\r\nin this experiment is the base Caffe [14] reference model,\r\nvery similar to the well-known AlexNet [22], pre-trained on\r\nILSVRC-2012 [32] as in Section 4.) Any further LSTMs\r\nin the stack transform the outputs of the LSTM below, and\r\nthe fourth LSTM’s outputs are inputs to the softmax which\r\nproduces a distribution over words p(wt|w1:t−1).\r\nFollowing [19], we refer to the use of the bottom-most\r\nLSTM to exclusively process the language input (with no\r\nvisual input) as the factored version of the model, and study\r\nthe importance of this by comparing it to an unfactored vari\u0002ant. See Figure 4 for details on the variants we study.\r\nWithout any explicit language modeling or defined syn\u0002tax structure, the described LRCN system learns mappings\r\nfrom pixel intensity values to natural language descriptions\r\nthat are often semantically descriptive and grammatically\r\ncorrect.\r\n5.1. Evaluation\r\nWe evaluate our image description model on both image\r\nretrieval and image annotation generation. We first show\r\nthe effectiveness of our model by quantitatively evaluating\r\nit on the image retrieval task proposed by [26] and seen in\r\n[25, 15, 35, 8, 18]. Our model is trained on the combined\r\ntraining sets of the Flickr30k [28] (28,000 training images)\r\nand COCO2014 [24] dataset (80,000 training images). We\r\nreport results on Flickr30k [28], with 30,000 images and\r\nfive sentence annotations per image. We use 1000 images\r\neach for test and validation and the remaining 28,000 for\r\ntraining.\r\nImage retrieval results are recorded in Table 2 and re\u0002port median rank, Medr, of the first retrieved ground truth\r\nimage and Recall@K, the number of sentences for which\r\nthe correct image is retrieved in the top-K. Our model\r\nconsistently outperforms the strong baselines from recent\r\nwork [18, 25, 15, 35, 8] as can be seen in Table 2. Here,\r\nwe make a note that the new OxfordNet model in [18] out\u0002performs our model on the retrieval task. However, Ox\u0002fordNet [18] utilizes a better-performing convolutional net\u0002work to get the additional edge over the base ConvNet [18].\r\nThe strength of our temporal model (and integration of the\r\ntemporal and visual models) can be more directly measured\r\nagainst the ConvNet [18] result, which uses the same base\r\nCNN architecture [22] pretrained on the same data.\r\nIn Table 3, we report image-to-caption retrieval results\r\nfor each of the architectural variants in Figure 4, as well as a\r\nfour-layer version (LRCN4f ) of the factored model. Based\r\non the facts that LRCN2f outperforms the LRCN4f model,\r\nand LRCN1u outperforms LRCN2u, there seems to be little\r\nto be gained from naively stacking additional LSTM layers\r\natop an existing network. On the other hand, a compari-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/ddc72660-1145-4175-9201-928ec073a127.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=02f62b96c5c473e656875979fc70ce0f88fccd120471795306385031edf15e54",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 885
      },
      {
        "segments": [
          {
            "segment_id": "64bfd7b1-e883-4f02-913a-cbaf1eeb8d19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "CNN\r\nLSTM1\r\n(t-1) LSTM1(t)\r\nword(t) word(t-1)\r\n...\r\nCNN\r\nLSTM1\r\n(t-1)\r\nLSTM2\r\n(t-1)\r\nLSTM1\r\n(t)\r\nLSTM2\r\n(t)\r\nword(t) word(t-1)\r\n...\r\n...\r\nCNN\r\nLSTM1\r\n(t-1)\r\nLSTM2\r\n(t-1)\r\nLSTM1\r\n(t)\r\nLSTM2\r\n(t)\r\nword(t) word(t-1)\r\n...\r\n...\r\nSingle Layer Two Layers, Unfactored Two Layers, Factored\r\nLRCN1u LRCN2u LRCN2f\r\nFigure 4: Three variations of the LRCN image captioning architecture that we experimentally evaluate. We explore the effect of depth in\r\nthe LSTM stack, and the effect of the “factorization” of the modalities (also explored in [19]).\r\nR@1 R@5 R@10 Medr\r\nDeViSE [8] 6.7 21.9 32.7 25\r\nSDT-RNN [35] 8.9 29.8 41.1 16\r\nDeFrag [15] 10.3 31.4 44.5 13\r\nm-RNN [25] 12.6 31.2 41.5 16\r\nConvNet [18] 10.4 31.0 43.7 14\r\nLRCN2f (ours) 17.5 40.3 50.8 9\r\nTable 2: Image description: Caption-to-image retrieval results for\r\nthe Flickr30k [28] dataset. R@K is the average recall at rank K\r\n(high is good). Medr is the median rank (low is good). Note that\r\n[18] achieves better retrieval performance using a stronger CNN\r\narchitecture see text.\r\nR@1 R@5 R@10 Medr\r\nLRCN1u 14.1 31.3 39.7 24\r\nLRCN2u 3.8 12.0 17.9 80\r\nLRCN2f 17.5 40.3 50.8 9\r\nLRCN4f 15.8 37.1 49.5 10\r\nTable 3: Flickr30k caption-to-image retrieval results for variants\r\nof the LRCN architectures. See Figure 4 for diagrams of these ar\u0002chitectures. The results indicate that the “factorization” is impor\u0002tant to the LRCN’s retrieval performance, while simply stacking\r\nadditional LSTM layers does not seem to improve performance.\r\nson of the LRCN2f and LRCN2u results indicatees that the\r\n“factorization” in the architecture is quite important to the\r\nmodel’s retrieval performance.\r\nTo evaluate sentence generation, we use the BLEU [27]\r\nmetric which was designed for automated evaluation of sta\u0002tistical machine translation. BLEU is a modified form of\r\nprecision that compares N-gram fragments of the hypothe\u0002sis translation with multiple reference translations. We use\r\nBLEU as a measure of similarity of the descriptions. The\r\nunigram scores (B-1) account for the adequacy of (or the\r\ninformation retained) by the translation, while longer N\u0002gram scores (B-2, B-3) account for the fluency. We com\u0002pare our results with [25] (on Flickr30k), and two strong\r\nFlickr30k [28]\r\nB-1 B-2 B-3 B-4\r\nm-RNN [25] 54.79 23.92 19.52 -\r\n1NN fc8 base (ours) 37.34 18.66 9.39 4.88\r\n1NN fc7 base (ours) 38.81 20.16 10.37 5.54\r\nLRCN (ours) 58.72 39.06 25.12 16.46\r\nCOCO 2014 [24]\r\nB-1 B-2 B-3 B-4\r\n1NN fc8 base (ours) 46.04 26.20 14.95 8.70\r\n1NN fc7 base (ours) 47.47 27.55 15.96 9.36\r\nLRCN (ours) 62.79 44.19 30.41 21.00\r\nTable 4: Image description: Sentence generation results (BLEU\r\nscores (%) – ours are adjusted with the brevity penalty) for the\r\nFlickr30k [28] and COCO 2014 [24] test sets.\r\nnearest neighbor baselines computed using AlexNet fc7 and\r\nfc8 layer outputs. We used 1-nearest neighbor to retrieve the\r\nmost similar image in the training database and average the\r\nBLEU score over the captions. The results on Flickr30k are\r\nreported in Table 4. Additionally, we report results on the\r\nnew COCO2014 [24] dataset which has 80,000 training im\u0002ages, and 40,000 validation images. Similar to Flickr30k,\r\neach image is annotated with 5 or more image annotations.\r\nWe isolate 5,000 images from the validation set for testing\r\npurposes and the results are reported in Table 4.\r\nBased on the B-1 scores in Table 4, generation using\r\nLRCN performs comparably with m-RNN [25] in terms of\r\nthe information conveyed in the description. Furthermore,\r\nLRCN significantly outperforms the baselines and the m\u0002RNN with regard to the fluency (B-2, B-3) of the genera\u0002tion, indicating the LRCN retains more of the bigrams and\r\ntrigrams from the human-annotated descriptions.\r\nIn addition to standard quantitative evaluations, we also\r\nemploy Amazon Mechnical Turkers (AMT) to evaluate the\r\ngenerated sentences. Given an image and a set of descrip\u0002tions from different models, we ask Turkers to rank the\r\nsentences based on correctness, grammar and relevance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/64bfd7b1-e883-4f02-913a-cbaf1eeb8d19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bc324a8cc3fd6914c8c059f7a983300a716e3be7a3138095e01fa675fa19c61f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 628
      },
      {
        "segments": [
          {
            "segment_id": "64bfd7b1-e883-4f02-913a-cbaf1eeb8d19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "CNN\r\nLSTM1\r\n(t-1) LSTM1(t)\r\nword(t) word(t-1)\r\n...\r\nCNN\r\nLSTM1\r\n(t-1)\r\nLSTM2\r\n(t-1)\r\nLSTM1\r\n(t)\r\nLSTM2\r\n(t)\r\nword(t) word(t-1)\r\n...\r\n...\r\nCNN\r\nLSTM1\r\n(t-1)\r\nLSTM2\r\n(t-1)\r\nLSTM1\r\n(t)\r\nLSTM2\r\n(t)\r\nword(t) word(t-1)\r\n...\r\n...\r\nSingle Layer Two Layers, Unfactored Two Layers, Factored\r\nLRCN1u LRCN2u LRCN2f\r\nFigure 4: Three variations of the LRCN image captioning architecture that we experimentally evaluate. We explore the effect of depth in\r\nthe LSTM stack, and the effect of the “factorization” of the modalities (also explored in [19]).\r\nR@1 R@5 R@10 Medr\r\nDeViSE [8] 6.7 21.9 32.7 25\r\nSDT-RNN [35] 8.9 29.8 41.1 16\r\nDeFrag [15] 10.3 31.4 44.5 13\r\nm-RNN [25] 12.6 31.2 41.5 16\r\nConvNet [18] 10.4 31.0 43.7 14\r\nLRCN2f (ours) 17.5 40.3 50.8 9\r\nTable 2: Image description: Caption-to-image retrieval results for\r\nthe Flickr30k [28] dataset. R@K is the average recall at rank K\r\n(high is good). Medr is the median rank (low is good). Note that\r\n[18] achieves better retrieval performance using a stronger CNN\r\narchitecture see text.\r\nR@1 R@5 R@10 Medr\r\nLRCN1u 14.1 31.3 39.7 24\r\nLRCN2u 3.8 12.0 17.9 80\r\nLRCN2f 17.5 40.3 50.8 9\r\nLRCN4f 15.8 37.1 49.5 10\r\nTable 3: Flickr30k caption-to-image retrieval results for variants\r\nof the LRCN architectures. See Figure 4 for diagrams of these ar\u0002chitectures. The results indicate that the “factorization” is impor\u0002tant to the LRCN’s retrieval performance, while simply stacking\r\nadditional LSTM layers does not seem to improve performance.\r\nson of the LRCN2f and LRCN2u results indicatees that the\r\n“factorization” in the architecture is quite important to the\r\nmodel’s retrieval performance.\r\nTo evaluate sentence generation, we use the BLEU [27]\r\nmetric which was designed for automated evaluation of sta\u0002tistical machine translation. BLEU is a modified form of\r\nprecision that compares N-gram fragments of the hypothe\u0002sis translation with multiple reference translations. We use\r\nBLEU as a measure of similarity of the descriptions. The\r\nunigram scores (B-1) account for the adequacy of (or the\r\ninformation retained) by the translation, while longer N\u0002gram scores (B-2, B-3) account for the fluency. We com\u0002pare our results with [25] (on Flickr30k), and two strong\r\nFlickr30k [28]\r\nB-1 B-2 B-3 B-4\r\nm-RNN [25] 54.79 23.92 19.52 -\r\n1NN fc8 base (ours) 37.34 18.66 9.39 4.88\r\n1NN fc7 base (ours) 38.81 20.16 10.37 5.54\r\nLRCN (ours) 58.72 39.06 25.12 16.46\r\nCOCO 2014 [24]\r\nB-1 B-2 B-3 B-4\r\n1NN fc8 base (ours) 46.04 26.20 14.95 8.70\r\n1NN fc7 base (ours) 47.47 27.55 15.96 9.36\r\nLRCN (ours) 62.79 44.19 30.41 21.00\r\nTable 4: Image description: Sentence generation results (BLEU\r\nscores (%) – ours are adjusted with the brevity penalty) for the\r\nFlickr30k [28] and COCO 2014 [24] test sets.\r\nnearest neighbor baselines computed using AlexNet fc7 and\r\nfc8 layer outputs. We used 1-nearest neighbor to retrieve the\r\nmost similar image in the training database and average the\r\nBLEU score over the captions. The results on Flickr30k are\r\nreported in Table 4. Additionally, we report results on the\r\nnew COCO2014 [24] dataset which has 80,000 training im\u0002ages, and 40,000 validation images. Similar to Flickr30k,\r\neach image is annotated with 5 or more image annotations.\r\nWe isolate 5,000 images from the validation set for testing\r\npurposes and the results are reported in Table 4.\r\nBased on the B-1 scores in Table 4, generation using\r\nLRCN performs comparably with m-RNN [25] in terms of\r\nthe information conveyed in the description. Furthermore,\r\nLRCN significantly outperforms the baselines and the m\u0002RNN with regard to the fluency (B-2, B-3) of the genera\u0002tion, indicating the LRCN retains more of the bigrams and\r\ntrigrams from the human-annotated descriptions.\r\nIn addition to standard quantitative evaluations, we also\r\nemploy Amazon Mechnical Turkers (AMT) to evaluate the\r\ngenerated sentences. Given an image and a set of descrip\u0002tions from different models, we ask Turkers to rank the\r\nsentences based on correctness, grammar and relevance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/64bfd7b1-e883-4f02-913a-cbaf1eeb8d19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bc324a8cc3fd6914c8c059f7a983300a716e3be7a3138095e01fa675fa19c61f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 628
      },
      {
        "segments": [
          {
            "segment_id": "5aee4ac8-071e-4afc-ab57-f3c74d1a6751",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Correctness Grammar Relevance\r\nTreeTalk [23] 4.08 4.35 3.98\r\nOxfordNet [18] 3.71 3.46 3.70\r\nNN [18] 3.44 3.20 3.49\r\nLRCN fc8 (ours) 3.74 3.19 3.72\r\nLRCN ft (ours) 3.47 3.01 3.50\r\nCaptions 2.55 3.72 2.59\r\nTable 5: Image description: Human evaluator rankings from 1-6\r\n(low is good) averaged for each method and criterion. We eval\u0002uated on 785 Flickr images selected by the authors of [18] for\r\nthe purposes of comparison against this similar contemporary ap\u0002proach.\r\nWe compared sentences from our model to the ones made\r\npublicly available by [18]. As seen in Table 5, our fine\u0002tuned (ft) LRCN model performs on par with the Nearest\r\nNeighbour (NN) on correctness and relevance, and better\r\non grammar. We show example sentence generations in Fig\u0002ure 6.\r\n6. Video description\r\nIn video description we must generate a variable length\r\nstream of words, similar to Section 5. [11, 30, 17, 3, 6, 17,\r\n40, 41] propose methods for generating sentence descrip\u0002tions for video, but to our knowledge we present the first\r\napplication of deep models to the vision description task.\r\nThe LSTM framework allows us to model the video as\r\na variable length input stream as discussed in Section 3.\r\nHowever, due to limitations of available video description\r\ndatasets we take a different path. We rely on more “tra\u0002ditional” activity and video recognition processing for the\r\ninput and use LSTMs for generating a sentence.\r\nWe first distinguish the following architectures for video\r\ndescription (see Figure 5). For each architecture, we assume\r\nwe have predictions of objects, subjects, and verbs present\r\nin the video from a CRF based on the full video input. In\r\nthis way, we observe the video as whole at each time step,\r\nnot incrementally frame by frame.\r\n(a) LSTM encoder & decoder with CRF max. (Fig\u0002ure 5(a)) The first architecture is motivated by the video de\u0002scription approach presented in [30]. They first recognize a\r\nsemantic representation of the video using the maximum a\r\nposterior estimate (MAP) of a CRF taking in video features\r\nas unaries. This representation, e.g. hperson,cut,cutting\r\nboardi, is then concatenated to a input sentence (person cut\r\ncutting board) which is translated to a natural sentence (a\r\nperson cuts on the board) using phrase-based statistical ma\u0002chine translation (SMT) [21]. We replace the SMT with an\r\nLSTM, which has shown state-of-the-art performance for\r\nmachine translation between languages [38, 5]. The archi\u0002tecture (shown in Figure 5(a)) has an encoder LSTM (or\u0002Architecture Input BLEU\r\nSMT [30] CRF max 24.9\r\nSMT [29] CRF prob 26.9\r\n(a) LSTM Encoder-Decoder (ours) CRF max 25.3\r\n(b) LSTM Decoder (ours) CRF max 27.4\r\n(c) LSTM Decoder (ours) CRF prob 28.8\r\nTable 6: Video description: Results on detailed description of\r\nTACoS multilevel[29], in %, see Section 6 for details.\r\nange) which encodes the one-hot vector (binary index vec\u0002tor in a vocabulary) of the input sentence as done in [38].\r\nThis allows for variable-length inputs. (Note that the input\r\nsentence might have a different number of words than el\u0002ements of the semantic representation.) At the end of the\r\nencoder stage, the final hidden unit must remember all nec\u0002essary information before being input into the decoder stage\r\n(pink) in which the hidden representation is decoded into a\r\nsentence, one word at each time step. We use the same two\u0002layer LSTM for encoding and decoding.\r\n(b) LSTM decoder with CRF max. (Figure 5(b)) In\r\nthis variant we exploit that the semantic representation can\r\nbe encoded as a single fixed length vector. We provide the\r\nentire visual input representation at each time step to the\r\nLSTM, analogous to how an entire image is provided as an\r\ninput to the LSTM in image description.\r\n(c) LSTM decoder with CRF prob. (Figure 5(c)) A\r\nbenefit of using LSTMs for machine translation compared\r\nto phrase-based SMT [21] is that it can naturally incorpo\u0002rate probability vectors during training and test time which\r\nallows the LSTM to learn uncertainties in visual generation\r\nrather than relying on MAP estimates. The architecture is\r\nthe the same as in (b), but we replace max predictions with\r\nprobability distributions.\r\n6.1. Evaluation\r\nWe evaluate our approach on the TACoS multilevel\r\n[29] dataset, which has 44,762 video/sentence pairs (about\r\n40,000 for training/validation). We compare to [30] who\r\nuse max prediction as well as a variant presented in [29]\r\nwhich takes CRF probabilities at test time and uses a word\r\nlattice to find an optimal sentence prediction. Since we use\r\nthe max prediction as well as the probability scores pro\u0002vided by [29], we have an identical visual representation.\r\n[29] uses dense trajectories [43] and SIFT features as well\r\nas temporal context reasoning modeled in a CRF.\r\nTable 6 shows the BLEU-4 score. The results show that\r\n(1) the LSTM outperforms an SMT-based approach to video\r\ndescription; (2) the simpler decoder architecture (b) and (c)\r\nachieve better performance than (a), likely because the in\u0002put does not need to be memorized; and (3) our approach\r\nachieves 28.8%, clearly outperforming the best reported\r\nnumber of 26.9% on TACoS multilevel by [29].\r\nMore broadly, these results show that our architecture",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/5aee4ac8-071e-4afc-ab57-f3c74d1a6751.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7067354d524c8ff1a737d8e466c4460aec30019daf3da334ad40d5de74ea7335",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 830
      },
      {
        "segments": [
          {
            "segment_id": "5aee4ac8-071e-4afc-ab57-f3c74d1a6751",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Correctness Grammar Relevance\r\nTreeTalk [23] 4.08 4.35 3.98\r\nOxfordNet [18] 3.71 3.46 3.70\r\nNN [18] 3.44 3.20 3.49\r\nLRCN fc8 (ours) 3.74 3.19 3.72\r\nLRCN ft (ours) 3.47 3.01 3.50\r\nCaptions 2.55 3.72 2.59\r\nTable 5: Image description: Human evaluator rankings from 1-6\r\n(low is good) averaged for each method and criterion. We eval\u0002uated on 785 Flickr images selected by the authors of [18] for\r\nthe purposes of comparison against this similar contemporary ap\u0002proach.\r\nWe compared sentences from our model to the ones made\r\npublicly available by [18]. As seen in Table 5, our fine\u0002tuned (ft) LRCN model performs on par with the Nearest\r\nNeighbour (NN) on correctness and relevance, and better\r\non grammar. We show example sentence generations in Fig\u0002ure 6.\r\n6. Video description\r\nIn video description we must generate a variable length\r\nstream of words, similar to Section 5. [11, 30, 17, 3, 6, 17,\r\n40, 41] propose methods for generating sentence descrip\u0002tions for video, but to our knowledge we present the first\r\napplication of deep models to the vision description task.\r\nThe LSTM framework allows us to model the video as\r\na variable length input stream as discussed in Section 3.\r\nHowever, due to limitations of available video description\r\ndatasets we take a different path. We rely on more “tra\u0002ditional” activity and video recognition processing for the\r\ninput and use LSTMs for generating a sentence.\r\nWe first distinguish the following architectures for video\r\ndescription (see Figure 5). For each architecture, we assume\r\nwe have predictions of objects, subjects, and verbs present\r\nin the video from a CRF based on the full video input. In\r\nthis way, we observe the video as whole at each time step,\r\nnot incrementally frame by frame.\r\n(a) LSTM encoder & decoder with CRF max. (Fig\u0002ure 5(a)) The first architecture is motivated by the video de\u0002scription approach presented in [30]. They first recognize a\r\nsemantic representation of the video using the maximum a\r\nposterior estimate (MAP) of a CRF taking in video features\r\nas unaries. This representation, e.g. hperson,cut,cutting\r\nboardi, is then concatenated to a input sentence (person cut\r\ncutting board) which is translated to a natural sentence (a\r\nperson cuts on the board) using phrase-based statistical ma\u0002chine translation (SMT) [21]. We replace the SMT with an\r\nLSTM, which has shown state-of-the-art performance for\r\nmachine translation between languages [38, 5]. The archi\u0002tecture (shown in Figure 5(a)) has an encoder LSTM (or\u0002Architecture Input BLEU\r\nSMT [30] CRF max 24.9\r\nSMT [29] CRF prob 26.9\r\n(a) LSTM Encoder-Decoder (ours) CRF max 25.3\r\n(b) LSTM Decoder (ours) CRF max 27.4\r\n(c) LSTM Decoder (ours) CRF prob 28.8\r\nTable 6: Video description: Results on detailed description of\r\nTACoS multilevel[29], in %, see Section 6 for details.\r\nange) which encodes the one-hot vector (binary index vec\u0002tor in a vocabulary) of the input sentence as done in [38].\r\nThis allows for variable-length inputs. (Note that the input\r\nsentence might have a different number of words than el\u0002ements of the semantic representation.) At the end of the\r\nencoder stage, the final hidden unit must remember all nec\u0002essary information before being input into the decoder stage\r\n(pink) in which the hidden representation is decoded into a\r\nsentence, one word at each time step. We use the same two\u0002layer LSTM for encoding and decoding.\r\n(b) LSTM decoder with CRF max. (Figure 5(b)) In\r\nthis variant we exploit that the semantic representation can\r\nbe encoded as a single fixed length vector. We provide the\r\nentire visual input representation at each time step to the\r\nLSTM, analogous to how an entire image is provided as an\r\ninput to the LSTM in image description.\r\n(c) LSTM decoder with CRF prob. (Figure 5(c)) A\r\nbenefit of using LSTMs for machine translation compared\r\nto phrase-based SMT [21] is that it can naturally incorpo\u0002rate probability vectors during training and test time which\r\nallows the LSTM to learn uncertainties in visual generation\r\nrather than relying on MAP estimates. The architecture is\r\nthe the same as in (b), but we replace max predictions with\r\nprobability distributions.\r\n6.1. Evaluation\r\nWe evaluate our approach on the TACoS multilevel\r\n[29] dataset, which has 44,762 video/sentence pairs (about\r\n40,000 for training/validation). We compare to [30] who\r\nuse max prediction as well as a variant presented in [29]\r\nwhich takes CRF probabilities at test time and uses a word\r\nlattice to find an optimal sentence prediction. Since we use\r\nthe max prediction as well as the probability scores pro\u0002vided by [29], we have an identical visual representation.\r\n[29] uses dense trajectories [43] and SIFT features as well\r\nas temporal context reasoning modeled in a CRF.\r\nTable 6 shows the BLEU-4 score. The results show that\r\n(1) the LSTM outperforms an SMT-based approach to video\r\ndescription; (2) the simpler decoder architecture (b) and (c)\r\nachieve better performance than (a), likely because the in\u0002put does not need to be memorized; and (3) our approach\r\nachieves 28.8%, clearly outperforming the best reported\r\nnumber of 26.9% on TACoS multilevel by [29].\r\nMore broadly, these results show that our architecture",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/5aee4ac8-071e-4afc-ab57-f3c74d1a6751.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7067354d524c8ff1a737d8e466c4460aec30019daf3da334ad40d5de74ea7335",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 830
      },
      {
        "segments": [
          {
            "segment_id": "322a2a15-617f-4d22-86dd-ba2eb29b05ad",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "[0.2,0.8,0,0] [0.1,0.7,0.3] [0.3,0.3,0.4]\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\ninput Visual Input sentence Predictions\r\nCRF\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nA\r\nman\r\ncuts\r\non\r\nthe\r\nboard\r\nEncoder\r\nDecoder\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nVisual Input CRF max Predictions\r\nCRF\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nA\r\nman\r\ncuts\r\non\r\nthe\r\nboard\r\nDecoder\r\n[0,1,0,0] [0,1,0] [0,0,1]\r\n[0,1,0,0] [0,1,0] [0,0,1]\r\n[0,1,0,0] [0,1,0] [0,0,1]\r\n[0,1,0,0] [0,1,0] [0,0,1]\r\n[0,1,0,0] [0,1,0] [0,0,1]\r\n[0,1,0,0] [0,1,0] [0,0,1]\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nVisual Input CRF prob Predictions\r\nCRF\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nLSTM\r\nA\r\nman\r\ncuts\r\non\r\nthe\r\nboard\r\nDecoder\r\n(a) (b) (c)\r\n[0.2,0.8,0,0] [0.1,0.7,0.3] [0.3,0.3,0.4]\r\n[0.2,0.8,0.0] [0.1,0.7,0.3] [0.3,0.3,0.4]\r\n[0.2,0.8,0,0] [0.1,0.7,0.3] [0.3,0.3,0.4]\r\n[0.2,0.8,0,0] [0.1,0.7,0.3] [0.3,0.3,0.4]\r\n[0.2,0.8,0,0] [0.1,0.7,0.3] [0.3,0.3,0.4]\r\nLSTM LSTM\r\nboard\r\ncutting\r\ncut\r\nperson\r\none\u0002hot\u0002vector\r\n[0,1,0,0,0,0]\r\n[0,0,0,1,0,0]\r\n[0,0,0,0,1,0]\r\n[1,0,0,0,0,0]\r\nFigure 5: Our approaches to video description. (a) LSTM encoder & decoder with CRF max (b) LSTM decoder with CRF max (c) LSTM\r\ndecoder with CRF probabilities. (For larger figure zoom or see supplemental).\r\nis not restricted to deep neural networks inputs but can be\r\ncleanly integrated with other fixed or variable length inputs\r\nfrom other vision systems.\r\n7. Conclusions\r\nWe’ve presented LRCN, a class of models that is both\r\nspatially and temporally deep, and has the flexibility to be\r\napplied to a variety of vision tasks involving sequential\r\ninputs and outputs. Our results consistently demonstrate\r\nthat by learning sequential dynamics with a deep sequence\r\nmodel, we can improve on previous methods which learn a\r\ndeep hierarchy of parameters only in the visual domain, and\r\non methods which take a fixed visual representation of the\r\ninput and only learn the dynamics of the output sequence.\r\nAs the field of computer vision matures beyond tasks\r\nwith static input and predictions, we envision that “dou\u0002bly deep” sequence modeling tools like LRCN will soon\r\nbecome central pieces of most vision systems, as convo\u0002lutional architectures recently have. The ease with which\r\nthese tools can be incorporated into existing visual recog\u0002nition pipelines makes them a natural choice for perceptual\r\nproblems with time-varying visual input or sequential out\u0002puts, which these methods are able to produce with little\r\ninput preprocessing and no hand-designed features.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/322a2a15-617f-4d22-86dd-ba2eb29b05ad.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=45f965b14a85d6fbf033d8644c1e91a3d77fa4cacc642116fb616a548010bfdb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 359
      },
      {
        "segments": [
          {
            "segment_id": "a90b6635-6f2c-4fe4-bf7b-b35191555468",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "A female tennis player in action on the\r\ncourt.\r\nA group of young men playing a game\r\nof soccer\r\nA man riding a wave on top of a surf\u0002board.\r\nA baseball game in progress with the\r\nbatter up to plate.\r\nA brown bear standing on top of a lush\r\ngreen field.\r\nA person holding a cell phone in their\r\nhand.\r\nA close up of a person brushing his\r\nteeth.\r\nA woman laying on a bed in a bedroom. A black and white cat is sitting on a\r\nchair.\r\nA large clock mounted to the side of a\r\nbuilding.\r\nA bunch of fruit that are sitting on a ta\u0002ble.A toothbrush holder sitting on top of a\r\nwhite sink.\r\nFigure 6: Image description: images with corresponding captions generated by our finetuned LRCN model. These are images 1-12 of our\r\nrandomly chosen validation set from COCO 2014 [24] (see Figure 7 for images 13-24). We used beam search with a beam size of 5 to\r\ngenerate the sentences, and display the top (highest likelihood) result above.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/a90b6635-6f2c-4fe4-bf7b-b35191555468.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7fba86fac369eb6706d83f68ba72111df1329f4d53ebcbb9fb1537d695f5bd1e",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c1f110bc-ba38-458d-b3b6-3b0f15636823",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "A close up of a hot dog on a bun. A boat on a river with a bridge in the\r\nbackground.\r\nA bath room with a toilet and a bath tub.\r\nA man that is standing in the dirt with a\r\nbat.\r\nA white toilet sitting in a bathroom next\r\nto a trash can.\r\nBlack and white photograph of a\r\nwoman sitting on a bench.\r\nA group of people walking down a\r\nstreet next to a traffic light.\r\nAn elephant standing in a grassy area\r\nwith tree in the background.\r\nA close up of a plate of food with broc\u0002coli.\r\nA bus parked on the side of a street next\r\nto a building.\r\nA group of people standing around a ta\u0002ble.A vase filled with flower sitting on a ta\u0002ble.\r\nFigure 7: Image description: images 13-24 (and LRCN-generated captions) from the set described in Figure 6.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/c1f110bc-ba38-458d-b3b6-3b0f15636823.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4b3b891c56d2b53cd60688d04f4387815a9bcb384d5cd1a03a245877d8ffb913",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 318
      },
      {
        "segments": [
          {
            "segment_id": "cef7ac85-1815-4b54-ac73-db5ae38dacea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "Acknowledgements\r\nThe authors thank Oriol Vinyals for valuable advice and\r\nhelpful discussion throughout this work. This work was\r\nsupported in part by DARPA’s MSEE and SMISC pro\u0002grams, NSF awards IIS-1427425, and IIS-1212798, IIS\u00021116411, Toyota, and the Berkeley Vision and Learning\r\nCenter. Marcus Rohrbach was supported by a fellowship\r\nwithin the FITweltweit-Program of the German Academic\r\nExchange Service (DAAD).\r\nReferences\r\n[1] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and\r\nA. Baskurt. Action classification in soccer videos with long\r\nshort-term memory recurrent neural networks. In ICANN.\r\n2010. 5\r\n[2] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and\r\nA. Baskurt. Sequential deep learning for human action\r\nrecognition. In Human Behavior Understanding. 2011. 2,\r\n5\r\n[3] A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickin\u0002son, S. Fidler, A. Michaux, S. Mussman, S. Narayanaswamy,\r\nD. Salvi, L. Schmidt, J. Shangguan, J. M. Siskind, J. Wag\u0002goner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. Video in sen\u0002tences out. In UAI, 2012. 8\r\n[4] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac\u0002curacy optical flow estimation based on a theory for warping.\r\nIn ECCV. 2004. 5\r\n[5] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. ¨\r\nOn the properties of neural machine translation: Encoder\u0002decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\r\n2, 3, 8\r\n[6] P. Das, C. Xu, R. Doell, and J. Corso. Thousand frames\r\nin just a few words: Lingual description of videos through\r\nlatent topics and sparse object stitching. In CVPR, 2013. 8\r\n[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei\u0002Fei. ImageNet: A large-scale hierarchical image database.\r\nIn CVPR, 2009. 5\r\n[8] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ran\u0002zato, and T. Mikolov. DeViSE: A deep visual-semantic em\u0002bedding model. In NIPS, 2013. 6, 7\r\n[9] A. Graves. Generating sequences with recurrent neural net\u0002works. arXiv preprint arXiv:1308.0850, 2013. 3\r\n[10] A. Graves and N. Jaitly. Towards end-to-end speech recog\u0002nition with recurrent neural networks. In ICML, 2014. 2,\r\n3\r\n[11] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,\r\nS. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.\r\nYouTube2Text: Recognizing and describing arbitrary activ\u0002ities using semantic hierarchies and zero-shoot recognition.\r\nIn ICCV, 2013. 8\r\n[12] S. Hochreiter and J. Schmidhuber. Long short-term memory.\r\nNeural Computation, 1997. 2, 3\r\n[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neu\u0002ral networks for human action recognition. Pattern Analysis\r\nand Machine Intelligence, IEEE Transactions on, 35(1):221–\r\n231, 2013. 2, 5\r\n[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir\u0002shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\r\narchitecture for fast feature embedding. In ACM MM, 2014.\r\n2, 5, 6\r\n[15] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em\u0002beddings for bidirectional image sentence mapping. NIPS,\r\n2014. 6, 7\r\n[16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\r\nand L. Fei-Fei. Large-scale video classification with convo\u0002lutional neural networks. In CVPR, 2014. 2, 5, 6\r\n[17] M. U. G. Khan, L. Zhang, and Y. Gotoh. Human focused\r\nvideo description. In Proceedings of the IEEE International\r\nConference on Computer Vision Workshops (ICCV Work\u0002shops), 2011. 8\r\n[18] R. Kiros, R. Salakhuditnov, and R. S. Zemel. Unifying\r\nvisual-semantic embeddings with multimodal neural lan\u0002guage models. arXiv preprint arXiv:1411.2539, 2014. 6,\r\n7, 8\r\n[19] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neu\u0002ral language models. In ICML, 2014. 6, 7\r\n[20] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neu\u0002ral language models. In Proc. NIPS Deep Learning Work\u0002shop, 2013. 6\r\n[21] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed\u0002erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,\r\nC. Dyer, O. Bojar, A. Constantin, and E. Herbst. Moses:\r\nOpen source toolkit for statistical machine translation. In\r\nACL, 2007. 8\r\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\r\nclassification with deep convolutional neural networks. In\r\nNIPS, 2012. 4, 5, 6\r\n[23] P. Kuznetsova, V. Ordonez, T. L. Berg, U. C. Hill, and\r\nY. Choi. Treetalk: Composition and compression of trees\r\nfor image descriptions. Transactions of the Association for\r\nComputational Linguistics, 2(10):351–362, 2014. 8\r\n[24] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra\u0002manan, P. Dollar, and C. L. Zitnick. Microsoft coco: Com- ´\r\nmon objects in context. arXiv preprint arXiv:1405.0312,\r\n2014. 6, 7, 10\r\n[25] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain\r\nimages with multimodal recurrent neural networks. arXiv\r\npreprint arXiv:1410.1090, 2014. 6, 7\r\n[26] P. Y. Micah Hodosh and J. Hockenmaier. Framing image\r\ndescription as a ranking task: Data, models and evaluation\r\nmetrics. JAIR, 47:853–899, 2013. 6\r\n[27] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a\r\nmethod for automatic evaluation of machine translation. In\r\nACL, 2002. 7\r\n[28] M. H. Peter Young, Alice Lai and J. Hockenmaier. From im\u0002age descriptions to visual denotations: New similarity met\u0002rics for semantic inference over event descriptions. TACL,\r\n2:67–78, 2014. 6, 7\r\n[29] A. Rohrbach, M. Rohrbach, W. Qiu, A. Friedrich, M. Pinkal,\r\nand B. Schiele. Coherent multi-sentence video description\r\nwith variable level of detail. In GCPR, 2014. 8\r\n[30] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and\r\nB. Schiele. Translating video content to natural language\r\ndescriptions. In ICCV, 2013. 2, 8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/cef7ac85-1815-4b54-ac73-db5ae38dacea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3f1a65e205933ad0be92ba2e6d4aca630ebc880112c346db823d1630b034046f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 862
      },
      {
        "segments": [
          {
            "segment_id": "cef7ac85-1815-4b54-ac73-db5ae38dacea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "Acknowledgements\r\nThe authors thank Oriol Vinyals for valuable advice and\r\nhelpful discussion throughout this work. This work was\r\nsupported in part by DARPA’s MSEE and SMISC pro\u0002grams, NSF awards IIS-1427425, and IIS-1212798, IIS\u00021116411, Toyota, and the Berkeley Vision and Learning\r\nCenter. Marcus Rohrbach was supported by a fellowship\r\nwithin the FITweltweit-Program of the German Academic\r\nExchange Service (DAAD).\r\nReferences\r\n[1] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and\r\nA. Baskurt. Action classification in soccer videos with long\r\nshort-term memory recurrent neural networks. In ICANN.\r\n2010. 5\r\n[2] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and\r\nA. Baskurt. Sequential deep learning for human action\r\nrecognition. In Human Behavior Understanding. 2011. 2,\r\n5\r\n[3] A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickin\u0002son, S. Fidler, A. Michaux, S. Mussman, S. Narayanaswamy,\r\nD. Salvi, L. Schmidt, J. Shangguan, J. M. Siskind, J. Wag\u0002goner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. Video in sen\u0002tences out. In UAI, 2012. 8\r\n[4] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac\u0002curacy optical flow estimation based on a theory for warping.\r\nIn ECCV. 2004. 5\r\n[5] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. ¨\r\nOn the properties of neural machine translation: Encoder\u0002decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\r\n2, 3, 8\r\n[6] P. Das, C. Xu, R. Doell, and J. Corso. Thousand frames\r\nin just a few words: Lingual description of videos through\r\nlatent topics and sparse object stitching. In CVPR, 2013. 8\r\n[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei\u0002Fei. ImageNet: A large-scale hierarchical image database.\r\nIn CVPR, 2009. 5\r\n[8] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ran\u0002zato, and T. Mikolov. DeViSE: A deep visual-semantic em\u0002bedding model. In NIPS, 2013. 6, 7\r\n[9] A. Graves. Generating sequences with recurrent neural net\u0002works. arXiv preprint arXiv:1308.0850, 2013. 3\r\n[10] A. Graves and N. Jaitly. Towards end-to-end speech recog\u0002nition with recurrent neural networks. In ICML, 2014. 2,\r\n3\r\n[11] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,\r\nS. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.\r\nYouTube2Text: Recognizing and describing arbitrary activ\u0002ities using semantic hierarchies and zero-shoot recognition.\r\nIn ICCV, 2013. 8\r\n[12] S. Hochreiter and J. Schmidhuber. Long short-term memory.\r\nNeural Computation, 1997. 2, 3\r\n[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neu\u0002ral networks for human action recognition. Pattern Analysis\r\nand Machine Intelligence, IEEE Transactions on, 35(1):221–\r\n231, 2013. 2, 5\r\n[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir\u0002shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\r\narchitecture for fast feature embedding. In ACM MM, 2014.\r\n2, 5, 6\r\n[15] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em\u0002beddings for bidirectional image sentence mapping. NIPS,\r\n2014. 6, 7\r\n[16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\r\nand L. Fei-Fei. Large-scale video classification with convo\u0002lutional neural networks. In CVPR, 2014. 2, 5, 6\r\n[17] M. U. G. Khan, L. Zhang, and Y. Gotoh. Human focused\r\nvideo description. In Proceedings of the IEEE International\r\nConference on Computer Vision Workshops (ICCV Work\u0002shops), 2011. 8\r\n[18] R. Kiros, R. Salakhuditnov, and R. S. Zemel. Unifying\r\nvisual-semantic embeddings with multimodal neural lan\u0002guage models. arXiv preprint arXiv:1411.2539, 2014. 6,\r\n7, 8\r\n[19] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neu\u0002ral language models. In ICML, 2014. 6, 7\r\n[20] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neu\u0002ral language models. In Proc. NIPS Deep Learning Work\u0002shop, 2013. 6\r\n[21] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed\u0002erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,\r\nC. Dyer, O. Bojar, A. Constantin, and E. Herbst. Moses:\r\nOpen source toolkit for statistical machine translation. In\r\nACL, 2007. 8\r\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\r\nclassification with deep convolutional neural networks. In\r\nNIPS, 2012. 4, 5, 6\r\n[23] P. Kuznetsova, V. Ordonez, T. L. Berg, U. C. Hill, and\r\nY. Choi. Treetalk: Composition and compression of trees\r\nfor image descriptions. Transactions of the Association for\r\nComputational Linguistics, 2(10):351–362, 2014. 8\r\n[24] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra\u0002manan, P. Dollar, and C. L. Zitnick. Microsoft coco: Com- ´\r\nmon objects in context. arXiv preprint arXiv:1405.0312,\r\n2014. 6, 7, 10\r\n[25] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain\r\nimages with multimodal recurrent neural networks. arXiv\r\npreprint arXiv:1410.1090, 2014. 6, 7\r\n[26] P. Y. Micah Hodosh and J. Hockenmaier. Framing image\r\ndescription as a ranking task: Data, models and evaluation\r\nmetrics. JAIR, 47:853–899, 2013. 6\r\n[27] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a\r\nmethod for automatic evaluation of machine translation. In\r\nACL, 2002. 7\r\n[28] M. H. Peter Young, Alice Lai and J. Hockenmaier. From im\u0002age descriptions to visual denotations: New similarity met\u0002rics for semantic inference over event descriptions. TACL,\r\n2:67–78, 2014. 6, 7\r\n[29] A. Rohrbach, M. Rohrbach, W. Qiu, A. Friedrich, M. Pinkal,\r\nand B. Schiele. Coherent multi-sentence video description\r\nwith variable level of detail. In GCPR, 2014. 8\r\n[30] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and\r\nB. Schiele. Translating video content to natural language\r\ndescriptions. In ICCV, 2013. 2, 8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/cef7ac85-1815-4b54-ac73-db5ae38dacea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3f1a65e205933ad0be92ba2e6d4aca630ebc880112c346db823d1630b034046f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 862
      },
      {
        "segments": [
          {
            "segment_id": "9fe9fef9-a682-4c4f-80d1-a5e5420a29f8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 13,
            "page_width": 612,
            "page_height": 792,
            "content": "[31] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn\u0002ing internal representations by error propagation. Technical\r\nreport, DTIC Document, 1985. 2\r\n[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\r\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\r\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\r\nRecognition Challenge, 2014. 5, 6\r\n[33] K. Simonyan and A. Zisserman. Two-stream convolutional\r\nnetworks for action recognition in videos. arXiv preprint\r\narXiv:1406.2199, 2014. 2, 5, 6\r\n[34] K. Simonyan and A. Zisserman. Very deep convolutional\r\nnetworks for large-scale image recognition. arXiv preprint\r\narXiv:1409.1556, 2014. 4\r\n[35] R. Socher, Q. Le, C. Manning, and A. Ng. Grounded com\u0002positional semantics for finding and describing images with\r\nsentences. In NIPS Deep Learning Workshop, 2013. 6, 7\r\n[36] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset\r\nof 101 human actions classes from videos in the wild. arXiv\r\npreprint arXiv:1212.0402, 2012. 5, 6\r\n[37] I. Sutskever, J. Martens, and G. E. Hinton. Generating text\r\nwith recurrent neural networks. In ICML, 2011. 2\r\n[38] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence\r\nlearning with neural networks. In NIPS, 2014. 2, 3, 8\r\n[39] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\r\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi\u0002novich. Going deeper with convolutions. arXiv preprint\r\narXiv:1409.4842, 2014. 4\r\n[40] C. C. Tan, Y.-G. Jiang, and C.-W. Ngo. Towards textually\r\ndescribing complex video contents with audio-visual concept\r\nclassifiers. In MM, 2011. 8\r\n[41] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko,\r\nand R. J. Mooney. Integrating language and vision to gen\u0002erate natural language descriptions of videos in the wild. In\r\nCOLING, 2014. 8\r\n[42] O. Vinyals, S. V. Ravuri, and D. Povey. Revisiting recurrent\r\nneural networks for robust ASR. In ICASSP, 2012. 2\r\n[43] H. Wang, A. Klaser, C. Schmid, and C. Liu. Dense trajecto- ¨\r\nries and motion boundary descriptors for action recognition.\r\nIJCV, 2013. 8\r\n[44] R. J. Williams and D. Zipser. A learning algorithm for con\u0002tinually running fully recurrent neural networks. Neural\r\nComputation, 1989. 2\r\n[45] W. Zaremba and I. Sutskever. Learning to execute. arXiv\r\npreprint arXiv:1410.4615, 2014. 3\r\n[46] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neu\u0002ral network regularization. arXiv preprint arXiv:1409.2329,\r\n2014. 2, 4\r\n[47] M. D. Zeiler and R. Fergus. Visualizing and understanding\r\nconvolutional networks. In ECCV. 2014. 5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1569b1e7-9963-43a0-a405-cc0324c4271f/images/9fe9fef9-a682-4c4f-80d1-a5e5420a29f8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041459Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6646adaa8d5cbb02c28efed95df037af83b7450c770a44571630508fd5d4a28b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 392
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "\"Long-term Recurrent Convolutional Networks for Visual Recognition and Description\"\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "2014"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "River, bridge, bathroom, baseball field, grassy area, street, tennis court, ocean, field.\n"
        }
      ]
    }
  }
}