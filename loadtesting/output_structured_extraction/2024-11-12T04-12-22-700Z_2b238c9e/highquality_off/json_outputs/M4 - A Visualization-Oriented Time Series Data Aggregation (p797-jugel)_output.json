{
  "file_name": "M4 - A Visualization-Oriented Time Series Data Aggregation (p797-jugel).pdf",
  "task_id": "0eb249eb-0919-46a9-8d0e-44cea543b5fa",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "d08b370a-f296-4991-80cc-0b65a7f10ebe",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "M4: A Visualization-Oriented Time Series Data Aggregation\r\nUwe Jugel, Zbigniew Jerzak,\r\nGregor Hackenbroich\r\nSAP AG\r\nChemnitzer Str. 48, 01187 Dresden, Germany\r\n{firstname}.{lastname}@sap.com\r\nVolker Markl\r\nTechnische Universitat Berlin ¨\r\nStraße des 17. Juni 135\r\n10623 Berlin, Germany\r\nvolker.markl@tu-berlin.de\r\nABSTRACT\r\nVisual analysis of high-volume time series data is ubiquitous\r\nin many industries, including finance, banking, and discrete\r\nmanufacturing. Contemporary, RDBMS-based systems for\r\nvisualization of high-volume time series data have difficulty\r\nto cope with the hard latency requirements and high inges\u0002tion rates of interactive visualizations. Existing solutions\r\nfor lowering the volume of time series data disregard the se\u0002mantics of visualizations and result in visualization errors.\r\nIn this work, we introduce M4, an aggregation-based time\r\nseries dimensionality reduction technique that provides error\u0002free visualizations at high data reduction rates. Focusing on\r\nline charts, as the predominant form of time series visualiza\u0002tion, we explain in detail the drawbacks of existing data re\u0002duction techniques and how our approach outperforms state\r\nof the art, by respecting the process of line rasterization.\r\nWe describe how to incorporate aggregation-based dimen\u0002sionality reduction at the query level in a visualization\u0002driven query rewriting system. Our approach is generic and\r\napplicable to any visualization system that uses an RDBMS\r\nas data source. Using real world data sets from high tech\r\nmanufacturing, stock markets, and sports analytics domains\r\nwe demonstrate that our visualization-oriented data aggre\u0002gation can reduce data volumes by up to two orders of mag\u0002nitude, while preserving perfect visualizations.\r\nKeywords: Relational databases, Query rewriting,\r\nDimensionality reduction, Line rasterization\r\n1. INTRODUCTION\r\nEnterprises are gathering petabytes of data in public and\r\nprivate clouds, with time series data originating from var\u0002ious sources, including sensor networks [15], smart grids,\r\nfinancial markets, and many more. Large volumes of col\u0002lected time series data are subsequently stored in relational\r\ndatabases. Relational databases, in turn, are used as back\u0002end by visual data analysis tools. Data analysts interact\r\nwith the visualizations and their actions are transformed by\r\nThis work is licensed under the Creative Commons Attribution\u0002NonCommercial-NoDerivs 3.0 Unported License. To view a copy of this li\u0002cense, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain per\u0002mission prior to any use beyond those covered by the license. Contact\r\ncopyright holder by emailing info@vldb.org. Articles from this volume\r\nwere invited to present their results at the 40th International Conference on\r\nVery Large Data Bases, September 1st - 5th 2014, Hangzhou, China.\r\nProceedings of the VLDB Endowment, Vol. 7, No. 10\r\nCopyright 2014 VLDB Endowment 2150-8097/14/06.\r\nthe visual data analysis tools into a series of queries that are\r\nissued against the relational database, holding the original\r\ntime series data. In state-of-the-art visual analytics tools,\r\ne.g., Tableau, QlikView, SAP Lumira, etc., such queries are\r\nissued to the database without considering the cardinality\r\nof the query result. However, when reading data from high\u0002volume data sources, result sets often contain millions of\r\nrows. This leads to very high bandwidth consumption be\u0002tween the visualization system and the database.\r\nLet us consider the following example. SAP customers in\r\nhigh tech manufacturing report that it is not uncommon for\r\n100 engineers to simultaneously access a global database,\r\ncontaining equipment monitoring data. Such monitoring\r\ndata originates from sensors embedded within the high tech\r\nmanufacturing machines. The common reporting frequency\r\nfor such embedded sensors is 100Hz [15]. An engineer usu\u0002ally accesses data which spans the last 12 hours for any given\r\nsensor. If the visualization system uses a non-aggregating\r\nquery, such as\r\nSELECT time,value FROM sensor WHERE time > NOW()-12*3600\r\nto retrieve the necessary data from the database, the total\r\namount of data to transfer is 100users · (12 · 3600)seconds ·\r\n100Hz = 432 million rows, i.e., over 4 million rows per visu\u0002alization client. Assuming a wire size of 60 bytes per row,\r\nthe total amount of data that needs to be transferred from\r\nthe database to all visualization clients is almost 26GB. Each\r\nuser will have to wait for nearly 260MB to be loaded to the\r\nvisualization client before he or she can examine a chart,\r\nshowing the sensor signal.\r\nWith the proliferation of high frequency data sources and\r\nreal-time visualization systems, the above concurrent-usage\r\npattern and its implications are observed by SAP not only in\r\nhigh tech manufacturing, but across a constantly increasing\r\nnumber of industries, including sports analytics [22], finance,\r\nand utilities.\r\nThe final visualization, which is presented to an engineer,\r\nis inherently restricted to displaying the retrieved data using\r\nwidth × height pixels - the area of the resulting chart. This\r\nimplies that a visualization system must perform a data re\u0002duction, transforming and projecting the received result set\r\nonto a width × height raster. This reduction is performed\r\nimplicitly by the visualization client and is applied to all re\u0002sult sets, regardless of the number of rows they contain. The\r\ngoal of this paper is to leverage this fundamental observa\u0002tion and apply an appropriate data reduction already at the\r\nquery level within the database. As illustrated in Figure 1,\r\nthe goal is to rewrite a visualization-related query Q using a\r\ndata reduction operator MR, such that the resulting query\r\n797",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/d08b370a-f296-4991-80cc-0b65a7f10ebe.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b6db025f5613bc2753028092ddd16665cc583b91c3a4a68fdd2a9e48f2066c57",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 824
      },
      {
        "segments": [
          {
            "segment_id": "d08b370a-f296-4991-80cc-0b65a7f10ebe",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "M4: A Visualization-Oriented Time Series Data Aggregation\r\nUwe Jugel, Zbigniew Jerzak,\r\nGregor Hackenbroich\r\nSAP AG\r\nChemnitzer Str. 48, 01187 Dresden, Germany\r\n{firstname}.{lastname}@sap.com\r\nVolker Markl\r\nTechnische Universitat Berlin ¨\r\nStraße des 17. Juni 135\r\n10623 Berlin, Germany\r\nvolker.markl@tu-berlin.de\r\nABSTRACT\r\nVisual analysis of high-volume time series data is ubiquitous\r\nin many industries, including finance, banking, and discrete\r\nmanufacturing. Contemporary, RDBMS-based systems for\r\nvisualization of high-volume time series data have difficulty\r\nto cope with the hard latency requirements and high inges\u0002tion rates of interactive visualizations. Existing solutions\r\nfor lowering the volume of time series data disregard the se\u0002mantics of visualizations and result in visualization errors.\r\nIn this work, we introduce M4, an aggregation-based time\r\nseries dimensionality reduction technique that provides error\u0002free visualizations at high data reduction rates. Focusing on\r\nline charts, as the predominant form of time series visualiza\u0002tion, we explain in detail the drawbacks of existing data re\u0002duction techniques and how our approach outperforms state\r\nof the art, by respecting the process of line rasterization.\r\nWe describe how to incorporate aggregation-based dimen\u0002sionality reduction at the query level in a visualization\u0002driven query rewriting system. Our approach is generic and\r\napplicable to any visualization system that uses an RDBMS\r\nas data source. Using real world data sets from high tech\r\nmanufacturing, stock markets, and sports analytics domains\r\nwe demonstrate that our visualization-oriented data aggre\u0002gation can reduce data volumes by up to two orders of mag\u0002nitude, while preserving perfect visualizations.\r\nKeywords: Relational databases, Query rewriting,\r\nDimensionality reduction, Line rasterization\r\n1. INTRODUCTION\r\nEnterprises are gathering petabytes of data in public and\r\nprivate clouds, with time series data originating from var\u0002ious sources, including sensor networks [15], smart grids,\r\nfinancial markets, and many more. Large volumes of col\u0002lected time series data are subsequently stored in relational\r\ndatabases. Relational databases, in turn, are used as back\u0002end by visual data analysis tools. Data analysts interact\r\nwith the visualizations and their actions are transformed by\r\nThis work is licensed under the Creative Commons Attribution\u0002NonCommercial-NoDerivs 3.0 Unported License. To view a copy of this li\u0002cense, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain per\u0002mission prior to any use beyond those covered by the license. Contact\r\ncopyright holder by emailing info@vldb.org. Articles from this volume\r\nwere invited to present their results at the 40th International Conference on\r\nVery Large Data Bases, September 1st - 5th 2014, Hangzhou, China.\r\nProceedings of the VLDB Endowment, Vol. 7, No. 10\r\nCopyright 2014 VLDB Endowment 2150-8097/14/06.\r\nthe visual data analysis tools into a series of queries that are\r\nissued against the relational database, holding the original\r\ntime series data. In state-of-the-art visual analytics tools,\r\ne.g., Tableau, QlikView, SAP Lumira, etc., such queries are\r\nissued to the database without considering the cardinality\r\nof the query result. However, when reading data from high\u0002volume data sources, result sets often contain millions of\r\nrows. This leads to very high bandwidth consumption be\u0002tween the visualization system and the database.\r\nLet us consider the following example. SAP customers in\r\nhigh tech manufacturing report that it is not uncommon for\r\n100 engineers to simultaneously access a global database,\r\ncontaining equipment monitoring data. Such monitoring\r\ndata originates from sensors embedded within the high tech\r\nmanufacturing machines. The common reporting frequency\r\nfor such embedded sensors is 100Hz [15]. An engineer usu\u0002ally accesses data which spans the last 12 hours for any given\r\nsensor. If the visualization system uses a non-aggregating\r\nquery, such as\r\nSELECT time,value FROM sensor WHERE time > NOW()-12*3600\r\nto retrieve the necessary data from the database, the total\r\namount of data to transfer is 100users · (12 · 3600)seconds ·\r\n100Hz = 432 million rows, i.e., over 4 million rows per visu\u0002alization client. Assuming a wire size of 60 bytes per row,\r\nthe total amount of data that needs to be transferred from\r\nthe database to all visualization clients is almost 26GB. Each\r\nuser will have to wait for nearly 260MB to be loaded to the\r\nvisualization client before he or she can examine a chart,\r\nshowing the sensor signal.\r\nWith the proliferation of high frequency data sources and\r\nreal-time visualization systems, the above concurrent-usage\r\npattern and its implications are observed by SAP not only in\r\nhigh tech manufacturing, but across a constantly increasing\r\nnumber of industries, including sports analytics [22], finance,\r\nand utilities.\r\nThe final visualization, which is presented to an engineer,\r\nis inherently restricted to displaying the retrieved data using\r\nwidth × height pixels - the area of the resulting chart. This\r\nimplies that a visualization system must perform a data re\u0002duction, transforming and projecting the received result set\r\nonto a width × height raster. This reduction is performed\r\nimplicitly by the visualization client and is applied to all re\u0002sult sets, regardless of the number of rows they contain. The\r\ngoal of this paper is to leverage this fundamental observa\u0002tion and apply an appropriate data reduction already at the\r\nquery level within the database. As illustrated in Figure 1,\r\nthe goal is to rewrite a visualization-related query Q using a\r\ndata reduction operator MR, such that the resulting query\r\n797",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/d08b370a-f296-4991-80cc-0b65a7f10ebe.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b6db025f5613bc2753028092ddd16665cc583b91c3a4a68fdd2a9e48f2066c57",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 824
      },
      {
        "segments": [
          {
            "segment_id": "bd043827-df4e-47f0-ac2a-e3a7574bc234",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "RDBMS\r\nQR= MR(Q)\r\nQ = SELECT t,v FROM T\r\n100k tuples (in 20s)\r\n10k tuples (in 2s)\r\na)\r\nb)\r\nvis1\r\n==\r\nvis2\r\nFigure 1: Time series visualization: a) based on\r\na unbounded query without reduction; b) using\r\nvisualization-oriented reduction at the query level.\r\nQR produces a much smaller result set, without impairing\r\nthe resulting visualization. Significantly reduced data vol\u0002umes mitigate high network bandwidth requirements and\r\nlead to shorter waiting times for the users of the visuali\u0002zation system. Note that the goal of our approach is not\r\nto compute images inside the database, since this prevents\r\nclient-side interaction with the data. Instead, our system\r\nshould select subsets of the original result set that can be\r\nconsumed transparently by any visualization client.\r\nTo achieve these goals, we present the following contribu\u0002tions. We first propose a visualization-driven query rewrit\u0002ing technique, relying on relational operators and parame\u0002trized with width and height of the desired visualization.\r\nSecondly, focusing on the detailed semantics of line charts,\r\nas the predominant form of time series visualization, we\r\ndevelop a visualization-driven aggregation that only selects\r\ndata points that are necessary to draw the correct visuali\u0002zation of the complete underlying data. Thereby, we model\r\nthe visualization process by selecting for every time interval,\r\nwhich corresponds to a pixel column in the final visualiza\u0002tion, the tuples with the minimum and maximum value, and\r\nadditionally the first and last tuples, having the minimum\r\nand maximum timestamp in that pixel column. To best of\r\nour knowledge, there is no application or previous discussion\r\nof this data reduction model in the literature, even though\r\nit provides superior results for the purpose of line visual\u0002izations. In this paper, we remedy this shortcoming and\r\nexplain the importance of the chosen min, max, and the ad\u0002ditional first and last tuples, in context of line rasterization.\r\nWe prove that the pixel-column-wise selection of these four\r\ntuples is required to ensure an error-free two-color (binary)\r\nline visualization. Furthermore, we denote this model as\r\nM4 aggregation and discuss and evaluate it for line visual\u0002izations in general, including anti-aliased (non-binary) line\r\nvisualizations.\r\nOur approach significantly differs from the state-of-the-art\r\ntime series dimensionality reduction techniques [11], which\r\nare often based on line simplification algorithms [25], such\r\nas the Ramer-Douglas-Peucker [6, 14] and the Visvalingam\u0002Whyatt algorithms [26]. These algorithms are computation\u0002ally expensive O(n log(n)) [19] and disregard the projection\r\nof the line to the width × height pixels of the final visualiza\u0002tion. In contrast, our approach has the complexity of O(n)\r\nand provides perfect visualizations.\r\nRelying only on relational operators for the data reduc\u0002tion, our visualization-driven query rewriting is generic and\r\ncan be applied to any RDBMS system. We demonstrate the\r\nimprovements of our techniques in a real world setting, us\u0002ing prototype implementations of our algorithms on top of\r\nSAP HANA [10] and Postgres (postgres.org).\r\nThe remainder of the paper is structured as follows. In\r\nSection 2, we present our system architecture and describe\r\nour query rewriting approach. In Section 3, we discuss our\r\nfocus on line charts. Thereafter, in Section 4, we provide the\r\ndetails of our visualization-oriented data aggregation model\r\nand discuss the proposed M4 aggregation. After describing\r\nthe drawbacks of existing time series dimensionality reduc\u0002tion techniques in Section 5, we compare our approach with\r\nthese techniques and evaluate the improvements regarding\r\nquery execution time, data efficiency, and visualization qual\u0002ity in Section 6. In Section 7, we discuss additional related\r\nwork, and we eventually conclude with Section 8.\r\n2. QUERY REWRITING\r\nIn this section, we describe our query rewriting approach to\r\nfacilitate data reduction for visualization systems that rely\r\non relational data sources.\r\nTo incorporate operators for data reduction, an original\r\nquery to a high-volume time series data source needs to be\r\nrewritten. The rewriting can either be done directly by the\r\nvisualization client or by an additional query interface to\r\nthe relational database management system (RDBMS). In\u0002dependent of where the query is rewritten, the actual data\r\nreduction will always be computed by the database itself.\r\nThe following Figure 2 illustrates this query rewriting and\r\ndata-centric dimensionality reduction approach.\r\nVisualization\r\nClient\r\nselected time range\r\nRDBMS Query Rewriter\r\ndata-reduced query result\r\nvisualization\r\nparameters\r\nquery\r\nreduction\r\ndata query\r\ndata reduction\r\ndata flow\r\n+\r\nFigure 2: Visualization system with query rewriter.\r\nQuery Definition. The definition of a query starts at\r\nthe visualization client, where the user first selects a time se\u0002ries data source, a time range, and the type of visualization.\r\nData source and time range usually define the main parts\r\nof the original query. Most queries, issued by our visualiza\u0002tion clients, are of the form SELECT time , value FROM series\r\nWHERE time > t1 AND time < t2. But practically, the visu\u0002alization client can define an arbitrary relational query, as\r\nlong as the result is a valid time series relation.\r\nTime Series Data Model. We regard time series as\r\nbinary relations T(t, v) with two numeric attributes: times\u0002tamp t ∈ R and value v ∈ R. Any other relation that has\r\nat least two numerical attributes can be easily projected\r\nto this model. For example, given a relation X(a, b, c),\r\nand knowing that a is a numerical timestamp and b and\r\nc are also numerical values, we can derive two separate time\r\nseries relations by means of projection and renaming, i.e.,\r\nTb(t, v) = πt←a,v←b(X) and Tc(t, v) = πt←a,v←c(X).\r\nVisualization Parameters. In addition to the query,\r\nthe visualization client must also provide the visualization\r\nparameters width w and height h, i.e., the exact pixel res\u0002olution of the desired visualization. Determining the exact\r\npixel resolution is very important, as we will later show in\r\nSection 6. For most visualizations the user-selected chart\r\nsize (wchart × hchart) is different from the actual resolution\r\n(w × h) of the canvas that is used for rasterization of the\r\ngeometry. Figure 3 depicts this difference using a schematic\r\nexample of a line chart that occupies 14×11 screen pixels in\r\n798",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/bd043827-df4e-47f0-ac2a-e3a7574bc234.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c955ff33b2fb94ef6beee8bb7a6842b352bfb5ff4d2d313bf75568af3526e474",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 975
      },
      {
        "segments": [
          {
            "segment_id": "bd043827-df4e-47f0-ac2a-e3a7574bc234",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "RDBMS\r\nQR= MR(Q)\r\nQ = SELECT t,v FROM T\r\n100k tuples (in 20s)\r\n10k tuples (in 2s)\r\na)\r\nb)\r\nvis1\r\n==\r\nvis2\r\nFigure 1: Time series visualization: a) based on\r\na unbounded query without reduction; b) using\r\nvisualization-oriented reduction at the query level.\r\nQR produces a much smaller result set, without impairing\r\nthe resulting visualization. Significantly reduced data vol\u0002umes mitigate high network bandwidth requirements and\r\nlead to shorter waiting times for the users of the visuali\u0002zation system. Note that the goal of our approach is not\r\nto compute images inside the database, since this prevents\r\nclient-side interaction with the data. Instead, our system\r\nshould select subsets of the original result set that can be\r\nconsumed transparently by any visualization client.\r\nTo achieve these goals, we present the following contribu\u0002tions. We first propose a visualization-driven query rewrit\u0002ing technique, relying on relational operators and parame\u0002trized with width and height of the desired visualization.\r\nSecondly, focusing on the detailed semantics of line charts,\r\nas the predominant form of time series visualization, we\r\ndevelop a visualization-driven aggregation that only selects\r\ndata points that are necessary to draw the correct visuali\u0002zation of the complete underlying data. Thereby, we model\r\nthe visualization process by selecting for every time interval,\r\nwhich corresponds to a pixel column in the final visualiza\u0002tion, the tuples with the minimum and maximum value, and\r\nadditionally the first and last tuples, having the minimum\r\nand maximum timestamp in that pixel column. To best of\r\nour knowledge, there is no application or previous discussion\r\nof this data reduction model in the literature, even though\r\nit provides superior results for the purpose of line visual\u0002izations. In this paper, we remedy this shortcoming and\r\nexplain the importance of the chosen min, max, and the ad\u0002ditional first and last tuples, in context of line rasterization.\r\nWe prove that the pixel-column-wise selection of these four\r\ntuples is required to ensure an error-free two-color (binary)\r\nline visualization. Furthermore, we denote this model as\r\nM4 aggregation and discuss and evaluate it for line visual\u0002izations in general, including anti-aliased (non-binary) line\r\nvisualizations.\r\nOur approach significantly differs from the state-of-the-art\r\ntime series dimensionality reduction techniques [11], which\r\nare often based on line simplification algorithms [25], such\r\nas the Ramer-Douglas-Peucker [6, 14] and the Visvalingam\u0002Whyatt algorithms [26]. These algorithms are computation\u0002ally expensive O(n log(n)) [19] and disregard the projection\r\nof the line to the width × height pixels of the final visualiza\u0002tion. In contrast, our approach has the complexity of O(n)\r\nand provides perfect visualizations.\r\nRelying only on relational operators for the data reduc\u0002tion, our visualization-driven query rewriting is generic and\r\ncan be applied to any RDBMS system. We demonstrate the\r\nimprovements of our techniques in a real world setting, us\u0002ing prototype implementations of our algorithms on top of\r\nSAP HANA [10] and Postgres (postgres.org).\r\nThe remainder of the paper is structured as follows. In\r\nSection 2, we present our system architecture and describe\r\nour query rewriting approach. In Section 3, we discuss our\r\nfocus on line charts. Thereafter, in Section 4, we provide the\r\ndetails of our visualization-oriented data aggregation model\r\nand discuss the proposed M4 aggregation. After describing\r\nthe drawbacks of existing time series dimensionality reduc\u0002tion techniques in Section 5, we compare our approach with\r\nthese techniques and evaluate the improvements regarding\r\nquery execution time, data efficiency, and visualization qual\u0002ity in Section 6. In Section 7, we discuss additional related\r\nwork, and we eventually conclude with Section 8.\r\n2. QUERY REWRITING\r\nIn this section, we describe our query rewriting approach to\r\nfacilitate data reduction for visualization systems that rely\r\non relational data sources.\r\nTo incorporate operators for data reduction, an original\r\nquery to a high-volume time series data source needs to be\r\nrewritten. The rewriting can either be done directly by the\r\nvisualization client or by an additional query interface to\r\nthe relational database management system (RDBMS). In\u0002dependent of where the query is rewritten, the actual data\r\nreduction will always be computed by the database itself.\r\nThe following Figure 2 illustrates this query rewriting and\r\ndata-centric dimensionality reduction approach.\r\nVisualization\r\nClient\r\nselected time range\r\nRDBMS Query Rewriter\r\ndata-reduced query result\r\nvisualization\r\nparameters\r\nquery\r\nreduction\r\ndata query\r\ndata reduction\r\ndata flow\r\n+\r\nFigure 2: Visualization system with query rewriter.\r\nQuery Definition. The definition of a query starts at\r\nthe visualization client, where the user first selects a time se\u0002ries data source, a time range, and the type of visualization.\r\nData source and time range usually define the main parts\r\nof the original query. Most queries, issued by our visualiza\u0002tion clients, are of the form SELECT time , value FROM series\r\nWHERE time > t1 AND time < t2. But practically, the visu\u0002alization client can define an arbitrary relational query, as\r\nlong as the result is a valid time series relation.\r\nTime Series Data Model. We regard time series as\r\nbinary relations T(t, v) with two numeric attributes: times\u0002tamp t ∈ R and value v ∈ R. Any other relation that has\r\nat least two numerical attributes can be easily projected\r\nto this model. For example, given a relation X(a, b, c),\r\nand knowing that a is a numerical timestamp and b and\r\nc are also numerical values, we can derive two separate time\r\nseries relations by means of projection and renaming, i.e.,\r\nTb(t, v) = πt←a,v←b(X) and Tc(t, v) = πt←a,v←c(X).\r\nVisualization Parameters. In addition to the query,\r\nthe visualization client must also provide the visualization\r\nparameters width w and height h, i.e., the exact pixel res\u0002olution of the desired visualization. Determining the exact\r\npixel resolution is very important, as we will later show in\r\nSection 6. For most visualizations the user-selected chart\r\nsize (wchart × hchart) is different from the actual resolution\r\n(w × h) of the canvas that is used for rasterization of the\r\ngeometry. Figure 3 depicts this difference using a schematic\r\nexample of a line chart that occupies 14×11 screen pixels in\r\n798",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/bd043827-df4e-47f0-ac2a-e3a7574bc234.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c955ff33b2fb94ef6beee8bb7a6842b352bfb5ff4d2d313bf75568af3526e474",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 975
      },
      {
        "segments": [
          {
            "segment_id": "52b38023-3d65-4668-a7b9-910959833d6f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "wchart = 14 \r\nh = 7\r\nhchart = 11\r\nh\r\nw = 9\r\nxstart = fx(tstart)\r\nxend = fx(tend)\r\nymin = fy(vmin)\r\nymax = fy(vmax)\r\nx-axis pixels\r\ny-axis pixels\r\nCanvas\r\nPadding\r\nPadding\r\nPadding\r\nFigure 3: Determining the visualization parameters.\r\ntotal, but only uses 9×7 pixels for drawing the actual lines.\r\nWhen deriving the data reduction operators from these vi\u0002sualization parameters w and h, our query rewriter assumes\r\nthe following.\r\nGiven the width w and height h of the canvas area, the\r\nvisualization client uses the following geometric transforma\u0002tion functions x = fx(t) and y = fy(v), x, y ∈ R to project\r\neach timestamp t and value v to the visualization’s coordi\u0002nate system.\r\nfx(t) = w · (t − tstart)/(tend − tstart)\r\nfy(v) = h · (v − vmin)/(vmax − vmin)\r\n(1)\r\nThe projected real-valued time series data is then traversed\r\nby the drawing routines of the visualization client to derive\r\nthe discrete pixels. We assume the projected minimum and\r\nmaximum timestamps and values of the selected time series\r\n(tstart, tend, vmin, vmax) match exactly with the real-valued\r\nleft, right, bottom, and top boundaries (xstart, xend, ymin,\r\nymax) of the canvas area, i.e., we assume that the drawing\r\nroutines do not apply an additional vertical or horizontal\r\ntranslation or rescaling operation to data. In our evalua\u0002tion, in Section 6, we will discuss potential issues of these\r\nassumptions.\r\nQuery Rewriting. In our system, the query rewriter\r\nhandles all visualization-related queries. Therefore, it re\u0002ceives a query Q and the additional visualization parameters\r\nwidth w and height h. The goal of the rewriting is to apply\r\nan additional data reduction to those queries, whose result\r\nset size exceeds a certain limit. The result of the rewrit\u0002ing process is exemplified in Figure 4a. In general, such a\r\nrewritten query QR contains the following subqueries.\r\n1) The original query Q,\r\n2) a cardinality query QC on Q,\r\n3) a cardinality check (conditional execution),\r\n3a) to either use the result of Q directly, or\r\n3b) to execute an additional data reduction QD on Q.\r\nOur system composes all relevant subqueries into one single\r\nSQL query to ensure a fast query execution. Thereby, we\r\nfirstly leverage that databases are able to reuse results of\r\nsubqueries, and secondly assume that the execution time of\r\ncounting the number of rows selected by the original query\r\nis negligibly small, compared to the actual query execu\u0002tion time. The two properties are true for most modern\r\na) Conditional query to apply PAA data reduction\r\nWITH Q AS (SELECT t,v FROM sensors WHERE\r\n id = 1 AND t >= $t1 AND t <= $t2),\r\n QC AS (SELECT count(*) c FROM Q)\r\nSELECT * FROM Q WHERE (SELECT c FROM QC) <= 800\r\nUNION\r\nSELECT * FROM (\r\n SELECT min(t),avg(v) FROM Q\r\n GROUP BY round(200*(t-$t1)/($t2-$t1))\r\n) AS QD WHERE (SELECT c FROM QC) > 800\r\n1) original query Q\r\n2) cardinality query QC\r\n3a) use Q if low card.\r\n3b) use QD if high card.\r\nreduction query QD:\r\ncompute aggregates\r\nfor each pixel-column\r\nb) resulting image c) expected image\r\nFigure 4: Query rewriting result and visualization.\r\ndatabases and the first facilitates the second. In practice,\r\nsubquery reuse can be achieved via common table expression\r\n[23], as defined by the SQL 1999 standard.\r\nThe SQL example in Figure 4a uses a width of w = 200\r\npixels, a cardinality limit of 4 · w = 800 tuples, and it ap\u0002plies a data reduction using a simple piece-wise aggregate\r\napproximation (PAA) [18] – a naive measure for time se\u0002ries dimensionality reduction. The corresponding visualiza\u0002tion is a line chart, for which we only consider the width w\r\nto define the PAA parameters. For such aggregation-based\r\ndata reduction operators, we align the time intervals with\r\nthe pixel columns to model the process of visualization at\r\nthe query level. We use the same geometric transformation\r\nx = fx(t), as is used by the visualization client and round\r\nthe resulting value to a discrete group key between 0 and\r\nw = 200, i.e., we use the following grouping function.\r\nfg(t) = round(w · (t − tstart)/(tend − tstart)) (2)\r\nFigure 4b shows the resulting image that the visualization\r\nclient derived from the reduced data, and Figure 4c shows\r\nthe resulting image derived from the raw time series data.\r\nThe averaging aggregation function significantly changes the\r\nactual shape of the time series. In Section 4, we will discuss\r\nthe utility of different types of aggregation functions for the\r\npurpose of visualization-oriented data reduction. For now,\r\nlet us focus on the overall structure of the query. Note that\r\nwe model the described conditional execution using a union\r\nof the different subqueries (3a) and (3b) that have contradic\u0002tory predicates based on the cardinality limit. This allows\r\nus to execute any user-defined query logic of the query Q\r\nonly once. The execution of the query Q and the data re\u0002duction QD is all covered by this single query, such that no\r\nhigh-volume data needs to be copied to an additional data\r\nreduction or compression component.\r\n3. TIME SERIES VISUALIZATION\r\nIn this work, we focus on line charts of high-volume time\r\nseries data. The following short digression on time series\r\nvisualization will explain our motivation.\r\nThere exist dozens of ways to visualize time series data,\r\nbut only a few of them work well with large data volumes.\r\nBar charts, pie charts, and similar simple chart types con\u0002sume too much space per displayed element [8]. The most\r\ncommon charts that suffice for high-volume data are shown\r\nin Figure 5. These are line charts and scatter plots where a\r\n799",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/52b38023-3d65-4668-a7b9-910959833d6f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2475da906633bd5f6430202182c51b983b22cce0b932be9430c5cd2a060f0986",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 923
      },
      {
        "segments": [
          {
            "segment_id": "52b38023-3d65-4668-a7b9-910959833d6f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "wchart = 14 \r\nh = 7\r\nhchart = 11\r\nh\r\nw = 9\r\nxstart = fx(tstart)\r\nxend = fx(tend)\r\nymin = fy(vmin)\r\nymax = fy(vmax)\r\nx-axis pixels\r\ny-axis pixels\r\nCanvas\r\nPadding\r\nPadding\r\nPadding\r\nFigure 3: Determining the visualization parameters.\r\ntotal, but only uses 9×7 pixels for drawing the actual lines.\r\nWhen deriving the data reduction operators from these vi\u0002sualization parameters w and h, our query rewriter assumes\r\nthe following.\r\nGiven the width w and height h of the canvas area, the\r\nvisualization client uses the following geometric transforma\u0002tion functions x = fx(t) and y = fy(v), x, y ∈ R to project\r\neach timestamp t and value v to the visualization’s coordi\u0002nate system.\r\nfx(t) = w · (t − tstart)/(tend − tstart)\r\nfy(v) = h · (v − vmin)/(vmax − vmin)\r\n(1)\r\nThe projected real-valued time series data is then traversed\r\nby the drawing routines of the visualization client to derive\r\nthe discrete pixels. We assume the projected minimum and\r\nmaximum timestamps and values of the selected time series\r\n(tstart, tend, vmin, vmax) match exactly with the real-valued\r\nleft, right, bottom, and top boundaries (xstart, xend, ymin,\r\nymax) of the canvas area, i.e., we assume that the drawing\r\nroutines do not apply an additional vertical or horizontal\r\ntranslation or rescaling operation to data. In our evalua\u0002tion, in Section 6, we will discuss potential issues of these\r\nassumptions.\r\nQuery Rewriting. In our system, the query rewriter\r\nhandles all visualization-related queries. Therefore, it re\u0002ceives a query Q and the additional visualization parameters\r\nwidth w and height h. The goal of the rewriting is to apply\r\nan additional data reduction to those queries, whose result\r\nset size exceeds a certain limit. The result of the rewrit\u0002ing process is exemplified in Figure 4a. In general, such a\r\nrewritten query QR contains the following subqueries.\r\n1) The original query Q,\r\n2) a cardinality query QC on Q,\r\n3) a cardinality check (conditional execution),\r\n3a) to either use the result of Q directly, or\r\n3b) to execute an additional data reduction QD on Q.\r\nOur system composes all relevant subqueries into one single\r\nSQL query to ensure a fast query execution. Thereby, we\r\nfirstly leverage that databases are able to reuse results of\r\nsubqueries, and secondly assume that the execution time of\r\ncounting the number of rows selected by the original query\r\nis negligibly small, compared to the actual query execu\u0002tion time. The two properties are true for most modern\r\na) Conditional query to apply PAA data reduction\r\nWITH Q AS (SELECT t,v FROM sensors WHERE\r\n id = 1 AND t >= $t1 AND t <= $t2),\r\n QC AS (SELECT count(*) c FROM Q)\r\nSELECT * FROM Q WHERE (SELECT c FROM QC) <= 800\r\nUNION\r\nSELECT * FROM (\r\n SELECT min(t),avg(v) FROM Q\r\n GROUP BY round(200*(t-$t1)/($t2-$t1))\r\n) AS QD WHERE (SELECT c FROM QC) > 800\r\n1) original query Q\r\n2) cardinality query QC\r\n3a) use Q if low card.\r\n3b) use QD if high card.\r\nreduction query QD:\r\ncompute aggregates\r\nfor each pixel-column\r\nb) resulting image c) expected image\r\nFigure 4: Query rewriting result and visualization.\r\ndatabases and the first facilitates the second. In practice,\r\nsubquery reuse can be achieved via common table expression\r\n[23], as defined by the SQL 1999 standard.\r\nThe SQL example in Figure 4a uses a width of w = 200\r\npixels, a cardinality limit of 4 · w = 800 tuples, and it ap\u0002plies a data reduction using a simple piece-wise aggregate\r\napproximation (PAA) [18] – a naive measure for time se\u0002ries dimensionality reduction. The corresponding visualiza\u0002tion is a line chart, for which we only consider the width w\r\nto define the PAA parameters. For such aggregation-based\r\ndata reduction operators, we align the time intervals with\r\nthe pixel columns to model the process of visualization at\r\nthe query level. We use the same geometric transformation\r\nx = fx(t), as is used by the visualization client and round\r\nthe resulting value to a discrete group key between 0 and\r\nw = 200, i.e., we use the following grouping function.\r\nfg(t) = round(w · (t − tstart)/(tend − tstart)) (2)\r\nFigure 4b shows the resulting image that the visualization\r\nclient derived from the reduced data, and Figure 4c shows\r\nthe resulting image derived from the raw time series data.\r\nThe averaging aggregation function significantly changes the\r\nactual shape of the time series. In Section 4, we will discuss\r\nthe utility of different types of aggregation functions for the\r\npurpose of visualization-oriented data reduction. For now,\r\nlet us focus on the overall structure of the query. Note that\r\nwe model the described conditional execution using a union\r\nof the different subqueries (3a) and (3b) that have contradic\u0002tory predicates based on the cardinality limit. This allows\r\nus to execute any user-defined query logic of the query Q\r\nonly once. The execution of the query Q and the data re\u0002duction QD is all covered by this single query, such that no\r\nhigh-volume data needs to be copied to an additional data\r\nreduction or compression component.\r\n3. TIME SERIES VISUALIZATION\r\nIn this work, we focus on line charts of high-volume time\r\nseries data. The following short digression on time series\r\nvisualization will explain our motivation.\r\nThere exist dozens of ways to visualize time series data,\r\nbut only a few of them work well with large data volumes.\r\nBar charts, pie charts, and similar simple chart types con\u0002sume too much space per displayed element [8]. The most\r\ncommon charts that suffice for high-volume data are shown\r\nin Figure 5. These are line charts and scatter plots where a\r\n799",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/52b38023-3d65-4668-a7b9-910959833d6f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2475da906633bd5f6430202182c51b983b22cce0b932be9430c5cd2a060f0986",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 923
      },
      {
        "segments": [
          {
            "segment_id": "b9b14a64-2e51-4e22-8548-44504b7c883e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "single data point can be presented using only a few pixels.\r\nRegarding space efficiency, these two simple chart types are\r\nonly surpassed by space-filling approaches [17]. However,\r\nthe most ubiquitous visualization is the line chart; found\r\nin spreadsheet applications, data analytics tools, charting\r\nframeworks, system monitoring applications, and many more.\r\nspace filling visualization\r\ntime value time\r\nvaluevalue\r\ntime\r\nline chart scatter plot\r\nFigure 5: Common time series visualizations.\r\nTo achieve an efficient data reduction, we need to consider\r\nhow a visualization is created from the underlying data. For\r\nscatter plots this process requires to shift, scale and round\r\nthe time series relation T(t, v) with t, v ∈ R to a relation\r\nof discrete pixels V (x, y) with x ∈ [1, w], y ∈ [1, h]. As al\u0002ready described in Section 2, we can reuse this geometric\r\ntransformation for data reduction. Space-filling visualiza\u0002tions are similar. They also project a time series T(t, v)\r\nto a sequence of discrete values V (i, l) with i ∈ [1, w · h],\r\nl ∈ [0, 255], where i is the position in the sequence and l is\r\nthe luminance to be used.\r\nAn appropriate data reduction for scatter plots is a two\u0002dimensional grouping of the time series, having a one-to-one\r\nmapping of pixels to groups. As a result, scatter plots (and\r\nalso space-filling approaches) require selecting up to w · h\r\ntuples from the original data to produce correct visualiza\u0002tions. This may add up to several hundred thousand tuples,\r\nespecially when considering today’s high screen resolutions.\r\nThe corresponding data reduction potential is limited.\r\nThis is not the case for line charts of high-volume time\r\nseries data. In Section 4.4, we will show that there is an\r\nupper bound of 4·w required tuples for two-color line charts.\r\n4. DATA REDUCTION OPERATORS\r\nThe goal of our query rewriting system is to apply visuali\u0002zation-oriented data reduction in the database by means of\r\nrelational operators, i.e., using data aggregation. In the lit\u0002erature, we did not find any comprehensive discussion that\r\ndescribes the effects of data aggregation on rasterized line\r\nvisualizations. Most time series dimensionality reduction\r\ntechniques [11] are too generic and are not designed specifi\u0002cally for line visualizations. We will now remedy this short\u0002coming and describe several classes of operators that we con\u0002sidered and evaluated for data reduction and discuss their\r\nutility for line visualizations.\r\n4.1 Visualization-Oriented Data Aggregation\r\nAs already described in Section 2, we model data reduction\r\nusing a time-based grouping, aligning the time intervals with\r\nthe pixel columns of the visualization. For each interval\r\nand thus for each pixel column we can compute aggregated\r\nvalues using one of the following options.\r\nNormal Aggregation. A simple form of data reduction\r\nis to compute an aggregated value and an aggregated times\u0002tamp using the aggregation functions min, max, avg, me\u0002dian, medoid, or mode. The resulting data reduction queries\r\non a time series relation T(t, v), using a (horizontal) group\u0002ing function fg and two aggregation functions ft and fv can\r\nbe defined in relational algebra:\r\nfg(t)Gft(t),fv(v)(T) (3)\r\nWe already used this simple form of aggregation in our query\r\nrewriting example (Figure 4), selecting a minimum (first)\r\ntimestamp and an average value to model a piece-wise ag\u0002gregate approximation (PAA) [18]. But any averaging func\u0002tion, i.e., using avg, median, medoid, or mode, will signifi\u0002cantly distort the actual shape of the time series (see Figure\r\n4b vs. 4c). To preserve the shape of the time series, we\r\nneed to focus on the extrema of each group. For example,\r\nwe want to select those tuples that have the minimum value\r\nvmin or maximum value vmax per group. Unfortunately, the\r\ngrouping semantics of the relational algebra does not allow\r\nselection of non-aggregated values.\r\nValue Preserving Aggregation. To select the corre\u0002sponding tuples, based on the computed aggregated values,\r\nwe need to join the aggregated data again with the underly\u0002ing time series. Therefore, we replace one of the aggregation\r\nfunctions with the time-based group key (result of fg) and\r\njoin the aggregation results with T on that group key and\r\non the aggregated value or timestamp. In particular, the\r\nfollowing query\r\nπt,v(T ./ fg(t)=k∧v=vg(fg(t)Gk←fg(t),vg←fv(v)(T))) (4)\r\nselects the corresponding timestamps t for each aggregated\r\nvalue vg = fv(v), and the following query\r\nπt,v(T ./ fg(t)=k∧t=tg(fg(t)Gk←fg(t),tg←ft(t)(T))) (5)\r\nselects the corresponding values v for each aggregated times\u0002tamp tg = ft(t). Note that these queries may select more\r\nthan one tuple per group, if there are duplicate values or\r\ntimestamps per group. However, in most of our high-volume\r\ntime series data sources, timestamps are unique and the val\u0002ues are real-valued numbers with multiple decimals places,\r\nsuch that the average number of duplicates is less than one\r\npercent of the overall data. In scenarios with more signifi\u0002cant duplicate ratios, the described queries (4) and (5) need\r\nto be encased with additional compensating aggregation op\u0002erators to ensure appropriate data reduction rates.\r\nSampling. Using the value preserving aggregation we\r\ncan express simple forms of systematic sampling, e.g., se\u0002lecting every first tuple per group using the following query.\r\nπt,v(T ./ fg(t)=k∧t=tmin (fg(t)Gk←fg(t),tmin←min(t)(T)))\r\nFor query level random sampling, we can also combine the\r\nvalue preserving aggregation with the SQL 2003 concept\r\nof the TABLESAMPLE or a selection operator involving\r\na random() function. While this allows us to conduct data\r\nsampling inside the database, we will show in our evalua\u0002tion that these simple forms sampling are not appropriate\r\nfor line visualizations.\r\nComposite Aggregations. In addition to the described\r\nqueries (3), (4), and (5) that yield a single aggregated value\r\nper group, we also consider composite queries with several\r\naggregated values per group. In relational algebra, we can\r\nmodel such queries as a union of two or more aggregating sub\r\nqueries that use the same grouping function. Alternatively,\r\nwe can modify the queries (4) and (5) to select multiple\r\naggregated values or timestamps per group before joining\r\nagain with the base relation. We then need to combine the\r\n800",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/b9b14a64-2e51-4e22-8548-44504b7c883e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d055fa746a00b2974a26f66032b29431715095730cd583b5dc14c21bbfa00a7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 974
      },
      {
        "segments": [
          {
            "segment_id": "b9b14a64-2e51-4e22-8548-44504b7c883e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "single data point can be presented using only a few pixels.\r\nRegarding space efficiency, these two simple chart types are\r\nonly surpassed by space-filling approaches [17]. However,\r\nthe most ubiquitous visualization is the line chart; found\r\nin spreadsheet applications, data analytics tools, charting\r\nframeworks, system monitoring applications, and many more.\r\nspace filling visualization\r\ntime value time\r\nvaluevalue\r\ntime\r\nline chart scatter plot\r\nFigure 5: Common time series visualizations.\r\nTo achieve an efficient data reduction, we need to consider\r\nhow a visualization is created from the underlying data. For\r\nscatter plots this process requires to shift, scale and round\r\nthe time series relation T(t, v) with t, v ∈ R to a relation\r\nof discrete pixels V (x, y) with x ∈ [1, w], y ∈ [1, h]. As al\u0002ready described in Section 2, we can reuse this geometric\r\ntransformation for data reduction. Space-filling visualiza\u0002tions are similar. They also project a time series T(t, v)\r\nto a sequence of discrete values V (i, l) with i ∈ [1, w · h],\r\nl ∈ [0, 255], where i is the position in the sequence and l is\r\nthe luminance to be used.\r\nAn appropriate data reduction for scatter plots is a two\u0002dimensional grouping of the time series, having a one-to-one\r\nmapping of pixels to groups. As a result, scatter plots (and\r\nalso space-filling approaches) require selecting up to w · h\r\ntuples from the original data to produce correct visualiza\u0002tions. This may add up to several hundred thousand tuples,\r\nespecially when considering today’s high screen resolutions.\r\nThe corresponding data reduction potential is limited.\r\nThis is not the case for line charts of high-volume time\r\nseries data. In Section 4.4, we will show that there is an\r\nupper bound of 4·w required tuples for two-color line charts.\r\n4. DATA REDUCTION OPERATORS\r\nThe goal of our query rewriting system is to apply visuali\u0002zation-oriented data reduction in the database by means of\r\nrelational operators, i.e., using data aggregation. In the lit\u0002erature, we did not find any comprehensive discussion that\r\ndescribes the effects of data aggregation on rasterized line\r\nvisualizations. Most time series dimensionality reduction\r\ntechniques [11] are too generic and are not designed specifi\u0002cally for line visualizations. We will now remedy this short\u0002coming and describe several classes of operators that we con\u0002sidered and evaluated for data reduction and discuss their\r\nutility for line visualizations.\r\n4.1 Visualization-Oriented Data Aggregation\r\nAs already described in Section 2, we model data reduction\r\nusing a time-based grouping, aligning the time intervals with\r\nthe pixel columns of the visualization. For each interval\r\nand thus for each pixel column we can compute aggregated\r\nvalues using one of the following options.\r\nNormal Aggregation. A simple form of data reduction\r\nis to compute an aggregated value and an aggregated times\u0002tamp using the aggregation functions min, max, avg, me\u0002dian, medoid, or mode. The resulting data reduction queries\r\non a time series relation T(t, v), using a (horizontal) group\u0002ing function fg and two aggregation functions ft and fv can\r\nbe defined in relational algebra:\r\nfg(t)Gft(t),fv(v)(T) (3)\r\nWe already used this simple form of aggregation in our query\r\nrewriting example (Figure 4), selecting a minimum (first)\r\ntimestamp and an average value to model a piece-wise ag\u0002gregate approximation (PAA) [18]. But any averaging func\u0002tion, i.e., using avg, median, medoid, or mode, will signifi\u0002cantly distort the actual shape of the time series (see Figure\r\n4b vs. 4c). To preserve the shape of the time series, we\r\nneed to focus on the extrema of each group. For example,\r\nwe want to select those tuples that have the minimum value\r\nvmin or maximum value vmax per group. Unfortunately, the\r\ngrouping semantics of the relational algebra does not allow\r\nselection of non-aggregated values.\r\nValue Preserving Aggregation. To select the corre\u0002sponding tuples, based on the computed aggregated values,\r\nwe need to join the aggregated data again with the underly\u0002ing time series. Therefore, we replace one of the aggregation\r\nfunctions with the time-based group key (result of fg) and\r\njoin the aggregation results with T on that group key and\r\non the aggregated value or timestamp. In particular, the\r\nfollowing query\r\nπt,v(T ./ fg(t)=k∧v=vg(fg(t)Gk←fg(t),vg←fv(v)(T))) (4)\r\nselects the corresponding timestamps t for each aggregated\r\nvalue vg = fv(v), and the following query\r\nπt,v(T ./ fg(t)=k∧t=tg(fg(t)Gk←fg(t),tg←ft(t)(T))) (5)\r\nselects the corresponding values v for each aggregated times\u0002tamp tg = ft(t). Note that these queries may select more\r\nthan one tuple per group, if there are duplicate values or\r\ntimestamps per group. However, in most of our high-volume\r\ntime series data sources, timestamps are unique and the val\u0002ues are real-valued numbers with multiple decimals places,\r\nsuch that the average number of duplicates is less than one\r\npercent of the overall data. In scenarios with more signifi\u0002cant duplicate ratios, the described queries (4) and (5) need\r\nto be encased with additional compensating aggregation op\u0002erators to ensure appropriate data reduction rates.\r\nSampling. Using the value preserving aggregation we\r\ncan express simple forms of systematic sampling, e.g., se\u0002lecting every first tuple per group using the following query.\r\nπt,v(T ./ fg(t)=k∧t=tmin (fg(t)Gk←fg(t),tmin←min(t)(T)))\r\nFor query level random sampling, we can also combine the\r\nvalue preserving aggregation with the SQL 2003 concept\r\nof the TABLESAMPLE or a selection operator involving\r\na random() function. While this allows us to conduct data\r\nsampling inside the database, we will show in our evalua\u0002tion that these simple forms sampling are not appropriate\r\nfor line visualizations.\r\nComposite Aggregations. In addition to the described\r\nqueries (3), (4), and (5) that yield a single aggregated value\r\nper group, we also consider composite queries with several\r\naggregated values per group. In relational algebra, we can\r\nmodel such queries as a union of two or more aggregating sub\r\nqueries that use the same grouping function. Alternatively,\r\nwe can modify the queries (4) and (5) to select multiple\r\naggregated values or timestamps per group before joining\r\nagain with the base relation. We then need to combine the\r\n800",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/b9b14a64-2e51-4e22-8548-44504b7c883e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d055fa746a00b2974a26f66032b29431715095730cd583b5dc14c21bbfa00a7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 974
      },
      {
        "segments": [
          {
            "segment_id": "833a11cd-d5b1-4762-93ae-d64d29927af3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "a) value-preserving MinMax aggregation query\r\nSELECT t,v FROM Q JOIN\r\n(SELECT round($w*(t-$t1)/($t2-$t1)) as k, -- define key\r\nmin(v) as v_min, max(v) as v_max -- get min,max\r\nFROM Q GROUP BY k) as QA -- group by k\r\nON k = round($w*(t-$t1)/($t2-$t1)) -- join on k\r\nAND (v = v_min OR v = v_max) -- &(min|max)\r\nb) resulting image c) expected image\r\nFigure 6: MinMax query and resulting visualization.\r\njoin predicates, such that all different aggregated values or\r\ntimestamps are correlated to either their missing timestamp\r\nor their missing value. The following MinMax aggregation\r\nwill provide an example of a composite aggregation.\r\nMinMax Aggregation. To preserve the shape of a time\r\nseries, the first intuition is to group-wise select the vertical\r\nextrema, which we denote as min and max tuples. This is a\r\ncomposite value-preserving aggregation, exemplified by the\r\nSQL query in Figure 6a. The query projects existing times\u0002tamps and values from a time series relation Q, after joining\r\nit with the aggregation results QA. Q is defined by an orig\u0002inal query, as already shown in Figure 4a. The group keys\r\nk are based on the rounded results of the (horizontal) geo\u0002metric transformation of Q to the visualization’s coordinate\r\nsystem. As stated before in Section 2, it is important that\r\nthis geometric transformation is exactly the same transfor\u0002mation as conducted by visualization client, before passing\r\nthe rescaled time series data to the line drawing routines for\r\nrasterization. Finally, the join of QA with Q is based on the\r\ngroup key k and on matching the values v in Q either with\r\nvmin or vmax from QA.\r\nFigure 6b shows the resulting visualization and we now\r\nobserve that it closely matches the expected visualization\r\nof the raw data 6c. In Section 4.3, we later discuss the\r\nremaining pixel errors; indicated by arrows in Figure 6b.\r\n4.2 The M4 Aggregation\r\nThe composite MinMax aggregation focuses on the vertical\r\nextrema of each pixel column, i.e., of each corresponding\r\ntime span. There are already existing approaches for se\u0002lecting extrema for the purpose of data reduction and data\r\nanalysis [12]. But most of them only partially consider the\r\nimplications for data visualization and neglect the final pro\u0002jection of the data to discrete screen pixels.\r\nA line chart that is based on a reduced data set, will al\u0002ways omit lines that would have connected the not selected\r\ntuples, and it will always add new approximating lines to\r\nbridge the not selected tuples between two consecutive se\u0002lected tuples. The resulting errors (in the real-valued vi\u0002sualization space) are significantly reduced by the final dis\u0002cretization process of the drawing routines. This effect is\r\nthe underlying principle of the proposed visualization-driven\r\ndata reduction.\r\nIntuitively, one may expect that selecting the minimum\r\nand maximum values, i.e., the tuples (tbottom, min(v)) and\r\n(ttop, max(v)), from exactly w groups, is sufficient to derive\r\na correct line visualization. This intuition is elusive, and this\r\na) value-preserving M4 aggregation query\r\nSELECT t,v FROM Q JOIN\r\n(SELECT round($w*(t-$t1)/($t2-$t1)) as k, --define key\r\nmin(v) as v_min, max(v) as v_max, --get min,max\r\nmin(t) as t_min, max(t) as t_max --get 1st,last\r\nFROM Q GROUP BY k) as QA --group by k\r\nON k = round($w*(t-$t1)/($t2-$t1)) --join on k\r\nAND (v = v_min OR v = v_max OR --&(min|max|\r\nt = t_min OR t = t_max) -- 1st|last)\r\nb) resulting image == expected image\r\nFigure 7: M4 query and resulting visualization.\r\nform of data reduction – provided by the MinMax aggrega\u0002tion – does not guarantee an error-free line visualization of\r\nthe time series. It ignores the important first and last tuples\r\nof the each group. We now introduce the M4 aggregation\r\nthat additionally selects these first and last tuples (min(t),\r\nvf irst) and (max(t), vlast). In Section 4.3, we then discuss\r\nhow M4 surpasses the MinMax intuition.\r\nM4 Aggregation. M4 is a composite value-preserving\r\naggregation (see Section 4.1) that groups a time series re\u0002lation into w equidistant time spans, such that each group\r\nexactly corresponds to a pixel column in the visualization.\r\nFor each group, M4 then computes the aggregates min(v),\r\nmax(v), min(t), and max(t) – hence the name M4 – and\r\nthen joins the aggregated data with the original time se\u0002ries, to add the missing timestamps tbottom and ttop and the\r\nmissing values vf irst and vlast.\r\nIn Figure 7a, we present an example query using the M4\r\naggregation. This SQL query is very similar to the MinMax\r\nquery in Figure 6a, adding only the min(t) and max(t) ag\u0002gregations and the additional join predicates based on the\r\nfirst and last timestamps tmin and tmax. Figure 7b depicts\r\nthe resulting visualization, which is now equal to the visu\u0002alization of the unreduced underlying time series.\r\nComplexity of M4. The required grouping and the\r\ncomputation of the aggregated values can be computed in\r\nO(n) for the n tuples of the base relation Q. The subse\u0002quent equi-join of the aggregated values with Q requires to\r\nmatch the n tuples in Q with 4 · w aggregated tuples, using\r\na hash-join in O(n+ 4 · w), but w does not depend on n and\r\nis inherently limited by physical display resolutions, e.g.,\r\nw = 5120 pixels for latest WHXGA displays. Therefore, the\r\ndescribed M4 aggregation of has complexity of O(n).\r\n4.3 Aggregation-Related Pixel Errors\r\nIn Figure 8 we compare three line visualizations: a) the\r\nschematic line visualization a time series Q, b) the visuali\u0002zation of M inMax(Q), and c) the visualization of M4(Q).\r\nM inMax(Q) does not select the first and last tuples per\r\npixel column, causing several types of line drawing errors.\r\nIn Figure 8b, the pixel (3,3) is not set correctly, since nei\u0002ther the start nor the end tuple of the corresponding line are\r\nincluded in the reduced data set. This kind of missing line\r\nerror E1 is distinctly visible with time series that have a very\r\nheterogeneous time distribution, i.e., notable gaps, resulting\r\nin pixel columns not holding any data (see pixel column 3\r\nin Figure 8b). A missing line error is often exacerbated by\r\n801",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/833a11cd-d5b1-4762-93ae-d64d29927af3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=878b74948fcb49e0699ebee53fac606673a9eadeceddf57f95c8c84ab82caea6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 994
      },
      {
        "segments": [
          {
            "segment_id": "833a11cd-d5b1-4762-93ae-d64d29927af3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "a) value-preserving MinMax aggregation query\r\nSELECT t,v FROM Q JOIN\r\n(SELECT round($w*(t-$t1)/($t2-$t1)) as k, -- define key\r\nmin(v) as v_min, max(v) as v_max -- get min,max\r\nFROM Q GROUP BY k) as QA -- group by k\r\nON k = round($w*(t-$t1)/($t2-$t1)) -- join on k\r\nAND (v = v_min OR v = v_max) -- &(min|max)\r\nb) resulting image c) expected image\r\nFigure 6: MinMax query and resulting visualization.\r\njoin predicates, such that all different aggregated values or\r\ntimestamps are correlated to either their missing timestamp\r\nor their missing value. The following MinMax aggregation\r\nwill provide an example of a composite aggregation.\r\nMinMax Aggregation. To preserve the shape of a time\r\nseries, the first intuition is to group-wise select the vertical\r\nextrema, which we denote as min and max tuples. This is a\r\ncomposite value-preserving aggregation, exemplified by the\r\nSQL query in Figure 6a. The query projects existing times\u0002tamps and values from a time series relation Q, after joining\r\nit with the aggregation results QA. Q is defined by an orig\u0002inal query, as already shown in Figure 4a. The group keys\r\nk are based on the rounded results of the (horizontal) geo\u0002metric transformation of Q to the visualization’s coordinate\r\nsystem. As stated before in Section 2, it is important that\r\nthis geometric transformation is exactly the same transfor\u0002mation as conducted by visualization client, before passing\r\nthe rescaled time series data to the line drawing routines for\r\nrasterization. Finally, the join of QA with Q is based on the\r\ngroup key k and on matching the values v in Q either with\r\nvmin or vmax from QA.\r\nFigure 6b shows the resulting visualization and we now\r\nobserve that it closely matches the expected visualization\r\nof the raw data 6c. In Section 4.3, we later discuss the\r\nremaining pixel errors; indicated by arrows in Figure 6b.\r\n4.2 The M4 Aggregation\r\nThe composite MinMax aggregation focuses on the vertical\r\nextrema of each pixel column, i.e., of each corresponding\r\ntime span. There are already existing approaches for se\u0002lecting extrema for the purpose of data reduction and data\r\nanalysis [12]. But most of them only partially consider the\r\nimplications for data visualization and neglect the final pro\u0002jection of the data to discrete screen pixels.\r\nA line chart that is based on a reduced data set, will al\u0002ways omit lines that would have connected the not selected\r\ntuples, and it will always add new approximating lines to\r\nbridge the not selected tuples between two consecutive se\u0002lected tuples. The resulting errors (in the real-valued vi\u0002sualization space) are significantly reduced by the final dis\u0002cretization process of the drawing routines. This effect is\r\nthe underlying principle of the proposed visualization-driven\r\ndata reduction.\r\nIntuitively, one may expect that selecting the minimum\r\nand maximum values, i.e., the tuples (tbottom, min(v)) and\r\n(ttop, max(v)), from exactly w groups, is sufficient to derive\r\na correct line visualization. This intuition is elusive, and this\r\na) value-preserving M4 aggregation query\r\nSELECT t,v FROM Q JOIN\r\n(SELECT round($w*(t-$t1)/($t2-$t1)) as k, --define key\r\nmin(v) as v_min, max(v) as v_max, --get min,max\r\nmin(t) as t_min, max(t) as t_max --get 1st,last\r\nFROM Q GROUP BY k) as QA --group by k\r\nON k = round($w*(t-$t1)/($t2-$t1)) --join on k\r\nAND (v = v_min OR v = v_max OR --&(min|max|\r\nt = t_min OR t = t_max) -- 1st|last)\r\nb) resulting image == expected image\r\nFigure 7: M4 query and resulting visualization.\r\nform of data reduction – provided by the MinMax aggrega\u0002tion – does not guarantee an error-free line visualization of\r\nthe time series. It ignores the important first and last tuples\r\nof the each group. We now introduce the M4 aggregation\r\nthat additionally selects these first and last tuples (min(t),\r\nvf irst) and (max(t), vlast). In Section 4.3, we then discuss\r\nhow M4 surpasses the MinMax intuition.\r\nM4 Aggregation. M4 is a composite value-preserving\r\naggregation (see Section 4.1) that groups a time series re\u0002lation into w equidistant time spans, such that each group\r\nexactly corresponds to a pixel column in the visualization.\r\nFor each group, M4 then computes the aggregates min(v),\r\nmax(v), min(t), and max(t) – hence the name M4 – and\r\nthen joins the aggregated data with the original time se\u0002ries, to add the missing timestamps tbottom and ttop and the\r\nmissing values vf irst and vlast.\r\nIn Figure 7a, we present an example query using the M4\r\naggregation. This SQL query is very similar to the MinMax\r\nquery in Figure 6a, adding only the min(t) and max(t) ag\u0002gregations and the additional join predicates based on the\r\nfirst and last timestamps tmin and tmax. Figure 7b depicts\r\nthe resulting visualization, which is now equal to the visu\u0002alization of the unreduced underlying time series.\r\nComplexity of M4. The required grouping and the\r\ncomputation of the aggregated values can be computed in\r\nO(n) for the n tuples of the base relation Q. The subse\u0002quent equi-join of the aggregated values with Q requires to\r\nmatch the n tuples in Q with 4 · w aggregated tuples, using\r\na hash-join in O(n+ 4 · w), but w does not depend on n and\r\nis inherently limited by physical display resolutions, e.g.,\r\nw = 5120 pixels for latest WHXGA displays. Therefore, the\r\ndescribed M4 aggregation of has complexity of O(n).\r\n4.3 Aggregation-Related Pixel Errors\r\nIn Figure 8 we compare three line visualizations: a) the\r\nschematic line visualization a time series Q, b) the visuali\u0002zation of M inMax(Q), and c) the visualization of M4(Q).\r\nM inMax(Q) does not select the first and last tuples per\r\npixel column, causing several types of line drawing errors.\r\nIn Figure 8b, the pixel (3,3) is not set correctly, since nei\u0002ther the start nor the end tuple of the corresponding line are\r\nincluded in the reduced data set. This kind of missing line\r\nerror E1 is distinctly visible with time series that have a very\r\nheterogeneous time distribution, i.e., notable gaps, resulting\r\nin pixel columns not holding any data (see pixel column 3\r\nin Figure 8b). A missing line error is often exacerbated by\r\n801",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/833a11cd-d5b1-4762-93ae-d64d29927af3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=878b74948fcb49e0699ebee53fac606673a9eadeceddf57f95c8c84ab82caea6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 994
      },
      {
        "segments": [
          {
            "segment_id": "e2d30349-0548-47e5-8675-f53efd112da9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "a) baseline vis. of Q\r\n1 2 3 4\r\nb) vis. of MinMax(Q) c) vis. of M4(Q)\r\n1\r\n2\r\n3\r\nE3\r\nE1\r\nE2\r\ninter\u0002group\r\nline\r\ninner\u0002group\r\nbackground pixels lines\r\nfore\u0002ground\r\npixels\r\nFigure 8: MinMax-related visualization error.\r\nan additional false line error E2, as the line drawing still\r\nrequires to connect two tuples to bridge the empty pixel\r\ncolumn. Furthermore, both error types may also occur in\r\nneighboring pixel columns, because the inner-column lines\r\ndo not always represent the complete set of pixels of a col\u0002umn. Additional inter-column pixels – below or above the\r\ninner-column pixels – can be derived from the inter-column\r\nlines. This will again cause missing or additional pixels if the\r\nreduced data set does not contain the correct tuples for the\r\ncorrect inter-column lines. In Figure 8b, the MinMax ag\u0002gregation causes such an error E3 by setting the undesired\r\npixel (1,2), derived from the false line between the maxi\u0002mum tuple in the first pixel column and the (consecutive)\r\nmaximum tuple in the second pixel column. Note that these\r\nerrors are independent of the resolution of the desired raster\r\nimage, i.e., of the chosen number of groups. If we do not ex\u0002plicitly select the first and last tuples, we cannot guarantee\r\nthat all tuples for drawing the correct inter-column lines are\r\nincluded.\r\n4.4 The M4 Upper Bound\r\nBased on the observation of the described errors, the ques\u0002tion arises if selecting only the four extremum tuples for each\r\npixel column guarantees an error-free visualization. In the\r\nfollowing, we prove the existence of an upper bound of tu\u0002ples necessary for an error-free, two-color line visualization.\r\nDefinition 1. A width-based grouping of an arbitrary\r\ntime series relation T(t, v) into w equidistant groups, de\u0002noted as G(T) = (B1, B2, ..., Bw), is derived from T using\r\nthe surjective grouping function i = fg(t) = round(w · (t −\r\ntstart)/(tend −tstart)) to assign any tuple of T to the groups\r\nBi. A tuple (t, v) is assigned to Bi if fg(t) = i.\r\nDefinition 2. A width-based M4 aggregation GM4(T)\r\nselects the extremum tuples (tbottomi, vmini), (ttopi, vmaxi),\r\n(tmini, vf irsti), and (tmax, vlasti) from each Bi of G(T).\r\nDefinition 3. A visualization relation V (x, y) with the\r\nattributes x ∈ N\r\n[1,w]\r\nand y ∈ N\r\n[1,h]\r\ncontains all foreground\r\npixels (x, y), representing all (black) pixels of all rasterized\r\nlines. V (x, y) contains none of the remaining (white) back\u0002ground pixels in N\r\n[1,w] × N[1,h]\r\n.\r\nDefinition 4. A line visualization operator viswh(T) →\r\nV defines, for all tuples (t, v) in T, the corresponding fore\u0002ground pixels (x, y) in V .\r\nThereby viswh first uses the linear transformation functions\r\nfx and fy to rescale all tuples in T to the coordinate system\r\nR\r\n[1,w] × R[1,h]\r\n(see Section 2) and then tests for all discrete\r\npixels and all non-discrete lines – defined by the consecutive\r\ntuples of the transformed time series – if a pixel is on the\r\nline or not on the line. For brevity, we omit a detailed de\u0002inner-column pixels\r\ndepend only on top &\r\nbottom pixels, derived\r\nfrom min/max tuples\r\nall non-inner\u0002column pixels\r\ncan be derived\r\nfrom first and\r\nlast tuples\r\ninner-column\r\npixel\r\ninter-column\r\npixels\r\nfirst and last tuples\r\nmin and max tuples\r\nFigure 9: Illustration of the Theorem 1.\r\nscription of line rasterization [2, 4], and assume the following\r\ntwo properties to be true.\r\nLemma 1. A rasterized line has no gaps.\r\nLemma 2. When rasterizing an inner-column line, no fore\u0002ground pixels are set outside of the pixel column.\r\nTheorem 1. Any two-color line visualization of an arbi\u0002trary time series T is equal to the two-color line visu\u0002alization of a time series T\r\n0\r\nthat contains at least the\r\nfour extrema of all groups of the width-based grouping\r\nof T, i.e., viswh(GM4(T)) = viswh(T).\r\nFigure 9 illustrates the reasoning of Theorem 1.\r\nProof. Suppose a visualization relation V = viswh(T) rep\u0002resents the pixels of a two-color line visualization of an ar\u0002bitrary time series T. Suppose a tuple can only be in one\r\nof the groups B1 to Bw that are defined by G(T) and that\r\ncorresponds to the pixel columns. Then there is only one\r\npair of consecutive tuples pj ∈ Bi and pj+1 ∈ Bi+1, i.e.,\r\nonly one inter-column line between each pair of consecutive\r\ngroups Bi and Bi+1. But then, all other lines defined by\r\nthe remaining pairs of consecutive tuples in T must define\r\ninner-column lines.\r\nAs of Lemmas 1 and 2, all inner-column pixels can be de\u0002fined from knowing the top and bottom inner-column pixels\r\nof each column. Furthermore, since fx and fy are linear\r\ntransformations, we can derive these top and bottom pixels\r\nof each column from the min and max tuples (tbottomi\r\n, vmini),\r\n(ttopi, vmaxi) for each group Bi. The remaining inter-column\r\npixels can be derived from all inter-column lines. Since there\r\nis only one inter-column line pjpj+1 between each pair of\r\nconsecutive groups Bi and Bi+1, the tuple pj ∈ Bi is the\r\nlast tuple (tmaxivlasti) of Bi and pj+1 ∈ Bi+1 is the first\r\ntuple (tmini+1 , vf irsti+1 ) of Bi+1.\r\nConsequently, all pixels of the visualization relation V can\r\nbe derived from the tuples (tbottomi, vmini), (ttopi, vmaxi),\r\n(tmaxivlasti), (tmini, vf irsti) of each group Bi, i.e., V =\r\nviswh(GM4(T)) = viswh(T). \u0003\r\nUsing Theorem 1, we can moreover derive\r\nTheorem 2. There exists an error-free two-color line vi\u0002sualization of an arbitrary time series T, based on a\r\nsubset T\r\n0\r\nof T, with |T\r\n0\r\n| ≤ 4 · w.\r\nNo matter how big T is, selecting the correct 4·w tuples from\r\nT allows us to create a perfect visualization of T. Clearly, for\r\nthe purpose of line visualization, M4 queries provide data\u0002efficient, predictable data reduction.\r\n5. TIME SERIES DATA REDUCTION\r\nIn Section 4, we discussed averaging, systematic sampling\r\nand random sampling as measures for data reduction. They\r\nprovide some utility for time series dimensionality reduction\r\n802",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/e2d30349-0548-47e5-8675-f53efd112da9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5fd5448de484aa3347e9047b265ff4ce7dfc472a6be2b2d5ee074be047ba8f38",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 986
      },
      {
        "segments": [
          {
            "segment_id": "e2d30349-0548-47e5-8675-f53efd112da9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "a) baseline vis. of Q\r\n1 2 3 4\r\nb) vis. of MinMax(Q) c) vis. of M4(Q)\r\n1\r\n2\r\n3\r\nE3\r\nE1\r\nE2\r\ninter\u0002group\r\nline\r\ninner\u0002group\r\nbackground pixels lines\r\nfore\u0002ground\r\npixels\r\nFigure 8: MinMax-related visualization error.\r\nan additional false line error E2, as the line drawing still\r\nrequires to connect two tuples to bridge the empty pixel\r\ncolumn. Furthermore, both error types may also occur in\r\nneighboring pixel columns, because the inner-column lines\r\ndo not always represent the complete set of pixels of a col\u0002umn. Additional inter-column pixels – below or above the\r\ninner-column pixels – can be derived from the inter-column\r\nlines. This will again cause missing or additional pixels if the\r\nreduced data set does not contain the correct tuples for the\r\ncorrect inter-column lines. In Figure 8b, the MinMax ag\u0002gregation causes such an error E3 by setting the undesired\r\npixel (1,2), derived from the false line between the maxi\u0002mum tuple in the first pixel column and the (consecutive)\r\nmaximum tuple in the second pixel column. Note that these\r\nerrors are independent of the resolution of the desired raster\r\nimage, i.e., of the chosen number of groups. If we do not ex\u0002plicitly select the first and last tuples, we cannot guarantee\r\nthat all tuples for drawing the correct inter-column lines are\r\nincluded.\r\n4.4 The M4 Upper Bound\r\nBased on the observation of the described errors, the ques\u0002tion arises if selecting only the four extremum tuples for each\r\npixel column guarantees an error-free visualization. In the\r\nfollowing, we prove the existence of an upper bound of tu\u0002ples necessary for an error-free, two-color line visualization.\r\nDefinition 1. A width-based grouping of an arbitrary\r\ntime series relation T(t, v) into w equidistant groups, de\u0002noted as G(T) = (B1, B2, ..., Bw), is derived from T using\r\nthe surjective grouping function i = fg(t) = round(w · (t −\r\ntstart)/(tend −tstart)) to assign any tuple of T to the groups\r\nBi. A tuple (t, v) is assigned to Bi if fg(t) = i.\r\nDefinition 2. A width-based M4 aggregation GM4(T)\r\nselects the extremum tuples (tbottomi, vmini), (ttopi, vmaxi),\r\n(tmini, vf irsti), and (tmax, vlasti) from each Bi of G(T).\r\nDefinition 3. A visualization relation V (x, y) with the\r\nattributes x ∈ N\r\n[1,w]\r\nand y ∈ N\r\n[1,h]\r\ncontains all foreground\r\npixels (x, y), representing all (black) pixels of all rasterized\r\nlines. V (x, y) contains none of the remaining (white) back\u0002ground pixels in N\r\n[1,w] × N[1,h]\r\n.\r\nDefinition 4. A line visualization operator viswh(T) →\r\nV defines, for all tuples (t, v) in T, the corresponding fore\u0002ground pixels (x, y) in V .\r\nThereby viswh first uses the linear transformation functions\r\nfx and fy to rescale all tuples in T to the coordinate system\r\nR\r\n[1,w] × R[1,h]\r\n(see Section 2) and then tests for all discrete\r\npixels and all non-discrete lines – defined by the consecutive\r\ntuples of the transformed time series – if a pixel is on the\r\nline or not on the line. For brevity, we omit a detailed de\u0002inner-column pixels\r\ndepend only on top &\r\nbottom pixels, derived\r\nfrom min/max tuples\r\nall non-inner\u0002column pixels\r\ncan be derived\r\nfrom first and\r\nlast tuples\r\ninner-column\r\npixel\r\ninter-column\r\npixels\r\nfirst and last tuples\r\nmin and max tuples\r\nFigure 9: Illustration of the Theorem 1.\r\nscription of line rasterization [2, 4], and assume the following\r\ntwo properties to be true.\r\nLemma 1. A rasterized line has no gaps.\r\nLemma 2. When rasterizing an inner-column line, no fore\u0002ground pixels are set outside of the pixel column.\r\nTheorem 1. Any two-color line visualization of an arbi\u0002trary time series T is equal to the two-color line visu\u0002alization of a time series T\r\n0\r\nthat contains at least the\r\nfour extrema of all groups of the width-based grouping\r\nof T, i.e., viswh(GM4(T)) = viswh(T).\r\nFigure 9 illustrates the reasoning of Theorem 1.\r\nProof. Suppose a visualization relation V = viswh(T) rep\u0002resents the pixels of a two-color line visualization of an ar\u0002bitrary time series T. Suppose a tuple can only be in one\r\nof the groups B1 to Bw that are defined by G(T) and that\r\ncorresponds to the pixel columns. Then there is only one\r\npair of consecutive tuples pj ∈ Bi and pj+1 ∈ Bi+1, i.e.,\r\nonly one inter-column line between each pair of consecutive\r\ngroups Bi and Bi+1. But then, all other lines defined by\r\nthe remaining pairs of consecutive tuples in T must define\r\ninner-column lines.\r\nAs of Lemmas 1 and 2, all inner-column pixels can be de\u0002fined from knowing the top and bottom inner-column pixels\r\nof each column. Furthermore, since fx and fy are linear\r\ntransformations, we can derive these top and bottom pixels\r\nof each column from the min and max tuples (tbottomi\r\n, vmini),\r\n(ttopi, vmaxi) for each group Bi. The remaining inter-column\r\npixels can be derived from all inter-column lines. Since there\r\nis only one inter-column line pjpj+1 between each pair of\r\nconsecutive groups Bi and Bi+1, the tuple pj ∈ Bi is the\r\nlast tuple (tmaxivlasti) of Bi and pj+1 ∈ Bi+1 is the first\r\ntuple (tmini+1 , vf irsti+1 ) of Bi+1.\r\nConsequently, all pixels of the visualization relation V can\r\nbe derived from the tuples (tbottomi, vmini), (ttopi, vmaxi),\r\n(tmaxivlasti), (tmini, vf irsti) of each group Bi, i.e., V =\r\nviswh(GM4(T)) = viswh(T). \u0003\r\nUsing Theorem 1, we can moreover derive\r\nTheorem 2. There exists an error-free two-color line vi\u0002sualization of an arbitrary time series T, based on a\r\nsubset T\r\n0\r\nof T, with |T\r\n0\r\n| ≤ 4 · w.\r\nNo matter how big T is, selecting the correct 4·w tuples from\r\nT allows us to create a perfect visualization of T. Clearly, for\r\nthe purpose of line visualization, M4 queries provide data\u0002efficient, predictable data reduction.\r\n5. TIME SERIES DATA REDUCTION\r\nIn Section 4, we discussed averaging, systematic sampling\r\nand random sampling as measures for data reduction. They\r\nprovide some utility for time series dimensionality reduction\r\n802",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/e2d30349-0548-47e5-8675-f53efd112da9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5fd5448de484aa3347e9047b265ff4ce7dfc472a6be2b2d5ee074be047ba8f38",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 986
      },
      {
        "segments": [
          {
            "segment_id": "b0ab1bcb-4c7f-48bb-a261-b2a8437e83a3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "in general, and may also provide useful data reduction for\r\ncertain types of visualizations. However, for rasterized line\r\nvisualizations, we have now proven the necessity of selecting\r\nthe correct min, max, first, and last tuples. As a result, any\r\naveraging, systematic, or random approach will fail to obtain\r\ngood results for line visualizations, if it does not ensure that\r\nthese important tuples are included.\r\nCompared to our approach, the most competitive appro\u0002aches, as found in the literature [11], are times series dimen\u0002sionality reduction techniques based on line simplification.\r\nFor example, Fu et al. reduce a time series based on percep\u0002tually important points (PIP) [12]. Such approaches usually\r\napply a distance measure defined between each three con\u0002secutive points of a line, and try to minimize this measure \u000f\r\nfor a selected data reduction rate. This min − \u000f problem is\r\ncomplemented with a min-# problem, where the minimum\r\nnumber of points needs to be found for a defined distance\r\n\u000f. However, due to the O(n\r\n2\r\n) worst case complexity [19]\r\nof this optimization problem, there are many heuristic algo\u0002rithms for line simplification. The fastest class of algorithms\r\nworks sequentially in O(n), processing every point of the line\r\nonly once. The two other main classes have complexity of\r\nO(n log(n)) and either merge points until some error crite\u0002rion is met (bottom up) or split the line (top down), starting\r\nwith an initial approximating line, defined by the first and\r\nlast points of the original line. The latter approaches usu\u0002ally provide a much better approximation of the original line\r\n[25]. Our aggregation-based data reduction, including M4,\r\nhas a complexity of O(n).\r\nApproximation Quality. To determine the approxima\u0002tion quality of a derived time series, most time series di\u0002mensionality reduction techniques use a geometric distance\r\nmeasure, defined between the segments of the approximat\u0002ing line and the (removed) points of the original line.\r\nThe most common measure is the Euclidean (perpendicu\u0002lar) distance, as shown in Figure 10a), which is the shortest\r\ndistance of a point pk to a line segment pipj . Other com\u0002monly applied distance measures are the area of the triangle\r\n(pi, pk, pj ) (Figure 10b) or the vertical distance of pk to pipj\r\n(Figure 10c).\r\noriginal line approximating line\r\nb) triangle\r\n area\r\na) perpendicular\r\n distance\r\nc) vertical\r\n distance\r\nFigure 10: Distance measures for line simplification.\r\nHowever, when visualizing a line using discrete pixels, the\r\nmain shortcoming of the described measures is their gener\u0002ality. They are geometric measures, defined in R × R, and\r\ndo not consider the discontinuities in discrete 2D space, as\r\ndefined by the cutting lines of two neighboring pixel rows or\r\ntwo neighboring pixel columns. For our approach, we con\u0002sult the actual visualization to determine the approximation\r\nquality of the data reduction operators.\r\nVisualization Quality. Two images of the same size\r\ncan be easily compared pixel by pixel. A simple, com\u0002monly applied error measure is the mean square error MSE\r\n=\r\n1\r\nwh\r\nPw\r\nx=1\r\nPh\r\ny=1(Ix,y(V1) − Ix,y(V2))2\r\n, with Ix,y defining\r\nthe luminance value of a pixel (x, y) of a visualization V .\r\nNonetheless, Wang et al. have shown [27] that MSE-based\r\nmeasures, including the commonly applied peak-signal-to\u0002noise-ratio (PSNR) [5], do not approximate the model of\r\nhuman perception very well and developed the Structural\r\nSimilarity Index (SSIM). For brevity and due to the com\u0002plexity of this measure, we have to pass on without a detailed\r\ndescription of this measure. The SSIM yields a similarity\r\nvalue between 1 and −1. The related normalized distance\r\nmeasure between two visualizations V1 and V2 is defined as:\r\nDSSIM(V1, V2) = 1 − SSIM(V1, V2)\r\n2\r\n(6)\r\nWe use DSSIM to evaluate the quality of a line visualization\r\nthat is based on a reduced time series data set, compar\u0002ing it with the original line visualization of the underlying\r\nunreduced time series data set.\r\n6. EVALUATION\r\nIn the following evaluation, we will compare the data re\u0002duction efficiency of the M4 aggregation with state-of-the\u0002art line simplification approaches and with commonly used\r\nnaive approaches, such as averaging, sampling, and round\u0002ing. Therefore, we apply all considered techniques to several\r\nreal world data sets and measure the resulting visualization\r\nquality. For all aggregation-based data reduction operators,\r\nwhich can be expressed using the relational algebra, we also\r\nevaluate the query execution performance.\r\n6.1 Real World Time Series Data\r\nWe consider three different data sets: the price of a single\r\nshare on the Frankfurt stock exchange over 6 weeks (700k\r\ntuples), 71 minutes from a speed sensor of a soccer ball\r\n[22](ball number 8, 7M rows), and one week of sensor data\r\nfrom an electrical power sensor of a semiconductor manufac\u0002turing machine [15](sensor MF03, 55M rows). In Figure 11,\r\nwe show excerpts of these data sets to display the differences\r\nin time and value distribution.\r\nt\r\nelectrical power\r\nP (W)\r\nc)\r\n€\r\nt\r\na) stock price\r\nv (µm/s)\r\nt\r\nb) ball speed\r\nFigure 11: a) financial, b) soccer, c) machine data.\r\nFor example, the financial data set (a) contains over 20000\r\ntuples per working day, with the share price changing only\r\nslowly over time. In contrast to that the soccer data set (b)\r\ncontains over 1500 readings per second and is best described\r\nas a sequence of bursts. Finally, the machine sensor data (c)\r\nat 100Hz constitutes a mixed signal that has time spans of\r\nlow, high, and also bursty variation.\r\n6.2 Query Execution Performance\r\nIn Section 4, we described how to express simple sampling or\r\naggregation-based data reduction operators using relational\r\nalgebra, including our proposed M4 aggregation. We now\r\nevaluate the query execution performance of these different\r\noperators. All evaluated queries were issued as SQL queries\r\n803",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/b0ab1bcb-4c7f-48bb-a261-b2a8437e83a3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f40b788bee79d908f128f9c7ba740cd2b9201e9f861edd421938288cf54e6eda",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 931
      },
      {
        "segments": [
          {
            "segment_id": "b0ab1bcb-4c7f-48bb-a261-b2a8437e83a3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "in general, and may also provide useful data reduction for\r\ncertain types of visualizations. However, for rasterized line\r\nvisualizations, we have now proven the necessity of selecting\r\nthe correct min, max, first, and last tuples. As a result, any\r\naveraging, systematic, or random approach will fail to obtain\r\ngood results for line visualizations, if it does not ensure that\r\nthese important tuples are included.\r\nCompared to our approach, the most competitive appro\u0002aches, as found in the literature [11], are times series dimen\u0002sionality reduction techniques based on line simplification.\r\nFor example, Fu et al. reduce a time series based on percep\u0002tually important points (PIP) [12]. Such approaches usually\r\napply a distance measure defined between each three con\u0002secutive points of a line, and try to minimize this measure \u000f\r\nfor a selected data reduction rate. This min − \u000f problem is\r\ncomplemented with a min-# problem, where the minimum\r\nnumber of points needs to be found for a defined distance\r\n\u000f. However, due to the O(n\r\n2\r\n) worst case complexity [19]\r\nof this optimization problem, there are many heuristic algo\u0002rithms for line simplification. The fastest class of algorithms\r\nworks sequentially in O(n), processing every point of the line\r\nonly once. The two other main classes have complexity of\r\nO(n log(n)) and either merge points until some error crite\u0002rion is met (bottom up) or split the line (top down), starting\r\nwith an initial approximating line, defined by the first and\r\nlast points of the original line. The latter approaches usu\u0002ally provide a much better approximation of the original line\r\n[25]. Our aggregation-based data reduction, including M4,\r\nhas a complexity of O(n).\r\nApproximation Quality. To determine the approxima\u0002tion quality of a derived time series, most time series di\u0002mensionality reduction techniques use a geometric distance\r\nmeasure, defined between the segments of the approximat\u0002ing line and the (removed) points of the original line.\r\nThe most common measure is the Euclidean (perpendicu\u0002lar) distance, as shown in Figure 10a), which is the shortest\r\ndistance of a point pk to a line segment pipj . Other com\u0002monly applied distance measures are the area of the triangle\r\n(pi, pk, pj ) (Figure 10b) or the vertical distance of pk to pipj\r\n(Figure 10c).\r\noriginal line approximating line\r\nb) triangle\r\n area\r\na) perpendicular\r\n distance\r\nc) vertical\r\n distance\r\nFigure 10: Distance measures for line simplification.\r\nHowever, when visualizing a line using discrete pixels, the\r\nmain shortcoming of the described measures is their gener\u0002ality. They are geometric measures, defined in R × R, and\r\ndo not consider the discontinuities in discrete 2D space, as\r\ndefined by the cutting lines of two neighboring pixel rows or\r\ntwo neighboring pixel columns. For our approach, we con\u0002sult the actual visualization to determine the approximation\r\nquality of the data reduction operators.\r\nVisualization Quality. Two images of the same size\r\ncan be easily compared pixel by pixel. A simple, com\u0002monly applied error measure is the mean square error MSE\r\n=\r\n1\r\nwh\r\nPw\r\nx=1\r\nPh\r\ny=1(Ix,y(V1) − Ix,y(V2))2\r\n, with Ix,y defining\r\nthe luminance value of a pixel (x, y) of a visualization V .\r\nNonetheless, Wang et al. have shown [27] that MSE-based\r\nmeasures, including the commonly applied peak-signal-to\u0002noise-ratio (PSNR) [5], do not approximate the model of\r\nhuman perception very well and developed the Structural\r\nSimilarity Index (SSIM). For brevity and due to the com\u0002plexity of this measure, we have to pass on without a detailed\r\ndescription of this measure. The SSIM yields a similarity\r\nvalue between 1 and −1. The related normalized distance\r\nmeasure between two visualizations V1 and V2 is defined as:\r\nDSSIM(V1, V2) = 1 − SSIM(V1, V2)\r\n2\r\n(6)\r\nWe use DSSIM to evaluate the quality of a line visualization\r\nthat is based on a reduced time series data set, compar\u0002ing it with the original line visualization of the underlying\r\nunreduced time series data set.\r\n6. EVALUATION\r\nIn the following evaluation, we will compare the data re\u0002duction efficiency of the M4 aggregation with state-of-the\u0002art line simplification approaches and with commonly used\r\nnaive approaches, such as averaging, sampling, and round\u0002ing. Therefore, we apply all considered techniques to several\r\nreal world data sets and measure the resulting visualization\r\nquality. For all aggregation-based data reduction operators,\r\nwhich can be expressed using the relational algebra, we also\r\nevaluate the query execution performance.\r\n6.1 Real World Time Series Data\r\nWe consider three different data sets: the price of a single\r\nshare on the Frankfurt stock exchange over 6 weeks (700k\r\ntuples), 71 minutes from a speed sensor of a soccer ball\r\n[22](ball number 8, 7M rows), and one week of sensor data\r\nfrom an electrical power sensor of a semiconductor manufac\u0002turing machine [15](sensor MF03, 55M rows). In Figure 11,\r\nwe show excerpts of these data sets to display the differences\r\nin time and value distribution.\r\nt\r\nelectrical power\r\nP (W)\r\nc)\r\n€\r\nt\r\na) stock price\r\nv (µm/s)\r\nt\r\nb) ball speed\r\nFigure 11: a) financial, b) soccer, c) machine data.\r\nFor example, the financial data set (a) contains over 20000\r\ntuples per working day, with the share price changing only\r\nslowly over time. In contrast to that the soccer data set (b)\r\ncontains over 1500 readings per second and is best described\r\nas a sequence of bursts. Finally, the machine sensor data (c)\r\nat 100Hz constitutes a mixed signal that has time spans of\r\nlow, high, and also bursty variation.\r\n6.2 Query Execution Performance\r\nIn Section 4, we described how to express simple sampling or\r\naggregation-based data reduction operators using relational\r\nalgebra, including our proposed M4 aggregation. We now\r\nevaluate the query execution performance of these different\r\noperators. All evaluated queries were issued as SQL queries\r\n803",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/b0ab1bcb-4c7f-48bb-a261-b2a8437e83a3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f40b788bee79d908f128f9c7ba740cd2b9201e9f861edd421938288cf54e6eda",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 931
      },
      {
        "segments": [
          {
            "segment_id": "21ec1d6a-15c4-45f0-829b-6ceec84320e3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 12: Query performance: (a,b,c,d) financial data, (e,f) soccer data, (g,h) machine data.\r\nvia ODBC over a (100Mbit) wide area network to a virtu\u0002alized, shared SAP HANA v1.00.70 instance, running in an\r\nSAP data center at a remote location.\r\nThe considered queries are: 1) a baseline query that se\u0002lects all tuples to be visualized, 2) a PAA-query that com\u0002putes up to 4·w average tuples, 3) a two-dimensional round\u0002ing query that selects up to w ·h rounded tuples, 4) a strati\u0002fied random sampling query that selects 4·w random tuples,\r\n5) a systematic sampling query that selects 4 ·w first tuples,\r\n6) a MinMax query that selects the two min and max tu\u0002ples from 2 · w groups, and finally 7) our M4 query selecting\r\nall four extrema from w groups. Note that we adjusted the\r\ngroup numbers to ensure a fair comparison at similar data\r\nreduction rates, such that any reduction query can at most\r\nproduce 4 · w tuples. All queries are parametrized using a\r\nwidth w = 1000, and (if required) a height h = 200.\r\nIn Figure 12, we plot the corresponding query execution\r\ntimes and total times for the three data sets. The total time\r\nmeasures the time from issuing the query to receiving all\r\nresults at the SQL client. For the financial data set, we first\r\nselected three days from the data (70k rows). We ran each\r\nquery 20 times and obtain the query execution times, shown\r\nin Figure 12a. The fastest query is the baseline query, as it is\r\na simple selection without additional operators. The other\r\nqueries are slower, as they have to compute the additional\r\ndata reduction. The slowest query is the rounding query,\r\nbecause it groups the data in two dimensions by w and h.\r\nThe other data reduction queries only require one horizontal\r\ngrouping. Comparing these execution times with the total\r\ntimes in Figure 12b, we see the baseline query losing its\r\nedge in query execution, ending up one order of magnitude\r\nslower than the other queries. Even for the low number of\r\n70k rows, the baseline query is dominated by the additional\r\ndata transport time. Regarding the resulting total times, all\r\ndata reduction queries are on the same level and manage to\r\nstay below one second. Note that the M4 aggregation does\r\nnot have significantly higher query execution times and total\r\ntimes than the other queries. The observations are similar\r\nwhen selecting 700k rows (30 days) from the financial data\r\nset (Figure 12c and 12d). The aggregation-based queries,\r\nincluding M4, are overall one order of magnitude faster than\r\nthe baseline at a negligible increase of query execution time.\r\nOur measurements show very similar results when running\r\nthe different types of data reduction queries on the soccer\r\nand machine data sets. We requested 1400k rows from the\r\nsoccer data set, and 3600k rows from the machine data set.\r\nThe results in Figure 12(e–h) again show an improvement\r\nof total time by up to one order of magnitude.\r\nnumber of underlying rows\r\ntotal time (s)\r\nFigure 13: Performance with varying row count.\r\nWe repeated all our tests, using a Postgres 8.4.11 RDBMS\r\nrunning on an Xeon E5-2620 with 2.00GHz, 64GB RAM,\r\n1TB HDD (no SSD) on Red Hat 6.3, hosted in the same\r\ndata center as the HANA instances. All data was served\r\nfrom a RAM disk. The Postgres working memory was set\r\nto 8GB. In Figure 13, we show the exemplary results for\r\nthe soccer data set, plotting the total time for increasing,\r\nrequested time spans, i.e., increasing number of underlying\r\nrows. We again observe the baseline query heavily depend\u0002ing on the limited network bandwidth. The aggregation\u0002based approaches again perform much better. We made\r\ncomparable observations with the finance data set and the\r\nmachine data set.\r\nThe results of both the Postgres and the HANA system\r\nshow that the baseline query, fetching all rows, mainly de\u0002pends on the database-outgoing network bandwidth. In con\u0002804",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/21ec1d6a-15c4-45f0-829b-6ceec84320e3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=47ac9deb4f0ce6f3a7aca5d558d8166038e6d49182518929de614b2533d5d815",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 652
      },
      {
        "segments": [
          {
            "segment_id": "21ec1d6a-15c4-45f0-829b-6ceec84320e3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 12: Query performance: (a,b,c,d) financial data, (e,f) soccer data, (g,h) machine data.\r\nvia ODBC over a (100Mbit) wide area network to a virtu\u0002alized, shared SAP HANA v1.00.70 instance, running in an\r\nSAP data center at a remote location.\r\nThe considered queries are: 1) a baseline query that se\u0002lects all tuples to be visualized, 2) a PAA-query that com\u0002putes up to 4·w average tuples, 3) a two-dimensional round\u0002ing query that selects up to w ·h rounded tuples, 4) a strati\u0002fied random sampling query that selects 4·w random tuples,\r\n5) a systematic sampling query that selects 4 ·w first tuples,\r\n6) a MinMax query that selects the two min and max tu\u0002ples from 2 · w groups, and finally 7) our M4 query selecting\r\nall four extrema from w groups. Note that we adjusted the\r\ngroup numbers to ensure a fair comparison at similar data\r\nreduction rates, such that any reduction query can at most\r\nproduce 4 · w tuples. All queries are parametrized using a\r\nwidth w = 1000, and (if required) a height h = 200.\r\nIn Figure 12, we plot the corresponding query execution\r\ntimes and total times for the three data sets. The total time\r\nmeasures the time from issuing the query to receiving all\r\nresults at the SQL client. For the financial data set, we first\r\nselected three days from the data (70k rows). We ran each\r\nquery 20 times and obtain the query execution times, shown\r\nin Figure 12a. The fastest query is the baseline query, as it is\r\na simple selection without additional operators. The other\r\nqueries are slower, as they have to compute the additional\r\ndata reduction. The slowest query is the rounding query,\r\nbecause it groups the data in two dimensions by w and h.\r\nThe other data reduction queries only require one horizontal\r\ngrouping. Comparing these execution times with the total\r\ntimes in Figure 12b, we see the baseline query losing its\r\nedge in query execution, ending up one order of magnitude\r\nslower than the other queries. Even for the low number of\r\n70k rows, the baseline query is dominated by the additional\r\ndata transport time. Regarding the resulting total times, all\r\ndata reduction queries are on the same level and manage to\r\nstay below one second. Note that the M4 aggregation does\r\nnot have significantly higher query execution times and total\r\ntimes than the other queries. The observations are similar\r\nwhen selecting 700k rows (30 days) from the financial data\r\nset (Figure 12c and 12d). The aggregation-based queries,\r\nincluding M4, are overall one order of magnitude faster than\r\nthe baseline at a negligible increase of query execution time.\r\nOur measurements show very similar results when running\r\nthe different types of data reduction queries on the soccer\r\nand machine data sets. We requested 1400k rows from the\r\nsoccer data set, and 3600k rows from the machine data set.\r\nThe results in Figure 12(e–h) again show an improvement\r\nof total time by up to one order of magnitude.\r\nnumber of underlying rows\r\ntotal time (s)\r\nFigure 13: Performance with varying row count.\r\nWe repeated all our tests, using a Postgres 8.4.11 RDBMS\r\nrunning on an Xeon E5-2620 with 2.00GHz, 64GB RAM,\r\n1TB HDD (no SSD) on Red Hat 6.3, hosted in the same\r\ndata center as the HANA instances. All data was served\r\nfrom a RAM disk. The Postgres working memory was set\r\nto 8GB. In Figure 13, we show the exemplary results for\r\nthe soccer data set, plotting the total time for increasing,\r\nrequested time spans, i.e., increasing number of underlying\r\nrows. We again observe the baseline query heavily depend\u0002ing on the limited network bandwidth. The aggregation\u0002based approaches again perform much better. We made\r\ncomparable observations with the finance data set and the\r\nmachine data set.\r\nThe results of both the Postgres and the HANA system\r\nshow that the baseline query, fetching all rows, mainly de\u0002pends on the database-outgoing network bandwidth. In con\u0002804",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/21ec1d6a-15c4-45f0-829b-6ceec84320e3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=47ac9deb4f0ce6f3a7aca5d558d8166038e6d49182518929de614b2533d5d815",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 652
      },
      {
        "segments": [
          {
            "segment_id": "030be5cd-d2df-4685-9ebe-19512be7978f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "a) M4 and MinMax vs. Aggregation b) M4 and MinMax vs. Line Simplification\r\nI binary\r\nround2d\r\nnh=w nh=2*w\r\nnumber of tuples number of tuples\r\nII anti-aliased\r\nnumber of tuples number of tuples\r\nFigure 14: Data efficiency of evaluated techniques, showing DSSIM over data volume.\r\ntrast, the size of the result sets of all aggregation-based\r\nqueries for any amount of underlying data is more or less\r\nconstant and below 4·w. Their total time mainly depends on\r\nthe database-internal query execution time. This evaluation\r\nalso shows that our proposed M4 aggregation is equally fast\r\nas common aggregation-based data reduction techniques.\r\nM4 can reduce the time the user has to wait for the data by\r\none order of magnitude in all our scenarios, and still provide\r\nthe correct tuples for high quality line visualizations.\r\n6.3 Visualization Quality and Data Efficiency\r\nWe now evaluate the robustness and the data efficiency re\u0002garding the achievable visualization quality. Therefore, we\r\ntest M4 and the other aggregation techniques using different\r\nnumbers of horizontal groups nh. We start with nh = 1 and\r\nend at nh = 2.5·w. Thereby we want to select at most 10·w\r\nrows, i.e., twice as much data as is actually required for an\r\nerror-free two-color line visualization. Based on the reduced\r\ndata sets we compute an (approximating) visualization and\r\ncompare it with the (baseline) visualization of the original\r\ndata set. All considered visualizations are drawn using the\r\nopen source Cairo graphics library (cairographics.org). The\r\ndistance measure is the DSSIM, as motivated in Section 5.\r\nThe underlying original time series of the evaluation sce\u0002nario are 70k tuples (3 days) from the financial data set.\r\nThe related visualization has w = 200 and h = 50. In the\r\nevaluated scenario, we allow the number of groups nh to be\r\ndifferent than the width w of the visualization. This will\r\nshow the robustness of our approach. However, in a real im\u0002plementation, the engineers have to make sure that nh = w\r\nto achieve the best results. In addition to the aggregation\u0002based operators, we also compare our approach with three\r\ndifferent line simplification approaches, as described in Sec\u0002tion 5. We use the Reumann-Wikham algorithm (reuwi)\r\n[24] as representative for sequential line simplification, the\r\ntop-down Ramer-Douglas-Peucker (RDP) algorithm [6, 14],\r\nand the bottom-up Visvalingam-Whyatts (visval) algorithm\r\n[26]. The RDP algorithm, does not allow setting a desired\r\ndata reduction ratio, thus we precomputed the minimal \u000f\r\nthat would produce a number of tuples proportional to the\r\nconsidered nh.\r\nIn Figure 14, we plot the measured, resulting visualization\r\nquality (DSSIM) over the resulting number of tuples of each\r\ndifferent groupings nh = 1 to nh = 2.5·w of an applied data\r\nreduction technique. For readability, we cut off all low qual\u0002ity results with DSSIM < 0.8. The lower the number of\r\ntuples and the higher the DSSIM, the more data efficient is\r\nthe corresponding technique for the purpose of line visuali\u0002zation. The Figures 14aI and 14bI depict these measures for\r\nbinary line visualizations and the Figures 14aII and 14bII\r\nfor anti-aliased line visualizations. We now observe the fol\u0002lowing results.\r\nSampling and Averaging operators (avg, first, and sran\u0002dom) select a single aggregated value per (horizontal) group.\r\nThey all show similar results and provide the lowest DSSIM.\r\nAs discussed in Section 4, they will often fail to select the\r\ntuples that are important for line rasterization, i.e., the min,\r\nmax, first, and last tuples that are required to set the correct\r\ninner-column and inter-column pixels.\r\n2D-Rounding requires an additional vertical grouping\r\ninto nv groups. We set nv = w/h·nh to a have proportional\r\nvertical grouping. The average visualization quality of 2D\u0002rounding is higher than that of averaging and sampling.\r\nMinMax queries select min(v) and max(v) and the cor\u0002responding timestamps per group. They provide very high\r\nDSSIM values already at low data volumes. On average,\r\nthey have a higher data efficiency than all aggregation based\r\ntechniques, (including M4, see Figure 14a), but are partially\r\nsurpassed by line simplification approaches (see Figure 14b).\r\nLine Simplification techniques (RDP and visval) on av\u0002erage provide better results than the aggregation-based tech\u0002niques (compare Figures 14a and 14b). As seen previously\r\n[25], top-down (RDP) and bottom-up (visval) algorithms\r\nperform much better than the sequential ones (reuwi). How\u0002ever, in context of rasterized line visualizations they are sur\u0002passed by M4 and also MinMax at nh = w and nh = 2 · w.\r\nThese techniques often miss one of the min, max, first, or last\r\ntuples, because these tuples must not necessarily comply to\r\nthe geometric distance measures used for line simplification,\r\nas described in Section 5.\r\nM4 queries select min(v), max(v), min(t), max(t) and\r\nthe corresponding timestamps and values per group. On av\u0002erage M4 provides a visualization quality of DSSIM > 0.9\r\n805",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/030be5cd-d2df-4685-9ebe-19512be7978f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ab805264658cdb8cc1b840c63321335ce68818c9f4fff95f337d5ecd41d9ef6d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 782
      },
      {
        "segments": [
          {
            "segment_id": "030be5cd-d2df-4685-9ebe-19512be7978f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "a) M4 and MinMax vs. Aggregation b) M4 and MinMax vs. Line Simplification\r\nI binary\r\nround2d\r\nnh=w nh=2*w\r\nnumber of tuples number of tuples\r\nII anti-aliased\r\nnumber of tuples number of tuples\r\nFigure 14: Data efficiency of evaluated techniques, showing DSSIM over data volume.\r\ntrast, the size of the result sets of all aggregation-based\r\nqueries for any amount of underlying data is more or less\r\nconstant and below 4·w. Their total time mainly depends on\r\nthe database-internal query execution time. This evaluation\r\nalso shows that our proposed M4 aggregation is equally fast\r\nas common aggregation-based data reduction techniques.\r\nM4 can reduce the time the user has to wait for the data by\r\none order of magnitude in all our scenarios, and still provide\r\nthe correct tuples for high quality line visualizations.\r\n6.3 Visualization Quality and Data Efficiency\r\nWe now evaluate the robustness and the data efficiency re\u0002garding the achievable visualization quality. Therefore, we\r\ntest M4 and the other aggregation techniques using different\r\nnumbers of horizontal groups nh. We start with nh = 1 and\r\nend at nh = 2.5·w. Thereby we want to select at most 10·w\r\nrows, i.e., twice as much data as is actually required for an\r\nerror-free two-color line visualization. Based on the reduced\r\ndata sets we compute an (approximating) visualization and\r\ncompare it with the (baseline) visualization of the original\r\ndata set. All considered visualizations are drawn using the\r\nopen source Cairo graphics library (cairographics.org). The\r\ndistance measure is the DSSIM, as motivated in Section 5.\r\nThe underlying original time series of the evaluation sce\u0002nario are 70k tuples (3 days) from the financial data set.\r\nThe related visualization has w = 200 and h = 50. In the\r\nevaluated scenario, we allow the number of groups nh to be\r\ndifferent than the width w of the visualization. This will\r\nshow the robustness of our approach. However, in a real im\u0002plementation, the engineers have to make sure that nh = w\r\nto achieve the best results. In addition to the aggregation\u0002based operators, we also compare our approach with three\r\ndifferent line simplification approaches, as described in Sec\u0002tion 5. We use the Reumann-Wikham algorithm (reuwi)\r\n[24] as representative for sequential line simplification, the\r\ntop-down Ramer-Douglas-Peucker (RDP) algorithm [6, 14],\r\nand the bottom-up Visvalingam-Whyatts (visval) algorithm\r\n[26]. The RDP algorithm, does not allow setting a desired\r\ndata reduction ratio, thus we precomputed the minimal \u000f\r\nthat would produce a number of tuples proportional to the\r\nconsidered nh.\r\nIn Figure 14, we plot the measured, resulting visualization\r\nquality (DSSIM) over the resulting number of tuples of each\r\ndifferent groupings nh = 1 to nh = 2.5·w of an applied data\r\nreduction technique. For readability, we cut off all low qual\u0002ity results with DSSIM < 0.8. The lower the number of\r\ntuples and the higher the DSSIM, the more data efficient is\r\nthe corresponding technique for the purpose of line visuali\u0002zation. The Figures 14aI and 14bI depict these measures for\r\nbinary line visualizations and the Figures 14aII and 14bII\r\nfor anti-aliased line visualizations. We now observe the fol\u0002lowing results.\r\nSampling and Averaging operators (avg, first, and sran\u0002dom) select a single aggregated value per (horizontal) group.\r\nThey all show similar results and provide the lowest DSSIM.\r\nAs discussed in Section 4, they will often fail to select the\r\ntuples that are important for line rasterization, i.e., the min,\r\nmax, first, and last tuples that are required to set the correct\r\ninner-column and inter-column pixels.\r\n2D-Rounding requires an additional vertical grouping\r\ninto nv groups. We set nv = w/h·nh to a have proportional\r\nvertical grouping. The average visualization quality of 2D\u0002rounding is higher than that of averaging and sampling.\r\nMinMax queries select min(v) and max(v) and the cor\u0002responding timestamps per group. They provide very high\r\nDSSIM values already at low data volumes. On average,\r\nthey have a higher data efficiency than all aggregation based\r\ntechniques, (including M4, see Figure 14a), but are partially\r\nsurpassed by line simplification approaches (see Figure 14b).\r\nLine Simplification techniques (RDP and visval) on av\u0002erage provide better results than the aggregation-based tech\u0002niques (compare Figures 14a and 14b). As seen previously\r\n[25], top-down (RDP) and bottom-up (visval) algorithms\r\nperform much better than the sequential ones (reuwi). How\u0002ever, in context of rasterized line visualizations they are sur\u0002passed by M4 and also MinMax at nh = w and nh = 2 · w.\r\nThese techniques often miss one of the min, max, first, or last\r\ntuples, because these tuples must not necessarily comply to\r\nthe geometric distance measures used for line simplification,\r\nas described in Section 5.\r\nM4 queries select min(v), max(v), min(t), max(t) and\r\nthe corresponding timestamps and values per group. On av\u0002erage M4 provides a visualization quality of DSSIM > 0.9\r\n805",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/030be5cd-d2df-4685-9ebe-19512be7978f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ab805264658cdb8cc1b840c63321335ce68818c9f4fff95f337d5ecd41d9ef6d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 782
      },
      {
        "segments": [
          {
            "segment_id": "1ee93660-542d-41b7-b0b3-d38b4c5a9bb9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "but is usually below MinMax and the line simplification\r\ntechniques. However, at nh = w, i.e., at any factor k of w,\r\nM4 provides perfect (error-free) visualizations. Any group\u0002ing with nh = k · w and k ∈ N\r\n+ also includes the min, max,\r\nfirst, and last tuples for nh = w.\r\nAnti-aliasing. The observed results for binary visuali\u0002zation (Figures 14I) and anti-aliased visualizations (Figures\r\n14II) are very similar. The absolute DSSIM values for anti\u0002aliased visualizations are even better than for binary ones.\r\nThis is caused by a single pixel error in a binary visualization\r\nimplying a full color swap from one extreme to the other,\r\ne.g., from back (0) to white (255). Pixel errors in anti-aliased\r\nvisualization are less distinct, especially in overplotted areas,\r\nwhich are common for high-volume time series data. For ex\u0002ample, a missing line will often result in a small increase in\r\nbrightness, rather than a complete swap of full color values,\r\nand an additional false line will result in a small decrease in\r\nbrightness of a pixel.\r\n6.4 Evaluation of Pixel Errors\r\nA visual result of M4, MinMax, RDP, and averaging (PAA),\r\napplied to 400 seconds (40k tuples) of the machine data\r\nset, is shown in Figure 15. We use only 100×20 pixels for\r\neach visualization to reveal the pixel errors of each opera\u0002tor. M4 thereby also presents the error-free baseline image.\r\nWe marked the pixel errors for MinMax, RDP, and PAA;\r\nblack represents additional pixels and white the missing pix\u0002els compared to the base image.\r\nM4/Baseline\r\nPAA\r\nMinMax\r\nRDP\r\nFigure 15: Projecting 40k tuples to 100x20 pixels.\r\nWe see how MinMax draws very long, false connection lines\r\n(right of each of the three main positive spikes of the chart).\r\nMinMax also has several smaller errors, caused by the same\r\neffect. In this regard, RDP is better, as the distance of the\r\nnot selected points to a long, false connection line is also very\r\nhigh, and RDP will have to split this line again. RDP also\r\napplies a slight averaging in areas where the time series has\r\na low variance, since the small distance between low varying\r\nvalues also decreases the corresponding measured distances.\r\nThe most pixel errors are produced by the PAA-based data\r\nreduction, mainly caused by the averaging of the vertical\r\nextrema. Overall, MinMax results in 30 false pixels, RDP\r\nin 39 false pixels, and PAA in over 100 false pixels. M4 stays\r\nerror-free.\r\nRelational\r\nSystem\r\nData Reduction\r\nSystem\r\ndata\r\nVisualization\r\nClient\r\nC) additional\r\n reduction\r\n \r\npixels pixels B) image-\r\n based\r\ndata pixels D) in-DB\r\n reduction\r\nQ(T)\r\nQ(T)\r\nQR(Q(T))\r\nA) without\r\n reduction Q(T)\r\ndata pixels\r\nDATA pixels\r\nInter\u0002acivityBand\u0002width\r\n++ – –\r\n+\r\n+ ++\r\n+\r\n–\r\nSystem\r\nType\r\n+\r\nFigure 16: Visualization system architectures.\r\n6.5 Data Reduction Potential\r\nLet us now go back to the motivating example in Section 1.\r\nFor the described scenario, we expected 100 users trying to\r\nvisually analyze 12 hours of sensor data, recorded at 100Hz.\r\nEach user has to wait for over 4 Million rows of data until\r\nhe or she can examine the sensor signal visually. Assuming\r\nthat the sensor data is visualized using a line chart that\r\nrelies on an M4-based aggregation, and that the maximum\r\nwidth of a chart is w = 2000 pixels. Then we know that\r\nM4 will at most select 4 · w = 8000 tuples from the time\r\nseries, independent of the chosen time span. The resulting\r\nmaximum amount of tuples, required to serve all 100 users\r\nwith error-free line charts, is 100users · 8000 = 800000 tuples;\r\ninstead of previously 463 million tuples. As a result, in this\r\nscenario we achieve a data reduction ratio of over 1 : 500.\r\n7. RELATED WORK\r\nIn this section, we discuss existing visualization systems and\r\nprovide an overview of related data reduction techniques,\r\ndiscussing the differences to our approach.\r\n7.1 Visualization Systems\r\nRegarding visualization-related data reduction, current state\u0002of-the-art visualization systems and tools fall into three cat\u0002egories. They (A) do not use any data reduction, or (B)\r\ncompute and send images instead of data to visualization\r\nclients, or (C) rely on additional data reduction outside of\r\nthe database. In Figure 16, we compare these systems to\r\nour solution (D), showing how each type of system applies\r\nand reduces a relational query Q on a time series relation T.\r\nNote that thin arrows indicate low-volume data flow, and\r\nthick arrows indicate that raw data needs to be transferred\r\nbetween the system’s components or to the client.\r\nVisual Analytics Tools. Many visual analytics tools\r\nare systems of type A that do not apply any visualization\u0002related data reduction, even though they often contain state\u0002of-the-art (relational) data engines [28] that could be used\r\nfor this purpose. For our visualization needs, we already\r\nevaluated four common candidates for such tools: Tableau\r\nDesktop 8.1 (tableausoftware.com), SAP Lumira 1.13 (sap\u0002lumira.com), QlikView 11.20 (clickview.com), and Datawatch\r\nDesktop 12.2 (datawatch.com). But none of these tools was\r\nable to quickly and easily visualize high-volume time series\r\ndata, having 1 million rows or more. Since all tools allow\r\nworking on data from a database or provide a tool-internal\r\ndata engine, we see a great opportunity for our approach\r\nto be implemented in such systems. For brevity, we cannot\r\nprovide a more detailed evaluation of these tools.\r\n806",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/1ee93660-542d-41b7-b0b3-d38b4c5a9bb9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3fec4a4b7c9a7ef0d3c6371ffa549709b265b749af1b711eef2db51b0b32fa7d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 872
      },
      {
        "segments": [
          {
            "segment_id": "1ee93660-542d-41b7-b0b3-d38b4c5a9bb9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "but is usually below MinMax and the line simplification\r\ntechniques. However, at nh = w, i.e., at any factor k of w,\r\nM4 provides perfect (error-free) visualizations. Any group\u0002ing with nh = k · w and k ∈ N\r\n+ also includes the min, max,\r\nfirst, and last tuples for nh = w.\r\nAnti-aliasing. The observed results for binary visuali\u0002zation (Figures 14I) and anti-aliased visualizations (Figures\r\n14II) are very similar. The absolute DSSIM values for anti\u0002aliased visualizations are even better than for binary ones.\r\nThis is caused by a single pixel error in a binary visualization\r\nimplying a full color swap from one extreme to the other,\r\ne.g., from back (0) to white (255). Pixel errors in anti-aliased\r\nvisualization are less distinct, especially in overplotted areas,\r\nwhich are common for high-volume time series data. For ex\u0002ample, a missing line will often result in a small increase in\r\nbrightness, rather than a complete swap of full color values,\r\nand an additional false line will result in a small decrease in\r\nbrightness of a pixel.\r\n6.4 Evaluation of Pixel Errors\r\nA visual result of M4, MinMax, RDP, and averaging (PAA),\r\napplied to 400 seconds (40k tuples) of the machine data\r\nset, is shown in Figure 15. We use only 100×20 pixels for\r\neach visualization to reveal the pixel errors of each opera\u0002tor. M4 thereby also presents the error-free baseline image.\r\nWe marked the pixel errors for MinMax, RDP, and PAA;\r\nblack represents additional pixels and white the missing pix\u0002els compared to the base image.\r\nM4/Baseline\r\nPAA\r\nMinMax\r\nRDP\r\nFigure 15: Projecting 40k tuples to 100x20 pixels.\r\nWe see how MinMax draws very long, false connection lines\r\n(right of each of the three main positive spikes of the chart).\r\nMinMax also has several smaller errors, caused by the same\r\neffect. In this regard, RDP is better, as the distance of the\r\nnot selected points to a long, false connection line is also very\r\nhigh, and RDP will have to split this line again. RDP also\r\napplies a slight averaging in areas where the time series has\r\na low variance, since the small distance between low varying\r\nvalues also decreases the corresponding measured distances.\r\nThe most pixel errors are produced by the PAA-based data\r\nreduction, mainly caused by the averaging of the vertical\r\nextrema. Overall, MinMax results in 30 false pixels, RDP\r\nin 39 false pixels, and PAA in over 100 false pixels. M4 stays\r\nerror-free.\r\nRelational\r\nSystem\r\nData Reduction\r\nSystem\r\ndata\r\nVisualization\r\nClient\r\nC) additional\r\n reduction\r\n \r\npixels pixels B) image-\r\n based\r\ndata pixels D) in-DB\r\n reduction\r\nQ(T)\r\nQ(T)\r\nQR(Q(T))\r\nA) without\r\n reduction Q(T)\r\ndata pixels\r\nDATA pixels\r\nInter\u0002acivityBand\u0002width\r\n++ – –\r\n+\r\n+ ++\r\n+\r\n–\r\nSystem\r\nType\r\n+\r\nFigure 16: Visualization system architectures.\r\n6.5 Data Reduction Potential\r\nLet us now go back to the motivating example in Section 1.\r\nFor the described scenario, we expected 100 users trying to\r\nvisually analyze 12 hours of sensor data, recorded at 100Hz.\r\nEach user has to wait for over 4 Million rows of data until\r\nhe or she can examine the sensor signal visually. Assuming\r\nthat the sensor data is visualized using a line chart that\r\nrelies on an M4-based aggregation, and that the maximum\r\nwidth of a chart is w = 2000 pixels. Then we know that\r\nM4 will at most select 4 · w = 8000 tuples from the time\r\nseries, independent of the chosen time span. The resulting\r\nmaximum amount of tuples, required to serve all 100 users\r\nwith error-free line charts, is 100users · 8000 = 800000 tuples;\r\ninstead of previously 463 million tuples. As a result, in this\r\nscenario we achieve a data reduction ratio of over 1 : 500.\r\n7. RELATED WORK\r\nIn this section, we discuss existing visualization systems and\r\nprovide an overview of related data reduction techniques,\r\ndiscussing the differences to our approach.\r\n7.1 Visualization Systems\r\nRegarding visualization-related data reduction, current state\u0002of-the-art visualization systems and tools fall into three cat\u0002egories. They (A) do not use any data reduction, or (B)\r\ncompute and send images instead of data to visualization\r\nclients, or (C) rely on additional data reduction outside of\r\nthe database. In Figure 16, we compare these systems to\r\nour solution (D), showing how each type of system applies\r\nand reduces a relational query Q on a time series relation T.\r\nNote that thin arrows indicate low-volume data flow, and\r\nthick arrows indicate that raw data needs to be transferred\r\nbetween the system’s components or to the client.\r\nVisual Analytics Tools. Many visual analytics tools\r\nare systems of type A that do not apply any visualization\u0002related data reduction, even though they often contain state\u0002of-the-art (relational) data engines [28] that could be used\r\nfor this purpose. For our visualization needs, we already\r\nevaluated four common candidates for such tools: Tableau\r\nDesktop 8.1 (tableausoftware.com), SAP Lumira 1.13 (sap\u0002lumira.com), QlikView 11.20 (clickview.com), and Datawatch\r\nDesktop 12.2 (datawatch.com). But none of these tools was\r\nable to quickly and easily visualize high-volume time series\r\ndata, having 1 million rows or more. Since all tools allow\r\nworking on data from a database or provide a tool-internal\r\ndata engine, we see a great opportunity for our approach\r\nto be implemented in such systems. For brevity, we cannot\r\nprovide a more detailed evaluation of these tools.\r\n806",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/1ee93660-542d-41b7-b0b3-d38b4c5a9bb9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3fec4a4b7c9a7ef0d3c6371ffa549709b265b749af1b711eef2db51b0b32fa7d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 872
      },
      {
        "segments": [
          {
            "segment_id": "bc775f57-9db8-4ca2-b2a2-09d02d8845bf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "Client-Server Systems. The second system type B is\r\ncommonly used in web-based solutions, e.g., financial web\u0002sites like Yahoo Finance (finance.yahoo.com) or Google Fi\u0002nance (google.com/finance). Those systems reduce the data\r\nvolumes by generating and caching raster images, and send\u0002ing those instead of the actual data for most of their smaller\r\nvisualizations. Purely image-based systems usually provide\r\npoor interactivity and are backed with a complementary sys\u0002tem of type C, implemented as a rich-client application that\r\nallows exploring the data interactively. Systems B and C\r\nusually rely on additional data reduction or image gener\u0002ation components between the data engine and the client.\r\nAssuming a system C that allows arbitrary non-aggregating\r\nuser queries Q, they will regularly need to transfer large\r\nquery results from the database to the external data re\u0002duction components. This may consume significant system\u0002internal bandwidth and heavily impact the overall perfor\u0002mance, as data transfer is one of the most costly operations.\r\nData-Centric System. Our visualization system (type\r\nD) can run expensive data reduction operations directly in\u0002side the data engine and still achieve the same level of in\u0002teractivity as provided by rich-client visualization systems\r\n(type C). Our system rewrites the original query Q, using\r\nadditional the data reduction operators, producing a new\r\nquery QR. When executing the new query, the data en\u0002gine can then jointly optimize all operators in one single\r\nquery graph, and the final (physical) operators can all di\u0002rectly access the shared in-memory data without requiring\r\nadditional, expensive data transfer.\r\n7.2 Data Reduction\r\nIn the following we give an overview on common data re\u0002duction methods and how they are related to visualizations.\r\nQuantization. Many visualization systems explicitly or\r\nimplicitly reduce continuous times-series data to discrete\r\nvalues, e.g., by generating images, or simply by rounding\r\nthe data, e.g., to have only two decimal places. A rounding\r\nfunction is a surjective function and does not allow correct\r\nreproduction of the original data. In our system we also con\u0002sider lossy, rounding-based reduction, and can even model it\r\nas relational query, facilitating a data-centric computation.\r\nTime Series Representation. There are many works\r\non time series representations [9], especially for the task of\r\ndata mining [11]. The goal of most approaches is, similar\r\nto our goal, to obtain a much smaller representation of a\r\ncomplete time series. In many cases, this is accomplished\r\nby splitting the time series (horizontally) into equidistant or\r\ndistribution-based time intervals and computing an aggre\u0002gated value (average) for each interval [18]. Further reduc\u0002tion is then achieved by mapping the aggregates to a limited\r\nalphabet, for example, based on the (vertical) distribution\r\nof the values. The results are, e.g., character sequences or\r\nlists of line segments (see Section 5) that approximate the\r\noriginal time series. The validity of a representation is then\r\ntested by using it in a data mining tasks, such as time\u0002series similarity matching [29]. The main difference of our\r\napproach is our focus on relational operators and our incor\u0002poration of the semantics of the visualizations. None of the\r\nexisting approaches discussed the related aspects of line ras\u0002terization that facilitate the high quality and data efficiency\r\nof our approach.\r\nOffline Aggregation and Synopsis. Traditionally, ag\u0002gregates of temporal business data in OLAP cubes are very\r\ncoarse grained. The number of aggregation levels is lim\u0002ited, e.g., to years, months, and days, and the aggregation\r\nfunctions are restricted, e.g., to count, avg, sum, min, and\r\nmax. For the purpose of visualization, such pre-aggregated\r\ndata might not represent the raw data very well, especially\r\nwhen considering high-volume time series data with a time\r\nresolution of a few milliseconds. The problem is partially\r\nmitigated by the provisioning of (hierarchical or amnesic)\r\ndata synopsis [7, 13]. However, synopsis techniques again\r\nrely on common time series dimensionality reduction tech\u0002niques [11], and thus are subject to approximation errors.\r\nIn this regard, we see the development of a visualization\u0002oriented data synopsis system that uses the proposed M4\r\naggregation to provide error-free visualizations as a chal\u0002lenging subject to future work.\r\nOnline Aggregation and Streaming. Even though\r\nthis paper focuses on aggregation of static data, our work\r\nwas initially driven by the need for interactive, real-time\r\nvisualizations of high-velocity streaming data [16]. Indeed,\r\nwe can apply the M4 aggregation for online aggregation,\r\ni.e., derive the four extremum tuples in O(n) and in a single\r\npass over the input stream. A custom M4 implementation\r\ncould scan the input data for the extremum tuples rather\r\nthan the extremum values, and thus avoid the subsequent\r\njoin, as required by the relational M4 (see Section 4.2).\r\nData Compression. We currently only consider data\r\nreduction at the application level. Any additional transport\u0002level data reduction technique, e.g., data packet compres\u0002sion or specialized compression of numerical data [20, 11], is\r\ncomplementary to our data reduction.\r\nContent Adaptation. Our approach is similar to con\u0002tent adaptation in general [21], which is widely used for\r\nimages, videos, and text in web-based systems. Content\r\nadaptation is one of our underlying ideas that we extended\r\ntowards a relational approach, with a special attention of\r\nthe semantics of line visualizations.\r\nStatistical Approaches. Statistical databases [1] can\r\nserve approximate results. They serve highly reduced ap\u0002proximate answers to user queries. Nevertheless, these an\u0002swers cannot very well represent the raw data for the pur\u0002pose of line visualization, since they apply simple random\r\nor systematic sampling, as discussed in Section 4. In theory,\r\nstatistical databases could be extended with our approach,\r\nto serve for example M4 or MinMax query results as ap\u0002proximating answers.\r\n7.3 Visualization-Driven Data Reduction\r\nThe usage of visualization parameters for data reduction has\r\nbeen partially described by Burtini et al. [3], where they use\r\nthe width and height of a visualization to define parameters\r\nfor some time series compression techniques. However, they\r\ndescribe a client-server system of type C (see Figure 16),\r\napplying the data reduction outside of the database. In our\r\nsystem, we push all data processing down to database by\r\nmeans of query rewriting. Furthermore, they use an average\r\naggregation with w groups, i.e., only 1 ·w tuples, as baseline\r\nand do not consider the visualization of the original time\r\nseries. Thereby, they overly simplify the actual problem\r\nand the resulting line charts will lose important detail in\r\nthe vertical extrema. They do not appropriately discuss the\r\nsemantics of rasterized line visualizations.\r\n807",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/bc775f57-9db8-4ca2-b2a2-09d02d8845bf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e4618cdede0328aadcd83b47e951f63c64307e679e0a91b2e2f67b3c07d0fc76",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1030
      },
      {
        "segments": [
          {
            "segment_id": "bc775f57-9db8-4ca2-b2a2-09d02d8845bf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "Client-Server Systems. The second system type B is\r\ncommonly used in web-based solutions, e.g., financial web\u0002sites like Yahoo Finance (finance.yahoo.com) or Google Fi\u0002nance (google.com/finance). Those systems reduce the data\r\nvolumes by generating and caching raster images, and send\u0002ing those instead of the actual data for most of their smaller\r\nvisualizations. Purely image-based systems usually provide\r\npoor interactivity and are backed with a complementary sys\u0002tem of type C, implemented as a rich-client application that\r\nallows exploring the data interactively. Systems B and C\r\nusually rely on additional data reduction or image gener\u0002ation components between the data engine and the client.\r\nAssuming a system C that allows arbitrary non-aggregating\r\nuser queries Q, they will regularly need to transfer large\r\nquery results from the database to the external data re\u0002duction components. This may consume significant system\u0002internal bandwidth and heavily impact the overall perfor\u0002mance, as data transfer is one of the most costly operations.\r\nData-Centric System. Our visualization system (type\r\nD) can run expensive data reduction operations directly in\u0002side the data engine and still achieve the same level of in\u0002teractivity as provided by rich-client visualization systems\r\n(type C). Our system rewrites the original query Q, using\r\nadditional the data reduction operators, producing a new\r\nquery QR. When executing the new query, the data en\u0002gine can then jointly optimize all operators in one single\r\nquery graph, and the final (physical) operators can all di\u0002rectly access the shared in-memory data without requiring\r\nadditional, expensive data transfer.\r\n7.2 Data Reduction\r\nIn the following we give an overview on common data re\u0002duction methods and how they are related to visualizations.\r\nQuantization. Many visualization systems explicitly or\r\nimplicitly reduce continuous times-series data to discrete\r\nvalues, e.g., by generating images, or simply by rounding\r\nthe data, e.g., to have only two decimal places. A rounding\r\nfunction is a surjective function and does not allow correct\r\nreproduction of the original data. In our system we also con\u0002sider lossy, rounding-based reduction, and can even model it\r\nas relational query, facilitating a data-centric computation.\r\nTime Series Representation. There are many works\r\non time series representations [9], especially for the task of\r\ndata mining [11]. The goal of most approaches is, similar\r\nto our goal, to obtain a much smaller representation of a\r\ncomplete time series. In many cases, this is accomplished\r\nby splitting the time series (horizontally) into equidistant or\r\ndistribution-based time intervals and computing an aggre\u0002gated value (average) for each interval [18]. Further reduc\u0002tion is then achieved by mapping the aggregates to a limited\r\nalphabet, for example, based on the (vertical) distribution\r\nof the values. The results are, e.g., character sequences or\r\nlists of line segments (see Section 5) that approximate the\r\noriginal time series. The validity of a representation is then\r\ntested by using it in a data mining tasks, such as time\u0002series similarity matching [29]. The main difference of our\r\napproach is our focus on relational operators and our incor\u0002poration of the semantics of the visualizations. None of the\r\nexisting approaches discussed the related aspects of line ras\u0002terization that facilitate the high quality and data efficiency\r\nof our approach.\r\nOffline Aggregation and Synopsis. Traditionally, ag\u0002gregates of temporal business data in OLAP cubes are very\r\ncoarse grained. The number of aggregation levels is lim\u0002ited, e.g., to years, months, and days, and the aggregation\r\nfunctions are restricted, e.g., to count, avg, sum, min, and\r\nmax. For the purpose of visualization, such pre-aggregated\r\ndata might not represent the raw data very well, especially\r\nwhen considering high-volume time series data with a time\r\nresolution of a few milliseconds. The problem is partially\r\nmitigated by the provisioning of (hierarchical or amnesic)\r\ndata synopsis [7, 13]. However, synopsis techniques again\r\nrely on common time series dimensionality reduction tech\u0002niques [11], and thus are subject to approximation errors.\r\nIn this regard, we see the development of a visualization\u0002oriented data synopsis system that uses the proposed M4\r\naggregation to provide error-free visualizations as a chal\u0002lenging subject to future work.\r\nOnline Aggregation and Streaming. Even though\r\nthis paper focuses on aggregation of static data, our work\r\nwas initially driven by the need for interactive, real-time\r\nvisualizations of high-velocity streaming data [16]. Indeed,\r\nwe can apply the M4 aggregation for online aggregation,\r\ni.e., derive the four extremum tuples in O(n) and in a single\r\npass over the input stream. A custom M4 implementation\r\ncould scan the input data for the extremum tuples rather\r\nthan the extremum values, and thus avoid the subsequent\r\njoin, as required by the relational M4 (see Section 4.2).\r\nData Compression. We currently only consider data\r\nreduction at the application level. Any additional transport\u0002level data reduction technique, e.g., data packet compres\u0002sion or specialized compression of numerical data [20, 11], is\r\ncomplementary to our data reduction.\r\nContent Adaptation. Our approach is similar to con\u0002tent adaptation in general [21], which is widely used for\r\nimages, videos, and text in web-based systems. Content\r\nadaptation is one of our underlying ideas that we extended\r\ntowards a relational approach, with a special attention of\r\nthe semantics of line visualizations.\r\nStatistical Approaches. Statistical databases [1] can\r\nserve approximate results. They serve highly reduced ap\u0002proximate answers to user queries. Nevertheless, these an\u0002swers cannot very well represent the raw data for the pur\u0002pose of line visualization, since they apply simple random\r\nor systematic sampling, as discussed in Section 4. In theory,\r\nstatistical databases could be extended with our approach,\r\nto serve for example M4 or MinMax query results as ap\u0002proximating answers.\r\n7.3 Visualization-Driven Data Reduction\r\nThe usage of visualization parameters for data reduction has\r\nbeen partially described by Burtini et al. [3], where they use\r\nthe width and height of a visualization to define parameters\r\nfor some time series compression techniques. However, they\r\ndescribe a client-server system of type C (see Figure 16),\r\napplying the data reduction outside of the database. In our\r\nsystem, we push all data processing down to database by\r\nmeans of query rewriting. Furthermore, they use an average\r\naggregation with w groups, i.e., only 1 ·w tuples, as baseline\r\nand do not consider the visualization of the original time\r\nseries. Thereby, they overly simplify the actual problem\r\nand the resulting line charts will lose important detail in\r\nthe vertical extrema. They do not appropriately discuss the\r\nsemantics of rasterized line visualizations.\r\n807",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/bc775f57-9db8-4ca2-b2a2-09d02d8845bf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e4618cdede0328aadcd83b47e951f63c64307e679e0a91b2e2f67b3c07d0fc76",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1030
      },
      {
        "segments": [
          {
            "segment_id": "a9b82bed-ae57-40c6-9073-220fd6f06aea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "8. CONCLUSION\r\nIn this paper, we introduced a visualization-driven query\r\nrewriting technique that facilitates a data-centric time se\u0002ries dimensionality reduction. We showed how to enclose\r\nall visualization-related queries to an RDBMS within addi\u0002tional data reduction operators. In particular, we considered\r\naggregation-based data reduction techniques and described\r\nhow they integrate with the proposed query-rewriting.\r\nFocusing on line charts, as the predominant form of time\r\nseries visualizations, our approach exploits the semantics of\r\nline rasterization to drive the data reduction of high-volume\r\ntime series data. We introduced the novel M4 aggregation\r\nthat selects the min, max, first, and last tuples from the time\r\nspans corresponding to the pixel columns of a line chart.\r\nUsing M4 we were able to reduce data volumes by two orders\r\nof magnitude and latencies by one order of magnitude, while\r\nensuring pixel-perfect line visualizations.\r\nIn the future, we want to extend our current focus on\r\nline visualizations to other forms of visualization, such as\r\nbar charts, scatter plots and space-filling visualizations. We\r\naim to provide a general framework for data-reduction that\r\nconsiders the rendering semantics of visualizations. We hope\r\nthat this in-depth, interdisciplinary database and computer\r\ngraphics research paper will inspire other researchers to in\u0002vestigate the boundaries between the two areas.\r\n9. REFERENCES\r\n[1] S. Agarwal, A. Panda, B. Mozafari, A. P. Iyer,\r\nS. Madden, and I. Stoica. Blink and it’s done:\r\nInteractive queries on very large data. PVLDB,\r\n5(12):1902–1905, 2012.\r\n[2] J. E. Bresenham. Algorithm for computer control of a\r\ndigital plotter. IBM Systems journal, 4(1):25–30, 1965.\r\n[3] G. Burtini, S. Fazackerley, and R. Lawrence. Time\r\nseries compression for adaptive chart generation. In\r\nCCECE, pages 1–6. IEEE, 2013.\r\n[4] J. X. Chen and X. Wang. Approximate line\r\nscan-conversion and antialiasing. In Computer\r\nGraphics Forum, pages 69–78. Wiley, 1999.\r\n[5] David Salomon. Data Compression. Springer, 2007.\r\n[6] D. H. Douglas and T. K. Peucker. Algorithms for the\r\nreduction of the number of points required to\r\nrepresent a digitized line or its caricature.\r\nCartographica Journal, 10(2):112–122, 1973.\r\n[7] Q. Duan, P. Wang, M. Wu, W. Wang, and S. Huang.\r\nApproximate query on historical stream data. In\r\nDEXA, pages 128–135. Springer, 2011.\r\n[8] S. G. Eick and A. F. Karr. Visual scalability. Journal\r\nof Computational and Graphical Statistics,\r\n11(1):22–43, 2002.\r\n[9] P. Esling and C. Agon. Time-series data mining. ACM\r\nComputing Surveys, 45(1):12–34, 2012.\r\n[10] F. F¨arber, S. K. Cha, J. Primsch, C. Bornh¨ovd,\r\nS. Sigg, and W. Lehner. SAP HANA Database-Data\r\nManagement for Modern Business Applications.\r\nSIGMOD Record, 40(4):45–51, 2012.\r\n[11] T. Fu. A review on time series data mining. EAAI\r\nJournal, 24(1):164–181, 2011.\r\n[12] T. Fu, F. Chung, R. Luk, and C. Ng. Representing\r\nfinancial time series based on data point importance.\r\nEAAI Journal, 21(2):277–300, 2008.\r\n[13] S. Gandhi, L. Foschini, and S. Suri. Space-efficient\r\nonline approximation of time series data: Streams,\r\namnesia, and out-of-order. In ICDE, pages 924–935.\r\nIEEE, 2010.\r\n[14] J. Hershberger and J. Snoeyink. Speeding up the\r\nDouglas-Peucker line-simplification algorithm.\r\nUniversity of British Columbia, Department of\r\nComputer Science, 1992.\r\n[15] Z. Jerzak, T. Heinze, M. Fehr, D. Gr¨ober, R. Hartung,\r\nand N. Stojanovic. The DEBS 2012 Grand Challenge.\r\nIn DEBS, pages 393–398. ACM, 2012.\r\n[16] U. Jugel and V. Markl. Interactive visualization of\r\nhigh-velocity event streams. In VLDB PhD Workshop.\r\nVLDB Endowment, 2012.\r\n[17] D. A. Keim, C. Panse, J. Schneidewind, M. Sips,\r\nM. C. Hao, and U. Dayal. Pushing the limit in visual\r\ndata exploration: Techniques and applications. Lecture\r\nnotes in artificial intelligence, (2821):37–51, 2003.\r\n[18] E. J. Keogh and Pazzani. A simple dimensionality\r\nreduction technique for fast similarity search in large\r\ntime series databases. In PAKDD, pages 122–133.\r\nSpringer, 2000.\r\n[19] A. Kolesnikov. Efficient algorithms for vectorization\r\nand polygonal approximation. University of Joensuu,\r\n2003.\r\n[20] P. Lindstrom and M. Isenburg. Fast and efficient\r\ncompression of floating-point data. In TVCG,\r\nvolume 12, pages 1245–1250. IEEE, 2006.\r\n[21] W.-Y. Ma, I. Bedner, G. Chang, A. Kuchinsky, and\r\nH. Zhang. A framework for adaptive content delivery\r\nin heterogeneous network environments. In Proc.\r\nSPIE, Multimedia Computing and Networking, volume\r\n3969, pages 86–100. SPIE, 2000.\r\n[22] C. Mutschler, H. Ziekow, and Z. Jerzak. The DEBS\r\n2013 Grand Challenge. In DEBS, pages 289–294.\r\nACM, 2013.\r\n[23] P. Przymus, A. Boniewicz, M. Burza´nska, and\r\nK. Stencel. Recursive query facilities in relational\r\ndatabases: a survey. In DTA and BSBT, pages 89–99.\r\nSpringer, 2010.\r\n[24] K. Reumann and A. P. M. Witkam. Optimizing curve\r\nsegmentation in computer graphics. In Proceedings of\r\nthe International Computing Symposium, pages\r\n467–472. North-Holland Publishing Company, 1974.\r\n[25] W. Shi and C. Cheung. Performance evaluation of line\r\nsimplification algorithms for vector generalization.\r\nThe Cartographic Journal, 43(1):27–44, 2006.\r\n[26] M. Visvalingam and J. Whyatt. Line generalisation by\r\nrepeated elimination of points. The Cartographic\r\nJournal, 30(1):46–51, 1993.\r\n[27] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P.\r\nSimoncelli. Image quality assessment: from error\r\nvisibility to structural similarity. IEEE Transactions\r\non Image Processing, 13(4):600–612, 2004.\r\n[28] R. Wesley, M. Eldridge, and P. Terlecki. An analytic\r\ndata engine for visualization in tableau. In SIGMOD,\r\npages 1185–1194. ACM, 2011.\r\n[29] Y. Wu, D. Agrawal, and A. El Abbadi. A comparison\r\nof DFT and DWT based similarity search in timeseries\r\ndatabases. In CIKM, pages 488–495. ACM, 2000.\r\n808",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/a9b82bed-ae57-40c6-9073-220fd6f06aea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b05741541d09d0667a7981fb5b8a60621660de6bb08adbe871fcb974daf25286",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 857
      },
      {
        "segments": [
          {
            "segment_id": "a9b82bed-ae57-40c6-9073-220fd6f06aea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "8. CONCLUSION\r\nIn this paper, we introduced a visualization-driven query\r\nrewriting technique that facilitates a data-centric time se\u0002ries dimensionality reduction. We showed how to enclose\r\nall visualization-related queries to an RDBMS within addi\u0002tional data reduction operators. In particular, we considered\r\naggregation-based data reduction techniques and described\r\nhow they integrate with the proposed query-rewriting.\r\nFocusing on line charts, as the predominant form of time\r\nseries visualizations, our approach exploits the semantics of\r\nline rasterization to drive the data reduction of high-volume\r\ntime series data. We introduced the novel M4 aggregation\r\nthat selects the min, max, first, and last tuples from the time\r\nspans corresponding to the pixel columns of a line chart.\r\nUsing M4 we were able to reduce data volumes by two orders\r\nof magnitude and latencies by one order of magnitude, while\r\nensuring pixel-perfect line visualizations.\r\nIn the future, we want to extend our current focus on\r\nline visualizations to other forms of visualization, such as\r\nbar charts, scatter plots and space-filling visualizations. We\r\naim to provide a general framework for data-reduction that\r\nconsiders the rendering semantics of visualizations. We hope\r\nthat this in-depth, interdisciplinary database and computer\r\ngraphics research paper will inspire other researchers to in\u0002vestigate the boundaries between the two areas.\r\n9. REFERENCES\r\n[1] S. Agarwal, A. Panda, B. Mozafari, A. P. Iyer,\r\nS. Madden, and I. Stoica. Blink and it’s done:\r\nInteractive queries on very large data. PVLDB,\r\n5(12):1902–1905, 2012.\r\n[2] J. E. Bresenham. Algorithm for computer control of a\r\ndigital plotter. IBM Systems journal, 4(1):25–30, 1965.\r\n[3] G. Burtini, S. Fazackerley, and R. Lawrence. Time\r\nseries compression for adaptive chart generation. In\r\nCCECE, pages 1–6. IEEE, 2013.\r\n[4] J. X. Chen and X. Wang. Approximate line\r\nscan-conversion and antialiasing. In Computer\r\nGraphics Forum, pages 69–78. Wiley, 1999.\r\n[5] David Salomon. Data Compression. Springer, 2007.\r\n[6] D. H. Douglas and T. K. Peucker. Algorithms for the\r\nreduction of the number of points required to\r\nrepresent a digitized line or its caricature.\r\nCartographica Journal, 10(2):112–122, 1973.\r\n[7] Q. Duan, P. Wang, M. Wu, W. Wang, and S. Huang.\r\nApproximate query on historical stream data. In\r\nDEXA, pages 128–135. Springer, 2011.\r\n[8] S. G. Eick and A. F. Karr. Visual scalability. Journal\r\nof Computational and Graphical Statistics,\r\n11(1):22–43, 2002.\r\n[9] P. Esling and C. Agon. Time-series data mining. ACM\r\nComputing Surveys, 45(1):12–34, 2012.\r\n[10] F. F¨arber, S. K. Cha, J. Primsch, C. Bornh¨ovd,\r\nS. Sigg, and W. Lehner. SAP HANA Database-Data\r\nManagement for Modern Business Applications.\r\nSIGMOD Record, 40(4):45–51, 2012.\r\n[11] T. Fu. A review on time series data mining. EAAI\r\nJournal, 24(1):164–181, 2011.\r\n[12] T. Fu, F. Chung, R. Luk, and C. Ng. Representing\r\nfinancial time series based on data point importance.\r\nEAAI Journal, 21(2):277–300, 2008.\r\n[13] S. Gandhi, L. Foschini, and S. Suri. Space-efficient\r\nonline approximation of time series data: Streams,\r\namnesia, and out-of-order. In ICDE, pages 924–935.\r\nIEEE, 2010.\r\n[14] J. Hershberger and J. Snoeyink. Speeding up the\r\nDouglas-Peucker line-simplification algorithm.\r\nUniversity of British Columbia, Department of\r\nComputer Science, 1992.\r\n[15] Z. Jerzak, T. Heinze, M. Fehr, D. Gr¨ober, R. Hartung,\r\nand N. Stojanovic. The DEBS 2012 Grand Challenge.\r\nIn DEBS, pages 393–398. ACM, 2012.\r\n[16] U. Jugel and V. Markl. Interactive visualization of\r\nhigh-velocity event streams. In VLDB PhD Workshop.\r\nVLDB Endowment, 2012.\r\n[17] D. A. Keim, C. Panse, J. Schneidewind, M. Sips,\r\nM. C. Hao, and U. Dayal. Pushing the limit in visual\r\ndata exploration: Techniques and applications. Lecture\r\nnotes in artificial intelligence, (2821):37–51, 2003.\r\n[18] E. J. Keogh and Pazzani. A simple dimensionality\r\nreduction technique for fast similarity search in large\r\ntime series databases. In PAKDD, pages 122–133.\r\nSpringer, 2000.\r\n[19] A. Kolesnikov. Efficient algorithms for vectorization\r\nand polygonal approximation. University of Joensuu,\r\n2003.\r\n[20] P. Lindstrom and M. Isenburg. Fast and efficient\r\ncompression of floating-point data. In TVCG,\r\nvolume 12, pages 1245–1250. IEEE, 2006.\r\n[21] W.-Y. Ma, I. Bedner, G. Chang, A. Kuchinsky, and\r\nH. Zhang. A framework for adaptive content delivery\r\nin heterogeneous network environments. In Proc.\r\nSPIE, Multimedia Computing and Networking, volume\r\n3969, pages 86–100. SPIE, 2000.\r\n[22] C. Mutschler, H. Ziekow, and Z. Jerzak. The DEBS\r\n2013 Grand Challenge. In DEBS, pages 289–294.\r\nACM, 2013.\r\n[23] P. Przymus, A. Boniewicz, M. Burza´nska, and\r\nK. Stencel. Recursive query facilities in relational\r\ndatabases: a survey. In DTA and BSBT, pages 89–99.\r\nSpringer, 2010.\r\n[24] K. Reumann and A. P. M. Witkam. Optimizing curve\r\nsegmentation in computer graphics. In Proceedings of\r\nthe International Computing Symposium, pages\r\n467–472. North-Holland Publishing Company, 1974.\r\n[25] W. Shi and C. Cheung. Performance evaluation of line\r\nsimplification algorithms for vector generalization.\r\nThe Cartographic Journal, 43(1):27–44, 2006.\r\n[26] M. Visvalingam and J. Whyatt. Line generalisation by\r\nrepeated elimination of points. The Cartographic\r\nJournal, 30(1):46–51, 1993.\r\n[27] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P.\r\nSimoncelli. Image quality assessment: from error\r\nvisibility to structural similarity. IEEE Transactions\r\non Image Processing, 13(4):600–612, 2004.\r\n[28] R. Wesley, M. Eldridge, and P. Terlecki. An analytic\r\ndata engine for visualization in tableau. In SIGMOD,\r\npages 1185–1194. ACM, 2011.\r\n[29] Y. Wu, D. Agrawal, and A. El Abbadi. A comparison\r\nof DFT and DWT based similarity search in timeseries\r\ndatabases. In CIKM, pages 488–495. ACM, 2000.\r\n808",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/0eb249eb-0919-46a9-8d0e-44cea543b5fa/images/a9b82bed-ae57-40c6-9073-220fd6f06aea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b05741541d09d0667a7981fb5b8a60621660de6bb08adbe871fcb974daf25286",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 857
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "SAP data center at a remote location.\n"
        }
      ]
    }
  }
}