{
  "file_name": "Kam1n0 - MapReduce-based Assembly Clone Search for Reverse Engineering - 2016 (DFC16kdd).pdf",
  "task_id": "47aa63f3-7883-4cce-bbcc-1809feb8df64",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "9613787f-ff91-472d-a67b-0ebc73951062",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Kam1n0: MapReduce-based Assembly Clone Search for\r\nReverse Engineering\r\nSteven H. H. Ding\u0002 Benjamin C. M. Fung\u0002 Philippe Charland†\r\n\u0002School of Information Studies, McGill University, Montreal, QC, Canada\r\n†Mission Critical Cyber Security Section, Defence R&D Canada - Valcartier, Quebec, QC, Canada\r\nsteven.h.ding@mail.mcgill.ca ben.fung@mcgill.ca philippe.charland@drdc-rddc.gc.ca\r\nABSTRACT\r\nAssembly code analysis is one of the critical processes for de\u0002tecting and proving software plagiarism and software patent\r\ninfringements when the source code is unavailable. It is also\r\na common practice to discover exploits and vulnerabilities\r\nin existing software. However, it is a manually intensive and\r\ntime-consuming process even for experienced reverse engi\u0002neers. An effective and efficient assembly code clone search\r\nengine can greatly reduce the effort of this process, since\r\nit can identify the cloned parts that have been previously\r\nanalyzed. The assembly code clone search problem belongs\r\nto the field of software engineering. However, it strongly\r\ndepends on practical nearest neighbor search techniques in\r\ndata mining and databases. By closely collaborating with\r\nreverse engineers and Defence Research and Development\r\nCanada (DRDC), we study the concerns and challenges that\r\nmake existing assembly code clone approaches not practi\u0002cally applicable from the perspective of data mining. We\r\npropose a new variant of LSH scheme and incorporate it with\r\ngraph matching to address these challenges. We implement\r\nan integrated assembly clone search engine called Kam1n0.\r\nIt is the first clone search engine that can efficiently identify\r\nthe given query assembly function’s subgraph clones from a\r\nlarge assembly code repository. Kam1n0 is built upon the\r\nApache Spark computation framework and Cassandra-like\r\nkey-value distributed storage. A deployed demo system is\r\npublicly available.1 Extensive experimental results suggest\r\nthat Kam1n0 is accurate, efficient, and scalable for handling\r\nlarge volume of assembly code.\r\nKeywords\r\nAssembly clone search; Information retrieval; Mining soft\u0002ware repositories\r\n1Kam1n0 online demo (no installation required). Both the user\r\nname and password are “sigkdd2016”. Use Chrome for best expe\u0002rience. http://dmas.lab.mcgill.ca/projects/kam1n0.htm\r\nPermission to make digital or hard copies of all or part of this work for personal or\r\nclassroom use is granted without fee provided that copies are not made or distributed\r\nfor profit or commercial advantage and that copies bear this notice and the full citation\r\non the first page. Copyrights for components of this work owned by others than the\r\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\r\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\r\nand/or a fee. Request permissions from permissions@acm.org.\r\nKDD ’16 August 13–17, 2016, San Francisco, CA, USA\r\n\u0002c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.\r\nISBN 978-1-4503-4232-2/16/08. . . $15.00\r\nDOI: http://dx.doi.org/XXXX.XXXX\r\n1. INTRODUCTION\r\nCode reuse is a common but uncontrolled issue in software\r\nengineering [15]. Mockus [25] found that more than 50%\r\nof files were reused in more than one open source project.\r\nSojer’s survey [29] indicates that more than 50% of the de\u0002velopers modify the components before reusing them. This\r\nmassively uncontrolled reuse of source code does not only\r\nintroduce legal issues such as GNU General Public License\r\n(GPL) violation [36, 17]. It also implies security concerns,\r\nas the source code and the vulnerabilities are uncontrollably\r\nshared between projects [4].\r\nIdentifying all these infringements and vulnerabilities re\u0002quires intensive effort from reverse engineers. However, the\r\nlearning curve to master reverse engineering is much steeper\r\nthan for programming [4]. Reverse engineering is a time\r\nconsuming process which involves inspecting the execution\r\nflow of the program in assembly code and determining the\r\nfunctionalities of the components. Given the fact that code\r\nreuse is prevalent in software development, there is a press\u0002ing need to develop an efficient and effective assembly clone\r\nsearch engine for reverse engineers. Previous clone search\r\napproaches only focus on the search accuracy. However,\r\ndesigning a practically useful clone search engine is a non\u0002trivial task which involves multiple factors to be considered.\r\nBy closely collaborating with reverse engineers and Defence\r\nResearch and Development Canada (DRDC), we outline the\r\ndeployment challenges and requirements as follows:\r\nInterpretability and usability: An assembly function\r\ncan be represented as a control flow graph consisting of con\u0002nected basic blocks. Given an assembly function as query, all\r\nof the previous assembly code clone search approaches [7, 6,\r\n18, 26] only provide the top-listed candidate assembly func\u0002tions. They are useful when there exists a function in the\r\nrepository that shares a high degree of similarity with the\r\nquery. However, due to the unpredictable effects of differ\u0002ent compilers, compiler optimization, and obfuscation tech\u0002niques, given an unknown function, it is less probable to have\r\na very similar function in the repository. Returning a list of\r\nclones with a low degree of similarity values is not useful.\r\nAs per our discussions with DRDC, a practical search en\u0002gine should be able to decompose the given query assembly\r\nfunction to different known subgraph clones which can help\r\nreverse engineers better understand the function’s composi\u0002tion. We define a subgraph clone as one of its subgraphs that\r\ncan be found in the other function. Refer to the example\r\nin Figure 1. The previous clone search approaches cannot\r\naddress this challenge.\r\n1\r\nhttp://dx.doi.org/10.1145/2939672.2939719",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/9613787f-ff91-472d-a67b-0ebc73951062.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a955407d623307883a85e7b28366e4f80cd829778b312c6045759d3051507cd7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 838
      },
      {
        "segments": [
          {
            "segment_id": "9613787f-ff91-472d-a67b-0ebc73951062",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Kam1n0: MapReduce-based Assembly Clone Search for\r\nReverse Engineering\r\nSteven H. H. Ding\u0002 Benjamin C. M. Fung\u0002 Philippe Charland†\r\n\u0002School of Information Studies, McGill University, Montreal, QC, Canada\r\n†Mission Critical Cyber Security Section, Defence R&D Canada - Valcartier, Quebec, QC, Canada\r\nsteven.h.ding@mail.mcgill.ca ben.fung@mcgill.ca philippe.charland@drdc-rddc.gc.ca\r\nABSTRACT\r\nAssembly code analysis is one of the critical processes for de\u0002tecting and proving software plagiarism and software patent\r\ninfringements when the source code is unavailable. It is also\r\na common practice to discover exploits and vulnerabilities\r\nin existing software. However, it is a manually intensive and\r\ntime-consuming process even for experienced reverse engi\u0002neers. An effective and efficient assembly code clone search\r\nengine can greatly reduce the effort of this process, since\r\nit can identify the cloned parts that have been previously\r\nanalyzed. The assembly code clone search problem belongs\r\nto the field of software engineering. However, it strongly\r\ndepends on practical nearest neighbor search techniques in\r\ndata mining and databases. By closely collaborating with\r\nreverse engineers and Defence Research and Development\r\nCanada (DRDC), we study the concerns and challenges that\r\nmake existing assembly code clone approaches not practi\u0002cally applicable from the perspective of data mining. We\r\npropose a new variant of LSH scheme and incorporate it with\r\ngraph matching to address these challenges. We implement\r\nan integrated assembly clone search engine called Kam1n0.\r\nIt is the first clone search engine that can efficiently identify\r\nthe given query assembly function’s subgraph clones from a\r\nlarge assembly code repository. Kam1n0 is built upon the\r\nApache Spark computation framework and Cassandra-like\r\nkey-value distributed storage. A deployed demo system is\r\npublicly available.1 Extensive experimental results suggest\r\nthat Kam1n0 is accurate, efficient, and scalable for handling\r\nlarge volume of assembly code.\r\nKeywords\r\nAssembly clone search; Information retrieval; Mining soft\u0002ware repositories\r\n1Kam1n0 online demo (no installation required). Both the user\r\nname and password are “sigkdd2016”. Use Chrome for best expe\u0002rience. http://dmas.lab.mcgill.ca/projects/kam1n0.htm\r\nPermission to make digital or hard copies of all or part of this work for personal or\r\nclassroom use is granted without fee provided that copies are not made or distributed\r\nfor profit or commercial advantage and that copies bear this notice and the full citation\r\non the first page. Copyrights for components of this work owned by others than the\r\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\r\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\r\nand/or a fee. Request permissions from permissions@acm.org.\r\nKDD ’16 August 13–17, 2016, San Francisco, CA, USA\r\n\u0002c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.\r\nISBN 978-1-4503-4232-2/16/08. . . $15.00\r\nDOI: http://dx.doi.org/XXXX.XXXX\r\n1. INTRODUCTION\r\nCode reuse is a common but uncontrolled issue in software\r\nengineering [15]. Mockus [25] found that more than 50%\r\nof files were reused in more than one open source project.\r\nSojer’s survey [29] indicates that more than 50% of the de\u0002velopers modify the components before reusing them. This\r\nmassively uncontrolled reuse of source code does not only\r\nintroduce legal issues such as GNU General Public License\r\n(GPL) violation [36, 17]. It also implies security concerns,\r\nas the source code and the vulnerabilities are uncontrollably\r\nshared between projects [4].\r\nIdentifying all these infringements and vulnerabilities re\u0002quires intensive effort from reverse engineers. However, the\r\nlearning curve to master reverse engineering is much steeper\r\nthan for programming [4]. Reverse engineering is a time\r\nconsuming process which involves inspecting the execution\r\nflow of the program in assembly code and determining the\r\nfunctionalities of the components. Given the fact that code\r\nreuse is prevalent in software development, there is a press\u0002ing need to develop an efficient and effective assembly clone\r\nsearch engine for reverse engineers. Previous clone search\r\napproaches only focus on the search accuracy. However,\r\ndesigning a practically useful clone search engine is a non\u0002trivial task which involves multiple factors to be considered.\r\nBy closely collaborating with reverse engineers and Defence\r\nResearch and Development Canada (DRDC), we outline the\r\ndeployment challenges and requirements as follows:\r\nInterpretability and usability: An assembly function\r\ncan be represented as a control flow graph consisting of con\u0002nected basic blocks. Given an assembly function as query, all\r\nof the previous assembly code clone search approaches [7, 6,\r\n18, 26] only provide the top-listed candidate assembly func\u0002tions. They are useful when there exists a function in the\r\nrepository that shares a high degree of similarity with the\r\nquery. However, due to the unpredictable effects of differ\u0002ent compilers, compiler optimization, and obfuscation tech\u0002niques, given an unknown function, it is less probable to have\r\na very similar function in the repository. Returning a list of\r\nclones with a low degree of similarity values is not useful.\r\nAs per our discussions with DRDC, a practical search en\u0002gine should be able to decompose the given query assembly\r\nfunction to different known subgraph clones which can help\r\nreverse engineers better understand the function’s composi\u0002tion. We define a subgraph clone as one of its subgraphs that\r\ncan be found in the other function. Refer to the example\r\nin Figure 1. The previous clone search approaches cannot\r\naddress this challenge.\r\n1\r\nhttp://dx.doi.org/10.1145/2939672.2939719",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/9613787f-ff91-472d-a67b-0ebc73951062.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a955407d623307883a85e7b28366e4f80cd829778b312c6045759d3051507cd7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 838
      },
      {
        "segments": [
          {
            "segment_id": "0b4f3d72-17e5-4842-b13c-6546e6dbc28e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": " push ebp\r\n mov ebp, esp\r\n push ecx\r\n mov [ebp+key], 1A9Ch\r\n mov eax, [ebp+msg]\r\n push eax\r\n push offset Format\r\n call ds:printf\r\n add esp, 8\r\n mov [ebp+msg], 1\r\n cmp [ebp+msg], 0\r\n jz short loc_40103E\r\nmov ecx, [ebp+key]\r\npush ecx\r\npush offset aTheKeyIsD\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_401050\r\nloc_40103E:\r\n mov edx, [ebp+arg_0]\r\n push edx\r\n push offset aInvMsg\r\n call ds:printf\r\n add esp, 8\r\nloc_401050:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\n push ebp\r\n mov ebp, esp\r\n push ecx\r\n mov [ebp+var_4], 1A9Ch\r\n mov eax, [ebp+arg_0]\r\n push eax\r\n push offset aD \r\n call ds:printf\r\n add esp, 8\r\n mov [ebp+arg_0], 1\r\n cmp [ebp+arg_0], 0\r\n jz short loc_4010BC\r\nmov ecx, [ebp+var_4]\r\npush ecx\r\npush offset aTheKeyIsD_0\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_4010D5\r\n loc_4010C3:\r\n mov eax, [ebp+arg_0]\r\n push eax\r\n push offset aInvMsg\r\n call ds:printf\r\n add esp, 8\r\nloc_4010D5:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\nmov edx, [ebp+var_4]\r\nimul edx, [ebp+arg_0]\r\npush edx\r\npush offset \r\naTheKeyIsD_1\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_4010D5\r\nloc_40109E:\r\nmov [ebp+arg_0], 2\r\ncmp [ebp+arg_0], 0\r\njz short loc_4010C3\r\n+push offset aWelcome\r\n+call ds:printf\r\n+add esp, 4\r\nloc_40109E\r\n[ebp+var_4]\r\nloc_4010D5\r\npush ebp\r\n mov ebp, esp\r\npush ecx\r\n mov [ebp+key], 1A9Ch\r\nmov eax, [ebp+msg]\r\n push eax\r\npush offset Format\r\n call ds:printf\r\nadd esp, 8\r\n mov [ebp+msg], 1\r\ncmp [ebp+msg], 0\r\njz short loc_40103E\r\nmov ecx, [ebp+key]\r\npush ecx\r\npush offset aTheKeyIsD\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_401050\r\nloc_40103E:\r\n mov edx, [ebp+arg_0]\r\npush edx\r\n push offset aInvMsg\r\ncall ds:printf\r\nadd esp, 8\r\nloc_401050:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\n push ebp\r\n mov ebp, esp\r\npush ecx\r\nmov [ebp+ , 1A9Ch\r\n mov eax, [ebp+\r\n push eax\r\npush offset\r\n call ds:printf\r\n add esp, 8\r\n mov [ebp+ , 1\r\n cmp [ebp+ , 0\r\n jz short\r\nmov ecx,\r\npush ecx\r\npush offset\r\ncall ds:printf\r\nadd esp, 8\r\njmp short \r\nloc_4010C3:\r\nmov eax, [ebp+arg_0]\r\n push eax\r\npush offset aInvMsg\r\n call ds:printf\r\nadd esp, 8\r\nloc_4010D5:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\nmov edx, [ebp+var_4]\r\nimul edx, [ebp+arg_0]\r\npush edx\r\npush offset \r\naTheKeyIsD_1\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_4010D5\r\nloc_40109E:\r\nmov [ebp+arg_0], 2\r\ncmp [ebp+arg_0], 0\r\njz short loc_4010C3\r\npush ecx\r\nmov [ bep+ 1A9Ch\r\n+push offset aWelcome\r\n+call ds:printf\r\n+add esp,4\r\n,\r\nloc_40109E\r\nl [p _ eb +var_4]\r\n8\r\nloc_4010D5\r\nt aD \r\nintf\r\noffset aMsg\r\nType III\r\n Clone\r\nType II\r\n Clone\r\nType III\r\n Clone\r\nType I\r\n Clone\r\nQuery: Target Function\r\n+ ]\r\n[ebp+\r\n+ var_4 ],\r\n+arg_0]\r\n]\r\n+amsg\r\n8\r\n+ arg_0 ]\r\n+arg 0]\r\nt\r\ng_\r\nloc 4\r\n+ g_ ]\r\n+arg 0]\r\narg_0\r\narg_0\r\nt aTh KeeyID0s _\r\nintf\r\naTheKeyIsD_0\r\nloc_401100:\r\nmov eax, 1\r\nmov esp, ebp\r\npop ebp\r\nretn\r\n add esp,8\r\n+cmp [ebp+msg], 0\r\n+Jz short loc_401100\r\nClone: Repository Function\r\nA basic block\r\nA block to block clone pair\r\nA jump link between two \r\nbasic blocks of the same \r\nfunction\r\nAn explanation label\r\nFigure 1: An example of the clone search problem.\r\nBasic blocks with a white background form a sub\u0002graph clone between two functions. Three types of\r\ncode clones are considered in this paper. Type I:\r\nliterally identical; Type II: syntactically equivalent;\r\nand Type III: minor modifications.\r\nEfficiency and Scalability: An efficient engine can help\r\nreverse engineers retrieve clone search results on-the-fly when\r\nthey are conducting an analysis. Instant feedback tells the\r\nreverse engineer the composition of a given function that is\r\nunder investigation. Scalability is a critical factor as the\r\nnumber of assembly functions in the repository needs to\r\nscale up to millions. The degradation of search performance\r\nagainst the repository size has to be considered. Previous\r\napproaches in [6, 7], which trade efficiency and scalability\r\nfor better accuracy, have a high latency for queries and are\r\nthus not practically applicable.\r\nIncremental updates: The clone search engine should\r\nsupport incremental updates to the repository, without re\u0002indexing existing assembly functions. [7] requires median\r\nstatistics to index each vector and [26] requires data-dependent\r\nsettings for its index, so they do not satisfy this requirement.\r\nClone search quality: Practically, clones among assem\u0002bly functions are one-to-many mappings, i.e., a function has\r\nmultiple cloned functions with different degrees of similar\u0002ity. However, all previous approaches [6, 7, 18, 26] assume\r\nthat clones are one-to-one mappings in the experiment. This\r\nis due to the difficulty of acquiring such a one-to-many la\u0002beled dataset. Moreover, they use different evaluation met\u0002rics. Therefore, it is difficult to have a direct comparison\r\namong them with respect to the search quality. We need to\r\ndevelop a one-to-many labeled dataset and an unified eval\u0002uation framework to quantify the clone search quality.\r\nTo address the above requirements and challenges, we pro\u0002pose a new variant of LSH scheme and incorporate it with\r\na graph matching technique. We also develop and deploy\r\na new assembly clone search engine called Kam1n0. Our\r\nmajor contributions can be summarized as follows:\r\n• Solved a challenging problem for the reverse en\u0002gineering community: Kam1n0 is the first assem\u0002bly code clone search engine that supports subgraph\r\nclone search. Refer to the example in Figure 1. It pro\u0002motes the interpretability and usability by providing\r\nsubgraph clones as results, which helps reverse engi\u0002neers analyzing new and unknown assembly functions.\r\nKam1n0 won the second prize at the 2015 Hex-Rays\r\nplugin Contest2 and its code is publicly accessible on\r\nGitHub.3\r\n• Efficient inexact assembly code search: The as\u0002sembly code vector space is highly skewed. Small blocks\r\ntend to be similar to each other and large blocks tend\r\nto be sparsely distributed in the space. Original hy\u0002perplane hashing with banding technique equally par\u0002titions the space and does not handle the unevenly dis\u0002tributed data well. We propose a new adaptive locality\r\nsensitive hashing (ALSH ) scheme to approximate the\r\ncosine similarity. To our best knowledge, ALSH is the\r\nfirst incremental locality sensitive hashing scheme that\r\nsolves this issue specifically for cosine space with theo\u0002retical guarantee. It retrieves fewer points for dense ar\u0002eas and more points for sparse ones in the cosine space.\r\nIt is therefore efficient in searching nearest neighbors.\r\n• Scalable sub-linear subgraph search: We propose\r\na MapReduce subgraph search algorithm based on the\r\nApache Spark computational framework without an\r\nadditional index. Different to the existing subgraph\r\nisomorphism search problem in data mining, we need\r\nto retrieve subgraphs that are both isomorphic to the\r\nquery and the repository functions as graphs. Thus,\r\nexisting algorithms are not directly applicable. Algo\u0002rithmically, our approach is bounded by polynomial\r\ncomplexity. However, our experiment suggests that it\r\nis sub-linear in practice.\r\n• Accurate and robust function clone search: Kam1n0\r\nis the first approach that integrates both the inexact\r\nassembly code and subgraph search. Previous solu\u0002tions do not consider both of them together. Our ex\u0002tensive experiments suggest that it boosts the clone\r\nsearch quality and yields stable results across different\r\ndatasets and metrics.\r\n• Develop a labeled dataset and benchmark state\u0002of-the-art assembly code clone solutions. We\r\ncarefully construct a new labeled one-to-many assem\u0002bly code clone dataset that is available to the research\r\ncommunity by linking the source code and assembly\r\nfunction level clones. We benchmark and report the\r\nperformance of twelve existing state-of-the-art solu\u0002tions with Kam1n0 on the dataset using several met\u0002rics. We also setup a mini-cluster to evaluate the scal\u0002ability of Kam1n0.\r\nThe remainder of this paper is organized as follows. Sec\u0002tion 2 situates our study within the literature of three differ\u0002ent research problems. Section 3 formally defines the studied\r\nproblem. Section 4 provides an overview of our solution and\r\nsystem design. Section 5 presents the preprocessing steps\r\nand the chosen vector space. Section 6 introduces our pro\u0002posed locality sensitive hashing scheme. Section 7 presents\r\nour graph search algorithm. Section 8 presents our bench\u0002mark experiments. Section 9 provides the conclusion.\r\n2https://hex-rays.com/contests/2015/#kam1n0\r\n3https://github.com/McGill-DMaS/Kam1n0-Plugin-IDA-Pro\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/0b4f3d72-17e5-4842-b13c-6546e6dbc28e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c64b22c6f966b96339f24cd81813c2b702ae40bcfa47ad74288cfd0ed17ecba3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1262
      },
      {
        "segments": [
          {
            "segment_id": "0b4f3d72-17e5-4842-b13c-6546e6dbc28e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": " push ebp\r\n mov ebp, esp\r\n push ecx\r\n mov [ebp+key], 1A9Ch\r\n mov eax, [ebp+msg]\r\n push eax\r\n push offset Format\r\n call ds:printf\r\n add esp, 8\r\n mov [ebp+msg], 1\r\n cmp [ebp+msg], 0\r\n jz short loc_40103E\r\nmov ecx, [ebp+key]\r\npush ecx\r\npush offset aTheKeyIsD\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_401050\r\nloc_40103E:\r\n mov edx, [ebp+arg_0]\r\n push edx\r\n push offset aInvMsg\r\n call ds:printf\r\n add esp, 8\r\nloc_401050:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\n push ebp\r\n mov ebp, esp\r\n push ecx\r\n mov [ebp+var_4], 1A9Ch\r\n mov eax, [ebp+arg_0]\r\n push eax\r\n push offset aD \r\n call ds:printf\r\n add esp, 8\r\n mov [ebp+arg_0], 1\r\n cmp [ebp+arg_0], 0\r\n jz short loc_4010BC\r\nmov ecx, [ebp+var_4]\r\npush ecx\r\npush offset aTheKeyIsD_0\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_4010D5\r\n loc_4010C3:\r\n mov eax, [ebp+arg_0]\r\n push eax\r\n push offset aInvMsg\r\n call ds:printf\r\n add esp, 8\r\nloc_4010D5:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\nmov edx, [ebp+var_4]\r\nimul edx, [ebp+arg_0]\r\npush edx\r\npush offset \r\naTheKeyIsD_1\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_4010D5\r\nloc_40109E:\r\nmov [ebp+arg_0], 2\r\ncmp [ebp+arg_0], 0\r\njz short loc_4010C3\r\n+push offset aWelcome\r\n+call ds:printf\r\n+add esp, 4\r\nloc_40109E\r\n[ebp+var_4]\r\nloc_4010D5\r\npush ebp\r\n mov ebp, esp\r\npush ecx\r\n mov [ebp+key], 1A9Ch\r\nmov eax, [ebp+msg]\r\n push eax\r\npush offset Format\r\n call ds:printf\r\nadd esp, 8\r\n mov [ebp+msg], 1\r\ncmp [ebp+msg], 0\r\njz short loc_40103E\r\nmov ecx, [ebp+key]\r\npush ecx\r\npush offset aTheKeyIsD\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_401050\r\nloc_40103E:\r\n mov edx, [ebp+arg_0]\r\npush edx\r\n push offset aInvMsg\r\ncall ds:printf\r\nadd esp, 8\r\nloc_401050:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\n push ebp\r\n mov ebp, esp\r\npush ecx\r\nmov [ebp+ , 1A9Ch\r\n mov eax, [ebp+\r\n push eax\r\npush offset\r\n call ds:printf\r\n add esp, 8\r\n mov [ebp+ , 1\r\n cmp [ebp+ , 0\r\n jz short\r\nmov ecx,\r\npush ecx\r\npush offset\r\ncall ds:printf\r\nadd esp, 8\r\njmp short \r\nloc_4010C3:\r\nmov eax, [ebp+arg_0]\r\n push eax\r\npush offset aInvMsg\r\n call ds:printf\r\nadd esp, 8\r\nloc_4010D5:\r\nxor eax, eax\r\nmov esp, ebp\r\npop ebp\r\nretn\r\nmov edx, [ebp+var_4]\r\nimul edx, [ebp+arg_0]\r\npush edx\r\npush offset \r\naTheKeyIsD_1\r\ncall ds:printf\r\nadd esp, 8\r\njmp short loc_4010D5\r\nloc_40109E:\r\nmov [ebp+arg_0], 2\r\ncmp [ebp+arg_0], 0\r\njz short loc_4010C3\r\npush ecx\r\nmov [ bep+ 1A9Ch\r\n+push offset aWelcome\r\n+call ds:printf\r\n+add esp,4\r\n,\r\nloc_40109E\r\nl [p _ eb +var_4]\r\n8\r\nloc_4010D5\r\nt aD \r\nintf\r\noffset aMsg\r\nType III\r\n Clone\r\nType II\r\n Clone\r\nType III\r\n Clone\r\nType I\r\n Clone\r\nQuery: Target Function\r\n+ ]\r\n[ebp+\r\n+ var_4 ],\r\n+arg_0]\r\n]\r\n+amsg\r\n8\r\n+ arg_0 ]\r\n+arg 0]\r\nt\r\ng_\r\nloc 4\r\n+ g_ ]\r\n+arg 0]\r\narg_0\r\narg_0\r\nt aTh KeeyID0s _\r\nintf\r\naTheKeyIsD_0\r\nloc_401100:\r\nmov eax, 1\r\nmov esp, ebp\r\npop ebp\r\nretn\r\n add esp,8\r\n+cmp [ebp+msg], 0\r\n+Jz short loc_401100\r\nClone: Repository Function\r\nA basic block\r\nA block to block clone pair\r\nA jump link between two \r\nbasic blocks of the same \r\nfunction\r\nAn explanation label\r\nFigure 1: An example of the clone search problem.\r\nBasic blocks with a white background form a sub\u0002graph clone between two functions. Three types of\r\ncode clones are considered in this paper. Type I:\r\nliterally identical; Type II: syntactically equivalent;\r\nand Type III: minor modifications.\r\nEfficiency and Scalability: An efficient engine can help\r\nreverse engineers retrieve clone search results on-the-fly when\r\nthey are conducting an analysis. Instant feedback tells the\r\nreverse engineer the composition of a given function that is\r\nunder investigation. Scalability is a critical factor as the\r\nnumber of assembly functions in the repository needs to\r\nscale up to millions. The degradation of search performance\r\nagainst the repository size has to be considered. Previous\r\napproaches in [6, 7], which trade efficiency and scalability\r\nfor better accuracy, have a high latency for queries and are\r\nthus not practically applicable.\r\nIncremental updates: The clone search engine should\r\nsupport incremental updates to the repository, without re\u0002indexing existing assembly functions. [7] requires median\r\nstatistics to index each vector and [26] requires data-dependent\r\nsettings for its index, so they do not satisfy this requirement.\r\nClone search quality: Practically, clones among assem\u0002bly functions are one-to-many mappings, i.e., a function has\r\nmultiple cloned functions with different degrees of similar\u0002ity. However, all previous approaches [6, 7, 18, 26] assume\r\nthat clones are one-to-one mappings in the experiment. This\r\nis due to the difficulty of acquiring such a one-to-many la\u0002beled dataset. Moreover, they use different evaluation met\u0002rics. Therefore, it is difficult to have a direct comparison\r\namong them with respect to the search quality. We need to\r\ndevelop a one-to-many labeled dataset and an unified eval\u0002uation framework to quantify the clone search quality.\r\nTo address the above requirements and challenges, we pro\u0002pose a new variant of LSH scheme and incorporate it with\r\na graph matching technique. We also develop and deploy\r\na new assembly clone search engine called Kam1n0. Our\r\nmajor contributions can be summarized as follows:\r\n• Solved a challenging problem for the reverse en\u0002gineering community: Kam1n0 is the first assem\u0002bly code clone search engine that supports subgraph\r\nclone search. Refer to the example in Figure 1. It pro\u0002motes the interpretability and usability by providing\r\nsubgraph clones as results, which helps reverse engi\u0002neers analyzing new and unknown assembly functions.\r\nKam1n0 won the second prize at the 2015 Hex-Rays\r\nplugin Contest2 and its code is publicly accessible on\r\nGitHub.3\r\n• Efficient inexact assembly code search: The as\u0002sembly code vector space is highly skewed. Small blocks\r\ntend to be similar to each other and large blocks tend\r\nto be sparsely distributed in the space. Original hy\u0002perplane hashing with banding technique equally par\u0002titions the space and does not handle the unevenly dis\u0002tributed data well. We propose a new adaptive locality\r\nsensitive hashing (ALSH ) scheme to approximate the\r\ncosine similarity. To our best knowledge, ALSH is the\r\nfirst incremental locality sensitive hashing scheme that\r\nsolves this issue specifically for cosine space with theo\u0002retical guarantee. It retrieves fewer points for dense ar\u0002eas and more points for sparse ones in the cosine space.\r\nIt is therefore efficient in searching nearest neighbors.\r\n• Scalable sub-linear subgraph search: We propose\r\na MapReduce subgraph search algorithm based on the\r\nApache Spark computational framework without an\r\nadditional index. Different to the existing subgraph\r\nisomorphism search problem in data mining, we need\r\nto retrieve subgraphs that are both isomorphic to the\r\nquery and the repository functions as graphs. Thus,\r\nexisting algorithms are not directly applicable. Algo\u0002rithmically, our approach is bounded by polynomial\r\ncomplexity. However, our experiment suggests that it\r\nis sub-linear in practice.\r\n• Accurate and robust function clone search: Kam1n0\r\nis the first approach that integrates both the inexact\r\nassembly code and subgraph search. Previous solu\u0002tions do not consider both of them together. Our ex\u0002tensive experiments suggest that it boosts the clone\r\nsearch quality and yields stable results across different\r\ndatasets and metrics.\r\n• Develop a labeled dataset and benchmark state\u0002of-the-art assembly code clone solutions. We\r\ncarefully construct a new labeled one-to-many assem\u0002bly code clone dataset that is available to the research\r\ncommunity by linking the source code and assembly\r\nfunction level clones. We benchmark and report the\r\nperformance of twelve existing state-of-the-art solu\u0002tions with Kam1n0 on the dataset using several met\u0002rics. We also setup a mini-cluster to evaluate the scal\u0002ability of Kam1n0.\r\nThe remainder of this paper is organized as follows. Sec\u0002tion 2 situates our study within the literature of three differ\u0002ent research problems. Section 3 formally defines the studied\r\nproblem. Section 4 provides an overview of our solution and\r\nsystem design. Section 5 presents the preprocessing steps\r\nand the chosen vector space. Section 6 introduces our pro\u0002posed locality sensitive hashing scheme. Section 7 presents\r\nour graph search algorithm. Section 8 presents our bench\u0002mark experiments. Section 9 provides the conclusion.\r\n2https://hex-rays.com/contests/2015/#kam1n0\r\n3https://github.com/McGill-DMaS/Kam1n0-Plugin-IDA-Pro\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/0b4f3d72-17e5-4842-b13c-6546e6dbc28e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c64b22c6f966b96339f24cd81813c2b702ae40bcfa47ad74288cfd0ed17ecba3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1262
      },
      {
        "segments": [
          {
            "segment_id": "b7682015-8e2e-472a-8e27-b9db86e067a7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "2. RELATED WORK\r\nLocality sensitive hashing. Locality sensitive hash\u0002ing (LSH ) has been studied for decades to solve the \u0002-\r\napproximated Nearest Neighbor (\u0002NN) problem, since exact\r\nnearest neighbor is not scalable to high dimensional data.\r\nOne of the prevalent problems of LSH is the uneven data dis\u0002tribution issue, as LSH equally partitions the data space. To\r\nmitigate this issue, several approaches have been proposed\r\nincluding the LSH-Forest [2], LSB-Forest [32], C2LSH [10]\r\nand SK-LSH [22]. It has been shown that the cosine vector\r\nspace is robust to different compiler settings [18] in assembly\r\ncode clone search. However, LSH-Forest, C2LSH and SK\u0002LSH are designed for the p-stable distribution, which does\r\nnot fit the cosine space. LSB-Forest dynamically and un\u0002equally partition the data space. As pointed out by [33],\r\nit requires the hash family to possess the (\u0002, f(\u0002)) property.\r\nHowever, to our best knowledge, such a family in cosine\r\nspace is still unknown. There are other learning-based ap\u0002proaches [11], which do not meet our incremental require\u0002ment. Wang et al. [35] provide a more comprehensive sur\u0002vey on LSH. To satisfy our requirements, we propose the\r\nALSH scheme specifically for the cosine space. Different to\r\nthe LSH-Forest, ALSH takes more than one bit when going\r\ndown the tree structure and does not require the (\u0002, f(\u0002))\r\nproperty for the LSH family to have theoretical guarantee.\r\nUnlike LSB forest [2], we dynamically construct the buckets\r\nto adapt to different data distributions.\r\nSubgraph isomorphism. Ullmann [34] proposed the\r\nfirst practical subgraph isomorphism algorithm for small\r\ngraphs. Several approaches were proposed afterwards for\r\nlarge scale graph data, such as TurboISO [13] and STwig [31].\r\nIt has been shown that they can solve the subgraph iso\u0002morphism problem in a reasonable time. However, they do\r\nnot completely meet our problem settings. The subgraph\r\nisomorphism problem needs to retrieve subgraphs that are\r\nisomorphic to the graphs in the repository and identical to\r\nthe query. However, we need to retrieve subgraphs that are\r\nisomorphic to the graphs in the repository and isomorphic\r\nto the graph of the query, which significantly increases the\r\ncomplexity. More details will be discussed in Section 7. Such\r\na difference requires us to propose a specialized search algo\u0002rithm. Lee et al. [21] provide a comprehensive survey and\r\nperformance benchmark on subgraph isomorphism.\r\nAssembly code clone search. The studies on the as\u0002sembly code clone search problem are recent. Only a few\r\napproaches exist [6, 7, 18, 26]. They all rely on the inexact\r\ntext search techniques of data mining. BinClone [7] models\r\nassembly code into an Euclidean space based on frequency\r\nvalues of selected features. It is inefficient and not scal\u0002able due to the exponential 2-combination of features which\r\napproximates the 2-norm distance. LSH-S in [26] models\r\nassembly code into a cosine space based on token frequency\r\nand approximate the distance by hyperplane hashing and\r\nbanding scheme. It equally partitions the space and suffers\r\nfrom the uneven data distribution problem. Graphlet [18]\r\nmodels assembly code into a cosine space based on extracted\r\nsignatures from assembly code. However, it cannot detect\r\nany subgraph clones that is smaller than the graphlet size.\r\nTracelet [6] models assembly code according to string edit\u0002ing distance. It compares function one by one, which is not\r\nscalable. Kam1n0 is fundamentally different to the previous\r\napproaches. It is an integration of inexact assembly code\r\nHTTP \r\nPOST/GET\r\nSubmit jobs\r\nPersist\u0002\r\ndata\r\nLoad/cache \r\ndata\r\nReturn \r\nresults\r\nResults in \r\nJ\u0003\u0004\u0005 format\r\nData \u0006torage layer (hard drive)\r\nGraph-based \r\ndata storage \r\nmodel\r\nNo-SQL database\r\nApache Cassandra\r\nBucket-based \r\ndata storage \r\nmodel\r\nDistributed/Local MapReduce compatible execution framework\r\n(\u0007emory and CPUs) ( y \r\nApache Spark:\r\nLocal or distributed mode\r\nt Web-based user interface and clone visualization\r\nJQuery D3js Dagre Bootstrap\r\nIDA\u0002Pro \bonnector\u0002\r\n(Plugin)\r\nKam1n0 engine\r\nLSH-based \r\nassembly code \r\nclone search\r\nMR-based \r\nsubgraph search\r\nAssembly code \r\nprocessing \r\nutilities\r\nRESTful API web services\r\nFigure 2: The overall solution stack of the Kam1n0\r\nengine.\r\nsearch and the subgraph search. It enables clone subgraph\r\nsearch of any size.\r\n3. PROBLEM STATEMENT\r\nReverse engineering starts from a binary file. After being\r\nunpacked and disassembled, it becomes a list of assembly\r\nfunctions. In this paper, function represents an assembly\r\nfunction; block represents a basic block; source function rep\u0002resents the actual function written in source code, such as\r\nC++; repository function stands for the assembly function\r\nthat is indexed inside the repository; target function denotes\r\nthe assembly function that is given as a query; and corre\u0002spondingly, repository blocks and target blocks refer to their\r\nrespective basic blocks. Each function f is represented as\r\na control flow graph denoted by (B,E), where B indicates\r\nits basic blocks and E, indicates the edges that connect the\r\nblocks. Let B(RP) be the complete set of basic blocks in\r\nthe repository and F(RP) be the complete set of functions\r\nin the repository. Given an assembly function, our goal is to\r\nsearch all its subgraph clones inside the repository RP. We\r\nformally define the search problem as follows:\r\nDefinition 1. (Assembly function subgraph clone search)\r\nGiven a target function ft and its control flow graph (Bt, Et),\r\nthe search problem is to retrieve all the repository func\u0002tions fs ∈ RP, which share at least one subgraph clone\r\nwith ft. The shared list of subgraph clones between fs and\r\nft is denoted by sgs[1 : a], where sgs[a] represents one of\r\nthem. A subgraph clone is a set of basic block clone pairs\r\nsgs[a] = {\u0003bt, bs\u0004,... } between fs and ft, where bt ∈ Bt,\r\nbs ∈ Bs, and \u0003bt, bs\u0004 is a type I, type II, or type III clone\r\n(see Figure 1). Formally, given ft, the problem is to retrieve\r\nall {fs|fs ∈ RP and |sgs| > 0}. \u0002\r\n4. OVERALL ARCHITECTURE\r\nThe Kam1n0 engine is designed for general key-value stor\u0002age and the Apache Spark 4 computational framework. Its\r\nsolution stack, as shown in Figure 2, consists of three layers.\r\nThe data storage layer is concerned with how the data is\r\nstored and indexed. The distributed/local execution layer\r\nmanages and executes the jobs submitted by the Kam1n0\r\n4Apache Spark, available at: http://spark.apache.org/\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/b7682015-8e2e-472a-8e27-b9db86e067a7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=aa7c7e4227421c528347debde610718345261e891dfa97ebb7bf085206c4bbd0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1001
      },
      {
        "segments": [
          {
            "segment_id": "b7682015-8e2e-472a-8e27-b9db86e067a7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "2. RELATED WORK\r\nLocality sensitive hashing. Locality sensitive hash\u0002ing (LSH ) has been studied for decades to solve the \u0002-\r\napproximated Nearest Neighbor (\u0002NN) problem, since exact\r\nnearest neighbor is not scalable to high dimensional data.\r\nOne of the prevalent problems of LSH is the uneven data dis\u0002tribution issue, as LSH equally partitions the data space. To\r\nmitigate this issue, several approaches have been proposed\r\nincluding the LSH-Forest [2], LSB-Forest [32], C2LSH [10]\r\nand SK-LSH [22]. It has been shown that the cosine vector\r\nspace is robust to different compiler settings [18] in assembly\r\ncode clone search. However, LSH-Forest, C2LSH and SK\u0002LSH are designed for the p-stable distribution, which does\r\nnot fit the cosine space. LSB-Forest dynamically and un\u0002equally partition the data space. As pointed out by [33],\r\nit requires the hash family to possess the (\u0002, f(\u0002)) property.\r\nHowever, to our best knowledge, such a family in cosine\r\nspace is still unknown. There are other learning-based ap\u0002proaches [11], which do not meet our incremental require\u0002ment. Wang et al. [35] provide a more comprehensive sur\u0002vey on LSH. To satisfy our requirements, we propose the\r\nALSH scheme specifically for the cosine space. Different to\r\nthe LSH-Forest, ALSH takes more than one bit when going\r\ndown the tree structure and does not require the (\u0002, f(\u0002))\r\nproperty for the LSH family to have theoretical guarantee.\r\nUnlike LSB forest [2], we dynamically construct the buckets\r\nto adapt to different data distributions.\r\nSubgraph isomorphism. Ullmann [34] proposed the\r\nfirst practical subgraph isomorphism algorithm for small\r\ngraphs. Several approaches were proposed afterwards for\r\nlarge scale graph data, such as TurboISO [13] and STwig [31].\r\nIt has been shown that they can solve the subgraph iso\u0002morphism problem in a reasonable time. However, they do\r\nnot completely meet our problem settings. The subgraph\r\nisomorphism problem needs to retrieve subgraphs that are\r\nisomorphic to the graphs in the repository and identical to\r\nthe query. However, we need to retrieve subgraphs that are\r\nisomorphic to the graphs in the repository and isomorphic\r\nto the graph of the query, which significantly increases the\r\ncomplexity. More details will be discussed in Section 7. Such\r\na difference requires us to propose a specialized search algo\u0002rithm. Lee et al. [21] provide a comprehensive survey and\r\nperformance benchmark on subgraph isomorphism.\r\nAssembly code clone search. The studies on the as\u0002sembly code clone search problem are recent. Only a few\r\napproaches exist [6, 7, 18, 26]. They all rely on the inexact\r\ntext search techniques of data mining. BinClone [7] models\r\nassembly code into an Euclidean space based on frequency\r\nvalues of selected features. It is inefficient and not scal\u0002able due to the exponential 2-combination of features which\r\napproximates the 2-norm distance. LSH-S in [26] models\r\nassembly code into a cosine space based on token frequency\r\nand approximate the distance by hyperplane hashing and\r\nbanding scheme. It equally partitions the space and suffers\r\nfrom the uneven data distribution problem. Graphlet [18]\r\nmodels assembly code into a cosine space based on extracted\r\nsignatures from assembly code. However, it cannot detect\r\nany subgraph clones that is smaller than the graphlet size.\r\nTracelet [6] models assembly code according to string edit\u0002ing distance. It compares function one by one, which is not\r\nscalable. Kam1n0 is fundamentally different to the previous\r\napproaches. It is an integration of inexact assembly code\r\nHTTP \r\nPOST/GET\r\nSubmit jobs\r\nPersist\u0002\r\ndata\r\nLoad/cache \r\ndata\r\nReturn \r\nresults\r\nResults in \r\nJ\u0003\u0004\u0005 format\r\nData \u0006torage layer (hard drive)\r\nGraph-based \r\ndata storage \r\nmodel\r\nNo-SQL database\r\nApache Cassandra\r\nBucket-based \r\ndata storage \r\nmodel\r\nDistributed/Local MapReduce compatible execution framework\r\n(\u0007emory and CPUs) ( y \r\nApache Spark:\r\nLocal or distributed mode\r\nt Web-based user interface and clone visualization\r\nJQuery D3js Dagre Bootstrap\r\nIDA\u0002Pro \bonnector\u0002\r\n(Plugin)\r\nKam1n0 engine\r\nLSH-based \r\nassembly code \r\nclone search\r\nMR-based \r\nsubgraph search\r\nAssembly code \r\nprocessing \r\nutilities\r\nRESTful API web services\r\nFigure 2: The overall solution stack of the Kam1n0\r\nengine.\r\nsearch and the subgraph search. It enables clone subgraph\r\nsearch of any size.\r\n3. PROBLEM STATEMENT\r\nReverse engineering starts from a binary file. After being\r\nunpacked and disassembled, it becomes a list of assembly\r\nfunctions. In this paper, function represents an assembly\r\nfunction; block represents a basic block; source function rep\u0002resents the actual function written in source code, such as\r\nC++; repository function stands for the assembly function\r\nthat is indexed inside the repository; target function denotes\r\nthe assembly function that is given as a query; and corre\u0002spondingly, repository blocks and target blocks refer to their\r\nrespective basic blocks. Each function f is represented as\r\na control flow graph denoted by (B,E), where B indicates\r\nits basic blocks and E, indicates the edges that connect the\r\nblocks. Let B(RP) be the complete set of basic blocks in\r\nthe repository and F(RP) be the complete set of functions\r\nin the repository. Given an assembly function, our goal is to\r\nsearch all its subgraph clones inside the repository RP. We\r\nformally define the search problem as follows:\r\nDefinition 1. (Assembly function subgraph clone search)\r\nGiven a target function ft and its control flow graph (Bt, Et),\r\nthe search problem is to retrieve all the repository func\u0002tions fs ∈ RP, which share at least one subgraph clone\r\nwith ft. The shared list of subgraph clones between fs and\r\nft is denoted by sgs[1 : a], where sgs[a] represents one of\r\nthem. A subgraph clone is a set of basic block clone pairs\r\nsgs[a] = {\u0003bt, bs\u0004,... } between fs and ft, where bt ∈ Bt,\r\nbs ∈ Bs, and \u0003bt, bs\u0004 is a type I, type II, or type III clone\r\n(see Figure 1). Formally, given ft, the problem is to retrieve\r\nall {fs|fs ∈ RP and |sgs| > 0}. \u0002\r\n4. OVERALL ARCHITECTURE\r\nThe Kam1n0 engine is designed for general key-value stor\u0002age and the Apache Spark 4 computational framework. Its\r\nsolution stack, as shown in Figure 2, consists of three layers.\r\nThe data storage layer is concerned with how the data is\r\nstored and indexed. The distributed/local execution layer\r\nmanages and executes the jobs submitted by the Kam1n0\r\n4Apache Spark, available at: http://spark.apache.org/\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/b7682015-8e2e-472a-8e27-b9db86e067a7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=aa7c7e4227421c528347debde610718345261e891dfa97ebb7bf085206c4bbd0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1001
      },
      {
        "segments": [
          {
            "segment_id": "d5b88a3e-5053-4c9a-8082-f527441cedab",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "Input formats\r\n\tinary file\r\n\nssembly file\r\nAssembly code processing utilities\r\nAssembly \r\nfunction \r\nparser\r\nAssembly \r\ncode \r\nnormalizer\r\nSubgraph search\r\nMappers\r\nReducer (self\u0002merge\r\nReducer\r\nAdaptive LSH module\r\nIndex\r\nCosine \u0006pace Extract \r\n\u000beatures\r\nAdaptive LSH forest\r\nProduce a binary surrogate \r\nFind exact/inexact clones of \r\nassembly blocks\r\nPairs of basic \r\nblock\r\nclones\r\nCloned \r\nsubgraphs of \r\neach function\r\nClone \u0006\f\r\u000e\b\u000f\r\nresults\r\nDisassembly \r\nfactory\r\nIDA\u0002Pro\r\nConstruct \r\n\u0010ector\r\nFigure 3: Assembly clone search data flow.\r\nengine. The Kam1n0 engine splits a search query into mul\u0002tiple jobs and coordinates their execution flow. It also pro\u0002vides the RESTful APIs. We have implemented a web-based\r\nuser interface and an Hex-Rays IDA Pro plugin5 as clients.\r\nIDA Pro is a popular interactive disassembler that is used\r\nby reverse engineers.\r\nFigure 3 depicts the data flow of the clone search pro\u0002cess. It consists of the following steps. Preprocessing: After\r\nparsing the input (either a binary file or assembly functions)\r\ninto control flow graphs, this step normalizes assembly code\r\ninto a general form, which will be elaborated in Section 5.\r\nFind basic blocks clone pairs: Given a list of assembly basic\r\nblocks from the previous step, it finds all the clone pairs of\r\nblocks using ALSH. Search the subgraph clones: Given the\r\nlist of clone block pairs, the MapReduce module merges and\r\nconstructs the subgraph clones. Note that this clone search\r\nprocess does not require any source code.\r\n5. PREPROCESSING AND VECTOR SPACE\r\nWe choose the cosine vector space to characterize the se\u0002mantic similarity of assembly code. It has been shown that\r\nthe cosine vector space is robust to different compiler set\u0002tings [18]. It can mitigate the linear transformation of as\u0002sembly code. For example, to optimize the program for\r\nspeed, the compiler may unroll and flatten a loop struc\u0002ture in assembly code by repeating the code inside the loop\r\nmultiple times. In this case, the cosine similarity between\r\nthe unrolled and original loop is still high, due to the fact\r\nthat the cosine distance only considers the included angle be\u0002tween two vectors. The features selected in Kam1n0 include\r\nmnemonics, combinations of mnemonics and operands, as\r\nwell as mnemonics n-gram, which are typically used in as\u0002sembly code analysis [7, 26]. The equivalent assembly code\r\nfragments can be represented in different forms. To miti\u0002gate this issue, we normalize the operands in assembly code\r\nduring the preprocessing. We extend the normalization tree\r\nused in BinClone [7] with more types. There are three nor\u0002malization levels: root, type, and specific. Each of them\r\ncorresponds to a different generalization level of assembly\r\ncode. More details can be found in our technical report.6\r\n6. LOCALITY SENSITIVE HASHING\r\nIn this section, we introduce an Adaptive Locality Sensi\u0002tive Hashing (ALSH ) scheme for searching the block-level\r\nsemantic clones. As discussed in Section 2, exact nearest\r\nneighbor search is not scalable. Thus, we started from the\r\nreduction of the \u0002-approximated k-NN problem:\r\n5IDA Pro, available at: http://www.hex-rays.com/\r\n6Kam1n0 technical report, available at:\r\nhttp://dmas.lab.mcgill.ca/fung/pub/DFC16kdd.pdf\r\nDefinition 2. (\u0002-approximated NN search problem) Given\r\na dataset D ⊂ Rd (R denotes real numbers) and a query\r\npoint q, let r denote the distance between the query point\r\nq and its nearest neighbor o∗. This problem is to find an\r\napproximated data point within the distance \u0002 × r where\r\n\u0002 > 1 \u0002\r\nThe \u0002-approximated k-NN search problem can be reduced\r\nto the \u0002NN problem by finding the k data points where each\r\nis an \u0002-approximated point of the exact k-NN of q [10]. The\r\nlocality sensitive hashing approaches do not solve the \u0002NN\r\nproblem directly. \u0002NN is further reduced into another prob\u0002lem: (\u0002, r)-approximated ball cover problem [1, 14].\r\nDefinition 3. ((\u0002, r)-approximated ball cover problem) Given\r\na dataset D ⊂ Rd and a query point q, let B(q, r) denote\r\na ball with center q and radius r. The query q returns the\r\nresults as follows:\r\n• if there exists a point o∗ ∈ B(q, R), then return a data\r\npoint from B(q, \u0002R)\r\n• if B(q, \u0002R) does not contain any data object in D, then\r\nreturn nothing. \u0002\r\nOne can solve the (\u0002, r) ball cover problem by using the\r\nLocality Sensitive Hashing (LSH) families. A locality sen\u0002sitive hashing family consists of hashing functions that can\r\npreserve the distance between points.\r\nDefinition 4. (Locality Sensitive Hashing Family) Given a\r\ndistance r under a specific metric space, an approximation\r\nratio \u0002, and two probabilities p1 > p2, a hash function family\r\nH→{h : Rd → U} is (r, \u0002r, p1, p2)-sensitive such that:\r\n• if o ∈ B(q, r), then P rH[h(q) = h(o)] \u0003 p1\r\n• if o /∈ B(q, \u0002r), then P rH[h(q) = h(o)] \u0004 p2 \u0002\r\nLSH families are available for many metric spaces such as\r\ncosine similarity [3], hamming distance [14], Jaccard coef\u0002ficient, and p-stable distributions [5]. Based on our chosen\r\ncosine vector space, we adopt the random hyperplane hash\r\n[3] family, where sign(·) output the sign of the input.\r\nh(\u0003o) = sign(\u0003o · \u0003a) (1)\r\nBy substituting the random vector \u0003a we can obtain different\r\nhash functions in the family. The collision probability of two\r\ndata points o\u00031 and o\u00032 on Equation 1 can be formulated as:\r\nP[h(o\u00031) = h(o\u00032)] = 1 − θo\u00031, \u0003o2\r\nπ (2)\r\nθo\u00031, \u0003o2 is the included angle between o\u00031 and o\u00032. The prob\u0002ability that two vectors have the same projected direction\r\non a random hyperplane is high when their included angle\r\nis small.\r\nTheorem 1. The random hyperplane hash function is a\r\n(r, \u0002r, 1 − r/π, 1 − \u0002r/π) sensitive hashing family. \u0002\r\nProof. According to Definition 4 and Equation 2: p1 =\r\n1 − r/π and p2 = 1 − \u0002r/π\r\nTo use the locality sensitive hashing families to solve the\r\nball cover problem, it needs a hashing scheme to meet the\r\nquality requirement. The E2LSH approach was originally\r\nproposed by [14] and extended by[5]. It concatenates k dif\u0002ferent hash functions [h1,...,hk] from a given LSH family H\r\ninto a function g(o)=(h1(o),...,hk(o)), and adopts l such\r\nfunctions. The parameters k and l are chosen to ensure the\r\nfollowing two properties are satisfied:\r\nProperty 1. (P1): if there exists p∗ ∈ B(q, r), then gj (p∗) =\r\ngj (q) from some j = 1 ...l. \u0002\r\nProperty 2. (P2): the total number of points ∈/ B(q, \u0002r)\r\nthat collides with q is less than 2l. \u0002\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/d5b88a3e-5053-4c9a-8082-f527441cedab.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f660a0ad6db24d55f51ad321827cbbdfdecf2c51d1c6fef2a2fe485fba98914a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1047
      },
      {
        "segments": [
          {
            "segment_id": "d5b88a3e-5053-4c9a-8082-f527441cedab",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "Input formats\r\n\tinary file\r\n\nssembly file\r\nAssembly code processing utilities\r\nAssembly \r\nfunction \r\nparser\r\nAssembly \r\ncode \r\nnormalizer\r\nSubgraph search\r\nMappers\r\nReducer (self\u0002merge\r\nReducer\r\nAdaptive LSH module\r\nIndex\r\nCosine \u0006pace Extract \r\n\u000beatures\r\nAdaptive LSH forest\r\nProduce a binary surrogate \r\nFind exact/inexact clones of \r\nassembly blocks\r\nPairs of basic \r\nblock\r\nclones\r\nCloned \r\nsubgraphs of \r\neach function\r\nClone \u0006\f\r\u000e\b\u000f\r\nresults\r\nDisassembly \r\nfactory\r\nIDA\u0002Pro\r\nConstruct \r\n\u0010ector\r\nFigure 3: Assembly clone search data flow.\r\nengine. The Kam1n0 engine splits a search query into mul\u0002tiple jobs and coordinates their execution flow. It also pro\u0002vides the RESTful APIs. We have implemented a web-based\r\nuser interface and an Hex-Rays IDA Pro plugin5 as clients.\r\nIDA Pro is a popular interactive disassembler that is used\r\nby reverse engineers.\r\nFigure 3 depicts the data flow of the clone search pro\u0002cess. It consists of the following steps. Preprocessing: After\r\nparsing the input (either a binary file or assembly functions)\r\ninto control flow graphs, this step normalizes assembly code\r\ninto a general form, which will be elaborated in Section 5.\r\nFind basic blocks clone pairs: Given a list of assembly basic\r\nblocks from the previous step, it finds all the clone pairs of\r\nblocks using ALSH. Search the subgraph clones: Given the\r\nlist of clone block pairs, the MapReduce module merges and\r\nconstructs the subgraph clones. Note that this clone search\r\nprocess does not require any source code.\r\n5. PREPROCESSING AND VECTOR SPACE\r\nWe choose the cosine vector space to characterize the se\u0002mantic similarity of assembly code. It has been shown that\r\nthe cosine vector space is robust to different compiler set\u0002tings [18]. It can mitigate the linear transformation of as\u0002sembly code. For example, to optimize the program for\r\nspeed, the compiler may unroll and flatten a loop struc\u0002ture in assembly code by repeating the code inside the loop\r\nmultiple times. In this case, the cosine similarity between\r\nthe unrolled and original loop is still high, due to the fact\r\nthat the cosine distance only considers the included angle be\u0002tween two vectors. The features selected in Kam1n0 include\r\nmnemonics, combinations of mnemonics and operands, as\r\nwell as mnemonics n-gram, which are typically used in as\u0002sembly code analysis [7, 26]. The equivalent assembly code\r\nfragments can be represented in different forms. To miti\u0002gate this issue, we normalize the operands in assembly code\r\nduring the preprocessing. We extend the normalization tree\r\nused in BinClone [7] with more types. There are three nor\u0002malization levels: root, type, and specific. Each of them\r\ncorresponds to a different generalization level of assembly\r\ncode. More details can be found in our technical report.6\r\n6. LOCALITY SENSITIVE HASHING\r\nIn this section, we introduce an Adaptive Locality Sensi\u0002tive Hashing (ALSH ) scheme for searching the block-level\r\nsemantic clones. As discussed in Section 2, exact nearest\r\nneighbor search is not scalable. Thus, we started from the\r\nreduction of the \u0002-approximated k-NN problem:\r\n5IDA Pro, available at: http://www.hex-rays.com/\r\n6Kam1n0 technical report, available at:\r\nhttp://dmas.lab.mcgill.ca/fung/pub/DFC16kdd.pdf\r\nDefinition 2. (\u0002-approximated NN search problem) Given\r\na dataset D ⊂ Rd (R denotes real numbers) and a query\r\npoint q, let r denote the distance between the query point\r\nq and its nearest neighbor o∗. This problem is to find an\r\napproximated data point within the distance \u0002 × r where\r\n\u0002 > 1 \u0002\r\nThe \u0002-approximated k-NN search problem can be reduced\r\nto the \u0002NN problem by finding the k data points where each\r\nis an \u0002-approximated point of the exact k-NN of q [10]. The\r\nlocality sensitive hashing approaches do not solve the \u0002NN\r\nproblem directly. \u0002NN is further reduced into another prob\u0002lem: (\u0002, r)-approximated ball cover problem [1, 14].\r\nDefinition 3. ((\u0002, r)-approximated ball cover problem) Given\r\na dataset D ⊂ Rd and a query point q, let B(q, r) denote\r\na ball with center q and radius r. The query q returns the\r\nresults as follows:\r\n• if there exists a point o∗ ∈ B(q, R), then return a data\r\npoint from B(q, \u0002R)\r\n• if B(q, \u0002R) does not contain any data object in D, then\r\nreturn nothing. \u0002\r\nOne can solve the (\u0002, r) ball cover problem by using the\r\nLocality Sensitive Hashing (LSH) families. A locality sen\u0002sitive hashing family consists of hashing functions that can\r\npreserve the distance between points.\r\nDefinition 4. (Locality Sensitive Hashing Family) Given a\r\ndistance r under a specific metric space, an approximation\r\nratio \u0002, and two probabilities p1 > p2, a hash function family\r\nH→{h : Rd → U} is (r, \u0002r, p1, p2)-sensitive such that:\r\n• if o ∈ B(q, r), then P rH[h(q) = h(o)] \u0003 p1\r\n• if o /∈ B(q, \u0002r), then P rH[h(q) = h(o)] \u0004 p2 \u0002\r\nLSH families are available for many metric spaces such as\r\ncosine similarity [3], hamming distance [14], Jaccard coef\u0002ficient, and p-stable distributions [5]. Based on our chosen\r\ncosine vector space, we adopt the random hyperplane hash\r\n[3] family, where sign(·) output the sign of the input.\r\nh(\u0003o) = sign(\u0003o · \u0003a) (1)\r\nBy substituting the random vector \u0003a we can obtain different\r\nhash functions in the family. The collision probability of two\r\ndata points o\u00031 and o\u00032 on Equation 1 can be formulated as:\r\nP[h(o\u00031) = h(o\u00032)] = 1 − θo\u00031, \u0003o2\r\nπ (2)\r\nθo\u00031, \u0003o2 is the included angle between o\u00031 and o\u00032. The prob\u0002ability that two vectors have the same projected direction\r\non a random hyperplane is high when their included angle\r\nis small.\r\nTheorem 1. The random hyperplane hash function is a\r\n(r, \u0002r, 1 − r/π, 1 − \u0002r/π) sensitive hashing family. \u0002\r\nProof. According to Definition 4 and Equation 2: p1 =\r\n1 − r/π and p2 = 1 − \u0002r/π\r\nTo use the locality sensitive hashing families to solve the\r\nball cover problem, it needs a hashing scheme to meet the\r\nquality requirement. The E2LSH approach was originally\r\nproposed by [14] and extended by[5]. It concatenates k dif\u0002ferent hash functions [h1,...,hk] from a given LSH family H\r\ninto a function g(o)=(h1(o),...,hk(o)), and adopts l such\r\nfunctions. The parameters k and l are chosen to ensure the\r\nfollowing two properties are satisfied:\r\nProperty 1. (P1): if there exists p∗ ∈ B(q, r), then gj (p∗) =\r\ngj (q) from some j = 1 ...l. \u0002\r\nProperty 2. (P2): the total number of points ∈/ B(q, \u0002r)\r\nthat collides with q is less than 2l. \u0002\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/d5b88a3e-5053-4c9a-8082-f527441cedab.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f660a0ad6db24d55f51ad321827cbbdfdecf2c51d1c6fef2a2fe485fba98914a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1047
      },
      {
        "segments": [
          {
            "segment_id": "4ab21e59-5f2e-494e-ad27-15ddfc006b06",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "It is proven that if the above two properties hold with\r\nconstant probability, the algorithm can correctly solve the\r\n(\u0002, r)-approximated ball cover problem [14]. For E2LSH, by\r\npicking k = logp2 (1/n) and l = nρ where ρ = ln1/p1\r\nln1/p2 , both\r\nProperties 1 and 2 hold with constant probability.\r\nHowever, the ball cover problem is a strong reduction to\r\nthe NN problem since it adopts the same radius r for all\r\npoints. Real-life data cannot always be evenly distributed.\r\nTherefore, it is difficult to pick an appropriate r. We de\u0002note this as the uneven data distribution issue. In [12],\r\na magic rm is adopted heuristically. But as pointed out\r\nby [32], such a magic radius may not exist. A weaker re\u0002duction was proposed in [14], where the NN problem is re\u0002duced to multiple (r, \u0002)-NN ball cover problems with varying\r\nr = {1, \u00022, \u00023,... }. The intuition is that points in different\r\ndensity areas can find a suitable r. However, such a reduc\u0002tion requires a large space consumption and longer response\r\ntime. Other indexing structures have been proposed to solve\r\nthis issue. Per our discussion in Section 2, existing tech\u0002niques do not meet our requirement. Thus, we customize\r\nthe LSH-forest approach and propose the ALSH structure.\r\n6.1 Adaptive LSH Structure\r\nWe found that the limitation of the expanding sequence\r\nof r in previous section is too strong. It is unnecessary to\r\nexactly follow the sequence r = {1, \u00022, \u00023,... }, as long as r\r\nis increasing in a similar manner to rt+1 = rt × \u0002. Thus, we\r\ncustomize the \u0002-approximated NN problem as follows:\r\nDefinition 5. (f(r)-approximated NN search problem) Given\r\na dataset D ⊂ Rd and a query point q, let r denote the dis\u0002tance between the query point q and its nearest neighbor o∗.\r\nThe problem is to find an approximated data point within\r\nthe distance f(r), where f(r)/r > 1 \u0002\r\nInstead of using a fix approximation ratio, we approxi\u0002mate the search by using a function on r. We issue a differ\u0002ent sequence of expanding r. The expanding sequence of r\r\nis formulated as r0, r1,...,rt, rt+1,...,rm, where rt < rt+1.\r\nSimilar to the E2LSH approach, we concatenate multiple\r\nhash functions from the random hyperplane hash family\r\nH into one. However, we concatenate different number of\r\nhash functions for different values of r. This number is de\u0002noted by kt for rt, and the sequence of k is denoted by\r\nk0, k1,...,kt, kt+1,...,km, where kt > kt+1. Recall that\r\nthe concatenated function is denoted by g. Consequently,\r\nthere will be a different function g at position t, which is de\u0002noted by gt. Yet, function gt and function g(t+1) can share\r\nkt+1 hash functions. With pm to be specified later, we set\r\nthe r value at position t as follows:\r\nrt = π × (1 − p(1/kt) m ) (3)\r\nThis allows us to have the effect of increasing the r value\r\nby decreasing the k value. We calculate the value of k at\r\nposition t as follows:\r\nkt = c × kt+1, where c > 1 (4)\r\nBy getting tt+1 from Equation 3, substituting kt using Equa\u0002tion 4, and substituting pm using Equation 3, we have:\r\nrt+1 = π × \u0002\r\n1 − (1 − rt\r\nπ )\r\nc\u0003\r\n= fc(rt) fc(rt)/rt > 1 (5)\r\nBy setting c equals to 2, we can get an approximately similar\r\ncurve of r sequence to the original sequence rt+1 = rt × \u0002\r\nrm\r\nrm-1\r\nrt+1\r\nrt\r\n......\r\nkm\r\nkm-1 = ckm\r\nkt+1 = ckt+2\r\nkt = ckt\u0011\u0012\r\nk0 r0\r\nk decreases\u0002 r increase\u0006\r\nLevel m\r\nLevel m-1\r\nLevel t+1\r\nLevel t\r\nLevel 0\r\n...... ......\r\n......\r\n......\r\nFigure 4: The index structure for the Adaptive Lo\u0002cality Sensitive Hashing (ALSH). There are m + 1\r\nlevels on this tree. Moving from level t to level t + 1\r\nis equivalent of increasing the search radius from rt\r\nto rt+1.\r\nwhere \u0002 equals to 2. Following the aforementioned logic, we\r\nconstruct an Adaptive Locality Sensitive Hashing (ALSH)\r\nindex in the form of prefix trees.\r\nAs shown in Figure 4, the index structure is a prefix tree of\r\nthe signature values calculated by G = {gm, gm−1,...,g0}.\r\nLevel t corresponds to the position t in the r expanding\r\nsequence. By introducing different values of kt, each level\r\nrepresents a different radius rt. Each level denotes a different\r\ngt function and the gt function is a concatenation of kt hash\r\nfunctions. Moving up from a node at level t to its parent at\r\nlevel t+1 indicates that it requires a shorter matched prefix.\r\nThe nodes which have the same parent at level t share the\r\nsame prefix that is generated by gt.\r\nTo locate the leaf for a given data point q ∈ Rd, ALSH\r\ndynamically constructs the hash functions by trying gt ∈\r\nG in sequence. The signature of gt can be generated by\r\npadding additional hash values to gt+1 since kt = c × kt+1.\r\nFollowing [14, 32], with l to be specified later, we adopt\r\nl such prefix trees as the ALSH’s index. Given a query\r\npoint q, we first locate the corresponding leaves in all prefix\r\ntrees. With l to be specified later, we collect the first 2l\r\npoints from all the leaf buckets. To index a point, we locate\r\nits corresponding leaf in each tree and insert it to the leaf\r\nbucket. Suppose a leaf is on level tt+1. If the number of\r\npoints in that leaf is more than 2l, we split all the data points\r\nof that leaf into the next level t by using gt. All the trees are\r\ndynamically constructed based on the incoming points to be\r\nindexed in sequence. Therefore, they can be incrementally\r\nmaintained. Unlike the learning-based LSH [11], Kam1n0\r\ndoes not require the whole repository to estimate the hash\r\nfunctions to build the index.\r\nIt can be easily proved that gt is a (rt, rt+1, pm, pc\r\nm)-sensi\u0002tive hash family and gt can correctly solve the (rt+1/rt, rt)-\r\napproximated ball cover problem by setting pc\r\nm = 1/n and\r\nl = n1/c. The proof follows [14]. Details and implemen\u0002tation on key-value data store can be found in our techni\u0002cal report.6 Another parameter rm controls the starting km\r\nvalue at the root value. It indicates the maximum distance\r\nthat two points can be considered as valid neighbors. For\r\nsparse points far away from each other, they are not consid\u0002ered as neighbors unless their distance is within rm.\r\nFor a single ALSH tree, the depth in the worst case is\r\nk0, and all the leaves are at level 0. In this case, the tree\r\nis equivalent to the E2LSH with k = k0. Therefore, the\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/4ab21e59-5f2e-494e-ad27-15ddfc006b06.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7777c5535babb4cef3b4df0bf79ef2329e996f5db20b3209b267cff0fb1e7d14",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1118
      },
      {
        "segments": [
          {
            "segment_id": "4ab21e59-5f2e-494e-ad27-15ddfc006b06",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "It is proven that if the above two properties hold with\r\nconstant probability, the algorithm can correctly solve the\r\n(\u0002, r)-approximated ball cover problem [14]. For E2LSH, by\r\npicking k = logp2 (1/n) and l = nρ where ρ = ln1/p1\r\nln1/p2 , both\r\nProperties 1 and 2 hold with constant probability.\r\nHowever, the ball cover problem is a strong reduction to\r\nthe NN problem since it adopts the same radius r for all\r\npoints. Real-life data cannot always be evenly distributed.\r\nTherefore, it is difficult to pick an appropriate r. We de\u0002note this as the uneven data distribution issue. In [12],\r\na magic rm is adopted heuristically. But as pointed out\r\nby [32], such a magic radius may not exist. A weaker re\u0002duction was proposed in [14], where the NN problem is re\u0002duced to multiple (r, \u0002)-NN ball cover problems with varying\r\nr = {1, \u00022, \u00023,... }. The intuition is that points in different\r\ndensity areas can find a suitable r. However, such a reduc\u0002tion requires a large space consumption and longer response\r\ntime. Other indexing structures have been proposed to solve\r\nthis issue. Per our discussion in Section 2, existing tech\u0002niques do not meet our requirement. Thus, we customize\r\nthe LSH-forest approach and propose the ALSH structure.\r\n6.1 Adaptive LSH Structure\r\nWe found that the limitation of the expanding sequence\r\nof r in previous section is too strong. It is unnecessary to\r\nexactly follow the sequence r = {1, \u00022, \u00023,... }, as long as r\r\nis increasing in a similar manner to rt+1 = rt × \u0002. Thus, we\r\ncustomize the \u0002-approximated NN problem as follows:\r\nDefinition 5. (f(r)-approximated NN search problem) Given\r\na dataset D ⊂ Rd and a query point q, let r denote the dis\u0002tance between the query point q and its nearest neighbor o∗.\r\nThe problem is to find an approximated data point within\r\nthe distance f(r), where f(r)/r > 1 \u0002\r\nInstead of using a fix approximation ratio, we approxi\u0002mate the search by using a function on r. We issue a differ\u0002ent sequence of expanding r. The expanding sequence of r\r\nis formulated as r0, r1,...,rt, rt+1,...,rm, where rt < rt+1.\r\nSimilar to the E2LSH approach, we concatenate multiple\r\nhash functions from the random hyperplane hash family\r\nH into one. However, we concatenate different number of\r\nhash functions for different values of r. This number is de\u0002noted by kt for rt, and the sequence of k is denoted by\r\nk0, k1,...,kt, kt+1,...,km, where kt > kt+1. Recall that\r\nthe concatenated function is denoted by g. Consequently,\r\nthere will be a different function g at position t, which is de\u0002noted by gt. Yet, function gt and function g(t+1) can share\r\nkt+1 hash functions. With pm to be specified later, we set\r\nthe r value at position t as follows:\r\nrt = π × (1 − p(1/kt) m ) (3)\r\nThis allows us to have the effect of increasing the r value\r\nby decreasing the k value. We calculate the value of k at\r\nposition t as follows:\r\nkt = c × kt+1, where c > 1 (4)\r\nBy getting tt+1 from Equation 3, substituting kt using Equa\u0002tion 4, and substituting pm using Equation 3, we have:\r\nrt+1 = π × \u0002\r\n1 − (1 − rt\r\nπ )\r\nc\u0003\r\n= fc(rt) fc(rt)/rt > 1 (5)\r\nBy setting c equals to 2, we can get an approximately similar\r\ncurve of r sequence to the original sequence rt+1 = rt × \u0002\r\nrm\r\nrm-1\r\nrt+1\r\nrt\r\n......\r\nkm\r\nkm-1 = ckm\r\nkt+1 = ckt+2\r\nkt = ckt\u0011\u0012\r\nk0 r0\r\nk decreases\u0002 r increase\u0006\r\nLevel m\r\nLevel m-1\r\nLevel t+1\r\nLevel t\r\nLevel 0\r\n...... ......\r\n......\r\n......\r\nFigure 4: The index structure for the Adaptive Lo\u0002cality Sensitive Hashing (ALSH). There are m + 1\r\nlevels on this tree. Moving from level t to level t + 1\r\nis equivalent of increasing the search radius from rt\r\nto rt+1.\r\nwhere \u0002 equals to 2. Following the aforementioned logic, we\r\nconstruct an Adaptive Locality Sensitive Hashing (ALSH)\r\nindex in the form of prefix trees.\r\nAs shown in Figure 4, the index structure is a prefix tree of\r\nthe signature values calculated by G = {gm, gm−1,...,g0}.\r\nLevel t corresponds to the position t in the r expanding\r\nsequence. By introducing different values of kt, each level\r\nrepresents a different radius rt. Each level denotes a different\r\ngt function and the gt function is a concatenation of kt hash\r\nfunctions. Moving up from a node at level t to its parent at\r\nlevel t+1 indicates that it requires a shorter matched prefix.\r\nThe nodes which have the same parent at level t share the\r\nsame prefix that is generated by gt.\r\nTo locate the leaf for a given data point q ∈ Rd, ALSH\r\ndynamically constructs the hash functions by trying gt ∈\r\nG in sequence. The signature of gt can be generated by\r\npadding additional hash values to gt+1 since kt = c × kt+1.\r\nFollowing [14, 32], with l to be specified later, we adopt\r\nl such prefix trees as the ALSH’s index. Given a query\r\npoint q, we first locate the corresponding leaves in all prefix\r\ntrees. With l to be specified later, we collect the first 2l\r\npoints from all the leaf buckets. To index a point, we locate\r\nits corresponding leaf in each tree and insert it to the leaf\r\nbucket. Suppose a leaf is on level tt+1. If the number of\r\npoints in that leaf is more than 2l, we split all the data points\r\nof that leaf into the next level t by using gt. All the trees are\r\ndynamically constructed based on the incoming points to be\r\nindexed in sequence. Therefore, they can be incrementally\r\nmaintained. Unlike the learning-based LSH [11], Kam1n0\r\ndoes not require the whole repository to estimate the hash\r\nfunctions to build the index.\r\nIt can be easily proved that gt is a (rt, rt+1, pm, pc\r\nm)-sensi\u0002tive hash family and gt can correctly solve the (rt+1/rt, rt)-\r\napproximated ball cover problem by setting pc\r\nm = 1/n and\r\nl = n1/c. The proof follows [14]. Details and implemen\u0002tation on key-value data store can be found in our techni\u0002cal report.6 Another parameter rm controls the starting km\r\nvalue at the root value. It indicates the maximum distance\r\nthat two points can be considered as valid neighbors. For\r\nsparse points far away from each other, they are not consid\u0002ered as neighbors unless their distance is within rm.\r\nFor a single ALSH tree, the depth in the worst case is\r\nk0, and all the leaves are at level 0. In this case, the tree\r\nis equivalent to the E2LSH with k = k0. Therefore, the\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/4ab21e59-5f2e-494e-ad27-15ddfc006b06.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7777c5535babb4cef3b4df0bf79ef2329e996f5db20b3209b267cff0fb1e7d14",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1118
      },
      {
        "segments": [
          {
            "segment_id": "90bf3db4-08ad-4bcc-9ffa-c783e4b12f84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "Mapper\r\nReducer\r\nA block-to-block\r\nclone pair\r\nA cloned subgraph\r\nCloned subgraphs \r\nof a function\r\nFigure 5: The MapReduce-based subgraph clone\r\nconstruction process.\r\nspace consumption for l = n1/c ALSH trees are bounded by\r\nO(dn+n1+1/c), where O(dn) is the data points in a dataset\r\nand O(n1+1/c) is the space of indexes for the trees. The\r\nquery time for a single ALSH prefix tree is bounded by its\r\nheight. Given the maximum k value k0 and the minimum\r\nk value km, its depth in the worst case is logc(k0/km) + 1.\r\nThus, the query time for l = n1/c prefix trees is bounded by\r\nO(logc(k0/km)×n1/c). The ALSH index needs to build n1/c\r\nprefix trees for the full theoretical quality to be guaranteed.\r\nBased on our observation, setting l to 1 and 2 is already\r\nsufficient for providing good quality assembly code clones.\r\n7. SUBGRAPH CLONE SEARCH\r\nEven subgraph isomorphism is NP-hard in theory [21, 28],\r\nmany algorithms have been proposed to solve it in a reason\u0002able time. Formally, the subgraph isomorphism algorithm\r\nsolved by most of these systems [13, 21, 31] is defined as:\r\nDefinition 6. (Subgraph Isomorphism Search) A graph is\r\ndenoted by a triplet (V,E,L) where V represents the set of\r\nvertices, E represents the set of edges, and L represents the\r\nlabels for each vertex. Given a query graph q = (V,E,L) and\r\na data graph g = (V \u0003\r\n, E\u0003, L\u0003) a subgraph isomorphism (also\r\nknown as embedding) is an injective function M : V → V \u0003\r\nsuch that the following conditions hold: (1) ∀u ∈ V,L(u) ∈\r\nL\u0003\r\n(M(u)), (2) ∀(ui, uj ) ∈ E,(M(ui), M(uj )) ∈ E\u0003, and (3)\r\nL(ui, uj ) = L\u0003\r\n(M(ui), M(uj )). The search problem is to\r\nfind all distinct embeddings of q in g. \u0002\r\nThe difference between this problem and ours in Defini\u0002tion 1 is two-fold. First, our problem is to retrieve all the\r\nsubgraph clones of the target function ft’s control flow graph\r\nfrom the repository. In contrast, this problem only needs to\r\nretrieve the exact matches of query graph q within g. Re\u0002fer to Conditions 1, 2, and 3 in Definition 6, or the termi\u0002nation condition of the procedure on Line 1 of subroutine\r\nSubgraphSearch in [21]. Our problem is more challenging\r\nand can be reduced to the problem in Definition 6 by issuing\r\nall the subgraphs of ft as queries, which introduces a higher\r\nalgorithmic complexity. Second, there is no such L data la\u0002bel attribute in our problem, but two types of edges: the\r\ncontrol flow graph which links the basic blocks and the se\u0002mantic relationship between basic blocks which is evaluated\r\nat the querying phase. Existing algorithms for subgraph iso\u0002Algorithm 1 Mapper\r\nInput A basic block clone pair \u0003bt, bs\u0004\r\nOutput A pair consisting of \u0003fs, sgs\u0004\r\n1: fs ← getFunctionId(bs)\r\n2: sgs ← [ ] \u0002 create an empty list of sub\u0002graph clones\r\n3: cloneGraph ← {\u0003bt, bs\u0004} \u0002 create a subgraph clone with\r\none clone pair\r\n4: sgs[0] ← cloneGraph \u0002 list of subgraph clones but at\r\nthis moment, it has only one.\r\n5: return \u0003fs, sgs\u0004 with fs as key index\r\nmorphism are not directly applicable. Assembly code con\u0002trol graphs are sparser than other graph data as there are\r\nless number of links between vertices and typically, basic\r\nblocks are only linked to each other within the same func\u0002tion. Given such properties, we can efficiently construct the\r\nsubgraph clones respectively for each repository function fs\r\nif it has more than one clone blocks in the previous step.\r\n7.1 MapReduce Subgraph Search\r\nWe adopt two functions in the Apache Spark MapRe\u0002duce execution framework, namely the map function and\r\nthe reduce-by-key function. In our case, the map function\r\ntransforms the clone pairs generated by ALSH (refer to the\r\ndata flow in Figure 3) and the reduce-by-key function con\u0002structs subgraph clone respectively for different repository\r\nfunction fs. Figure 5 shows the overview of our subgraph\r\nclone search approach.\r\nThe signature for the map function (Algorithm 1) is \u0003bt, bs\u0004\r\n→ \u0003fs, sgs[1 : a]\u0004. Each execution of the map function takes\r\na clone pair \u0003bt, bs\u0004 produced by ALSH and transforms it to\r\n\u0003fs, sgs[1 : a]\u0004, which is a pair of repository function id fs\r\nand its list of subgraph clones sgs in Definition 1. The map\r\nfunctions are independent to each other.\r\nThe outputs of the map functions correspond to the first\r\nrow in Figure 5. A red circle represents a target basic block\r\nbs and a green triangle represents a source basic block bt.\r\nThe link between them indicates that they are a block-to\u0002block clone pair \u0003bt, bs\u0004, which is produced in the previous\r\nstep. A white rectangle represents a list of subgraph clones\r\nand the colored rectangle inside it represents a subgraph\r\nclone. Algorithm 1 maps each clone pair into a list of sub\u0002graph clones which contains only one subgraph clone. Each\r\nsubgraph clone is initialized with only one clone pair.\r\nAfter the map transformation functions, the reduce-by\u0002key function reduces the produced lists of subgraph clones.\r\nThe reducer merges a pair of lists sg1\r\ns and sg2s into a single\r\none, by considering their subgraph clones’ connectivity. The\r\nreduce process is executed for the links from the second row\r\nto the last row in Figure 5. Only the lists of subgraph clones\r\nwith the same fs will be merged. As indicated by the links\r\nbetween the first and second row in Figure 5, only rectangles\r\nwith the same orange background can be reduced together.\r\nRectangles with other background colors are reduced with\r\ntheir own group.\r\nAlgorithm 2 shows the reduce function in details. Given\r\ntwo lists of subgraph clones under the same repository func\u0002tion fs, the reduce function compares their subgraph clones\r\n(Lines 1 and 2) and checks if two graphs can be connected\r\n(Lines 4 to 13) by referring to the the control flow graph\r\nedges Es and Et . If the two subgraph clones can be con\u0002nected by one clone pair, then they can be merged into a\r\nsingle one (Lines 6 and 7). If a subgraph clone from sg2\r\ns\r\ncannot be merged into any subgraph clones in sg2\r\ns , it will be\r\nappended to the list sg1\r\ns (Lines 20 and 21). At the end of the\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/90bf3db4-08ad-4bcc-9ffa-c783e4b12f84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=144635ed2b410cf88bc0f6adcb1faa4535d6e701468e4aa109554316a4f04292",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1045
      },
      {
        "segments": [
          {
            "segment_id": "90bf3db4-08ad-4bcc-9ffa-c783e4b12f84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "Mapper\r\nReducer\r\nA block-to-block\r\nclone pair\r\nA cloned subgraph\r\nCloned subgraphs \r\nof a function\r\nFigure 5: The MapReduce-based subgraph clone\r\nconstruction process.\r\nspace consumption for l = n1/c ALSH trees are bounded by\r\nO(dn+n1+1/c), where O(dn) is the data points in a dataset\r\nand O(n1+1/c) is the space of indexes for the trees. The\r\nquery time for a single ALSH prefix tree is bounded by its\r\nheight. Given the maximum k value k0 and the minimum\r\nk value km, its depth in the worst case is logc(k0/km) + 1.\r\nThus, the query time for l = n1/c prefix trees is bounded by\r\nO(logc(k0/km)×n1/c). The ALSH index needs to build n1/c\r\nprefix trees for the full theoretical quality to be guaranteed.\r\nBased on our observation, setting l to 1 and 2 is already\r\nsufficient for providing good quality assembly code clones.\r\n7. SUBGRAPH CLONE SEARCH\r\nEven subgraph isomorphism is NP-hard in theory [21, 28],\r\nmany algorithms have been proposed to solve it in a reason\u0002able time. Formally, the subgraph isomorphism algorithm\r\nsolved by most of these systems [13, 21, 31] is defined as:\r\nDefinition 6. (Subgraph Isomorphism Search) A graph is\r\ndenoted by a triplet (V,E,L) where V represents the set of\r\nvertices, E represents the set of edges, and L represents the\r\nlabels for each vertex. Given a query graph q = (V,E,L) and\r\na data graph g = (V \u0003\r\n, E\u0003, L\u0003) a subgraph isomorphism (also\r\nknown as embedding) is an injective function M : V → V \u0003\r\nsuch that the following conditions hold: (1) ∀u ∈ V,L(u) ∈\r\nL\u0003\r\n(M(u)), (2) ∀(ui, uj ) ∈ E,(M(ui), M(uj )) ∈ E\u0003, and (3)\r\nL(ui, uj ) = L\u0003\r\n(M(ui), M(uj )). The search problem is to\r\nfind all distinct embeddings of q in g. \u0002\r\nThe difference between this problem and ours in Defini\u0002tion 1 is two-fold. First, our problem is to retrieve all the\r\nsubgraph clones of the target function ft’s control flow graph\r\nfrom the repository. In contrast, this problem only needs to\r\nretrieve the exact matches of query graph q within g. Re\u0002fer to Conditions 1, 2, and 3 in Definition 6, or the termi\u0002nation condition of the procedure on Line 1 of subroutine\r\nSubgraphSearch in [21]. Our problem is more challenging\r\nand can be reduced to the problem in Definition 6 by issuing\r\nall the subgraphs of ft as queries, which introduces a higher\r\nalgorithmic complexity. Second, there is no such L data la\u0002bel attribute in our problem, but two types of edges: the\r\ncontrol flow graph which links the basic blocks and the se\u0002mantic relationship between basic blocks which is evaluated\r\nat the querying phase. Existing algorithms for subgraph iso\u0002Algorithm 1 Mapper\r\nInput A basic block clone pair \u0003bt, bs\u0004\r\nOutput A pair consisting of \u0003fs, sgs\u0004\r\n1: fs ← getFunctionId(bs)\r\n2: sgs ← [ ] \u0002 create an empty list of sub\u0002graph clones\r\n3: cloneGraph ← {\u0003bt, bs\u0004} \u0002 create a subgraph clone with\r\none clone pair\r\n4: sgs[0] ← cloneGraph \u0002 list of subgraph clones but at\r\nthis moment, it has only one.\r\n5: return \u0003fs, sgs\u0004 with fs as key index\r\nmorphism are not directly applicable. Assembly code con\u0002trol graphs are sparser than other graph data as there are\r\nless number of links between vertices and typically, basic\r\nblocks are only linked to each other within the same func\u0002tion. Given such properties, we can efficiently construct the\r\nsubgraph clones respectively for each repository function fs\r\nif it has more than one clone blocks in the previous step.\r\n7.1 MapReduce Subgraph Search\r\nWe adopt two functions in the Apache Spark MapRe\u0002duce execution framework, namely the map function and\r\nthe reduce-by-key function. In our case, the map function\r\ntransforms the clone pairs generated by ALSH (refer to the\r\ndata flow in Figure 3) and the reduce-by-key function con\u0002structs subgraph clone respectively for different repository\r\nfunction fs. Figure 5 shows the overview of our subgraph\r\nclone search approach.\r\nThe signature for the map function (Algorithm 1) is \u0003bt, bs\u0004\r\n→ \u0003fs, sgs[1 : a]\u0004. Each execution of the map function takes\r\na clone pair \u0003bt, bs\u0004 produced by ALSH and transforms it to\r\n\u0003fs, sgs[1 : a]\u0004, which is a pair of repository function id fs\r\nand its list of subgraph clones sgs in Definition 1. The map\r\nfunctions are independent to each other.\r\nThe outputs of the map functions correspond to the first\r\nrow in Figure 5. A red circle represents a target basic block\r\nbs and a green triangle represents a source basic block bt.\r\nThe link between them indicates that they are a block-to\u0002block clone pair \u0003bt, bs\u0004, which is produced in the previous\r\nstep. A white rectangle represents a list of subgraph clones\r\nand the colored rectangle inside it represents a subgraph\r\nclone. Algorithm 1 maps each clone pair into a list of sub\u0002graph clones which contains only one subgraph clone. Each\r\nsubgraph clone is initialized with only one clone pair.\r\nAfter the map transformation functions, the reduce-by\u0002key function reduces the produced lists of subgraph clones.\r\nThe reducer merges a pair of lists sg1\r\ns and sg2s into a single\r\none, by considering their subgraph clones’ connectivity. The\r\nreduce process is executed for the links from the second row\r\nto the last row in Figure 5. Only the lists of subgraph clones\r\nwith the same fs will be merged. As indicated by the links\r\nbetween the first and second row in Figure 5, only rectangles\r\nwith the same orange background can be reduced together.\r\nRectangles with other background colors are reduced with\r\ntheir own group.\r\nAlgorithm 2 shows the reduce function in details. Given\r\ntwo lists of subgraph clones under the same repository func\u0002tion fs, the reduce function compares their subgraph clones\r\n(Lines 1 and 2) and checks if two graphs can be connected\r\n(Lines 4 to 13) by referring to the the control flow graph\r\nedges Es and Et . If the two subgraph clones can be con\u0002nected by one clone pair, then they can be merged into a\r\nsingle one (Lines 6 and 7). If a subgraph clone from sg2\r\ns\r\ncannot be merged into any subgraph clones in sg2\r\ns , it will be\r\nappended to the list sg1\r\ns (Lines 20 and 21). At the end of the\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/90bf3db4-08ad-4bcc-9ffa-c783e4b12f84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=144635ed2b410cf88bc0f6adcb1faa4535d6e701468e4aa109554316a4f04292",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1045
      },
      {
        "segments": [
          {
            "segment_id": "f4231a01-8880-4f43-9e45-390563d2176b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "(l<100 ,p=0.983) (l<200 ,p=0.994)\r\n0.00\r\n0.25\r\n0.50\r\n0.75\r\n1.00\r\n0 100 200 300\r\nPercentage (p)\r\n0.00\r\n0.05\r\n0.10\r\n0.15\r\n0 10 20 30 40\r\nDensity\r\n(s<1.00 ,p=0.26)\r\n(s<0.40 ,p=0.11)\r\n0.00\r\n0.25\r\n0.50\r\n0.75\r\n1.00\r\n0.00 0.25 0.50 0.75 1.00\r\nSource Function Similarity (s)\r\nPercentage (p)\r\nEmpirical Distribution Function (EDF)\r\nDensity\r\nNumber of Basic Blocks per Function Number of Basic Blocks per Function\r\nEmpirical Distribution Function (EDF) Kernel Density & Histogram\r\n(a) (c) (d)\r\n0\r\n5\r\n10\r\n15\r\n0.6 0.7 0.8 0.9 1.0\r\nCosine Similarity of the 20th-nearest Neighbour.\r\nKernel Density & Histogram\r\n(b)\r\nFigure 6: (a) the EDF function on repository function clone pair similarity, (b) the kernel density & histogram\r\nof the cosine similarity of each basic block’s 20th-nearest neighbor, (c) the EDF on per assembly function\r\nblock count |Bs|, and (d) the kernel density & histogram on assembly function block count |Bs| < 40.\r\nAlgorithm 2 Reducer\r\nInput subgraph lists of same fs: sg1\r\ns[1 : a1] and sg2s[1 : a2]\r\nOutput a single subgraph list sg1\r\ns[1 : a]\r\n1: for a1 → |sg1\r\ns| do\r\n2: for a2 → |sg2\r\ns| do\r\n3: canMerge ← false\r\n4: for each \u0002b\r\na1\r\nt , ba1s\r\n\u0003 ∈ sg1\r\ns[a1] do\r\n5: for each \u0002b\r\na2\r\nt , ba2s\r\n\u0003 ∈ sg2\r\ns[a2] do\r\n6: if Et(b\r\na1\r\nt , ba2t ) exists then\r\n7: if Es(ba1s , ba2s ) exists then\r\n8: canMerge ← true\r\n9: goto Line 14.\r\n10: end if\r\n11: end if\r\n12: end for\r\n13: end for\r\n14: if canMerge is true then\r\n15: sg1\r\ns[a1] ← sg1s[a1]\r\n\u0004 sg2\r\ns[a2]\r\n16: sg2\r\ns ← sg2s − sg2s[a2]\r\n17: end if\r\n18: end for\r\n19: end for\r\n20: if sg2\r\ns is not ∅ then \u0002 for graphs in sg2s that cannot be\r\nmerged, append them to the list\r\n21: sg1\r\ns ← sg1s\r\n\u0004 sg2\r\ns\r\n22: end if\r\n23: return sg1\r\ns\r\ngraph search algorithm, we solve the problem in Definition 1.\r\nIn order to obtain a ranked list of repository functions for\r\nfs, we calculate the similarity value by checking how much\r\nits subgraphs sgs cover the graph of the query ft: sims =\r\n(|uniqueEdges(sgs)| + |uniqueNodes(sgs)|)/(|Bt| + |Et|).\r\nCompared to other join-based or graph-exploration-based\r\nsearch approach, our MapReduce-based search procedure\r\navoids recursive search and is bounded by polynomial com\u0002plexity. Let ms be the number of clone pairs for a target\r\nfunction ft. There are at most O(m2\r\ns) connectivity checks\r\nbetween the clone pairs (no merge can be found) and the\r\nmap function requires O(ms) executions. ms corresponds\r\nto the number of rectangles in the second row of Figure 5.\r\nRefer from the second row to the last one. The reduce func\u0002tion are bounded by O(m2\r\ns) comparisons. ms is bounded\r\nby O(|Bt|×|Bs|), which implies that each basic block of\r\nft is a clone with all the basic blocks of fs. However, this\r\nextreme case rarely happens. Given the nature of assembly\r\nfunctions and search scenarios, ms is sufficiently bounded\r\nby O(max(|Bt|, |Bs|)). According to the descriptive statis\u0002tics of our experiment, 99% of them have less than 200 basic\r\nblocks.\r\n8. EXPERIMENTS\r\nThis section presents comprehensive experimental results\r\nfor the task of assembly code clone search. First, we ex\u0002Library\r\nName\r\nBranch\r\nCount\r\nFunction\r\nCount\r\nBlock\r\nCount\r\nClone Pair\r\nCount\r\nbzip2 5 590 15,181 1,329\r\ncurl 16 9,468 176,174 49,317\r\nexpat 3 2,025 35,801 14,054\r\njsoncpp 14 11,779 43,734 204,701\r\nlibpng 9 4,605 82,257 18,946\r\nlibtiff 13 7,276 124,974 51,925\r\nopenssl 9 13,732 200,415 29,767\r\nsqlite 12 9,437 202,777 23,674\r\ntinyxml 7 3,286 30,401 22,798\r\nzlib 8 1,741 30,585 6,854\r\ntotal 96 63,939 942,299 423,365\r\nTable 1: The assembly code clone dataset summary.\r\nplain how to construct a labeled dataset that can be used\r\nfor benchmarking in future research. Then, we evaluate the\r\neffect of assembly code normalization. Although normaliza\u0002tion has been extensively used in previous works, its effects\r\nhave not been thoroughly studied yet. Next, we present\r\nthe benchmark results which compares Kam1n0 with state\u0002of-the-art clone search approaches in terms of clone search\r\nquality. Finally, we demonstrate the scalability and capac\u0002ity of the Kam1n0 engine by presenting the experimental\r\nresults from a mini-cluster.\r\n8.1 Labeled Dataset Generation\r\nOne of the challenging problems for assembly code clone\r\nsearch is the lack of labeled (ground truth) dataset, since\r\nthe most effective labeled dataset requires intensive manual\r\nidentification of assembly code clones [7]. To facilitate future\r\nstudies on assembly clone search, we have developed a tool to\r\nsystematically generate the assembly function clones based\r\non source code clones. The tool performs four steps: Step 1:\r\nParse all the source code functions from different branches or\r\nversions of a project and identify all the function-to-function\r\nclones using CCFINDERX [16], which estimates source code\r\nsimilarity based on the sequence of normalized tokens. Step\r\n2: Compile the given branches or versions of a project with\r\nan additional debug flag to enable the compiler output debug\r\nsymbols. Step 3: Link the source code functions to the\r\nassembly code functions using the compiler output debug\r\nsymbols, where such information is available. Step 4: For\r\neach pair of source code clone, generate a pair of assembly\r\nfunction clone and transfer the similarity to the new pair.\r\nThe intuition is that the source code function-level clones\r\nindicate the functional clones between their corresponding\r\nassembly code. In [7, 25], the source code and assembly\r\ncode are manually linked with an injected identifier in the\r\nform of variable declarations. However, after the automa\u0002tion of such process, we find that the link rate is very low\r\ndue to the impact of compiler optimizations. The generated\r\n7",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/f4231a01-8880-4f43-9e45-390563d2176b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b6caaffd38b13c87776acfc9d5a0d43c7c398510080107038fe49483dafe5505",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 934
      },
      {
        "segments": [
          {
            "segment_id": "f4231a01-8880-4f43-9e45-390563d2176b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "(l<100 ,p=0.983) (l<200 ,p=0.994)\r\n0.00\r\n0.25\r\n0.50\r\n0.75\r\n1.00\r\n0 100 200 300\r\nPercentage (p)\r\n0.00\r\n0.05\r\n0.10\r\n0.15\r\n0 10 20 30 40\r\nDensity\r\n(s<1.00 ,p=0.26)\r\n(s<0.40 ,p=0.11)\r\n0.00\r\n0.25\r\n0.50\r\n0.75\r\n1.00\r\n0.00 0.25 0.50 0.75 1.00\r\nSource Function Similarity (s)\r\nPercentage (p)\r\nEmpirical Distribution Function (EDF)\r\nDensity\r\nNumber of Basic Blocks per Function Number of Basic Blocks per Function\r\nEmpirical Distribution Function (EDF) Kernel Density & Histogram\r\n(a) (c) (d)\r\n0\r\n5\r\n10\r\n15\r\n0.6 0.7 0.8 0.9 1.0\r\nCosine Similarity of the 20th-nearest Neighbour.\r\nKernel Density & Histogram\r\n(b)\r\nFigure 6: (a) the EDF function on repository function clone pair similarity, (b) the kernel density & histogram\r\nof the cosine similarity of each basic block’s 20th-nearest neighbor, (c) the EDF on per assembly function\r\nblock count |Bs|, and (d) the kernel density & histogram on assembly function block count |Bs| < 40.\r\nAlgorithm 2 Reducer\r\nInput subgraph lists of same fs: sg1\r\ns[1 : a1] and sg2s[1 : a2]\r\nOutput a single subgraph list sg1\r\ns[1 : a]\r\n1: for a1 → |sg1\r\ns| do\r\n2: for a2 → |sg2\r\ns| do\r\n3: canMerge ← false\r\n4: for each \u0002b\r\na1\r\nt , ba1s\r\n\u0003 ∈ sg1\r\ns[a1] do\r\n5: for each \u0002b\r\na2\r\nt , ba2s\r\n\u0003 ∈ sg2\r\ns[a2] do\r\n6: if Et(b\r\na1\r\nt , ba2t ) exists then\r\n7: if Es(ba1s , ba2s ) exists then\r\n8: canMerge ← true\r\n9: goto Line 14.\r\n10: end if\r\n11: end if\r\n12: end for\r\n13: end for\r\n14: if canMerge is true then\r\n15: sg1\r\ns[a1] ← sg1s[a1]\r\n\u0004 sg2\r\ns[a2]\r\n16: sg2\r\ns ← sg2s − sg2s[a2]\r\n17: end if\r\n18: end for\r\n19: end for\r\n20: if sg2\r\ns is not ∅ then \u0002 for graphs in sg2s that cannot be\r\nmerged, append them to the list\r\n21: sg1\r\ns ← sg1s\r\n\u0004 sg2\r\ns\r\n22: end if\r\n23: return sg1\r\ns\r\ngraph search algorithm, we solve the problem in Definition 1.\r\nIn order to obtain a ranked list of repository functions for\r\nfs, we calculate the similarity value by checking how much\r\nits subgraphs sgs cover the graph of the query ft: sims =\r\n(|uniqueEdges(sgs)| + |uniqueNodes(sgs)|)/(|Bt| + |Et|).\r\nCompared to other join-based or graph-exploration-based\r\nsearch approach, our MapReduce-based search procedure\r\navoids recursive search and is bounded by polynomial com\u0002plexity. Let ms be the number of clone pairs for a target\r\nfunction ft. There are at most O(m2\r\ns) connectivity checks\r\nbetween the clone pairs (no merge can be found) and the\r\nmap function requires O(ms) executions. ms corresponds\r\nto the number of rectangles in the second row of Figure 5.\r\nRefer from the second row to the last one. The reduce func\u0002tion are bounded by O(m2\r\ns) comparisons. ms is bounded\r\nby O(|Bt|×|Bs|), which implies that each basic block of\r\nft is a clone with all the basic blocks of fs. However, this\r\nextreme case rarely happens. Given the nature of assembly\r\nfunctions and search scenarios, ms is sufficiently bounded\r\nby O(max(|Bt|, |Bs|)). According to the descriptive statis\u0002tics of our experiment, 99% of them have less than 200 basic\r\nblocks.\r\n8. EXPERIMENTS\r\nThis section presents comprehensive experimental results\r\nfor the task of assembly code clone search. First, we ex\u0002Library\r\nName\r\nBranch\r\nCount\r\nFunction\r\nCount\r\nBlock\r\nCount\r\nClone Pair\r\nCount\r\nbzip2 5 590 15,181 1,329\r\ncurl 16 9,468 176,174 49,317\r\nexpat 3 2,025 35,801 14,054\r\njsoncpp 14 11,779 43,734 204,701\r\nlibpng 9 4,605 82,257 18,946\r\nlibtiff 13 7,276 124,974 51,925\r\nopenssl 9 13,732 200,415 29,767\r\nsqlite 12 9,437 202,777 23,674\r\ntinyxml 7 3,286 30,401 22,798\r\nzlib 8 1,741 30,585 6,854\r\ntotal 96 63,939 942,299 423,365\r\nTable 1: The assembly code clone dataset summary.\r\nplain how to construct a labeled dataset that can be used\r\nfor benchmarking in future research. Then, we evaluate the\r\neffect of assembly code normalization. Although normaliza\u0002tion has been extensively used in previous works, its effects\r\nhave not been thoroughly studied yet. Next, we present\r\nthe benchmark results which compares Kam1n0 with state\u0002of-the-art clone search approaches in terms of clone search\r\nquality. Finally, we demonstrate the scalability and capac\u0002ity of the Kam1n0 engine by presenting the experimental\r\nresults from a mini-cluster.\r\n8.1 Labeled Dataset Generation\r\nOne of the challenging problems for assembly code clone\r\nsearch is the lack of labeled (ground truth) dataset, since\r\nthe most effective labeled dataset requires intensive manual\r\nidentification of assembly code clones [7]. To facilitate future\r\nstudies on assembly clone search, we have developed a tool to\r\nsystematically generate the assembly function clones based\r\non source code clones. The tool performs four steps: Step 1:\r\nParse all the source code functions from different branches or\r\nversions of a project and identify all the function-to-function\r\nclones using CCFINDERX [16], which estimates source code\r\nsimilarity based on the sequence of normalized tokens. Step\r\n2: Compile the given branches or versions of a project with\r\nan additional debug flag to enable the compiler output debug\r\nsymbols. Step 3: Link the source code functions to the\r\nassembly code functions using the compiler output debug\r\nsymbols, where such information is available. Step 4: For\r\neach pair of source code clone, generate a pair of assembly\r\nfunction clone and transfer the similarity to the new pair.\r\nThe intuition is that the source code function-level clones\r\nindicate the functional clones between their corresponding\r\nassembly code. In [7, 25], the source code and assembly\r\ncode are manually linked with an injected identifier in the\r\nform of variable declarations. However, after the automa\u0002tion of such process, we find that the link rate is very low\r\ndue to the impact of compiler optimizations. The generated\r\n7",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/f4231a01-8880-4f43-9e45-390563d2176b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b6caaffd38b13c87776acfc9d5a0d43c7c398510080107038fe49483dafe5505",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 934
      },
      {
        "segments": [
          {
            "segment_id": "c7bf1e1a-d244-4447-98f6-12e6679c7323",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "M Approach Bzip2 Curl Expat Jsoncpp Libpng Libtiff Openssl Sqlite Tinyxml Zlib Avg.\r\nA\r\nU\r\nR\r\nO\r\nC\r\nBinClone .985 Ø Ø Ø Ø Ø Ø Ø Ø *.894 .188\r\nComposite .857 .766 .693 .725 .814 .772 .688 .726 .688 .729 .746\r\nConstant .769 .759 .723 .665 .829 .764 .689 .776 .683 .768 .743\r\nGraphlet .775 .688 .673 .563 .714 .653 .682 .746 .676 .685 .685\r\nGraphlet-C .743 .761 .705 .604 .764 .729 .731 .748 .677 .668 .713\r\nGraphlet-E .523 .526 .505 .516 .519 .521 .512 .513 .524 .514 .517\r\nMixGram .900 .840 .728 .726 .830 .808 .809 .765 .707 .732 .785\r\nMixGraph .769 .733 .706 .587 .755 .692 .713 .765 .674 .708 .710\r\nN-gram .950 .860 .727 .713 .843 .809 .819 .789 .714 .766 .799\r\nN-perm .886 .847 .731 .729 .834 .813 .811 .769 .709 .736 .787\r\nTracelet .830 Ø Ø Ø Ø Ø Ø Ø Ø .799 .163\r\nLSH-S .965 .901 .794 .854 .894 .922 .882 .845 .768 .758 .858\r\nKam1n0 *.992 *.989 *.843 *.890 *.944 *.967 *.891 *.895 *.864 .830 *.911\r\nA\r\nU\r\nP\r\nR\r\nBinClone .294 Ø Ø Ø Ø Ø Ø Ø Ø .091 .038\r\nComposite .645 .495 .375 .353 *.541 .482 .288 .405 .261 .409 .425\r\nConstant .247 .280 .301 .158 .311 .349 .072 .157 .142 .240 .226\r\nGraphlet .162 .133 .138 .051 .115 .103 .041 .108 .150 .106 .111\r\nGraphlet-C .455 .482 .296 .176 .413 .369 .366 .437 .245 .338 .358\r\nGraphlet-E .022 .024 .013 .020 .012 .015 .004 .010 .020 .026 .017\r\nMixGram .727 .598 .363 .337 .513 .512 *.464 .471 .286 .383 .465\r\nMixGraph .247 .242 .228 .098 .196 .184 .078 .180 .163 .175 .179\r\nN-gram .638 .491 .297 .275 .408 .428 .301 .417 .264 .314 .383\r\nN-perm .613 .589 .360 .344 .523 *.515 .438 .465 .288 .370 .450\r\nTracelet .057 Ø Ø Ø Ø Ø Ø Ø Ø .027 .008\r\nLSH-S .227 .014 .095 .049 .035 .038 .012 .018 .079 .041 .061\r\nKam1n0 *.780 *.633 *.473 *.504 .477 .387 .411 *.610 *.413 *.465 *.515\r\nM\r\nA\r\nP\r\n@\r\n10\r\nBinClone .495 Ø Ø Ø Ø Ø Ø Ø Ø .398 .089\r\nComposite .505 .525 .489 .190 .493 .536 .238 .382 .303 .472 .413\r\nConstant .354 .459 .539 .132 .473 .502 .199 .379 .229 .495 .376\r\nGraphlet .274 .309 .408 .030 .264 .276 .154 .303 .233 .302 .255\r\nGraphlet-C .339 .499 .586 .084 .412 .449 .284 .416 .272 .361 .370\r\nGraphlet-E .021 .053 .010 .011 .024 .040 .012 .019 .039 .028 .026\r\nMixGram .559 .641 .625 .191 .511 .589 .392 .445 .321 .474 .475\r\nMixGraph .334 .407 .572 .064 .345 .351 .211 .387 .244 .371 .329\r\nN-gram .620 .636 .615 .176 .512 .567 .398 .481 .310 .506 .482\r\nN-perm .532 .653 .628 .191 .516 .597 .394 .452 .317 .483 .476\r\nTracelet .228 Ø Ø Ø Ø Ø Ø Ø Ø .265 .049\r\nLSH-S .322 .069 .198 .032 .145 .078 .086 .111 .130 .101 .127\r\nKam1n0 *.672 *.680 *.690 *.196 *.548 *.587 *.434 *.605 *.375 *.573 *.536\r\nTable 2: Benchmark results of different assembly code clone search approaches. We employed three evaluation\r\nmetrics: the Area Under the Receiver Operating Characteristic Curve (AUROC ), the Area Under the Precision\u0002Recall Curve (AUPR), and the Mean Average Precision at Position 10 (MAP@10). Ø denotes that the method\r\nis not scalable and we cannot obtain a result for this dataset within 24 hours.\r\nassembly code clone is in fact the combined result of source\r\ncode patches and compiler optimizations. The source code\r\nevolves from version to version, and different versions may\r\nhave different default compiler settings. Thus, the labeled\r\ndataset simulates the real-world assembly clone evolvement.\r\nThis tool is applicable only if the source code is available.\r\nRefer to Table 1 for some popular open source libraries\r\nwith different versions. We applied the aforementioned tool\r\non them to generate the labeled dataset for the experiments.\r\nThere are 63,939 assembly functions which successfully link\r\nto the source code functions. The labeled dataset is a list\r\nof one-to-multiple assembly function clones with the trans\u0002ferred similarity from their source code clones.\r\nSee Figure 6c and Figure 6d. The assembly function basic\r\nblock count follows a long-tail distribution. Most of them\r\nhave between 0 and 5 assembly basic blocks, and 99% of\r\nthem is bounded by 200. We find that this is the typical\r\ndistribution of assembly function block count. This distri\u0002bution facilitates our graph search because the worst case\r\nis bounded by O(|Bt|×|Bs|) and P[|Bs| < 200] > 0.99.\r\nFigure 6b shows the cosine similarity distribution of each\r\nbasic block’s 20th-nearest neighbor. It reflects variations of\r\ndensity in the vector space and calls for an adaptive LSH.\r\n74% of the source code clones given by CCFINDERX are\r\nexact clones (see Figure 6a). However, by applying a strong\r\nhash on their assembly code, we find that only 30% of them\r\nare exact clones (Type I clones). Thus, the total percentage\r\nof inexact clones is 70%×74%+26% = 77.8%. CCFINDERX\r\nclassifies tokens in source code into different types before\r\nclone detection. If two source code fragments are identified\r\nas clones with a low similarity, there is a higher chance that\r\nthe underlying assembly code is indeed not a clone due to\r\nthe normalization of the source code. To mitigate this issue,\r\nwe heuristically set a 0.4 threshold for clones to be included\r\nin our dataset. Thus, we have 66.8% of inexact clones.\r\n8.2 Normalization Level\r\nNone Root Specific\r\nRoot < 2e−16 - -\r\nSpecific < 2e−16 1 -\r\nType < 2e−16 1 1\r\nTable 3: Paired t-test on the normalization level.\r\nAssembly code normalization is used in [7, 27]. However,\r\nits effects were not formally studied. In this section, we\r\npresent the results of the statistical test on the effects of the\r\nnormalization level. Details on normalization can be found\r\nin our technical report.6 We start by using a strong hash\r\nclone search with different normalization levels on each of\r\nthe generated datasets. Then, we collect the corresponding\r\nprecision value to the given normalization level as samples\r\nand test the relationship between precision and the chosen\r\nnormalization level. Normalization can increase the recall,\r\nbut we want to evaluate the trade-off between the precision\r\nand different normalization levels. According to the ANOVA\r\n8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/c7bf1e1a-d244-4447-98f6-12e6679c7323.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e65fe81c6de08fc0443be54cc586aab6af8214730cddc3df86ad5b49e19f7675",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1030
      },
      {
        "segments": [
          {
            "segment_id": "c7bf1e1a-d244-4447-98f6-12e6679c7323",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "M Approach Bzip2 Curl Expat Jsoncpp Libpng Libtiff Openssl Sqlite Tinyxml Zlib Avg.\r\nA\r\nU\r\nR\r\nO\r\nC\r\nBinClone .985 Ø Ø Ø Ø Ø Ø Ø Ø *.894 .188\r\nComposite .857 .766 .693 .725 .814 .772 .688 .726 .688 .729 .746\r\nConstant .769 .759 .723 .665 .829 .764 .689 .776 .683 .768 .743\r\nGraphlet .775 .688 .673 .563 .714 .653 .682 .746 .676 .685 .685\r\nGraphlet-C .743 .761 .705 .604 .764 .729 .731 .748 .677 .668 .713\r\nGraphlet-E .523 .526 .505 .516 .519 .521 .512 .513 .524 .514 .517\r\nMixGram .900 .840 .728 .726 .830 .808 .809 .765 .707 .732 .785\r\nMixGraph .769 .733 .706 .587 .755 .692 .713 .765 .674 .708 .710\r\nN-gram .950 .860 .727 .713 .843 .809 .819 .789 .714 .766 .799\r\nN-perm .886 .847 .731 .729 .834 .813 .811 .769 .709 .736 .787\r\nTracelet .830 Ø Ø Ø Ø Ø Ø Ø Ø .799 .163\r\nLSH-S .965 .901 .794 .854 .894 .922 .882 .845 .768 .758 .858\r\nKam1n0 *.992 *.989 *.843 *.890 *.944 *.967 *.891 *.895 *.864 .830 *.911\r\nA\r\nU\r\nP\r\nR\r\nBinClone .294 Ø Ø Ø Ø Ø Ø Ø Ø .091 .038\r\nComposite .645 .495 .375 .353 *.541 .482 .288 .405 .261 .409 .425\r\nConstant .247 .280 .301 .158 .311 .349 .072 .157 .142 .240 .226\r\nGraphlet .162 .133 .138 .051 .115 .103 .041 .108 .150 .106 .111\r\nGraphlet-C .455 .482 .296 .176 .413 .369 .366 .437 .245 .338 .358\r\nGraphlet-E .022 .024 .013 .020 .012 .015 .004 .010 .020 .026 .017\r\nMixGram .727 .598 .363 .337 .513 .512 *.464 .471 .286 .383 .465\r\nMixGraph .247 .242 .228 .098 .196 .184 .078 .180 .163 .175 .179\r\nN-gram .638 .491 .297 .275 .408 .428 .301 .417 .264 .314 .383\r\nN-perm .613 .589 .360 .344 .523 *.515 .438 .465 .288 .370 .450\r\nTracelet .057 Ø Ø Ø Ø Ø Ø Ø Ø .027 .008\r\nLSH-S .227 .014 .095 .049 .035 .038 .012 .018 .079 .041 .061\r\nKam1n0 *.780 *.633 *.473 *.504 .477 .387 .411 *.610 *.413 *.465 *.515\r\nM\r\nA\r\nP\r\n@\r\n10\r\nBinClone .495 Ø Ø Ø Ø Ø Ø Ø Ø .398 .089\r\nComposite .505 .525 .489 .190 .493 .536 .238 .382 .303 .472 .413\r\nConstant .354 .459 .539 .132 .473 .502 .199 .379 .229 .495 .376\r\nGraphlet .274 .309 .408 .030 .264 .276 .154 .303 .233 .302 .255\r\nGraphlet-C .339 .499 .586 .084 .412 .449 .284 .416 .272 .361 .370\r\nGraphlet-E .021 .053 .010 .011 .024 .040 .012 .019 .039 .028 .026\r\nMixGram .559 .641 .625 .191 .511 .589 .392 .445 .321 .474 .475\r\nMixGraph .334 .407 .572 .064 .345 .351 .211 .387 .244 .371 .329\r\nN-gram .620 .636 .615 .176 .512 .567 .398 .481 .310 .506 .482\r\nN-perm .532 .653 .628 .191 .516 .597 .394 .452 .317 .483 .476\r\nTracelet .228 Ø Ø Ø Ø Ø Ø Ø Ø .265 .049\r\nLSH-S .322 .069 .198 .032 .145 .078 .086 .111 .130 .101 .127\r\nKam1n0 *.672 *.680 *.690 *.196 *.548 *.587 *.434 *.605 *.375 *.573 *.536\r\nTable 2: Benchmark results of different assembly code clone search approaches. We employed three evaluation\r\nmetrics: the Area Under the Receiver Operating Characteristic Curve (AUROC ), the Area Under the Precision\u0002Recall Curve (AUPR), and the Mean Average Precision at Position 10 (MAP@10). Ø denotes that the method\r\nis not scalable and we cannot obtain a result for this dataset within 24 hours.\r\nassembly code clone is in fact the combined result of source\r\ncode patches and compiler optimizations. The source code\r\nevolves from version to version, and different versions may\r\nhave different default compiler settings. Thus, the labeled\r\ndataset simulates the real-world assembly clone evolvement.\r\nThis tool is applicable only if the source code is available.\r\nRefer to Table 1 for some popular open source libraries\r\nwith different versions. We applied the aforementioned tool\r\non them to generate the labeled dataset for the experiments.\r\nThere are 63,939 assembly functions which successfully link\r\nto the source code functions. The labeled dataset is a list\r\nof one-to-multiple assembly function clones with the trans\u0002ferred similarity from their source code clones.\r\nSee Figure 6c and Figure 6d. The assembly function basic\r\nblock count follows a long-tail distribution. Most of them\r\nhave between 0 and 5 assembly basic blocks, and 99% of\r\nthem is bounded by 200. We find that this is the typical\r\ndistribution of assembly function block count. This distri\u0002bution facilitates our graph search because the worst case\r\nis bounded by O(|Bt|×|Bs|) and P[|Bs| < 200] > 0.99.\r\nFigure 6b shows the cosine similarity distribution of each\r\nbasic block’s 20th-nearest neighbor. It reflects variations of\r\ndensity in the vector space and calls for an adaptive LSH.\r\n74% of the source code clones given by CCFINDERX are\r\nexact clones (see Figure 6a). However, by applying a strong\r\nhash on their assembly code, we find that only 30% of them\r\nare exact clones (Type I clones). Thus, the total percentage\r\nof inexact clones is 70%×74%+26% = 77.8%. CCFINDERX\r\nclassifies tokens in source code into different types before\r\nclone detection. If two source code fragments are identified\r\nas clones with a low similarity, there is a higher chance that\r\nthe underlying assembly code is indeed not a clone due to\r\nthe normalization of the source code. To mitigate this issue,\r\nwe heuristically set a 0.4 threshold for clones to be included\r\nin our dataset. Thus, we have 66.8% of inexact clones.\r\n8.2 Normalization Level\r\nNone Root Specific\r\nRoot < 2e−16 - -\r\nSpecific < 2e−16 1 -\r\nType < 2e−16 1 1\r\nTable 3: Paired t-test on the normalization level.\r\nAssembly code normalization is used in [7, 27]. However,\r\nits effects were not formally studied. In this section, we\r\npresent the results of the statistical test on the effects of the\r\nnormalization level. Details on normalization can be found\r\nin our technical report.6 We start by using a strong hash\r\nclone search with different normalization levels on each of\r\nthe generated datasets. Then, we collect the corresponding\r\nprecision value to the given normalization level as samples\r\nand test the relationship between precision and the chosen\r\nnormalization level. Normalization can increase the recall,\r\nbut we want to evaluate the trade-off between the precision\r\nand different normalization levels. According to the ANOVA\r\n8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/c7bf1e1a-d244-4447-98f6-12e6679c7323.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e65fe81c6de08fc0443be54cc586aab6af8214730cddc3df86ad5b49e19f7675",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1030
      },
      {
        "segments": [
          {
            "segment_id": "7be51b0d-0a49-4fe3-9af6-e1badb78d063",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "6.5\r\n11.5\r\n16.5\r\n21.5\r\n26.5\r\n31.5\r\n36.5\r\n41.5\r\n46.5\r\n51.5\r\n56.5\r\n61.5\r\n10,000\r\n110,000 210,000 310,000\r\n410,000 510,000 610,000 710,000 810,000\r\n910,000\r\n1,010,000 1,110,000 1,210,000\r\n1,310,000 1,410,000 1,510,000 1,610,000 1,710,000 1,810,000 1,910,000 2,010,000 2,110,000 2,210,000 2,310,000\r\nNumber of Functions in Repository\r\nTime used to Index (ms)\r\nAverage Indexing Time per−Function\r\n797\r\n1297\r\n1797\r\n2297\r\n2797\r\n3297\r\n10,000\r\n110,000 210,000 310,000\r\n410,000 510,000 610,000 710,000 810,000 910,000\r\n1,010,000 1,110,000 1,210,000 1,310,000 1,410,000 1,510,000 1,610,000 1,710,000 1,810,000\r\n1,910,000 2,010,000 2,110,000 2,210,000 2,310,000\r\nNumber of Functions in Repository\r\nAverage Query Reponse Time (ms)\r\nAverage Query Response Time per−Function\r\n(a)\r\n(b)\r\nFigure 7: Scalability study. (a): Average Indexing\r\nTime vs. Number of Functions in the Repository.\r\n(b): Average Query Response Time vs. Number of\r\nFunctions in the Repository. The red line represents\r\nthe plotted time and the blue line represents the\r\nsmoothed polynomial approximation.\r\ntest (p < 2e−16), the difference of applying normalization or\r\nnot is statistically significant. Consider Table 3. However,\r\nthe difference of applying different levels, namely Root, Type,\r\nor Specific, is not statistically significant.\r\n8.3 Clone Search Approach Benchmark\r\nIn this section, we benchmark twelve assembly code clone\r\nsearch approaches: BinClone [8, 19], Graphlets [17, 18],\r\nLSH-S [29], and Tracelet [6]. [18] includes several approaches:\r\nmnemonic n-grams (denoted as n-gram), mnemonic n-perms\r\n(denoted as n-perm), Graphlets (denoted as Graphlet), Ex\u0002tended Graphlets (denoted as Graphlet-E), Colored Graphlets\r\n(denoted as Graphlet-C), Mixed Graphlets (denoted as Mix\u0002Graph), Mixed n-grams/perms (denoted as MixGram), Con\u0002stants, and the Composite of n-grams/perms and Graphlets\r\n(denoted as Composite). The idea of using Graphlet orig\u0002inated from [20]. We re-implemented all these approaches\r\nunder a unified evaluation framework and all parameters\r\nwere configured according to the papers. We did not in\u0002clude the re-write engine in [6] because it is not scalable.\r\nSeveral metrics are used in previous research to evaluate\r\nthe clone search quality, but there is no common agreement\r\non what should be used. Precision, recall and F1 are used in\r\nBinClone [7], while [36] maintains that a F2 measure is more\r\nappropriate. However, these two values will change as the\r\nsearch similarity threshold value changes. To evaluate the\r\ntrade-off between recall and precision, we use three typical\r\ninformation retrieval metrics, namely Area Under the Re\u0002ceiver Operating Characteristic Curve (AUROC), Area Un\u0002der the Precision-Recall Curve (AUPR), and Mean Average\r\nPrecision at Position 10 (MAP@10 ). These three metrics\r\nflavor different information retrieval scenarios. Therefore,\r\nwe employ all of them. AUROC and AUPR can test a clas\u0002sifier by issuing different threshold values consecutively [9,\r\n24, 30], while MAP@10 can evaluate the quality of the top\u0002ranked list simulating the real user experience [23].\r\nTable 2 presents the benchmark results. The highest score\r\nof each evaluation metric is highlighted for each dataset.\r\nAlso, the micro-average of all the results for each approach\r\nis given in the Avg column. Kam1n0 out-performs the other\r\napproaches in almost every case for all evaluation metrics.\r\nKam1n0 also achieves the best averaged AUROC, AUPRC,\r\nand MAP@10 scores. The overall performance also suggests\r\nthat it is the most stable one. Each approach is given a\r\n24-hour time frame to finish the clone search and it is only\r\nallowed to use a single thread. Some results for BinClone\r\nand Tracelet are empty, which indicate that they are not\r\nscalable enough to obtain the search result within the given\r\ntime frame. Also, we notice that BinClone consumes more\r\nmemory than the others for building the index, due to its\r\ncombination of features which enlarges the feature space.\r\nWe notice that the experimental results are limited within\r\nthe context of CCFinder. In the future, we will investigate\r\nother source code clone detection techniques to generate the\r\nground truth data.\r\n8.4 Scalability Study\r\nIn this section, we evaluate Kam1n0’s scalability on a large\r\nrepository of assembly functions. We set up a mini-cluster\r\non Google Cloud with four computational nodes. Each of\r\nthem is a n1-highmem-4 machine with 2 virtual cores and\r\n13 GB of RAM. We only use regular persistent disks rather\r\nthan solid state drives. Each machine is given 500 GB of disk\r\nstorage. All the machines run on CentOS. Three machines\r\nrun the Spark Computation Framework and the Apache Cas\u0002sandra Database and the other runs our Kam1n0 engine.\r\nTo conduct the experiment, we prepare a large collection\r\nof binary files. All these files are either open source li\u0002braries or applications, such as Chromium. In total, there\r\nare more than 2,310,000 assembly functions and 27,666,692\r\nbasic blocks. Altogether, there are more than 8 GB of as\u0002sembly code. We gradually index this collection of binaries\r\nin random order, and query the zlib binary file of version\r\n2.7.0 on Kam1n0 at every 10,000 assembly function index\u0002ing interval. As zlib is a widely used library, it is expected\r\nthat it has a large number of clones in the repository. We\r\ncollect the average indexing time for each function to be in\u0002dexed, as well as the average time it takes to respond to a\r\nfunction query. Figure 7 depicts the average indexing and\r\nquery response time for each function. The two diagrams\r\nsuggest that Kam1n0 has a good scalability with respect to\r\nthe repository size. Even as the number of functions in the\r\nrepository increases from 10,000 to 2,310,000, the impact on\r\nthe response time is negligible. There is a spike up at 910,000\r\ndue to the regular compaction routine in Cassandra, which\r\nincreases I/O contention in the database.\r\n9. CONCLUSION AND LESSON LEARNED\r\nThrough the collaboration with Defence Research and De\u0002velopment Canada (DRDC), we learned that scalability, which\r\nwas not considered in previous studies, is a critical issue for\r\ndeploying a successful assembly clone search engine. To ad\u0002dress this, we present the first assembly search engine that\r\ncombines LSH and subgraph search. Existing off-the-shelf\r\nLSH nearest neighbor algorithms and subgraph isomorphism\r\nsearch techniques do not fit our problem setting. Therefore,\r\nwe propose new variants of LSH scheme and incorporate it\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/7be51b0d-0a49-4fe3-9af6-e1badb78d063.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a16ca5daf4f268e6412865633a9f69e376b5fe991976bec9d38957de774b79d0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 976
      },
      {
        "segments": [
          {
            "segment_id": "7be51b0d-0a49-4fe3-9af6-e1badb78d063",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "6.5\r\n11.5\r\n16.5\r\n21.5\r\n26.5\r\n31.5\r\n36.5\r\n41.5\r\n46.5\r\n51.5\r\n56.5\r\n61.5\r\n10,000\r\n110,000 210,000 310,000\r\n410,000 510,000 610,000 710,000 810,000\r\n910,000\r\n1,010,000 1,110,000 1,210,000\r\n1,310,000 1,410,000 1,510,000 1,610,000 1,710,000 1,810,000 1,910,000 2,010,000 2,110,000 2,210,000 2,310,000\r\nNumber of Functions in Repository\r\nTime used to Index (ms)\r\nAverage Indexing Time per−Function\r\n797\r\n1297\r\n1797\r\n2297\r\n2797\r\n3297\r\n10,000\r\n110,000 210,000 310,000\r\n410,000 510,000 610,000 710,000 810,000 910,000\r\n1,010,000 1,110,000 1,210,000 1,310,000 1,410,000 1,510,000 1,610,000 1,710,000 1,810,000\r\n1,910,000 2,010,000 2,110,000 2,210,000 2,310,000\r\nNumber of Functions in Repository\r\nAverage Query Reponse Time (ms)\r\nAverage Query Response Time per−Function\r\n(a)\r\n(b)\r\nFigure 7: Scalability study. (a): Average Indexing\r\nTime vs. Number of Functions in the Repository.\r\n(b): Average Query Response Time vs. Number of\r\nFunctions in the Repository. The red line represents\r\nthe plotted time and the blue line represents the\r\nsmoothed polynomial approximation.\r\ntest (p < 2e−16), the difference of applying normalization or\r\nnot is statistically significant. Consider Table 3. However,\r\nthe difference of applying different levels, namely Root, Type,\r\nor Specific, is not statistically significant.\r\n8.3 Clone Search Approach Benchmark\r\nIn this section, we benchmark twelve assembly code clone\r\nsearch approaches: BinClone [8, 19], Graphlets [17, 18],\r\nLSH-S [29], and Tracelet [6]. [18] includes several approaches:\r\nmnemonic n-grams (denoted as n-gram), mnemonic n-perms\r\n(denoted as n-perm), Graphlets (denoted as Graphlet), Ex\u0002tended Graphlets (denoted as Graphlet-E), Colored Graphlets\r\n(denoted as Graphlet-C), Mixed Graphlets (denoted as Mix\u0002Graph), Mixed n-grams/perms (denoted as MixGram), Con\u0002stants, and the Composite of n-grams/perms and Graphlets\r\n(denoted as Composite). The idea of using Graphlet orig\u0002inated from [20]. We re-implemented all these approaches\r\nunder a unified evaluation framework and all parameters\r\nwere configured according to the papers. We did not in\u0002clude the re-write engine in [6] because it is not scalable.\r\nSeveral metrics are used in previous research to evaluate\r\nthe clone search quality, but there is no common agreement\r\non what should be used. Precision, recall and F1 are used in\r\nBinClone [7], while [36] maintains that a F2 measure is more\r\nappropriate. However, these two values will change as the\r\nsearch similarity threshold value changes. To evaluate the\r\ntrade-off between recall and precision, we use three typical\r\ninformation retrieval metrics, namely Area Under the Re\u0002ceiver Operating Characteristic Curve (AUROC), Area Un\u0002der the Precision-Recall Curve (AUPR), and Mean Average\r\nPrecision at Position 10 (MAP@10 ). These three metrics\r\nflavor different information retrieval scenarios. Therefore,\r\nwe employ all of them. AUROC and AUPR can test a clas\u0002sifier by issuing different threshold values consecutively [9,\r\n24, 30], while MAP@10 can evaluate the quality of the top\u0002ranked list simulating the real user experience [23].\r\nTable 2 presents the benchmark results. The highest score\r\nof each evaluation metric is highlighted for each dataset.\r\nAlso, the micro-average of all the results for each approach\r\nis given in the Avg column. Kam1n0 out-performs the other\r\napproaches in almost every case for all evaluation metrics.\r\nKam1n0 also achieves the best averaged AUROC, AUPRC,\r\nand MAP@10 scores. The overall performance also suggests\r\nthat it is the most stable one. Each approach is given a\r\n24-hour time frame to finish the clone search and it is only\r\nallowed to use a single thread. Some results for BinClone\r\nand Tracelet are empty, which indicate that they are not\r\nscalable enough to obtain the search result within the given\r\ntime frame. Also, we notice that BinClone consumes more\r\nmemory than the others for building the index, due to its\r\ncombination of features which enlarges the feature space.\r\nWe notice that the experimental results are limited within\r\nthe context of CCFinder. In the future, we will investigate\r\nother source code clone detection techniques to generate the\r\nground truth data.\r\n8.4 Scalability Study\r\nIn this section, we evaluate Kam1n0’s scalability on a large\r\nrepository of assembly functions. We set up a mini-cluster\r\non Google Cloud with four computational nodes. Each of\r\nthem is a n1-highmem-4 machine with 2 virtual cores and\r\n13 GB of RAM. We only use regular persistent disks rather\r\nthan solid state drives. Each machine is given 500 GB of disk\r\nstorage. All the machines run on CentOS. Three machines\r\nrun the Spark Computation Framework and the Apache Cas\u0002sandra Database and the other runs our Kam1n0 engine.\r\nTo conduct the experiment, we prepare a large collection\r\nof binary files. All these files are either open source li\u0002braries or applications, such as Chromium. In total, there\r\nare more than 2,310,000 assembly functions and 27,666,692\r\nbasic blocks. Altogether, there are more than 8 GB of as\u0002sembly code. We gradually index this collection of binaries\r\nin random order, and query the zlib binary file of version\r\n2.7.0 on Kam1n0 at every 10,000 assembly function index\u0002ing interval. As zlib is a widely used library, it is expected\r\nthat it has a large number of clones in the repository. We\r\ncollect the average indexing time for each function to be in\u0002dexed, as well as the average time it takes to respond to a\r\nfunction query. Figure 7 depicts the average indexing and\r\nquery response time for each function. The two diagrams\r\nsuggest that Kam1n0 has a good scalability with respect to\r\nthe repository size. Even as the number of functions in the\r\nrepository increases from 10,000 to 2,310,000, the impact on\r\nthe response time is negligible. There is a spike up at 910,000\r\ndue to the regular compaction routine in Cassandra, which\r\nincreases I/O contention in the database.\r\n9. CONCLUSION AND LESSON LEARNED\r\nThrough the collaboration with Defence Research and De\u0002velopment Canada (DRDC), we learned that scalability, which\r\nwas not considered in previous studies, is a critical issue for\r\ndeploying a successful assembly clone search engine. To ad\u0002dress this, we present the first assembly search engine that\r\ncombines LSH and subgraph search. Existing off-the-shelf\r\nLSH nearest neighbor algorithms and subgraph isomorphism\r\nsearch techniques do not fit our problem setting. Therefore,\r\nwe propose new variants of LSH scheme and incorporate it\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/7be51b0d-0a49-4fe3-9af6-e1badb78d063.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a16ca5daf4f268e6412865633a9f69e376b5fe991976bec9d38957de774b79d0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 976
      },
      {
        "segments": [
          {
            "segment_id": "6d2388d6-f380-4b8a-8072-e5921be907b9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "with graph search to address the challenges. Experimen\u0002tal results suggest that our proposed MapReduce-based sys\u0002tem, Kam1n0, is accurate, efficient, and scalable. Currently\r\nKam1n0 can only identify clones for x86/amd64 processor.\r\nIn the future, we will extend it to the other processors and\r\ninvestigate approaches that can find clones between different\r\nprocessors. Kam1n0 provides a practical solution of assem\u0002bly clone search for both DRDC and the reverse engineering\r\ncommunity. The contribution is partially reflected by the\r\naward received at the 2015 Hex-Rays Plug-In Contest.\r\n10. ACKNOWLEDGMENT\r\nThe authors would like to thank the reviewers for the thor\u0002ough reviews and valuable comments. This research is sup\u0002ported by Defence Research and Development Canada (con\u0002tract no. W7701-155902/001/QCL) and Canada Research\r\nChairs Program (950-230623).\r\n11. REFERENCES\r\n[1] A. Andoni and P. Indyk. Near-optimal hashing\r\nalgorithms for approximate nearest neighbor in high\r\ndimensions. Commun. ACM, 51(1), 2008.\r\n[2] M. Bawa, T. Condie, and P. Ganesan. LSH forest:\r\nself-tuning indexes for similarity search. In Proc. of\r\nWWW‘05, 2005.\r\n[3] M. Charikar. Similarity estimation techniques from\r\nrounding algorithms. In Proceedings on 34th Annual\r\nACM STOC’02, 2002.\r\n[4] P. Charland, B. C. M. Fung, and M. R. Farhadi.\r\nClone search for malicious code correlation. In Proc. of\r\nthe NATO RTO Symposium on Information\r\nAssurance and Cyber Defense (IST-111), 2012.\r\n[5] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni.\r\nLocality-sensitive hashing scheme based on p-stable\r\ndistributions. In Proc. of ACM SoCG’04, 2004.\r\n[6] Y. David and E. Yahav. Tracelet-based code search in\r\nexecutables. In Proc. of SIGPLAN’14, 2014.\r\n[7] M. R. Farhadi, B. C. M. Fung, P. Charland, and\r\nM. Debbabi. Binclone: Detecting code clones in\r\nmalware. In Proc. of the 8th International Conference\r\non Software Security and Reliability, 2014.\r\n[8] M. R. Farhadi, B. C. M. Fung, Y. B. Fung,\r\nP. Charland, S. Preda, and M. Debbabi. Scalable code\r\nclone search for malware analysis. Digital\r\nInvestigation, 2015.\r\n[9] T. Fawcett. An introduction to roc analysis. Pattern\r\nrecognition letters, 27(8), 2006.\r\n[10] J. Gan, J. Feng, Q. Fang, and W. Ng.\r\nLocality-sensitive hashing scheme based on dynamic\r\ncollision counting. In Proc. of SIGMOD’12, 2012.\r\n[11] J. Gao, H. V. Jagadish, W. Lu, and B. C. Ooi. Dsh:\r\nData sensitive hashing for high-dimensional\r\nk-nnsearch. In Proc. of SIGMOD’14. ACM, 2014.\r\n[12] A. Gionis, P. Indyk, and R. Motwani. Similarity\r\nsearch in high dimensions via hashing. In Proc. of the\r\nVLDB, 1999.\r\n[13] W. Han, J. Lee, and J. Lee. Turboiso: towards\r\nultrafast and robust subgraph isomorphism search in\r\nlarge graph databases. In Proc. of SIGMOD’13, 2013.\r\n[14] S. Har-Peled, P. Indyk, and R. Motwani. Approximate\r\nnearest neighbor: Towards removing the curse of\r\ndimensionality. Theory of Computing, 8(1), 2012.\r\n[15] E. Juergens et al. Why and how to control cloning in\r\nsoftware artifacts. Technische Universit¨at Munchen ¨ ,\r\n2011.\r\n[16] T. Kamiya, S. Kusumoto, and K. Inoue. Ccfinder: a\r\nmultilinguistic token-based code clone detection\r\nsystem for large scale source code. IEEE TSE, 28(7),\r\n2002.\r\n[17] W. M. Khoo. Decompilation as search. University of\r\nCambridge, Computer Laboratory, Technical Report,\r\n2013.\r\n[18] W. M. Khoo, A. Mycroft, and R. J. Anderson.\r\nRendezvous: a search engine for binary code. In Proc.\r\nof MSR’13, 2013.\r\n[19] V. Komsiyski. Binary differencing for media files. 2013.\r\n[20] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and\r\nG. Vigna. Polymorphic worm detection using\r\nstructural information of executables. In Proc. of\r\nRAID’06. Springer, 2006.\r\n[21] J. Lee, W. Han, R. Kasperovics, and J. Lee. An\r\nin-depth comparison of subgraph isomorphism\r\nalgorithms in graph databases. PVLDB, 6(2), 2012.\r\n[22] Y. Liu, J. Cui, Z. Huang, H. Li, and H. T. Shen.\r\nSK-LSH: an efficient index structure for approximate\r\nnearest neighbor search. PVLDB, 7(9), 2014.\r\n[23] C. D. Manning, P. Raghavan, H. Schutze, et al. ¨\r\nIntroduction to information retrieval, volume 1.\r\nCambridge University Press, 2008.\r\n[24] C. D. Manning and H. Schutze.\r\n¨ Foundations of\r\nstatistical natural language processing. MIT press,\r\n1999.\r\n[25] A. Mockus. Large-scale code reuse in open source\r\nsoftware. In Proc. of FLOSS’07. IEEE, 2007.\r\n[26] A. Saebjornsen. Detecting Fine-Grained Similarity in\r\nBinaries. PhD thesis, UC Davis, 2014.\r\n[27] A. Sæbjørnsen, J. Willcock, T. Panas, D. J. Quinlan,\r\nand Z. Su. Detecting code clones in binary\r\nexecutables. In Proc. of the 18th International\r\nSymposium on Software Testing and Analysis, 2009.\r\n[28] R. Shamir and D. Tsur. Faster subtree isomorphism.\r\nIn Proc. of IEEE ISTCS’97, 1997.\r\n[29] M. Sojer and J. Henkel. Code reuse in open source\r\nsoftware development: Quantitative evidence, drivers,\r\nand impediments. JAIS, 11(12), 2010.\r\n[30] S. Sonnenburg, G. R¨atsch, C. Sch¨afer, and\r\nB. Sch¨olkopf. Large scale multiple kernel learning.\r\nJMLR, 7, 2006.\r\n[31] Z. Sun, H. Wang, H. Wang, B. Shao, and J. Li.\r\nEfficient subgraph matching on billion node graphs.\r\nThe VLDB Endowment, 5(9), 2012.\r\n[32] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Quality and\r\nefficiency in high dimensional nearest neighbor search.\r\nIn Proc. of SIGMOD’09, 2009.\r\n[33] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Efficient and\r\naccurate nearest neighbor and closest pair search in\r\nhigh-dimensional space. ACM TODS, 35(3), 2010.\r\n[34] J. R. Ullmann. An algorithm for subgraph\r\nisomorphism. ACM JACM, 23(1), 1976.\r\n[35] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for\r\nsimilarity search: A survey. arXiv:1408.2927, 2014.\r\n[36] H. Welte. Current developments in GPL compliance,\r\n2012.\r\n10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/6d2388d6-f380-4b8a-8072-e5921be907b9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8338deff5c6b03ba3f5c404222193b5f1c01a50b4f491d707101f68339082626",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 874
      },
      {
        "segments": [
          {
            "segment_id": "6d2388d6-f380-4b8a-8072-e5921be907b9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "with graph search to address the challenges. Experimen\u0002tal results suggest that our proposed MapReduce-based sys\u0002tem, Kam1n0, is accurate, efficient, and scalable. Currently\r\nKam1n0 can only identify clones for x86/amd64 processor.\r\nIn the future, we will extend it to the other processors and\r\ninvestigate approaches that can find clones between different\r\nprocessors. Kam1n0 provides a practical solution of assem\u0002bly clone search for both DRDC and the reverse engineering\r\ncommunity. The contribution is partially reflected by the\r\naward received at the 2015 Hex-Rays Plug-In Contest.\r\n10. ACKNOWLEDGMENT\r\nThe authors would like to thank the reviewers for the thor\u0002ough reviews and valuable comments. This research is sup\u0002ported by Defence Research and Development Canada (con\u0002tract no. W7701-155902/001/QCL) and Canada Research\r\nChairs Program (950-230623).\r\n11. REFERENCES\r\n[1] A. Andoni and P. Indyk. Near-optimal hashing\r\nalgorithms for approximate nearest neighbor in high\r\ndimensions. Commun. ACM, 51(1), 2008.\r\n[2] M. Bawa, T. Condie, and P. Ganesan. LSH forest:\r\nself-tuning indexes for similarity search. In Proc. of\r\nWWW‘05, 2005.\r\n[3] M. Charikar. Similarity estimation techniques from\r\nrounding algorithms. In Proceedings on 34th Annual\r\nACM STOC’02, 2002.\r\n[4] P. Charland, B. C. M. Fung, and M. R. Farhadi.\r\nClone search for malicious code correlation. In Proc. of\r\nthe NATO RTO Symposium on Information\r\nAssurance and Cyber Defense (IST-111), 2012.\r\n[5] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni.\r\nLocality-sensitive hashing scheme based on p-stable\r\ndistributions. In Proc. of ACM SoCG’04, 2004.\r\n[6] Y. David and E. Yahav. Tracelet-based code search in\r\nexecutables. In Proc. of SIGPLAN’14, 2014.\r\n[7] M. R. Farhadi, B. C. M. Fung, P. Charland, and\r\nM. Debbabi. Binclone: Detecting code clones in\r\nmalware. In Proc. of the 8th International Conference\r\non Software Security and Reliability, 2014.\r\n[8] M. R. Farhadi, B. C. M. Fung, Y. B. Fung,\r\nP. Charland, S. Preda, and M. Debbabi. Scalable code\r\nclone search for malware analysis. Digital\r\nInvestigation, 2015.\r\n[9] T. Fawcett. An introduction to roc analysis. Pattern\r\nrecognition letters, 27(8), 2006.\r\n[10] J. Gan, J. Feng, Q. Fang, and W. Ng.\r\nLocality-sensitive hashing scheme based on dynamic\r\ncollision counting. In Proc. of SIGMOD’12, 2012.\r\n[11] J. Gao, H. V. Jagadish, W. Lu, and B. C. Ooi. Dsh:\r\nData sensitive hashing for high-dimensional\r\nk-nnsearch. In Proc. of SIGMOD’14. ACM, 2014.\r\n[12] A. Gionis, P. Indyk, and R. Motwani. Similarity\r\nsearch in high dimensions via hashing. In Proc. of the\r\nVLDB, 1999.\r\n[13] W. Han, J. Lee, and J. Lee. Turboiso: towards\r\nultrafast and robust subgraph isomorphism search in\r\nlarge graph databases. In Proc. of SIGMOD’13, 2013.\r\n[14] S. Har-Peled, P. Indyk, and R. Motwani. Approximate\r\nnearest neighbor: Towards removing the curse of\r\ndimensionality. Theory of Computing, 8(1), 2012.\r\n[15] E. Juergens et al. Why and how to control cloning in\r\nsoftware artifacts. Technische Universit¨at Munchen ¨ ,\r\n2011.\r\n[16] T. Kamiya, S. Kusumoto, and K. Inoue. Ccfinder: a\r\nmultilinguistic token-based code clone detection\r\nsystem for large scale source code. IEEE TSE, 28(7),\r\n2002.\r\n[17] W. M. Khoo. Decompilation as search. University of\r\nCambridge, Computer Laboratory, Technical Report,\r\n2013.\r\n[18] W. M. Khoo, A. Mycroft, and R. J. Anderson.\r\nRendezvous: a search engine for binary code. In Proc.\r\nof MSR’13, 2013.\r\n[19] V. Komsiyski. Binary differencing for media files. 2013.\r\n[20] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and\r\nG. Vigna. Polymorphic worm detection using\r\nstructural information of executables. In Proc. of\r\nRAID’06. Springer, 2006.\r\n[21] J. Lee, W. Han, R. Kasperovics, and J. Lee. An\r\nin-depth comparison of subgraph isomorphism\r\nalgorithms in graph databases. PVLDB, 6(2), 2012.\r\n[22] Y. Liu, J. Cui, Z. Huang, H. Li, and H. T. Shen.\r\nSK-LSH: an efficient index structure for approximate\r\nnearest neighbor search. PVLDB, 7(9), 2014.\r\n[23] C. D. Manning, P. Raghavan, H. Schutze, et al. ¨\r\nIntroduction to information retrieval, volume 1.\r\nCambridge University Press, 2008.\r\n[24] C. D. Manning and H. Schutze.\r\n¨ Foundations of\r\nstatistical natural language processing. MIT press,\r\n1999.\r\n[25] A. Mockus. Large-scale code reuse in open source\r\nsoftware. In Proc. of FLOSS’07. IEEE, 2007.\r\n[26] A. Saebjornsen. Detecting Fine-Grained Similarity in\r\nBinaries. PhD thesis, UC Davis, 2014.\r\n[27] A. Sæbjørnsen, J. Willcock, T. Panas, D. J. Quinlan,\r\nand Z. Su. Detecting code clones in binary\r\nexecutables. In Proc. of the 18th International\r\nSymposium on Software Testing and Analysis, 2009.\r\n[28] R. Shamir and D. Tsur. Faster subtree isomorphism.\r\nIn Proc. of IEEE ISTCS’97, 1997.\r\n[29] M. Sojer and J. Henkel. Code reuse in open source\r\nsoftware development: Quantitative evidence, drivers,\r\nand impediments. JAIS, 11(12), 2010.\r\n[30] S. Sonnenburg, G. R¨atsch, C. Sch¨afer, and\r\nB. Sch¨olkopf. Large scale multiple kernel learning.\r\nJMLR, 7, 2006.\r\n[31] Z. Sun, H. Wang, H. Wang, B. Shao, and J. Li.\r\nEfficient subgraph matching on billion node graphs.\r\nThe VLDB Endowment, 5(9), 2012.\r\n[32] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Quality and\r\nefficiency in high dimensional nearest neighbor search.\r\nIn Proc. of SIGMOD’09, 2009.\r\n[33] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Efficient and\r\naccurate nearest neighbor and closest pair search in\r\nhigh-dimensional space. ACM TODS, 35(3), 2010.\r\n[34] J. R. Ullmann. An algorithm for subgraph\r\nisomorphism. ACM JACM, 23(1), 1976.\r\n[35] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for\r\nsimilarity search: A survey. arXiv:1408.2927, 2014.\r\n[36] H. Welte. Current developments in GPL compliance,\r\n2012.\r\n10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/47aa63f3-7883-4cce-bbcc-1809feb8df64/images/6d2388d6-f380-4b8a-8072-e5921be907b9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041623Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8338deff5c6b03ba3f5c404222193b5f1c01a50b4f491d707101f68339082626",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 874
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\n \"title\": \"Kam1n0: MapReduce-based Assembly Clone Search for Reverse Engineering\"\n}\n```"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "{\n \"date_published\": \"August 13–17, 2016\"\n}"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "No response"
        }
      ]
    }
  }
}