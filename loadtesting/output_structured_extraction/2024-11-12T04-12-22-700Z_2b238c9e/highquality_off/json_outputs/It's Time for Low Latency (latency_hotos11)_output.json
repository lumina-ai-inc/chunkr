{
  "file_name": "It's Time for Low Latency (latency_hotos11).pdf",
  "task_id": "5a84d437-ed96-444f-8254-19917e8d16bc",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "57cccf4f-2e2c-44b2-941e-85a6f23a3626",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "It’s Time for Low Latency\r\nStephen M. Rumble, Diego Ongaro, Ryan Stutsman,\r\nMendel Rosenblum, and John K. Ousterhout\r\nStanford University\r\nAbstract\r\nThe operating systems community has ignored network\r\nlatency for too long. In the past, speed-of-light delays\r\nin wide area networks and unoptimized network hard\u0002ware have made sub-100µs round-trip times impossible.\r\nHowever, in the next few years datacenters will be de\u0002ployed with low-latency Ethernet. Without the burden\r\nof propagation delays in the datacenter campus and net\u0002work delays in the Ethernet devices, it will be up to us\r\nto finish the job and see this benefit through to applica\u0002tions. We argue that OS researchers must lead the charge\r\nin rearchitecting systems to push the boundaries of low\u0002latency datacenter communication. 5-10µs remote pro\u0002cedure calls are possible in the short term – two orders\r\nof magnitude better than today. In the long term, moving\r\nthe network interface on to the CPU core will make 1µs\r\ntimes feasible.\r\n1 Introduction\r\nNetwork latency has been an increasing source of frustra\u0002tion and disappointment over the last thirty years. While\r\nnearly every other metric of computer performance has\r\nimproved drastically, the latency of network communica\u0002tion has not. System designers have consistently chosen\r\nto sacrifice latency in favor of other goals such as band\u0002width, and software developers have focused their efforts\r\nmore on tolerating latency than improving it.\r\nRecent developments are finally bringing low latency\r\nwithin reach. Fast communication is available today for\r\nthose willing to use specialized hardware and software\r\ndeveloped by the high-performance computing (HPC)\r\ncommunity, and pieces of the low latency puzzle are be\u0002coming available for general purpose computing.\r\nIn this position paper we argue that it should be pos\u0002sible to achieve end-to-end remote procedure call (RPC)\r\nlatencies of 5-10µs in large datacenters using commod\u0002ity hardware and software within a few years. How\u0002ever, achieving this goal will require the creation of a\r\nnew software architecture for networking with a differ\u0002ent division of responsibility between operating system,\r\nhardware, and application. In addition, we will probably\r\nneed new network protocols optimized for low latency in\r\nlarge datacenters. Although several hardware improve\u00021983 2011 Improved\r\nCPU Speed 1x10Mhz 4x3GHz > 1,000x\r\nMemory Size ≤ 2MB 8GB ≥ 4,000x\r\nDisk Capacity ≤ 30MB 2TB > 60,000x\r\nNet Bwidth 3Mbps 10Gbps > 3,000x\r\nRTT 2.54ms 80µs 32x\r\nTable 1: Network latency has improved far more slowly over\r\nthe last three decades than other performance metrics for com\u0002modity computers. The V Distributed System [5] achieved\r\nround-trip RPC times of 2.54ms. Today, a pair of modern Linux\r\nservers require 80µs for 16-byte RPCs over TCP with 10Gb\r\nEthernet.\r\nments will also be required to reach this goal, the oper\u0002ating system community is in the best position to coordi\u0002nate all of these changes and create the right end-to-end\r\narchitecture. Over the longer-term, and with more radi\u0002cal hardware changes (such as moving the NIC onto the\r\nCPU chip), we think 1µs datacenter round-trips can be\r\nachieved.\r\nThere will be many benefits for datacenter computing\r\nif we succeed. Lower latency will simplify application\r\ndevelopment, increase web application scalability, and\r\nenable new kinds of data-intensive applications that are\r\nnot possible today.\r\n2 A History of High Latency\r\nNetwork latency has failed to keep up with other im\u0002provements in computer performance. Consider technol\u0002ogy evolution over the last 30 years (Table 1). In 1983,\r\nthe V Distributed System [5] had 2.54ms RPC round\u0002trip times on a SUN Workstation [2]. The last three\r\ndecades have seen only a 30x reduction in latency, while\r\nnetwork bandwidth has improved more than 3,000x over\r\nthe same period, and processor throughput, disk capac\u0002ity, and memory capacity have also had large gains. Sev\u0002eral research projects in the mid- and late-1990s attacked\r\nthe latency problem [15, 6, 7, 12], but unfortunately their\r\ntechniques were not adopted by mainstream manufactur\u0002ers.\r\nLatency is even worse in large datacenters with tens\r\nof thousands of servers. Round-trip times are typically\r\n200-500µs [13, 9] and congestion can cause spikes up\r\n1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/57cccf4f-2e2c-44b2-941e-85a6f23a3626.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4ddd2be419ecc20edffa6060f1816d86ab55318da3d10ed3e42670b98301bec4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 650
      },
      {
        "segments": [
          {
            "segment_id": "57cccf4f-2e2c-44b2-941e-85a6f23a3626",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "It’s Time for Low Latency\r\nStephen M. Rumble, Diego Ongaro, Ryan Stutsman,\r\nMendel Rosenblum, and John K. Ousterhout\r\nStanford University\r\nAbstract\r\nThe operating systems community has ignored network\r\nlatency for too long. In the past, speed-of-light delays\r\nin wide area networks and unoptimized network hard\u0002ware have made sub-100µs round-trip times impossible.\r\nHowever, in the next few years datacenters will be de\u0002ployed with low-latency Ethernet. Without the burden\r\nof propagation delays in the datacenter campus and net\u0002work delays in the Ethernet devices, it will be up to us\r\nto finish the job and see this benefit through to applica\u0002tions. We argue that OS researchers must lead the charge\r\nin rearchitecting systems to push the boundaries of low\u0002latency datacenter communication. 5-10µs remote pro\u0002cedure calls are possible in the short term – two orders\r\nof magnitude better than today. In the long term, moving\r\nthe network interface on to the CPU core will make 1µs\r\ntimes feasible.\r\n1 Introduction\r\nNetwork latency has been an increasing source of frustra\u0002tion and disappointment over the last thirty years. While\r\nnearly every other metric of computer performance has\r\nimproved drastically, the latency of network communica\u0002tion has not. System designers have consistently chosen\r\nto sacrifice latency in favor of other goals such as band\u0002width, and software developers have focused their efforts\r\nmore on tolerating latency than improving it.\r\nRecent developments are finally bringing low latency\r\nwithin reach. Fast communication is available today for\r\nthose willing to use specialized hardware and software\r\ndeveloped by the high-performance computing (HPC)\r\ncommunity, and pieces of the low latency puzzle are be\u0002coming available for general purpose computing.\r\nIn this position paper we argue that it should be pos\u0002sible to achieve end-to-end remote procedure call (RPC)\r\nlatencies of 5-10µs in large datacenters using commod\u0002ity hardware and software within a few years. How\u0002ever, achieving this goal will require the creation of a\r\nnew software architecture for networking with a differ\u0002ent division of responsibility between operating system,\r\nhardware, and application. In addition, we will probably\r\nneed new network protocols optimized for low latency in\r\nlarge datacenters. Although several hardware improve\u00021983 2011 Improved\r\nCPU Speed 1x10Mhz 4x3GHz > 1,000x\r\nMemory Size ≤ 2MB 8GB ≥ 4,000x\r\nDisk Capacity ≤ 30MB 2TB > 60,000x\r\nNet Bwidth 3Mbps 10Gbps > 3,000x\r\nRTT 2.54ms 80µs 32x\r\nTable 1: Network latency has improved far more slowly over\r\nthe last three decades than other performance metrics for com\u0002modity computers. The V Distributed System [5] achieved\r\nround-trip RPC times of 2.54ms. Today, a pair of modern Linux\r\nservers require 80µs for 16-byte RPCs over TCP with 10Gb\r\nEthernet.\r\nments will also be required to reach this goal, the oper\u0002ating system community is in the best position to coordi\u0002nate all of these changes and create the right end-to-end\r\narchitecture. Over the longer-term, and with more radi\u0002cal hardware changes (such as moving the NIC onto the\r\nCPU chip), we think 1µs datacenter round-trips can be\r\nachieved.\r\nThere will be many benefits for datacenter computing\r\nif we succeed. Lower latency will simplify application\r\ndevelopment, increase web application scalability, and\r\nenable new kinds of data-intensive applications that are\r\nnot possible today.\r\n2 A History of High Latency\r\nNetwork latency has failed to keep up with other im\u0002provements in computer performance. Consider technol\u0002ogy evolution over the last 30 years (Table 1). In 1983,\r\nthe V Distributed System [5] had 2.54ms RPC round\u0002trip times on a SUN Workstation [2]. The last three\r\ndecades have seen only a 30x reduction in latency, while\r\nnetwork bandwidth has improved more than 3,000x over\r\nthe same period, and processor throughput, disk capac\u0002ity, and memory capacity have also had large gains. Sev\u0002eral research projects in the mid- and late-1990s attacked\r\nthe latency problem [15, 6, 7, 12], but unfortunately their\r\ntechniques were not adopted by mainstream manufactur\u0002ers.\r\nLatency is even worse in large datacenters with tens\r\nof thousands of servers. Round-trip times are typically\r\n200-500µs [13, 9] and congestion can cause spikes up\r\n1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/57cccf4f-2e2c-44b2-941e-85a6f23a3626.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4ddd2be419ecc20edffa6060f1816d86ab55318da3d10ed3e42670b98301bec4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 650
      },
      {
        "segments": [
          {
            "segment_id": "a9c164e0-ee3a-4cb8-a701-97ee8265ee7b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "Component Delay Round-Trip\r\nNetwork Switch 10-30µs 100-300µs\r\nNetwork Interface Card 2.5-32µs 10-128µs\r\nOS Network Stack 15µs 60µs\r\nSpeed of Light (in Fiber) 5ns/m 0.6-1.2µs\r\nTable 2: Factors that contribute to latency in TCP datacenter\r\ncommunication. “Delay” indicates the cost of a single traver\u0002sal of the component, and “Round-Trip” indicates the total im\u0002pact on round-trip time. Messages typically traverse 5 switches\r\nin each direction in a large datacenter network and must pass\r\nthrough the OS stack 4 times.\r\nto tens of milliseconds. Table 2 breaks down the ma\u0002jor components of latency in datacenters today. Store\u0002and-forward network switches are the largest single con\u0002tributor, adding tens of microseconds for each hop, but\r\nnetwork interface cards and operating system proto\u0002col stacks also present major obstacles to low latency.\r\nSpeed-of-light delays are not a major factor.\r\nHigh latency is not fundamental or inevitable: design\u0002ers have chosen to sacrifice latency in order to achieve\r\nother goals or work around problems elsewhere in the\r\nsystem. For example, network switches typically em\u0002ploy large packet buffers, which are needed because\r\n(a) networks are oversubscribed, resulting in congestion,\r\nand (b) the TCP protocol behaves poorly if packets are\r\ndropped because of congestion. Unfortunately, the more\r\nthat buffers are used, the worse latency becomes. Both\r\noperating systems and NICs are optimized for bandwidth\r\nat the expense of latency; for example, many NICs inten\u0002tionally delay the delivery of interrupts as much as 30µs\r\nin order to allow several packets to be processed with a\r\nsingle interrupt.\r\n3 Impact on Applications\r\nAlthough it has been convenient for system designers to\r\nsacrifice latency, this has not been so convenient for ap\u0002plication developers. For example, high latency limits\r\nFacebook’s applications to 100-150 sequential data ac\u0002cesses within the datacenter for each Web page returned\r\nto a browser (any more would result in unacceptable re\u0002sponse time for users). As a result, Facebook has re\u0002sorted to parallel accesses and de-normalized data, both\r\nof which add to application complexity [13]. Some fea\u0002tures are simply not possible within this constraint.\r\nBecause of the complexity of dealing with latency,\r\nconsiderable effort has been expended in recent years to\r\ndevelop application frameworks that are insensitive to la\u0002tency. For example, MapReduce [10] organizes large\u0002scale applications as a series of parallel stages where\r\ndata is accessed sequentially in large blocks; as a re\u0002sult, application speed is limited by bandwidth, not la\u0002tency. However, its dependence on sequential data ac\u0002cess makes MapReduce difficult to use for applications\r\nthat require random accesses. Furthermore, MapReduce\r\nis useful only for long-running batch jobs, not for inter\u0002active tasks.\r\nHigh latency rules out entire classes of applications.\r\nFor example, if an application needs to harness thousands\r\nof machines in a datacenter and intensively access ran\u0002dom bits of data distributed across the main memories of\r\nthose machines (e.g., for large-scale graph algorithms),\r\nthe application is not practical today: it will bottleneck\r\non the network. Network latency was less of an issue\r\nin the past when most network requests resulted in disk\r\nI/Os, but as the primary locus of data moves from disk to\r\nflash or even DRAM, the network is becoming the pri\u0002mary source of latency in remote data accesses.\r\nIf latency can be improved by 1-2 orders of mag\u0002nitude, we believe it will have a revolutionary impact\r\non applications. The most immediate benefit will be\r\nfor existing applications that suffer from high latency,\r\nsuch as Facebook or Google’s statistical machine transla\u0002tion [3]. These applications will become much simpler to\r\ndevelop, since it will no longer be necessary to employ\r\ncomplex workarounds for high latency. They will also\r\nbecome faster and more scalable. More in-depth data ex\u0002ploration will be possible in real-time.\r\nMore importantly, we speculate that low latency will\r\nenable a new breed of data-intensive applications. The\r\nWeb has made it possible to assemble enormous datasets,\r\nbut high latency severely restricts the kinds of operations\r\nthat can be performed on those datasets, particularly in\r\nreal time. Low-latency networking will allow more in\u0002tensive and interactive manipulation of large datasets\r\nthan has ever been possible in the past. It is hard to pre\u0002dict the nature of these applications, since they could not\r\nexist today, but one possible example is collaboration at\r\nthe level of large crowds (e.g., in massive virtual worlds);\r\nanother is very large-scale machine learning applications\r\nand other large graph algorithms.\r\nThe popularity of memcached [1] and “NoSQL” stor\u0002age systems provides another indication of the benefits of\r\nlow latency. Their rapid adoption demonstrates just how\r\ndesirable and powerful fast storage access is. Developers\r\nare clamoring for even faster data access, but the network\r\nis now the bottleneck: more than 80% of the memcached\r\nlatency for Facebook is due to the network.\r\nLow latency has the potential to reduce or eliminate\r\nother problems that have plagued system designers. For\r\nexample, many NoSQL systems have sacrificed consis\u0002tency guarantees by limiting atomic updates to a sin\u0002gle row or offering only eventual consistency [11, 4, 8].\r\nStrong consistency is expensive to implement when there\r\nare many transactions executing concurrently, since this\r\nincreases the likelihood of expensive conflicts. In a\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/a9c164e0-ee3a-4cb8-a701-97ee8265ee7b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=798cba63ef72cbc7787c6ec260b9d475679e4192e489fb638a0bf3ba52760ad9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 835
      },
      {
        "segments": [
          {
            "segment_id": "a9c164e0-ee3a-4cb8-a701-97ee8265ee7b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "Component Delay Round-Trip\r\nNetwork Switch 10-30µs 100-300µs\r\nNetwork Interface Card 2.5-32µs 10-128µs\r\nOS Network Stack 15µs 60µs\r\nSpeed of Light (in Fiber) 5ns/m 0.6-1.2µs\r\nTable 2: Factors that contribute to latency in TCP datacenter\r\ncommunication. “Delay” indicates the cost of a single traver\u0002sal of the component, and “Round-Trip” indicates the total im\u0002pact on round-trip time. Messages typically traverse 5 switches\r\nin each direction in a large datacenter network and must pass\r\nthrough the OS stack 4 times.\r\nto tens of milliseconds. Table 2 breaks down the ma\u0002jor components of latency in datacenters today. Store\u0002and-forward network switches are the largest single con\u0002tributor, adding tens of microseconds for each hop, but\r\nnetwork interface cards and operating system proto\u0002col stacks also present major obstacles to low latency.\r\nSpeed-of-light delays are not a major factor.\r\nHigh latency is not fundamental or inevitable: design\u0002ers have chosen to sacrifice latency in order to achieve\r\nother goals or work around problems elsewhere in the\r\nsystem. For example, network switches typically em\u0002ploy large packet buffers, which are needed because\r\n(a) networks are oversubscribed, resulting in congestion,\r\nand (b) the TCP protocol behaves poorly if packets are\r\ndropped because of congestion. Unfortunately, the more\r\nthat buffers are used, the worse latency becomes. Both\r\noperating systems and NICs are optimized for bandwidth\r\nat the expense of latency; for example, many NICs inten\u0002tionally delay the delivery of interrupts as much as 30µs\r\nin order to allow several packets to be processed with a\r\nsingle interrupt.\r\n3 Impact on Applications\r\nAlthough it has been convenient for system designers to\r\nsacrifice latency, this has not been so convenient for ap\u0002plication developers. For example, high latency limits\r\nFacebook’s applications to 100-150 sequential data ac\u0002cesses within the datacenter for each Web page returned\r\nto a browser (any more would result in unacceptable re\u0002sponse time for users). As a result, Facebook has re\u0002sorted to parallel accesses and de-normalized data, both\r\nof which add to application complexity [13]. Some fea\u0002tures are simply not possible within this constraint.\r\nBecause of the complexity of dealing with latency,\r\nconsiderable effort has been expended in recent years to\r\ndevelop application frameworks that are insensitive to la\u0002tency. For example, MapReduce [10] organizes large\u0002scale applications as a series of parallel stages where\r\ndata is accessed sequentially in large blocks; as a re\u0002sult, application speed is limited by bandwidth, not la\u0002tency. However, its dependence on sequential data ac\u0002cess makes MapReduce difficult to use for applications\r\nthat require random accesses. Furthermore, MapReduce\r\nis useful only for long-running batch jobs, not for inter\u0002active tasks.\r\nHigh latency rules out entire classes of applications.\r\nFor example, if an application needs to harness thousands\r\nof machines in a datacenter and intensively access ran\u0002dom bits of data distributed across the main memories of\r\nthose machines (e.g., for large-scale graph algorithms),\r\nthe application is not practical today: it will bottleneck\r\non the network. Network latency was less of an issue\r\nin the past when most network requests resulted in disk\r\nI/Os, but as the primary locus of data moves from disk to\r\nflash or even DRAM, the network is becoming the pri\u0002mary source of latency in remote data accesses.\r\nIf latency can be improved by 1-2 orders of mag\u0002nitude, we believe it will have a revolutionary impact\r\non applications. The most immediate benefit will be\r\nfor existing applications that suffer from high latency,\r\nsuch as Facebook or Google’s statistical machine transla\u0002tion [3]. These applications will become much simpler to\r\ndevelop, since it will no longer be necessary to employ\r\ncomplex workarounds for high latency. They will also\r\nbecome faster and more scalable. More in-depth data ex\u0002ploration will be possible in real-time.\r\nMore importantly, we speculate that low latency will\r\nenable a new breed of data-intensive applications. The\r\nWeb has made it possible to assemble enormous datasets,\r\nbut high latency severely restricts the kinds of operations\r\nthat can be performed on those datasets, particularly in\r\nreal time. Low-latency networking will allow more in\u0002tensive and interactive manipulation of large datasets\r\nthan has ever been possible in the past. It is hard to pre\u0002dict the nature of these applications, since they could not\r\nexist today, but one possible example is collaboration at\r\nthe level of large crowds (e.g., in massive virtual worlds);\r\nanother is very large-scale machine learning applications\r\nand other large graph algorithms.\r\nThe popularity of memcached [1] and “NoSQL” stor\u0002age systems provides another indication of the benefits of\r\nlow latency. Their rapid adoption demonstrates just how\r\ndesirable and powerful fast storage access is. Developers\r\nare clamoring for even faster data access, but the network\r\nis now the bottleneck: more than 80% of the memcached\r\nlatency for Facebook is due to the network.\r\nLow latency has the potential to reduce or eliminate\r\nother problems that have plagued system designers. For\r\nexample, many NoSQL systems have sacrificed consis\u0002tency guarantees by limiting atomic updates to a sin\u0002gle row or offering only eventual consistency [11, 4, 8].\r\nStrong consistency is expensive to implement when there\r\nare many transactions executing concurrently, since this\r\nincreases the likelihood of expensive conflicts. In a\r\n2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/a9c164e0-ee3a-4cb8-a701-97ee8265ee7b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=798cba63ef72cbc7787c6ec260b9d475679e4192e489fb638a0bf3ba52760ad9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 835
      },
      {
        "segments": [
          {
            "segment_id": "b63ee464-5afc-483c-8c2f-7a39ed72074b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "system with very low latency, transactions finish more\r\nquickly, reducing the overlap between transactions and\r\nminimizing conflicts. We speculate that low-latency sys\u0002tems may be able to provide stronger consistency guar\u0002antees at much larger system scale.\r\nLow latency may also reduce the incast problems ex\u0002perienced by many applications. With high latency, ap\u0002plications are forced to issue concurrent data requests in\r\norder to meet user response deadlines. However, such\r\nrequests can lead to simultaneous responses, which can\r\ncause congestion near the client, overflow small switch\r\nbuffers, and result in packet loss. This scenario could be\r\navoided if sequential accesses were sufficiently fast that\r\nconcurrent requests are no longer needed.\r\n4 Low Latency is Within Reach\r\nSeveral recent developments are putting low latency net\u0002working within reach. These include the rise of the dat\u0002acenter, the next generation of Ethernet switching chips,\r\nand faster NICs. The HPC community has shown us that\r\nlow latency is possible using special-purpose intercon\u0002nects, but now we are on the brink of achieving the same\r\nwith commodity hardware.\r\nOne of the interesting properties of datacenters is that\r\nthey pack large amounts of computation and storage\r\nclose together. While the speed of light limits latency\r\nin wide area networks, electrons can traverse 100m of\r\ncopper cables and back in about 1µs. This means that\r\nmicrosecond-level RPCs are physically possible at very\r\nlarge scale, and the datacenter is the perfect environment\r\nto take advantage of low latency networking.\r\nNew cut-through switching chips designed for 10Gb\r\nEthernet are dramatically lowering the cost of both la\u0002tency and bandwidth. For example, switches from\r\nArista, which are based on chips from Fulcrum Mi\u0002crosystems, offer switching delays less than 1µs, which\r\nis more than an order of magnitude improvement over\r\nthe times in Table 2. The cost/port of these switches is\r\nstill high compared to commodity 1Gb switches, but will\r\ndrop rapidly over the next few years. Furthermore, the\r\nswitching chips should get a double benefit from Moore’s\r\nLaw: not only will they improve in latency but the num\u0002ber of ports will increase, which will reduce the number\r\nof switching levels required to traverse a datacenter.\r\nThe new switching chips also promise to make band\u0002width plentiful and cheap. Most existing datacenter net\u0002works cannot afford enough bandwidth in the switching\r\nfabric for all nodes to communicate randomly at full line\r\nrates. As a result, upper layers in the switching fabric\r\nare oversubscribed (the total bandwidth of the top layer\r\nis typically 100-500x less than the total bandwidth out of\r\nindividual servers). The resulting congestion can result\r\nin delays of tens of milliseconds as packets work their\r\nway through deep buffers. The new switching chips are\r\ncheap enough to make full bisection bandwidth afford\u0002able in datacenter networks, eliminating congestion and\r\nthe associated delays.\r\nNew network interface controllers from companies\r\nsuch as Mellanox are significantly reducing two other\r\nmajor sources of latency. First, they have been opti\u0002mized to reduce latency within the NIC itself (less than\r\n1µs in each direction). Second, they allow direct access\r\nfrom user space, which eliminates the overhead of pass\u0002ing through the kernel. These interfaces can be used in a\r\npolling mode, which also eliminates the latency associ\u0002ated with interrupt handling and context switching. Un\u0002fortunately most of these interfaces are designed for spe\u0002cialized interconnects such as Infiniband and Myrinet, so\r\nthey do not support the standard protocols and APIs ex\u0002pected by most applications.\r\nThe HPC community has already produced special\u0002ized systems that combine all the benefits above. Using\r\nInfiniband switches and NICs from Mellanox, we have\r\nmeasured round-trip times less than 5µs in small-scale\r\nnetworks with reliable delivery protocols analogous to\r\nTCP. HPC vendors have demonstrated that low latency\r\nis possible, and it seems likely that some of the tech\u0002niques used in HPC hardware will migrate to mainstream\r\nnetworking. Unfortunately, the interconnects, protocols,\r\nand APIs of these systems are very different from the\r\ncommodity Ethernet/IP/TCP approaches used in most\r\ndatacenters, so low latency is still beyond the reach of\r\nmost applications today.\r\nFuthermore, HPC approaches are unlikely to be\r\nadopted wholesale. First of all, Ethernet’s ubiquity, mar\u0002ket dominance, and economies of scale will make it dif\u0002ficult to compete with. Second, the HPC strategy has\r\nbeen to move functions to network interfaces to over\u0002come OS inefficiencies and allow more complex offload\u0002ing of functionality. This makes the NICs more expen\u0002sive, but more importantly, it makes the network less\r\nflexible. In contrast, Ethernet’s simplicity promotes in\u0002novation and makes for an excellent research vehicle.\r\n5 The OS Community’s Role\r\nUntil recently there was little reason for operating sys\u0002tems to worry about latency: external factors such as\r\nspeed-of-light propagation for long-haul networks and\r\nslow switches in datacenters overshadowed any ineffi\u0002ciencies in the operating system. However, the improve\u0002ments discussed in Section 4 are dramatically reducing\r\nthe external factors; within a few years the operating sys\u0002tem could become the largest remaining obstacle to low\r\nlatency RPCs in datacenters. Thus, it is now time to re\u0002think the role of the operating system in networking.\r\nAs a community, we should set a goal of making\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/b63ee464-5afc-483c-8c2f-7a39ed72074b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ef5d85590fc6bae97673f37a0e8a3adfbe7283d28d713b93c5f02f9ccd6a102",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 831
      },
      {
        "segments": [
          {
            "segment_id": "b63ee464-5afc-483c-8c2f-7a39ed72074b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "system with very low latency, transactions finish more\r\nquickly, reducing the overlap between transactions and\r\nminimizing conflicts. We speculate that low-latency sys\u0002tems may be able to provide stronger consistency guar\u0002antees at much larger system scale.\r\nLow latency may also reduce the incast problems ex\u0002perienced by many applications. With high latency, ap\u0002plications are forced to issue concurrent data requests in\r\norder to meet user response deadlines. However, such\r\nrequests can lead to simultaneous responses, which can\r\ncause congestion near the client, overflow small switch\r\nbuffers, and result in packet loss. This scenario could be\r\navoided if sequential accesses were sufficiently fast that\r\nconcurrent requests are no longer needed.\r\n4 Low Latency is Within Reach\r\nSeveral recent developments are putting low latency net\u0002working within reach. These include the rise of the dat\u0002acenter, the next generation of Ethernet switching chips,\r\nand faster NICs. The HPC community has shown us that\r\nlow latency is possible using special-purpose intercon\u0002nects, but now we are on the brink of achieving the same\r\nwith commodity hardware.\r\nOne of the interesting properties of datacenters is that\r\nthey pack large amounts of computation and storage\r\nclose together. While the speed of light limits latency\r\nin wide area networks, electrons can traverse 100m of\r\ncopper cables and back in about 1µs. This means that\r\nmicrosecond-level RPCs are physically possible at very\r\nlarge scale, and the datacenter is the perfect environment\r\nto take advantage of low latency networking.\r\nNew cut-through switching chips designed for 10Gb\r\nEthernet are dramatically lowering the cost of both la\u0002tency and bandwidth. For example, switches from\r\nArista, which are based on chips from Fulcrum Mi\u0002crosystems, offer switching delays less than 1µs, which\r\nis more than an order of magnitude improvement over\r\nthe times in Table 2. The cost/port of these switches is\r\nstill high compared to commodity 1Gb switches, but will\r\ndrop rapidly over the next few years. Furthermore, the\r\nswitching chips should get a double benefit from Moore’s\r\nLaw: not only will they improve in latency but the num\u0002ber of ports will increase, which will reduce the number\r\nof switching levels required to traverse a datacenter.\r\nThe new switching chips also promise to make band\u0002width plentiful and cheap. Most existing datacenter net\u0002works cannot afford enough bandwidth in the switching\r\nfabric for all nodes to communicate randomly at full line\r\nrates. As a result, upper layers in the switching fabric\r\nare oversubscribed (the total bandwidth of the top layer\r\nis typically 100-500x less than the total bandwidth out of\r\nindividual servers). The resulting congestion can result\r\nin delays of tens of milliseconds as packets work their\r\nway through deep buffers. The new switching chips are\r\ncheap enough to make full bisection bandwidth afford\u0002able in datacenter networks, eliminating congestion and\r\nthe associated delays.\r\nNew network interface controllers from companies\r\nsuch as Mellanox are significantly reducing two other\r\nmajor sources of latency. First, they have been opti\u0002mized to reduce latency within the NIC itself (less than\r\n1µs in each direction). Second, they allow direct access\r\nfrom user space, which eliminates the overhead of pass\u0002ing through the kernel. These interfaces can be used in a\r\npolling mode, which also eliminates the latency associ\u0002ated with interrupt handling and context switching. Un\u0002fortunately most of these interfaces are designed for spe\u0002cialized interconnects such as Infiniband and Myrinet, so\r\nthey do not support the standard protocols and APIs ex\u0002pected by most applications.\r\nThe HPC community has already produced special\u0002ized systems that combine all the benefits above. Using\r\nInfiniband switches and NICs from Mellanox, we have\r\nmeasured round-trip times less than 5µs in small-scale\r\nnetworks with reliable delivery protocols analogous to\r\nTCP. HPC vendors have demonstrated that low latency\r\nis possible, and it seems likely that some of the tech\u0002niques used in HPC hardware will migrate to mainstream\r\nnetworking. Unfortunately, the interconnects, protocols,\r\nand APIs of these systems are very different from the\r\ncommodity Ethernet/IP/TCP approaches used in most\r\ndatacenters, so low latency is still beyond the reach of\r\nmost applications today.\r\nFuthermore, HPC approaches are unlikely to be\r\nadopted wholesale. First of all, Ethernet’s ubiquity, mar\u0002ket dominance, and economies of scale will make it dif\u0002ficult to compete with. Second, the HPC strategy has\r\nbeen to move functions to network interfaces to over\u0002come OS inefficiencies and allow more complex offload\u0002ing of functionality. This makes the NICs more expen\u0002sive, but more importantly, it makes the network less\r\nflexible. In contrast, Ethernet’s simplicity promotes in\u0002novation and makes for an excellent research vehicle.\r\n5 The OS Community’s Role\r\nUntil recently there was little reason for operating sys\u0002tems to worry about latency: external factors such as\r\nspeed-of-light propagation for long-haul networks and\r\nslow switches in datacenters overshadowed any ineffi\u0002ciencies in the operating system. However, the improve\u0002ments discussed in Section 4 are dramatically reducing\r\nthe external factors; within a few years the operating sys\u0002tem could become the largest remaining obstacle to low\r\nlatency RPCs in datacenters. Thus, it is now time to re\u0002think the role of the operating system in networking.\r\nAs a community, we should set a goal of making\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/b63ee464-5afc-483c-8c2f-7a39ed72074b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ef5d85590fc6bae97673f37a0e8a3adfbe7283d28d713b93c5f02f9ccd6a102",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 831
      },
      {
        "segments": [
          {
            "segment_id": "dc544046-dfc0-4b5f-9a91-0c570364342c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "5-10µs RPC times (end-to-end between applications)\r\neasily accessible to mainstream datacenter applications\r\nwithin a few years. This section describes some of the\r\nissues to address in order to achieve this goal. Section 6\r\nwill then argue that we can do even better and should set\r\na longer-term goal of 1µs round-trip times.\r\nThe first and most important task is to create a new\r\nsystem architecture for networking with a different di\u0002vision of responsibility between NIC hardware, operat\u0002ing system, and application. The operating system can\u0002not be in the loop for normal message exchanges: data\r\nmust pass directly between the application and the NIC.\r\nWe should think of network operations more like mem\u0002ory references and less like disk I/Os: in the same way\r\nthat the operating system sets up page tables and then\r\nlets memory references operate at hardware speeds, the\r\nOS should communicate with the NIC to establish map\u0002pings for packet demultiplexing (perhaps using mecha\u0002nisms like those defined for OpenFlow [14]), then get\r\nout of the way during normal processing. In addition,\r\nthe implementation of network protocols may need to be\r\nshared between the operating system and applications.\r\nThe new networking architecture must also be based\r\non a polling approach to communication, where threads\r\nremain on their CPUs while waiting for packets to ar\u0002rive; it makes no sense to switch contexts during an\r\nRPC when the RPC latency is comparable to the con\u0002text switch time. However, polling may not scale well\r\nas more and more applications begin to use it. For ex\u0002ample, how should the system behave if there were more\r\nthreads polling than there were cores/hyperthreads? It\r\nmay make sense to introduce new synchronization and\r\nscheduling mechanisms that combine polling with tradi\u0002tional context switching.\r\nAlthough future NICs may need to take on some ad\u0002ditional functions to enable direct application-level ac\u0002cess, in general we argue for onloading from the NIC.\r\nIn recent years some NIC vendors have attempted to of\u0002fload as much functionality as possible from the CPU to\r\nthe NIC, including significant portions of network proto\u0002cols, but we argue that this is the wrong approach. For\r\noptimal performance, operations should be carried out\r\non the fastest processor, which is the main CPU; cycles\r\nthere are now plentiful, thanks to increases in the number\r\nof cores. Functionality implemented in the NIC is also\r\nharder to change. The NIC should contain the minimum\r\nfeature set needed to move bits as efficiently as possible\r\nbetween the CPU and the network; all other functions\r\nshould be implemented in the main processor.\r\nAchieving low latency may also require the develop\u0002ment of new network protocols. Our measurements in\u0002dicate that current TCP implementations account for 25-\r\n50µs of latency in round-trip RPC times. Furthermore,\r\nTCP is currently optimized for large unidirectional flows\r\nSource of Delay Quantity, Rate R-trip Latency\r\nPropagation 50m, 5ns/m 250ns x 2\r\nTransmission 100B, 32Gb/s 25ns x 2\r\nSwitching 5 hops, 100ns/hop 500ns x 2\r\nTotal: 1.55µs\r\nTable 3: End-host processing aside, round-trip network laten\u0002cies as low as 1.55µs are currently possible for large datacen\u0002ters. While propagation delays will not improve, we can expect\r\ntransmission times to drop with higher bitrates, switching la\u0002tencies to fall, and hop counts to decrease with Moore’s Law.\r\nrather than small RPC-like exchanges; it is not designed\r\nto capitalize on new datacenter switching fabrics (e.g.,\r\nit behaves poorly if randomized routing is used to mini\u0002mize congestion); and it does not behave gracefully in the\r\nface of incast. We think a two-pronged approach makes\r\nsense, where one group of researchers attempts to opti\u0002mize TCP to minimize its latency and fix its other prob\u0002lems, while a second group makes a clean-slate design of\r\na new network protocol for small low-latency RPC ex\u0002changes within large datacenters. One of the advantages\r\nof datacenters is that they form their own closed ecosys\u0002tems: a new protocol can succeed within a datacenter\r\nwithout having to be implemented on every machine in\r\nthe Internet.\r\n6 Pushing the Envelope: Integrated NICs\r\n5-10µs round-trip latencies seem achievable within a few\r\nyears, but we believe it is possible to do much better in\r\nthe longer term. It appears technologically feasible to re\u0002duce datacenter RPC latency to 1µs before speed-of-light\r\ndelays limit further progress. However, 1µs round-trips\r\nwill require the integration of NIC functionality onto the\r\nmain CPU die.\r\nTable 3 breaks down network fabric latencies that are\r\nachievable today within a 50m diameter using Infiniband\r\nQDR switches with 100ns latency and 32Gbps effective\r\nline rates. Although propagation delay is significant, it\r\naccounts for less than half the total latency. We can ex\u0002pect significant improvements in the 68% of the time\r\nspent in transmission and switching delays. For instance,\r\n100Gb Ethernet is on the horizon, and Moore’s Law will\r\nenable higher switch port densities, which will reduce\r\nthe total number of hops. In a future scenario with 30ns\r\nswitch latencies, 8 hops per round-trip, and 100Gbps line\r\nrates, round-trip fabric latency could halve to 750ns.\r\nThe next major challenge in reducing latency is to\r\neliminate latency on the motherboard. Unfortunately,\r\nusing off-processor NIC chips introduces significant de\u0002lays. In order to move data from the CPU to the network,\r\nthe CPU must flush the data to memory and then the NIC\r\nmust read the data from memory. Each of these trans\u0002fers introduces around 100ns of delay, and for most NICs\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/dc544046-dfc0-4b5f-9a91-0c570364342c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0892b5ca652d7908f2963c6946152977411db72c764c6d676463ab3039291e3f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 879
      },
      {
        "segments": [
          {
            "segment_id": "dc544046-dfc0-4b5f-9a91-0c570364342c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "5-10µs RPC times (end-to-end between applications)\r\neasily accessible to mainstream datacenter applications\r\nwithin a few years. This section describes some of the\r\nissues to address in order to achieve this goal. Section 6\r\nwill then argue that we can do even better and should set\r\na longer-term goal of 1µs round-trip times.\r\nThe first and most important task is to create a new\r\nsystem architecture for networking with a different di\u0002vision of responsibility between NIC hardware, operat\u0002ing system, and application. The operating system can\u0002not be in the loop for normal message exchanges: data\r\nmust pass directly between the application and the NIC.\r\nWe should think of network operations more like mem\u0002ory references and less like disk I/Os: in the same way\r\nthat the operating system sets up page tables and then\r\nlets memory references operate at hardware speeds, the\r\nOS should communicate with the NIC to establish map\u0002pings for packet demultiplexing (perhaps using mecha\u0002nisms like those defined for OpenFlow [14]), then get\r\nout of the way during normal processing. In addition,\r\nthe implementation of network protocols may need to be\r\nshared between the operating system and applications.\r\nThe new networking architecture must also be based\r\non a polling approach to communication, where threads\r\nremain on their CPUs while waiting for packets to ar\u0002rive; it makes no sense to switch contexts during an\r\nRPC when the RPC latency is comparable to the con\u0002text switch time. However, polling may not scale well\r\nas more and more applications begin to use it. For ex\u0002ample, how should the system behave if there were more\r\nthreads polling than there were cores/hyperthreads? It\r\nmay make sense to introduce new synchronization and\r\nscheduling mechanisms that combine polling with tradi\u0002tional context switching.\r\nAlthough future NICs may need to take on some ad\u0002ditional functions to enable direct application-level ac\u0002cess, in general we argue for onloading from the NIC.\r\nIn recent years some NIC vendors have attempted to of\u0002fload as much functionality as possible from the CPU to\r\nthe NIC, including significant portions of network proto\u0002cols, but we argue that this is the wrong approach. For\r\noptimal performance, operations should be carried out\r\non the fastest processor, which is the main CPU; cycles\r\nthere are now plentiful, thanks to increases in the number\r\nof cores. Functionality implemented in the NIC is also\r\nharder to change. The NIC should contain the minimum\r\nfeature set needed to move bits as efficiently as possible\r\nbetween the CPU and the network; all other functions\r\nshould be implemented in the main processor.\r\nAchieving low latency may also require the develop\u0002ment of new network protocols. Our measurements in\u0002dicate that current TCP implementations account for 25-\r\n50µs of latency in round-trip RPC times. Furthermore,\r\nTCP is currently optimized for large unidirectional flows\r\nSource of Delay Quantity, Rate R-trip Latency\r\nPropagation 50m, 5ns/m 250ns x 2\r\nTransmission 100B, 32Gb/s 25ns x 2\r\nSwitching 5 hops, 100ns/hop 500ns x 2\r\nTotal: 1.55µs\r\nTable 3: End-host processing aside, round-trip network laten\u0002cies as low as 1.55µs are currently possible for large datacen\u0002ters. While propagation delays will not improve, we can expect\r\ntransmission times to drop with higher bitrates, switching la\u0002tencies to fall, and hop counts to decrease with Moore’s Law.\r\nrather than small RPC-like exchanges; it is not designed\r\nto capitalize on new datacenter switching fabrics (e.g.,\r\nit behaves poorly if randomized routing is used to mini\u0002mize congestion); and it does not behave gracefully in the\r\nface of incast. We think a two-pronged approach makes\r\nsense, where one group of researchers attempts to opti\u0002mize TCP to minimize its latency and fix its other prob\u0002lems, while a second group makes a clean-slate design of\r\na new network protocol for small low-latency RPC ex\u0002changes within large datacenters. One of the advantages\r\nof datacenters is that they form their own closed ecosys\u0002tems: a new protocol can succeed within a datacenter\r\nwithout having to be implemented on every machine in\r\nthe Internet.\r\n6 Pushing the Envelope: Integrated NICs\r\n5-10µs round-trip latencies seem achievable within a few\r\nyears, but we believe it is possible to do much better in\r\nthe longer term. It appears technologically feasible to re\u0002duce datacenter RPC latency to 1µs before speed-of-light\r\ndelays limit further progress. However, 1µs round-trips\r\nwill require the integration of NIC functionality onto the\r\nmain CPU die.\r\nTable 3 breaks down network fabric latencies that are\r\nachievable today within a 50m diameter using Infiniband\r\nQDR switches with 100ns latency and 32Gbps effective\r\nline rates. Although propagation delay is significant, it\r\naccounts for less than half the total latency. We can ex\u0002pect significant improvements in the 68% of the time\r\nspent in transmission and switching delays. For instance,\r\n100Gb Ethernet is on the horizon, and Moore’s Law will\r\nenable higher switch port densities, which will reduce\r\nthe total number of hops. In a future scenario with 30ns\r\nswitch latencies, 8 hops per round-trip, and 100Gbps line\r\nrates, round-trip fabric latency could halve to 750ns.\r\nThe next major challenge in reducing latency is to\r\neliminate latency on the motherboard. Unfortunately,\r\nusing off-processor NIC chips introduces significant de\u0002lays. In order to move data from the CPU to the network,\r\nthe CPU must flush the data to memory and then the NIC\r\nmust read the data from memory. Each of these trans\u0002fers introduces around 100ns of delay, and for most NICs\r\n4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/dc544046-dfc0-4b5f-9a91-0c570364342c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0892b5ca652d7908f2963c6946152977411db72c764c6d676463ab3039291e3f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 879
      },
      {
        "segments": [
          {
            "segment_id": "b8f4d5c7-d22d-4f73-bf34-f80c1ee4ed6f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "multiple memory operations are required (e.g., not only\r\nmust the packet be stored in memory, but separate ring\r\nbuffer pointers must also be manipulated). Each RPC re\u0002quires data to pass through NIC chips four times for a\r\ntotal delay of at least 1-2µs. If any direct manipulation\r\nof NIC device registers over PCIe is required, such as\r\npolling a device register for incoming data, it adds hun\u0002dreds more nanoseconds of latency.\r\nAs a result, 1µs round-trip times cannot be achieved\r\nwith off-processor NICs. Moreover, there must not be\r\nany memory accesses in the fast path: information must\r\nmove directly between on-chip caches and the network.\r\nThis will require the integration of NIC functionality\r\nonto the main processor chip. Although we realize that\r\nsuch a change will not happen overnight, we argue that\r\nfast network communication is as important for large\u0002scale datacenter applications as fast floating-point arith\u0002metic is for scientific applications and that integrating\r\nNIC functionality should be a top priority for the pro\u0002cessor design community. Using chip real estate for an\r\nintegrated NIC is likely to improve overall system perfor\u0002mance more than adding cores that software developers\r\ndo not know how to utilize.\r\nWe urge everyone in the OS community to apply pres\u0002sure on hardware architects for integrated NICs, and we\r\nbelieve the OS community should drive the architecture\r\nfor on-chip networking in order to ensure the best distri\u0002bution of functionality between hardware, OS, and appli\u0002cation. Processor designers are already putting intercon\u0002nection networks on-die for core-to-core communication\r\n– we need to help them think bigger. If we can make the\r\nleap to on-chip NICs, 1µs round-trip times could become\r\nwidely available within ten years.\r\n7 Conclusion\r\nWe are on the cusp of a two-order-of-magnitude im\u0002provement in the latency of RPC communication. It is\r\ntime for the operating systems community to implement\r\na new networking architecture and new protocols that\r\nsolve the latency problem end-to-end and make fast net\u0002working easily available to applications. If we can do\r\nthis, we will not only simplify the development of cur\u0002rent applications but also enable new kinds of applica\u0002tions that manipulate large-scale datasets in ways never\r\nbefore imaginable.\r\n8 Acknowledgements\r\nThis work was supported in part by the Gigascale Sys\u0002tems Research Center and the Multiscale Systems Cen\u0002ter, two of six research centers funded under the Focus\r\nCenter Research Program, a Semiconductor Research\r\nCorporation program, and by gifts from SAP, NetApp,\r\nFacebook, and Mellanox. This work was also supported\r\nin part by an NSERC Post Graduate Scholarship. Nandu\r\nJayakumar provided comments that improved the pre\u0002sentation of the paper.\r\nReferences\r\n[1] memcached: a distributed memory object caching system, Jan.\r\n2011. http://www.memcached.org/.\r\n[2] BECHTOLSHEIM, A. The SUN workstation architecture. Tech.\r\nrep., Stanford, CA, USA, 1982.\r\n[3] BRANTS, T., POPAT, A. C., XU, P., OCH, F. J., AND DEAN,\r\nJ. Large language models in machine translation. In Proceed\u0002ings of the 2007 Joint Conference on Empirical Methods in Nat\u0002ural Language Processing and Computational Natural Language\r\nLearning (2007), EMNLP-CoNLL ’07, pp. 858–867.\r\n[4] CHANG, F., DEAN, J., GHEMAWAT, S., HS IEH, W. C., WAL\u0002LACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND\r\nGRUBER, R. E. Bigtable: a distributed storage system for struc\u0002tured data. In Proceedings of the 7th USENIX Symposium on Op\u0002erating Systems Design and Implementation - Volume 7 (Berke\u0002ley, CA, USA, 2006), USENIX Association, pp. 15–15.\r\n[5] CHERITON, D. R., AND ZWAENEPOEL, W. The distributed v\r\nkernel and its performance for diskless workstations. In Proceed\u0002ings of the ninth ACM symposium on Operating systems princi\u0002ples (New York, NY, USA, 1983), SOSP ’83, ACM, pp. 129–140.\r\n[6] CHIOLA, G., AND CIACCIO, G. Gamma: a low-cost network\r\nof workstations based on active messages. In In Proc. Euromicro\r\nPDP’97 (1997), IEEE Computer Society.\r\n[7] CHUN, B. N., MAINWARING, A. M., AND CULLER, D. E. Vir\u0002tual network transport protocols for myrinet. IEEE Micro 18\r\n(1998), 53–63.\r\n[8] COOPER, B. F., RAMAKRISHNAN, R., SRIVASTAVA, U., SIL\u0002BERSTEIN, A., BOHANNON, P., JACOBSEN, H.-A., PUZ, N.,\r\nWEAVER, D., AND YERNENI, R. Pnuts: Yahoo!’s hosted data\r\nserving platform. Proc. VLDB Endow. 1 (August 2008), 1277–\r\n1288.\r\n[9] DEAN, J. Keynote talk: Designs, lessons and advice from build\u0002ing large distributed systems. In The 3rd ACM SIGOPS Interna\u0002tional Workshop on Large Scale Distributed Systems and Middle\u0002ware (October 2009).\r\n[10] DEAN, J., AND GHEMAWAT, S. Mapreduce: simplified data\r\nprocessing on large clusters. Commun. ACM 51 (January 2008),\r\n107–113.\r\n[11] DECANDIA, G., HASTORUN, D., JAMPANI, M., KAKULAPATI,\r\nG., LAKSHMAN, A., PILCHIN, A., SIVASUBRAMANIAN, S.,\r\nVOSSHALL, P., AND VOGELS, W. Dynamo: amazon’s highly\r\navailable key-value store. In Proceedings of twenty-first ACM\r\nSIGOPS symposium on Operating systems principles (New York,\r\nNY, USA, 2007), SOSP ’07, ACM, pp. 205–220.\r\n[12] DITTIA, Z. Integrated hardware/software design of a high per\u0002formance network interface. PhD thesis, St. Louis, MO, USA,\r\n2001. AAI3016230.\r\n[13] JOHNSON, R., AND ROTHSCHILD, J. Personal Communica\u0002tions, March 24 and August 20, 2009.\r\n[14] MCKEOWN, N., ANDERSON, T., BALAKRISHNAN, H.,\r\nPARULKAR, G., PETERSON, L., REXFORD, J., SHENKER, S.,\r\nAND TURNER, J. Openflow: enabling innovation in campus net\u0002works. SIGCOMM Comput. Commun. Rev. 38 (March 2008),\r\n69–74.\r\n[15] VON EICKEN, T., BASU, A., BUCH, V., AND VOGELS, W. U\u0002net: a user-level network interface for parallel and distributed\r\ncomputing. In Proceedings of the fifteenth ACM symposium on\r\nOperating systems principles (New York, NY, USA, 1995), SOSP\r\n’95, ACM, pp. 40–53.\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/b8f4d5c7-d22d-4f73-bf34-f80c1ee4ed6f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e307ac3c8d3c3979aba723fd4dbbb0676fb232ac73b5aa1d956fdc88b2d36a37",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 872
      },
      {
        "segments": [
          {
            "segment_id": "b8f4d5c7-d22d-4f73-bf34-f80c1ee4ed6f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "multiple memory operations are required (e.g., not only\r\nmust the packet be stored in memory, but separate ring\r\nbuffer pointers must also be manipulated). Each RPC re\u0002quires data to pass through NIC chips four times for a\r\ntotal delay of at least 1-2µs. If any direct manipulation\r\nof NIC device registers over PCIe is required, such as\r\npolling a device register for incoming data, it adds hun\u0002dreds more nanoseconds of latency.\r\nAs a result, 1µs round-trip times cannot be achieved\r\nwith off-processor NICs. Moreover, there must not be\r\nany memory accesses in the fast path: information must\r\nmove directly between on-chip caches and the network.\r\nThis will require the integration of NIC functionality\r\nonto the main processor chip. Although we realize that\r\nsuch a change will not happen overnight, we argue that\r\nfast network communication is as important for large\u0002scale datacenter applications as fast floating-point arith\u0002metic is for scientific applications and that integrating\r\nNIC functionality should be a top priority for the pro\u0002cessor design community. Using chip real estate for an\r\nintegrated NIC is likely to improve overall system perfor\u0002mance more than adding cores that software developers\r\ndo not know how to utilize.\r\nWe urge everyone in the OS community to apply pres\u0002sure on hardware architects for integrated NICs, and we\r\nbelieve the OS community should drive the architecture\r\nfor on-chip networking in order to ensure the best distri\u0002bution of functionality between hardware, OS, and appli\u0002cation. Processor designers are already putting intercon\u0002nection networks on-die for core-to-core communication\r\n– we need to help them think bigger. If we can make the\r\nleap to on-chip NICs, 1µs round-trip times could become\r\nwidely available within ten years.\r\n7 Conclusion\r\nWe are on the cusp of a two-order-of-magnitude im\u0002provement in the latency of RPC communication. It is\r\ntime for the operating systems community to implement\r\na new networking architecture and new protocols that\r\nsolve the latency problem end-to-end and make fast net\u0002working easily available to applications. If we can do\r\nthis, we will not only simplify the development of cur\u0002rent applications but also enable new kinds of applica\u0002tions that manipulate large-scale datasets in ways never\r\nbefore imaginable.\r\n8 Acknowledgements\r\nThis work was supported in part by the Gigascale Sys\u0002tems Research Center and the Multiscale Systems Cen\u0002ter, two of six research centers funded under the Focus\r\nCenter Research Program, a Semiconductor Research\r\nCorporation program, and by gifts from SAP, NetApp,\r\nFacebook, and Mellanox. This work was also supported\r\nin part by an NSERC Post Graduate Scholarship. Nandu\r\nJayakumar provided comments that improved the pre\u0002sentation of the paper.\r\nReferences\r\n[1] memcached: a distributed memory object caching system, Jan.\r\n2011. http://www.memcached.org/.\r\n[2] BECHTOLSHEIM, A. The SUN workstation architecture. Tech.\r\nrep., Stanford, CA, USA, 1982.\r\n[3] BRANTS, T., POPAT, A. C., XU, P., OCH, F. J., AND DEAN,\r\nJ. Large language models in machine translation. In Proceed\u0002ings of the 2007 Joint Conference on Empirical Methods in Nat\u0002ural Language Processing and Computational Natural Language\r\nLearning (2007), EMNLP-CoNLL ’07, pp. 858–867.\r\n[4] CHANG, F., DEAN, J., GHEMAWAT, S., HS IEH, W. C., WAL\u0002LACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND\r\nGRUBER, R. E. Bigtable: a distributed storage system for struc\u0002tured data. In Proceedings of the 7th USENIX Symposium on Op\u0002erating Systems Design and Implementation - Volume 7 (Berke\u0002ley, CA, USA, 2006), USENIX Association, pp. 15–15.\r\n[5] CHERITON, D. R., AND ZWAENEPOEL, W. The distributed v\r\nkernel and its performance for diskless workstations. In Proceed\u0002ings of the ninth ACM symposium on Operating systems princi\u0002ples (New York, NY, USA, 1983), SOSP ’83, ACM, pp. 129–140.\r\n[6] CHIOLA, G., AND CIACCIO, G. Gamma: a low-cost network\r\nof workstations based on active messages. In In Proc. Euromicro\r\nPDP’97 (1997), IEEE Computer Society.\r\n[7] CHUN, B. N., MAINWARING, A. M., AND CULLER, D. E. Vir\u0002tual network transport protocols for myrinet. IEEE Micro 18\r\n(1998), 53–63.\r\n[8] COOPER, B. F., RAMAKRISHNAN, R., SRIVASTAVA, U., SIL\u0002BERSTEIN, A., BOHANNON, P., JACOBSEN, H.-A., PUZ, N.,\r\nWEAVER, D., AND YERNENI, R. Pnuts: Yahoo!’s hosted data\r\nserving platform. Proc. VLDB Endow. 1 (August 2008), 1277–\r\n1288.\r\n[9] DEAN, J. Keynote talk: Designs, lessons and advice from build\u0002ing large distributed systems. In The 3rd ACM SIGOPS Interna\u0002tional Workshop on Large Scale Distributed Systems and Middle\u0002ware (October 2009).\r\n[10] DEAN, J., AND GHEMAWAT, S. Mapreduce: simplified data\r\nprocessing on large clusters. Commun. ACM 51 (January 2008),\r\n107–113.\r\n[11] DECANDIA, G., HASTORUN, D., JAMPANI, M., KAKULAPATI,\r\nG., LAKSHMAN, A., PILCHIN, A., SIVASUBRAMANIAN, S.,\r\nVOSSHALL, P., AND VOGELS, W. Dynamo: amazon’s highly\r\navailable key-value store. In Proceedings of twenty-first ACM\r\nSIGOPS symposium on Operating systems principles (New York,\r\nNY, USA, 2007), SOSP ’07, ACM, pp. 205–220.\r\n[12] DITTIA, Z. Integrated hardware/software design of a high per\u0002formance network interface. PhD thesis, St. Louis, MO, USA,\r\n2001. AAI3016230.\r\n[13] JOHNSON, R., AND ROTHSCHILD, J. Personal Communica\u0002tions, March 24 and August 20, 2009.\r\n[14] MCKEOWN, N., ANDERSON, T., BALAKRISHNAN, H.,\r\nPARULKAR, G., PETERSON, L., REXFORD, J., SHENKER, S.,\r\nAND TURNER, J. Openflow: enabling innovation in campus net\u0002works. SIGCOMM Comput. Commun. Rev. 38 (March 2008),\r\n69–74.\r\n[15] VON EICKEN, T., BASU, A., BUCH, V., AND VOGELS, W. U\u0002net: a user-level network interface for parallel and distributed\r\ncomputing. In Proceedings of the fifteenth ACM symposium on\r\nOperating systems principles (New York, NY, USA, 1995), SOSP\r\n’95, ACM, pp. 40–53.\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/5a84d437-ed96-444f-8254-19917e8d16bc/images/b8f4d5c7-d22d-4f73-bf34-f80c1ee4ed6f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041952Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e307ac3c8d3c3979aba723fd4dbbb0676fb232ac73b5aa1d956fdc88b2d36a37",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 872
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "It's Time for Low Latency\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Stephen M. Rumble, Diego Ongaro, Ryan Stutsman, Mendel Rosenblum, and John K. Ousterhout\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "I am unable to extract the publication date of this document as it is not mentioned in the text provided."
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "Stanford University\n"
        }
      ]
    }
  }
}