{
  "file_name": "Magazines and Vmem- Extending the Slab Allocator to Many CPUs and Arbitrary Resources.pdf",
  "task_id": "bf84d87b-1de9-435a-8297-0beb924e3c21",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "0f55c401-728d-410a-af92-fbb1bd774d15",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "USENIX Association\r\nProceedings of the\r\n2001 USENIX Annual\r\nTechnical Conference\r\nBoston, Massachusetts, USA\r\nJune 25–30, 2001\r\nTHE ADVANCED COMPUTING SYSTEMS ASSOCIATION\r\n© 2001 by The USENIX Association All Rights Reserved For more information about the USENIX Association:\r\nPhone: 1 510 528 8649 FAX: 1 510 548 5738 Email: office@usenix.org WWW: http://www.usenix.org\r\nRights to individual papers remain with the author or the author's employer.\r\n Permission is granted for noncommercial reproduction of the work for educational or research purposes.\r\nThis copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/0f55c401-728d-410a-af92-fbb1bd774d15.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ebe70d3e2be2c1fc12a9665ca85a134a8eace744aaaa24c929daf85728ba3c51",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 92
      },
      {
        "segments": [
          {
            "segment_id": "aa100314-1c4d-4a32-884a-5fa87b452a43",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": " \r\n1. Introduction\r\nThe slab allocator [Bonwick94] has taken on a life of\r\nits own since its introduction in these pages seven\r\nyears ago. Initially deployed in Solaris 2.4, it has\r\nsince been adopted in whole or in part by several other\r\noperating systems including Linux, FreeBSD,\r\nNetBSD, OpenBSD, EROS, and Nemesis. It has also\r\nbeen adapted to applications such as BIRD and Perl.\r\nSlab allocation is now described in several OS\r\ntextbooks [Bovet00, Mauro00, Vahalia96] and is part\r\nof the curriculum at major universities worldwide.\r\nMeanwhile, the Solaris slab allocator has continued to\r\nevolve. It now provides per−CPU memory allocation,\r\nmore general resource allocation, and is available as a\r\nuser−level library. We describe these developments in\r\nseven sections as follows:\r\n§2. Slab Allocator Review. We begin with brief\r\nreview of the original slab allocator.\r\n§3. Magazines: Per−CPU Memory Allocation. As\r\nservers with many CPUs became more common and\r\nmemory latencies continued to grow relative to\r\nprocessor speed, the slab allocator’s original locking\r\nstrategy became a performance bottleneck. We\r\naddressed this by introducing a per−CPU caching\r\nscheme called the magazine layer.\r\n§4. Vmem: Fast, General Resource Allocation. The\r\nslab allocator caches relatively small objects and relies\r\non a more general−purpose backing store to provide\r\nslabs and satisfy large allocations. We describe a new\r\nresource allocator, vmem, that can manage arbitrary\r\nsets of integers − anything from virtual memory\r\naddresses to minor device numbers to process IDs.\r\nVmem acts as a universal backing store for the slab\r\nallocator, and provides powerful new interfaces to\r\naddress more complex resource allocation problems.\r\nVmem appears to be the first resource allocator that\r\ncan satisfy allocations and frees of any size in\r\nguaranteed constant time.\r\n§5. Vmem−Related Slab Allocator Improvements.\r\nWe describe two key improvements to the slab\r\nallocator itself: it now provides object caching for any\r\nvmem arena, and can issue reclaim callbacks to notify\r\nclients when the arena’s resources are running low.\r\n§6. libumem: A User−Level Slab Allocator. We\r\ndescribe what was necessary to transplant the slab\r\nallocator from kernel to user context, and show that\r\nthe resulting libumem outperforms even the current\r\nbest−of−breed multithreaded user−level allocators.\r\n§7. Conclusions. We conclude with some observa−\r\ntions about how these technologies have influenced\r\nSolaris development in general.\r\nMagazines and Vmem:\r\nExtending the Slab Allocator to Many CPUs and Arbitrary Resources\r\nJeff Bonwick, Sun Microsystems\r\nJonathan Adams, California Institute of Technology\r\nAbstract\r\nThe slab allocator [Bonwick94] provides efficient object caching but has two significant\r\nlimitations: its global locking doesn’t scale to many CPUs, and the allocator can’t manage\r\nresources other than kernel memory. To provide scalability we introduce a per−processor\r\ncaching scheme called the magazine layer that provides linear scaling to any number of\r\nCPUs. To support more general resource allocation we introduce a new virtual memory\r\nallocator, vmem, which acts as a universal backing store for the slab allocator. Vmem is a\r\ncomplete general−purpose resource allocator in its own right, providing several important\r\nnew services; it also appears to be the first resource allocator that can satisfy arbitrary−size\r\nallocations in constant time. Magazines and vmem have yielded performance gains\r\nexceeding 50% on system−level benchmarks like LADDIS and SPECweb99.\r\nWe ported these technologies from kernel to user context and found that the resulting\r\nlibumem outperforms the current best−of−breed user−level memory allocators. libumem also\r\nprovides a richer programming model and can be used to manage other user−level resources.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/aa100314-1c4d-4a32-884a-5fa87b452a43.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a3ff12e64b346276aaa4463d0a60152cca6485ca1369eba2b89e143c7306eb28",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "aa100314-1c4d-4a32-884a-5fa87b452a43",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": " \r\n1. Introduction\r\nThe slab allocator [Bonwick94] has taken on a life of\r\nits own since its introduction in these pages seven\r\nyears ago. Initially deployed in Solaris 2.4, it has\r\nsince been adopted in whole or in part by several other\r\noperating systems including Linux, FreeBSD,\r\nNetBSD, OpenBSD, EROS, and Nemesis. It has also\r\nbeen adapted to applications such as BIRD and Perl.\r\nSlab allocation is now described in several OS\r\ntextbooks [Bovet00, Mauro00, Vahalia96] and is part\r\nof the curriculum at major universities worldwide.\r\nMeanwhile, the Solaris slab allocator has continued to\r\nevolve. It now provides per−CPU memory allocation,\r\nmore general resource allocation, and is available as a\r\nuser−level library. We describe these developments in\r\nseven sections as follows:\r\n§2. Slab Allocator Review. We begin with brief\r\nreview of the original slab allocator.\r\n§3. Magazines: Per−CPU Memory Allocation. As\r\nservers with many CPUs became more common and\r\nmemory latencies continued to grow relative to\r\nprocessor speed, the slab allocator’s original locking\r\nstrategy became a performance bottleneck. We\r\naddressed this by introducing a per−CPU caching\r\nscheme called the magazine layer.\r\n§4. Vmem: Fast, General Resource Allocation. The\r\nslab allocator caches relatively small objects and relies\r\non a more general−purpose backing store to provide\r\nslabs and satisfy large allocations. We describe a new\r\nresource allocator, vmem, that can manage arbitrary\r\nsets of integers − anything from virtual memory\r\naddresses to minor device numbers to process IDs.\r\nVmem acts as a universal backing store for the slab\r\nallocator, and provides powerful new interfaces to\r\naddress more complex resource allocation problems.\r\nVmem appears to be the first resource allocator that\r\ncan satisfy allocations and frees of any size in\r\nguaranteed constant time.\r\n§5. Vmem−Related Slab Allocator Improvements.\r\nWe describe two key improvements to the slab\r\nallocator itself: it now provides object caching for any\r\nvmem arena, and can issue reclaim callbacks to notify\r\nclients when the arena’s resources are running low.\r\n§6. libumem: A User−Level Slab Allocator. We\r\ndescribe what was necessary to transplant the slab\r\nallocator from kernel to user context, and show that\r\nthe resulting libumem outperforms even the current\r\nbest−of−breed multithreaded user−level allocators.\r\n§7. Conclusions. We conclude with some observa−\r\ntions about how these technologies have influenced\r\nSolaris development in general.\r\nMagazines and Vmem:\r\nExtending the Slab Allocator to Many CPUs and Arbitrary Resources\r\nJeff Bonwick, Sun Microsystems\r\nJonathan Adams, California Institute of Technology\r\nAbstract\r\nThe slab allocator [Bonwick94] provides efficient object caching but has two significant\r\nlimitations: its global locking doesn’t scale to many CPUs, and the allocator can’t manage\r\nresources other than kernel memory. To provide scalability we introduce a per−processor\r\ncaching scheme called the magazine layer that provides linear scaling to any number of\r\nCPUs. To support more general resource allocation we introduce a new virtual memory\r\nallocator, vmem, which acts as a universal backing store for the slab allocator. Vmem is a\r\ncomplete general−purpose resource allocator in its own right, providing several important\r\nnew services; it also appears to be the first resource allocator that can satisfy arbitrary−size\r\nallocations in constant time. Magazines and vmem have yielded performance gains\r\nexceeding 50% on system−level benchmarks like LADDIS and SPECweb99.\r\nWe ported these technologies from kernel to user context and found that the resulting\r\nlibumem outperforms the current best−of−breed user−level memory allocators. libumem also\r\nprovides a richer programming model and can be used to manage other user−level resources.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/aa100314-1c4d-4a32-884a-5fa87b452a43.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a3ff12e64b346276aaa4463d0a60152cca6485ca1369eba2b89e143c7306eb28",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "e9b5e9cb-fe72-4674-822c-f85df4a60f38",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "2. Slab Allocator Review\r\n2.1. Object Caches\r\nPrograms often cache their frequently used objects to\r\nimprove performance. If a program frequently\r\nallocates and frees foo structures, it is likely to\r\nemploy highly optimized foo_alloc() and\r\nfoo_free() routines to “avoid the overhead of\r\nmalloc.” The usual strategy is to cache foo objects on\r\na simple freelist so that most allocations and frees take\r\njust a handful of instructions. Further optimization is\r\npossible if foo objects naturally return to a partially\r\ninitialized state before they’re freed, in which case\r\nfoo_alloc() can assume that an object on the\r\nfreelist is already partially initialized.\r\nWe refer to the techniques described above as object\r\ncaching. Traditional malloc implementations cannot\r\nprovide object caching because the malloc/free\r\ninterface is typeless, so the slab allocator introduced\r\nan explicit object cache programming model with\r\ninterfaces to create and destroy object caches, and\r\nallocate and free objects from them (see Figure 2.1).\r\nThe allocator and its clients cooperate to maintain an\r\nobject’s partially initialized, or constructed, state. The\r\nallocator guarantees that an object will be in this state\r\nwhen allocated; the client guarantees that it will be in\r\nthis state when freed. Thus, we can allocate and free\r\nan object many times without destroying and\r\nreinitializing its locks, condition variables, reference\r\ncounts, and other invariant state each time.\r\n2.2. Slabs\r\nA slab is one or more pages of virtually contiguous\r\nmemory, carved up into equal−size chunks, with a\r\nreference count indicating how many of those chunks\r\nare currently allocated. To create new objects the\r\nallocator creates a slab, applies the constructor to each\r\nchunk, and adds the resulting objects to the cache. If\r\nsystem memory runs low the allocator can reclaim any\r\nslabs whose reference count is zero by applying the\r\ndestructor to each object and returning memory to the\r\nVM system. Once a cache is populated, allocations\r\nand frees are very fast: they just move an object to or\r\nfrom a freelist and update its slab reference count.\r\nFigure 2.1: Slab Allocator Interface Summary\r\nkmem_cache_t *kmem_cache_create(\r\nchar *name, /* descriptive name for this cache */\r\nsize_t size, /* size of the objects it manages */\r\nsize_t align, /* minimum object alignment */\r\nint (*constructor)(void *obj, void *private, int kmflag),\r\nvoid (*destructor)(void *obj, void *private),\r\nvoid (*reclaim)(void *private), /* memory reclaim callback */\r\nvoid *private, /* argument to the above callbacks */\r\nvmem_t *vmp, /* vmem source for slab creation */\r\nint cflags); /* cache creation flags */\r\nCreates a cache of objects, each of size size, aligned on an align boundary. name identifies the cache\r\nfor statistics and debugging. constructor and destructor convert plain memory into objects and\r\nback again; constructor may fail if it needs to allocate memory but can’t. reclaim is a callback\r\nissued by the allocator when system−wide resources are running low (see §5.2). private is a\r\nparameter passed to the constructor, destructor and reclaim callbacks to support parameterized\r\ncaches (e.g. a separate packet cache for each instance of a SCSI HBA driver). vmp is the vmem source\r\nthat provides memory to create slabs (see §4 and §5.1). cflags indicates special cache properties.\r\nkmem_cache_create() returns an opaque pointer to the object cache (a.k.a. kmem cache).\r\nvoid kmem_cache_destroy(kmem_cache_t *cp);\r\nDestroys the cache and releases all associated resources. All allocated objects must have been freed.\r\nvoid *kmem_cache_alloc(kmem_cache_t *cp, int kmflag);\r\nGets an object from the cache. The object will be in its constructed state. kmflag is either KM_SLEEP\r\nor KM_NOSLEEP, indicating whether it’s acceptable to wait for memory if none is currently available. \r\nvoid kmem_cache_free(kmem_cache_t *cp, void *obj);\r\nReturns an object to the cache. The object must be in its constructed state.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/e9b5e9cb-fe72-4674-822c-f85df4a60f38.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=38d18bad47a252ba86913d4193ccfd5dd25884c0e1e3a1c5857b08777ad89e1c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 604
      },
      {
        "segments": [
          {
            "segment_id": "e9b5e9cb-fe72-4674-822c-f85df4a60f38",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "2. Slab Allocator Review\r\n2.1. Object Caches\r\nPrograms often cache their frequently used objects to\r\nimprove performance. If a program frequently\r\nallocates and frees foo structures, it is likely to\r\nemploy highly optimized foo_alloc() and\r\nfoo_free() routines to “avoid the overhead of\r\nmalloc.” The usual strategy is to cache foo objects on\r\na simple freelist so that most allocations and frees take\r\njust a handful of instructions. Further optimization is\r\npossible if foo objects naturally return to a partially\r\ninitialized state before they’re freed, in which case\r\nfoo_alloc() can assume that an object on the\r\nfreelist is already partially initialized.\r\nWe refer to the techniques described above as object\r\ncaching. Traditional malloc implementations cannot\r\nprovide object caching because the malloc/free\r\ninterface is typeless, so the slab allocator introduced\r\nan explicit object cache programming model with\r\ninterfaces to create and destroy object caches, and\r\nallocate and free objects from them (see Figure 2.1).\r\nThe allocator and its clients cooperate to maintain an\r\nobject’s partially initialized, or constructed, state. The\r\nallocator guarantees that an object will be in this state\r\nwhen allocated; the client guarantees that it will be in\r\nthis state when freed. Thus, we can allocate and free\r\nan object many times without destroying and\r\nreinitializing its locks, condition variables, reference\r\ncounts, and other invariant state each time.\r\n2.2. Slabs\r\nA slab is one or more pages of virtually contiguous\r\nmemory, carved up into equal−size chunks, with a\r\nreference count indicating how many of those chunks\r\nare currently allocated. To create new objects the\r\nallocator creates a slab, applies the constructor to each\r\nchunk, and adds the resulting objects to the cache. If\r\nsystem memory runs low the allocator can reclaim any\r\nslabs whose reference count is zero by applying the\r\ndestructor to each object and returning memory to the\r\nVM system. Once a cache is populated, allocations\r\nand frees are very fast: they just move an object to or\r\nfrom a freelist and update its slab reference count.\r\nFigure 2.1: Slab Allocator Interface Summary\r\nkmem_cache_t *kmem_cache_create(\r\nchar *name, /* descriptive name for this cache */\r\nsize_t size, /* size of the objects it manages */\r\nsize_t align, /* minimum object alignment */\r\nint (*constructor)(void *obj, void *private, int kmflag),\r\nvoid (*destructor)(void *obj, void *private),\r\nvoid (*reclaim)(void *private), /* memory reclaim callback */\r\nvoid *private, /* argument to the above callbacks */\r\nvmem_t *vmp, /* vmem source for slab creation */\r\nint cflags); /* cache creation flags */\r\nCreates a cache of objects, each of size size, aligned on an align boundary. name identifies the cache\r\nfor statistics and debugging. constructor and destructor convert plain memory into objects and\r\nback again; constructor may fail if it needs to allocate memory but can’t. reclaim is a callback\r\nissued by the allocator when system−wide resources are running low (see §5.2). private is a\r\nparameter passed to the constructor, destructor and reclaim callbacks to support parameterized\r\ncaches (e.g. a separate packet cache for each instance of a SCSI HBA driver). vmp is the vmem source\r\nthat provides memory to create slabs (see §4 and §5.1). cflags indicates special cache properties.\r\nkmem_cache_create() returns an opaque pointer to the object cache (a.k.a. kmem cache).\r\nvoid kmem_cache_destroy(kmem_cache_t *cp);\r\nDestroys the cache and releases all associated resources. All allocated objects must have been freed.\r\nvoid *kmem_cache_alloc(kmem_cache_t *cp, int kmflag);\r\nGets an object from the cache. The object will be in its constructed state. kmflag is either KM_SLEEP\r\nor KM_NOSLEEP, indicating whether it’s acceptable to wait for memory if none is currently available. \r\nvoid kmem_cache_free(kmem_cache_t *cp, void *obj);\r\nReturns an object to the cache. The object must be in its constructed state.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/e9b5e9cb-fe72-4674-822c-f85df4a60f38.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=38d18bad47a252ba86913d4193ccfd5dd25884c0e1e3a1c5857b08777ad89e1c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 604
      },
      {
        "segments": [
          {
            "segment_id": "052d1660-07e7-448b-8dd5-8165f781152a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "3. Magazines\r\nThe biggest limitation of the original slab allocator is\r\nthat it lacks multiprocessor scalability. To allocate an\r\nobject the allocator must acquire the lock that protects\r\nthe cache’s slab list, thus serializing all allocations.\r\nTo allow all CPUs to allocate in parallel we need some\r\nform of per−CPU caching.\r\nOur basic approach is to give each CPU an M−element\r\ncache of objects called a magazine, by analogy with\r\nautomatic weapons. Each CPU’s magazine can satisfy\r\nM allocations before the CPU needs to reload – that is,\r\nexchange its empty magazine for a full one. The CPU\r\ndoesn’t access any global data when allocating from\r\nits magazine, so we can increase scalability arbitrarily\r\nby increasing the magazine size (M).\r\nIn this section we describe how the magazine layer\r\nworks and how it performs in practice. Figure 3\r\n(below) illustrates the key concepts.\r\n“Adding per−CPU caches to the slab algorithm would\r\nprovide an excellent allocator.”\r\nUresh Vahalia, UNIX Internals: The New Frontiers\r\nFigure 3: Structure of an Object Cache − The Magazine and Slab Layers\r\nSlab Layer (unconstructed) CPU Layer Depot Magazine Layer (constructed)\r\nLoaded\r\n[3 rounds]\r\nPrevious\r\n[full]\r\ncache_cpu[0]\r\nLoaded\r\n[5 rounds]\r\nPrevious\r\n[empty]\r\ncache_cpu[1]\r\nLoaded\r\n[2 rounds]\r\nPrevious\r\n[empty]\r\ncache_cpu[2]\r\n. . .\r\nLoaded\r\n[4 rounds]\r\nPrevious\r\n[full]\r\ncache_cpu[NCPU-1]\r\nFull\r\nMagazines\r\nEmpty\r\nMagazines\r\nSlab List slab\r\nbufctl bufctl bufctl\r\none or more pages from cache’s vmem source\r\ncolor buffer buffer buffer\r\nVmem Arena",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/052d1660-07e7-448b-8dd5-8165f781152a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5cf15f8bf65880e2239b350bff7e770e8056d2a8b337c9aa7b1dd7bc1ec2bc55",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 235
      },
      {
        "segments": [
          {
            "segment_id": "7fed2398-4171-4300-b8e7-f76d8aa59768",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. Overview\r\nA magazine is an M−element array of pointers to\r\nobjects* with a count of the number of rounds (valid\r\npointers) currently in the array. Conceptually, a\r\nmagazine works like a stack. To allocate an object\r\nfrom a magazine we pop its top element:\r\nobj = magazine[--rounds];\r\nTo free an object to a magazine we push it on top:\r\nmagazine[rounds++] = obj;\r\nWe use magazines to provide each object cache with a\r\nsmall per−CPU object supply. Each CPU has its own\r\nloaded magazine, so transactions (allocations and\r\nfrees) can proceed in parallel on all CPUs.\r\nThe interesting question is what to do if the loaded\r\nmagazine is empty when we want to allocate an object\r\n(or full when we want to free one). We cannot just\r\nfall through to the slab layer, because then a long run\r\nof allocations would miss in the CPU layer every time,\r\nruining scalability. Each object cache therefore keeps\r\na global stockpile of magazines, the depot, to replenish\r\nits CPU layer. We refer to the CPU and depot layers\r\ncollectively as the magazine layer.\r\nWith M−round magazines we would intuitively expect\r\nthe CPU layer’s miss rate to be at most 1/M, but in\r\nfact a tight loop of two allocations followed by two\r\nfrees can cause thrashing, with half of all transactions\r\naccessing the globally−locked depot regardless of M,\r\nas shown in Figure 3.1a below.\r\n*We use an array of object pointers, rather than just linking objects\r\ntogether on a freelist, for two reasons: first, freelist linkage would\r\noverwrite an object’s constructed state; and second, we plan to use\r\nthe slab allocator to manage arbitrary resources, so we can’t assume\r\nthat the objects we’re managing are backed by writable memory.\r\nWe address this by keeping the previously loaded\r\nmagazine in the CPU layer, as shown in Figure 3\r\n(previous page). If the loaded magazine cannot satisfy\r\na transaction but the previous magazine can, we\r\nexchange loaded with previous and try again. If\r\nneither magazine can satisfy the transaction, we return\r\nprevious to the depot, move loaded to previous, and\r\nload a new magazine from the depot.\r\nThe key observation is that the only reason to load a\r\nnew magazine is to replace a full with an empty or\r\nvice versa, so we know that after each reload the CPU\r\neither has a full loaded magazine and an empty\r\nprevious magazine or vice versa. The CPU can\r\ntherefore satisfy at least M allocations and at least M\r\nfrees entirely with CPU−local magazines before it\r\nmust access the depot again, so the CPU layer’s\r\nworst−case miss rate is bounded by 1/M regardless of\r\nworkload.\r\nIn the common case of short−lived objects with a high\r\nallocation rate there are two performance advantages\r\nto this scheme. First, balanced alloc/free pairs on the\r\nsame CPU can almost all be satisfied by the loaded\r\nmagazine; therefore we can expect the actual miss rate\r\nto be even lower than 1/M. Second, the LIFO nature\r\nof magazines implies that we tend to reuse the same\r\nobjects over and over again. This is advantageous in\r\nhardware because the CPU will already own the cache\r\nlines for recently modified memory.\r\nFigure 3.1b (next page) summarizes the overall\r\nmagazine algorithm in pseudo−code. Figure 3.1c\r\nshows the actual code for the hot path (i.e. hitting in\r\nthe loaded magazine) to illustrate how little work is\r\nrequired.\r\n3.2. Object Construction\r\nThe original slab allocator applied constructors at slab\r\ncreation time. This can be wasteful for objects whose\r\nconstructors allocate additional memory. To take an\r\nextreme example, suppose an 8−byte object’s\r\nconstructor attaches a 1K buffer to it. Assuming 8K\r\npages, one slab would contain about 1000 objects,\r\nwhich after construction would consume 1MB of\r\nmemory. If only a few of these objects were ever\r\nallocated, most of that 1MB would be wasted.\r\nWe addressed this by moving object construction up to\r\nthe magazine layer and keeping only raw buffers in the\r\nslab layer. Now a buffer becomes an object (has its\r\nconstructor applied) when it moves from the slab layer\r\nup to the magazine layer, and an object becomes a raw\r\nbuffer (has its destructor applied) when it moves from\r\nthe magazine layer back down to the slab layer.\r\nFigure 3.1a: Thrashing at a Magazine Boundary\r\ntime\r\nallocation allocation free free\r\ntakes last gets full adds one gets empty\r\nround from magazine round to magazine\r\nmagazine, from depot magazine, from depot\r\nleaving it and takes leaving it and adds\r\nempty one round full one round\r\nCPU Layer\r\nDepot",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/7fed2398-4171-4300-b8e7-f76d8aa59768.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0cf84b0725aa5a4550a559b5d0aa282ab8b13d7a7f5b3f69006331b83ae97e95",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 751
      },
      {
        "segments": [
          {
            "segment_id": "7fed2398-4171-4300-b8e7-f76d8aa59768",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. Overview\r\nA magazine is an M−element array of pointers to\r\nobjects* with a count of the number of rounds (valid\r\npointers) currently in the array. Conceptually, a\r\nmagazine works like a stack. To allocate an object\r\nfrom a magazine we pop its top element:\r\nobj = magazine[--rounds];\r\nTo free an object to a magazine we push it on top:\r\nmagazine[rounds++] = obj;\r\nWe use magazines to provide each object cache with a\r\nsmall per−CPU object supply. Each CPU has its own\r\nloaded magazine, so transactions (allocations and\r\nfrees) can proceed in parallel on all CPUs.\r\nThe interesting question is what to do if the loaded\r\nmagazine is empty when we want to allocate an object\r\n(or full when we want to free one). We cannot just\r\nfall through to the slab layer, because then a long run\r\nof allocations would miss in the CPU layer every time,\r\nruining scalability. Each object cache therefore keeps\r\na global stockpile of magazines, the depot, to replenish\r\nits CPU layer. We refer to the CPU and depot layers\r\ncollectively as the magazine layer.\r\nWith M−round magazines we would intuitively expect\r\nthe CPU layer’s miss rate to be at most 1/M, but in\r\nfact a tight loop of two allocations followed by two\r\nfrees can cause thrashing, with half of all transactions\r\naccessing the globally−locked depot regardless of M,\r\nas shown in Figure 3.1a below.\r\n*We use an array of object pointers, rather than just linking objects\r\ntogether on a freelist, for two reasons: first, freelist linkage would\r\noverwrite an object’s constructed state; and second, we plan to use\r\nthe slab allocator to manage arbitrary resources, so we can’t assume\r\nthat the objects we’re managing are backed by writable memory.\r\nWe address this by keeping the previously loaded\r\nmagazine in the CPU layer, as shown in Figure 3\r\n(previous page). If the loaded magazine cannot satisfy\r\na transaction but the previous magazine can, we\r\nexchange loaded with previous and try again. If\r\nneither magazine can satisfy the transaction, we return\r\nprevious to the depot, move loaded to previous, and\r\nload a new magazine from the depot.\r\nThe key observation is that the only reason to load a\r\nnew magazine is to replace a full with an empty or\r\nvice versa, so we know that after each reload the CPU\r\neither has a full loaded magazine and an empty\r\nprevious magazine or vice versa. The CPU can\r\ntherefore satisfy at least M allocations and at least M\r\nfrees entirely with CPU−local magazines before it\r\nmust access the depot again, so the CPU layer’s\r\nworst−case miss rate is bounded by 1/M regardless of\r\nworkload.\r\nIn the common case of short−lived objects with a high\r\nallocation rate there are two performance advantages\r\nto this scheme. First, balanced alloc/free pairs on the\r\nsame CPU can almost all be satisfied by the loaded\r\nmagazine; therefore we can expect the actual miss rate\r\nto be even lower than 1/M. Second, the LIFO nature\r\nof magazines implies that we tend to reuse the same\r\nobjects over and over again. This is advantageous in\r\nhardware because the CPU will already own the cache\r\nlines for recently modified memory.\r\nFigure 3.1b (next page) summarizes the overall\r\nmagazine algorithm in pseudo−code. Figure 3.1c\r\nshows the actual code for the hot path (i.e. hitting in\r\nthe loaded magazine) to illustrate how little work is\r\nrequired.\r\n3.2. Object Construction\r\nThe original slab allocator applied constructors at slab\r\ncreation time. This can be wasteful for objects whose\r\nconstructors allocate additional memory. To take an\r\nextreme example, suppose an 8−byte object’s\r\nconstructor attaches a 1K buffer to it. Assuming 8K\r\npages, one slab would contain about 1000 objects,\r\nwhich after construction would consume 1MB of\r\nmemory. If only a few of these objects were ever\r\nallocated, most of that 1MB would be wasted.\r\nWe addressed this by moving object construction up to\r\nthe magazine layer and keeping only raw buffers in the\r\nslab layer. Now a buffer becomes an object (has its\r\nconstructor applied) when it moves from the slab layer\r\nup to the magazine layer, and an object becomes a raw\r\nbuffer (has its destructor applied) when it moves from\r\nthe magazine layer back down to the slab layer.\r\nFigure 3.1a: Thrashing at a Magazine Boundary\r\ntime\r\nallocation allocation free free\r\ntakes last gets full adds one gets empty\r\nround from magazine round to magazine\r\nmagazine, from depot magazine, from depot\r\nleaving it and takes leaving it and adds\r\nempty one round full one round\r\nCPU Layer\r\nDepot",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/7fed2398-4171-4300-b8e7-f76d8aa59768.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0cf84b0725aa5a4550a559b5d0aa282ab8b13d7a7f5b3f69006331b83ae97e95",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 751
      },
      {
        "segments": [
          {
            "segment_id": "9482cc72-5440-4c5c-88d7-e651b8c5fad0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 3.1b: The Magazine Algorithm\r\nThe allocation and free paths through the magazine layer are almost completely symmetric, as shown below.\r\nThe only asymmetry is that the free path is responsible for populating the depot with empty magazines, as\r\nexplained in §3.3.\r\nFigure 3.1c: The Hot Path in the Magazine Layer\r\nvoid *\r\nkmem_cache_alloc(kmem_cache_t *cp, int kmflag)\r\n{\r\nkmem_cpu_cache_t *ccp = &cp->cache_cpu[CPU->cpu_id];\r\nmutex_enter(&ccp->cc_lock);\r\nif (ccp->cc_rounds > 0) {\r\nkmem_magazine_t *mp = ccp->cc_loaded;\r\nvoid *obj = mp->mag_round[--ccp->cc_rounds];\r\nmutex_exit(&ccp->cc_lock);\r\nreturn (obj);\r\n}\r\n...\r\n}\r\nvoid\r\nkmem_cache_free(kmem_cache_t *cp, void *obj)\r\n{\r\nkmem_cpu_cache_t *ccp = &cp->cache_cpu[CPU->cpu_id];\r\nmutex_enter(&ccp->cc_lock);\r\nif (ccp->cc_rounds < ccp->cc_magsize) {\r\nkmem_magazine_t *mp = ccp->cc_loaded;\r\nmp->mag_round[ccp->cc_rounds++] = obj;\r\nmutex_exit(&ccp->cc_lock);\r\nreturn;\r\n}\r\n...\r\n}\r\nAlloc:\r\n if (the CPU's loaded magazine isn't empty)\r\n pop the top object and return it;\r\n if (the CPU's previous magazine is full)\r\n exchange loaded with previous,\r\n goto Alloc;\r\n if (the depot has any full magazines)\r\n return previous to depot,\r\n move loaded to previous,\r\n load the full magazine,\r\n goto Alloc;\r\n allocate an object from the slab layer,\r\n apply its constructor, and return it;\r\nFree:\r\n if (the CPU's loaded magazine isn't full)\r\n push the object on top and return;\r\n if (the CPU's previous magazine is empty)\r\n exchange loaded with previous,\r\n goto Free;\r\n if (the depot has any empty magazines)\r\n return previous to depot,\r\n move loaded to previous,\r\n load the empty magazine,\r\n goto Free;\r\n if (an empty magazine can be allocated)\r\n put it in the depot and goto Free;\r\n apply the object's destructor\r\n and return it to the slab layer",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/9482cc72-5440-4c5c-88d7-e651b8c5fad0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9ea2ea76cb387418860f2946c6752d86990c3c1145bdcb4a06f893a4555e2b26",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 247
      },
      {
        "segments": [
          {
            "segment_id": "f72bdbbf-a746-4c6c-aaa3-6b67a7b9e6b6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. Populating the Magazine Layer\r\nWe have described how the magazine layer works\r\nonce it’s populated, but how does it get populated?\r\nThere are two distinct problems here: we must allocate\r\nobjects, and we must allocate magazines to hold them.\r\n Object allocation. In the allocation path, if the\r\ndepot has no full magazines, we allocate a single\r\nobject from the slab layer and construct it.\r\n Magazine allocation. In the free path, if the depot\r\nhas no empty magazines, we allocate one.\r\nWe never allocate full magazines explicitly, because\r\nit’s not necessary: empty magazines are eventually\r\nfilled by frees, so it suffices to create empty magazines\r\nand let full ones emerge as a side effect of normal\r\nallocation/free traffic.\r\nWe allocate the magazines themselves (i.e. the arrays\r\nof pointers) from object caches, just like everything\r\nelse; there is no need for a special magazine allocator.*\r\n3.4. Dynamic Magazine Resizing\r\nThus far we have discussed M−element magazines\r\nwithout specifying how M is determined. We’ve\r\nobserved that we can make the CPU layer’s miss rate\r\nas low as we like by increasing M, but making M\r\nlarger than necessary would waste memory. We\r\ntherefore seek the smallest value of M that delivers\r\nlinear scalability.\r\nRather than picking some “magic value,” we designed\r\nthe magazine layer to tune itself dynamically. We\r\nstart each object cache with a small value of M and\r\nobserve the contention rate on the depot lock. We do\r\nthis by using a non−blocking trylock primitive on the\r\ndepot lock; if that fails we use the ordinary blocking\r\nlock primitive and increment a contention count. If\r\nthe contention rate exceeds a fixed threshold we\r\nincrease the cache’s magazine size. We enforce a\r\nmaximum magazine size to ensure that this feedback\r\nloop can’t get out of control, but in practice the\r\nalgorithm behaves extremely well on everything from\r\ndesktops to 64−CPU Starfires. The algorithm\r\ngenerally stabilizes after several minutes of load with\r\nreasonable magazine sizes and depot lock contention\r\nrates of less than once per second.\r\n*Note that if we allocated full magazines in the allocation path, this\r\nwould cause infinite recursion the first time we tried to allocate a\r\nmagazine for one of the magazine caches. There is no such problem\r\nwith allocating empty magazines in the free path.\r\n3.5. Protecting Per−CPU State\r\nAn object cache’s CPU layer contains per−CPU state\r\nthat must be protected either by per−CPU locking or\r\nby disabling interrupts. We selected per−CPU locking\r\nfor several reasons:\r\n Programming Model. Some operations, such as\r\nchanging a cache’s magazine size, require the\r\nallocator to modify the state of each CPU. This is\r\ntrivial if the CPU layer is protected by locks.\r\n Real−time. Disabling interrupts increases dispatch\r\nlatency (because it disables preemption), which is\r\nunacceptable in a real−time operating system like\r\nSolaris [Khanna92].\r\n Performance. On most modern processors,\r\ngrabbing an uncontended lock is cheaper than\r\nmodifying the processor interrupt level.\r\n3.6. Hardware Cache Effects\r\nEven per−CPU algorithms don’t scale if they suffer\r\nfrom false sharing (contention for ownership of a\r\ncache line that can occur when multiple CPUs modify\r\nlogically unrelated data that happens to reside in the\r\nsame physical cache line). We carefully pad and align\r\nthe magazine layer’s per−CPU data structures so that\r\neach one has its own cache line. We found that doing\r\nso is critical for linear scalability on modern hardware.\r\nAn allocator can also induce false sharing by handing\r\nout objects smaller than a cache line to more than one\r\nCPU [Berger00]. We haven’t found this to be a\r\nproblem in practice, however, because most kernel\r\ndata structures are larger than a cache line.\r\n3.7. Using the Depot as a Working Set\r\nWhen the system is in steady state, allocations and\r\nfrees must be roughly in balance (because memory\r\nusage is roughly constant). The variation in memory\r\nconsumption over a fixed period of time defines a\r\nform of working set [Denning68]; specifically, it\r\ndefines how many magazines the depot must have on\r\nhand to keep the allocator working mostly out of its\r\nhigh−performance magazine layer. For example, if\r\nthe depot’s full magazine list varies between 37 and 47\r\nmagazines over a given period, then the working set is\r\n10 magazines; the other 37 are eligible for reclaiming.\r\nThe depot continuously tracks the working set sizes of\r\nits full and empty magazine lists, but does not actually\r\nfree excess magazines unless memory runs low.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/f72bdbbf-a746-4c6c-aaa3-6b67a7b9e6b6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cee7d29e889ccc6e9a9290c2eaeeda3cd2efc7c69291a7eff677253516634364",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 732
      },
      {
        "segments": [
          {
            "segment_id": "f72bdbbf-a746-4c6c-aaa3-6b67a7b9e6b6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. Populating the Magazine Layer\r\nWe have described how the magazine layer works\r\nonce it’s populated, but how does it get populated?\r\nThere are two distinct problems here: we must allocate\r\nobjects, and we must allocate magazines to hold them.\r\n Object allocation. In the allocation path, if the\r\ndepot has no full magazines, we allocate a single\r\nobject from the slab layer and construct it.\r\n Magazine allocation. In the free path, if the depot\r\nhas no empty magazines, we allocate one.\r\nWe never allocate full magazines explicitly, because\r\nit’s not necessary: empty magazines are eventually\r\nfilled by frees, so it suffices to create empty magazines\r\nand let full ones emerge as a side effect of normal\r\nallocation/free traffic.\r\nWe allocate the magazines themselves (i.e. the arrays\r\nof pointers) from object caches, just like everything\r\nelse; there is no need for a special magazine allocator.*\r\n3.4. Dynamic Magazine Resizing\r\nThus far we have discussed M−element magazines\r\nwithout specifying how M is determined. We’ve\r\nobserved that we can make the CPU layer’s miss rate\r\nas low as we like by increasing M, but making M\r\nlarger than necessary would waste memory. We\r\ntherefore seek the smallest value of M that delivers\r\nlinear scalability.\r\nRather than picking some “magic value,” we designed\r\nthe magazine layer to tune itself dynamically. We\r\nstart each object cache with a small value of M and\r\nobserve the contention rate on the depot lock. We do\r\nthis by using a non−blocking trylock primitive on the\r\ndepot lock; if that fails we use the ordinary blocking\r\nlock primitive and increment a contention count. If\r\nthe contention rate exceeds a fixed threshold we\r\nincrease the cache’s magazine size. We enforce a\r\nmaximum magazine size to ensure that this feedback\r\nloop can’t get out of control, but in practice the\r\nalgorithm behaves extremely well on everything from\r\ndesktops to 64−CPU Starfires. The algorithm\r\ngenerally stabilizes after several minutes of load with\r\nreasonable magazine sizes and depot lock contention\r\nrates of less than once per second.\r\n*Note that if we allocated full magazines in the allocation path, this\r\nwould cause infinite recursion the first time we tried to allocate a\r\nmagazine for one of the magazine caches. There is no such problem\r\nwith allocating empty magazines in the free path.\r\n3.5. Protecting Per−CPU State\r\nAn object cache’s CPU layer contains per−CPU state\r\nthat must be protected either by per−CPU locking or\r\nby disabling interrupts. We selected per−CPU locking\r\nfor several reasons:\r\n Programming Model. Some operations, such as\r\nchanging a cache’s magazine size, require the\r\nallocator to modify the state of each CPU. This is\r\ntrivial if the CPU layer is protected by locks.\r\n Real−time. Disabling interrupts increases dispatch\r\nlatency (because it disables preemption), which is\r\nunacceptable in a real−time operating system like\r\nSolaris [Khanna92].\r\n Performance. On most modern processors,\r\ngrabbing an uncontended lock is cheaper than\r\nmodifying the processor interrupt level.\r\n3.6. Hardware Cache Effects\r\nEven per−CPU algorithms don’t scale if they suffer\r\nfrom false sharing (contention for ownership of a\r\ncache line that can occur when multiple CPUs modify\r\nlogically unrelated data that happens to reside in the\r\nsame physical cache line). We carefully pad and align\r\nthe magazine layer’s per−CPU data structures so that\r\neach one has its own cache line. We found that doing\r\nso is critical for linear scalability on modern hardware.\r\nAn allocator can also induce false sharing by handing\r\nout objects smaller than a cache line to more than one\r\nCPU [Berger00]. We haven’t found this to be a\r\nproblem in practice, however, because most kernel\r\ndata structures are larger than a cache line.\r\n3.7. Using the Depot as a Working Set\r\nWhen the system is in steady state, allocations and\r\nfrees must be roughly in balance (because memory\r\nusage is roughly constant). The variation in memory\r\nconsumption over a fixed period of time defines a\r\nform of working set [Denning68]; specifically, it\r\ndefines how many magazines the depot must have on\r\nhand to keep the allocator working mostly out of its\r\nhigh−performance magazine layer. For example, if\r\nthe depot’s full magazine list varies between 37 and 47\r\nmagazines over a given period, then the working set is\r\n10 magazines; the other 37 are eligible for reclaiming.\r\nThe depot continuously tracks the working set sizes of\r\nits full and empty magazine lists, but does not actually\r\nfree excess magazines unless memory runs low.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/f72bdbbf-a746-4c6c-aaa3-6b67a7b9e6b6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cee7d29e889ccc6e9a9290c2eaeeda3cd2efc7c69291a7eff677253516634364",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 732
      },
      {
        "segments": [
          {
            "segment_id": "6646ae08-853e-4d10-8da5-5c982a1e5685",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "3.8. Microbenchmark Performance\r\nThe two key metrics for an MT−hot memory allocator\r\nare latency and scalability. We measured latency as\r\nthe average time per iteration of a tight alloc/free loop.\r\nWe measured scalability by running multiple instances\r\nof the latency test on a 333MHz 16−CPU Starfire.\r\nThe latency test revealed that the magazine layer\r\nimproves even single−CPU performance (356ns per\r\nalloc/free pair vs. 743ns for the original slab allocator)\r\nbecause the hot path is so simple (see Figure 3.1c).\r\nIndeed, there is little room for further improvement in\r\nlatency because the cost of locking imposes a lower\r\nbound of 186ns.\r\nAs we increased the number of threads the magazine\r\nlayer exhibited perfect linear scaling, as shown below.\r\nWithout the magazine layer, throughput was actually\r\nlower with additional threads due to increasingly\r\npathological lock contention. With 16 threads (all 16\r\nCPUs busy) the magazine layer delivered 16 times\r\nhigher throughput than a single CPU (and 340 times\r\nhigher throughput than the original allocator), with the\r\nsame 356ns latency.\r\n3.9. System−Level Performance\r\nWe ran several system−level benchmarks both with\r\nand without the magazine layer to assess the magazine\r\nlayer’s effectiveness.* The system was uniformly\r\nfaster with magazines, with the greatest improvements\r\nin allocator−intensive workloads like network I/O. \r\n*Unfortunately we could not make direct comparisons with other\r\nkernel memory allocators because the Solaris kernel makes extensive\r\nuse of the object cache interfaces, which are simply not available in\r\nother allocators. We will, however, provide direct comparisons with\r\nbest−of−breed user−level allocators in §6.\r\n3.9.1. SPECweb99\r\nWe ran the industry−standard SPECweb99 web server\r\nbenchmark [SPEC01] on an 8−CPU E4500. The\r\nmagazine layer more than doubled performance, from\r\n995 to 2037 simultaneous connections. The gain is so\r\ndramatic because every network packet comes from\r\nthe allocator.\r\n3.9.2. TPC−C\r\nWe ran the industry−standard TPC−C database\r\nbenchmark [TPC01] on an 8−CPU E6000. Magazines\r\nimproved performance by 7%. The gain here is much\r\nmore modest than with SPECweb99 because TPC−C\r\nis not very demanding of the kernel memory allocator.\r\n3.9.3. Kenbus\r\nWe ran Kenbus, a precursor to the SPEC SMT\r\n(System Multi−Tasking) benchmark currently under\r\ndevelopment [SPEC01], on a 24−CPU E6000. The\r\nmagazine layer improved peak throughput by 13% and\r\nimproved the system’s ability to sustain peak\r\nthroughput as load increased. At maximum tested\r\nload (6000 users) the magazine layer improved system\r\nthroughput by 23%.\r\n3.10. Summary\r\nThe magazine layer provides efficient object caching\r\nwith very low latency and linear scaling to any number\r\nof CPUs. We discussed the magazine layer in the\r\ncontext of the slab allocator, but in fact the algorithms\r\nare completely general. A magazine layer can be\r\nadded to any memory allocator to make it scale.\r\nFigure 3.8: Allocation Scalability\r\nNumber of threads (16-CPU system)\r\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\r\nThroughput (alloc/sec)\r\n0\r\n10M\r\n20M\r\n30M\r\n40M\r\n50M with magazines\r\nwithout magazines\r\nperfect scaling Figure 3.9.3: Kenbus Performance\r\nNumber of Simulated Users\r\n0 1000 2000 3000 4000 5000 6000\r\nThroughput (scripts/hour)\r\n0\r\n10000\r\n20000\r\n30000\r\n40000\r\n50000 with magazines\r\nwithout magazines",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/6646ae08-853e-4d10-8da5-5c982a1e5685.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a7c72636f991247fcc0e1d702e0ed5354281a8864c957504be443a1e227d3aae",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 508
      },
      {
        "segments": [
          {
            "segment_id": "dc198e52-f593-411f-904f-f9f5af7ccb51",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "4. Vmem\r\nThe slab allocator relies on two lower−level system\r\nservices to create slabs: a virtual address allocator to\r\nprovide kernel virtual addresses, and VM routines to\r\nback those addresses with physical pages and establish\r\nvirtual−to−physical translations.\r\nIncredibly, we found that our largest systems were\r\nscalability−limited by the old virtual address allocator.\r\nIt tended to fragment the address space badly over\r\ntime, its latency was linear in the number of\r\nfragments, and the whole thing was single−threaded.\r\nVirtual address allocation is just one example of the\r\nmore general problem of resource allocation. For our\r\npurposes, a resource is anything that can be described\r\nby a set of integers. For example, virtual addresses are\r\nsubsets of the 64−bit integers; process IDs are subsets\r\nof the integers [0, 30000]; and minor device numbers\r\nare subsets of the 32−bit integers.\r\nResource allocation (in the sense described above) is a\r\nfundamental problem that every operating system must\r\nsolve, yet it is surprisingly absent in the literature. It\r\nappears that 40 years of research on memory allocators\r\nhas simply never been applied to resource allocators.\r\nThe resource allocators for Linux, all the BSD kernels,\r\nand Solaris 7 or earlier all use linear−time algorithms.\r\nIn this section we describe a new general−purpose\r\nresource allocator, vmem, which provides guaranteed\r\nconstant−time performance with low fragmentation.\r\nVmem appears to be the first resource allocator that\r\ncan do this.\r\nWe begin by providing background on the current\r\nstate of the art. We then lay out our objectives in\r\ncreating vmem, describe the vmem interfaces, explain\r\nthe implementation in detail, and discuss vmem’s\r\nperformance (fragmentation, latency, and scalability)\r\nunder both benchmarks and real−world conditions.\r\n4.1. Background\r\nAlmost all versions of Unix have a resource map\r\nallocator called rmalloc() [Vahalia96]. A resource\r\nmap can be any set of integers, though it’s most often\r\nan address range like [0xe0000000, 0xf0000000).\r\nThe interface is simple: rmalloc(map, size)\r\nallocates a segment of the specified size from map,\r\nand rmfree(map, size, addr) gives it back.\r\nLinux’s resource allocator and BSD’s extent allocator\r\nprovide roughly the same services. All three suffer\r\nfrom serious flaws in both design and implementation:\r\n Linear−time performance. All three allocators\r\nmaintain a list of free segments, sorted in address\r\norder so the allocator can detect when coalescing is\r\npossible: if segments [a, b) and [b, c) are both\r\nfree, they can be merged into a single free segment\r\n[a, c) to reduce fragmentation. The allocation\r\ncode performs a linear search to find a segment\r\nlarge enough to satisfy the allocation. The free\r\ncode uses insertion sort (also a linear algorithm) to\r\nreturn a segment to the free segment list. It can\r\ntake several milliseconds to allocate or free a\r\nsegment once the resource becomes fragmented.\r\n Implementation exposure. A resource allocator\r\nneeds data structures to keep information about its\r\nfree segments. In various ways, all three allocators\r\nmake this your problem:\r\n rmalloc() requires the creator of the resource\r\nmap to specify the maximum possible number\r\nof free segments at map creation time. If the\r\nmap ever gets more fragmented than that, the\r\nallocator throws away resources in rmfree()\r\nbecause it has nowhere to put them. (!)\r\n Linux puts the burden on its clients to supply a\r\nsegment structure with each allocation to hold\r\nthe allocator’s internal data. (!)\r\n BSD allocates segment structures dynamically,\r\nbut in so doing creates an awkward failure\r\nmode: extent_free() fails if it can’t allocate\r\na segment structure. It’s difficult to deal with\r\nan allocator that won’t let you give stuff back.\r\nWe concluded that it was time to abandon our stone\r\ntools and bring modern technology to the problem.\r\n4.2. Objectives\r\nWe believe a good resource allocator should have the\r\nfollowing properties:\r\n A powerful interface that can cleanly express the\r\nmost common resource allocation problems.\r\n Constant−time performance, regardless of the size\r\nof the request or the degree of fragmentation.\r\n Linear scalability to any number of CPUs.\r\n Low fragmentation, even if the operating system\r\nruns at full throttle for years.\r\nWe’ll begin by discussing the interface considerations,\r\nthen drill down to the implementation details.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/dc198e52-f593-411f-904f-f9f5af7ccb51.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6ddfc8dbc365bd11605bbff56302296c96b0df64adc392ca423efaf1adbf8328",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 682
      },
      {
        "segments": [
          {
            "segment_id": "dc198e52-f593-411f-904f-f9f5af7ccb51",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "4. Vmem\r\nThe slab allocator relies on two lower−level system\r\nservices to create slabs: a virtual address allocator to\r\nprovide kernel virtual addresses, and VM routines to\r\nback those addresses with physical pages and establish\r\nvirtual−to−physical translations.\r\nIncredibly, we found that our largest systems were\r\nscalability−limited by the old virtual address allocator.\r\nIt tended to fragment the address space badly over\r\ntime, its latency was linear in the number of\r\nfragments, and the whole thing was single−threaded.\r\nVirtual address allocation is just one example of the\r\nmore general problem of resource allocation. For our\r\npurposes, a resource is anything that can be described\r\nby a set of integers. For example, virtual addresses are\r\nsubsets of the 64−bit integers; process IDs are subsets\r\nof the integers [0, 30000]; and minor device numbers\r\nare subsets of the 32−bit integers.\r\nResource allocation (in the sense described above) is a\r\nfundamental problem that every operating system must\r\nsolve, yet it is surprisingly absent in the literature. It\r\nappears that 40 years of research on memory allocators\r\nhas simply never been applied to resource allocators.\r\nThe resource allocators for Linux, all the BSD kernels,\r\nand Solaris 7 or earlier all use linear−time algorithms.\r\nIn this section we describe a new general−purpose\r\nresource allocator, vmem, which provides guaranteed\r\nconstant−time performance with low fragmentation.\r\nVmem appears to be the first resource allocator that\r\ncan do this.\r\nWe begin by providing background on the current\r\nstate of the art. We then lay out our objectives in\r\ncreating vmem, describe the vmem interfaces, explain\r\nthe implementation in detail, and discuss vmem’s\r\nperformance (fragmentation, latency, and scalability)\r\nunder both benchmarks and real−world conditions.\r\n4.1. Background\r\nAlmost all versions of Unix have a resource map\r\nallocator called rmalloc() [Vahalia96]. A resource\r\nmap can be any set of integers, though it’s most often\r\nan address range like [0xe0000000, 0xf0000000).\r\nThe interface is simple: rmalloc(map, size)\r\nallocates a segment of the specified size from map,\r\nand rmfree(map, size, addr) gives it back.\r\nLinux’s resource allocator and BSD’s extent allocator\r\nprovide roughly the same services. All three suffer\r\nfrom serious flaws in both design and implementation:\r\n Linear−time performance. All three allocators\r\nmaintain a list of free segments, sorted in address\r\norder so the allocator can detect when coalescing is\r\npossible: if segments [a, b) and [b, c) are both\r\nfree, they can be merged into a single free segment\r\n[a, c) to reduce fragmentation. The allocation\r\ncode performs a linear search to find a segment\r\nlarge enough to satisfy the allocation. The free\r\ncode uses insertion sort (also a linear algorithm) to\r\nreturn a segment to the free segment list. It can\r\ntake several milliseconds to allocate or free a\r\nsegment once the resource becomes fragmented.\r\n Implementation exposure. A resource allocator\r\nneeds data structures to keep information about its\r\nfree segments. In various ways, all three allocators\r\nmake this your problem:\r\n rmalloc() requires the creator of the resource\r\nmap to specify the maximum possible number\r\nof free segments at map creation time. If the\r\nmap ever gets more fragmented than that, the\r\nallocator throws away resources in rmfree()\r\nbecause it has nowhere to put them. (!)\r\n Linux puts the burden on its clients to supply a\r\nsegment structure with each allocation to hold\r\nthe allocator’s internal data. (!)\r\n BSD allocates segment structures dynamically,\r\nbut in so doing creates an awkward failure\r\nmode: extent_free() fails if it can’t allocate\r\na segment structure. It’s difficult to deal with\r\nan allocator that won’t let you give stuff back.\r\nWe concluded that it was time to abandon our stone\r\ntools and bring modern technology to the problem.\r\n4.2. Objectives\r\nWe believe a good resource allocator should have the\r\nfollowing properties:\r\n A powerful interface that can cleanly express the\r\nmost common resource allocation problems.\r\n Constant−time performance, regardless of the size\r\nof the request or the degree of fragmentation.\r\n Linear scalability to any number of CPUs.\r\n Low fragmentation, even if the operating system\r\nruns at full throttle for years.\r\nWe’ll begin by discussing the interface considerations,\r\nthen drill down to the implementation details.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/dc198e52-f593-411f-904f-f9f5af7ccb51.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6ddfc8dbc365bd11605bbff56302296c96b0df64adc392ca423efaf1adbf8328",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 682
      },
      {
        "segments": [
          {
            "segment_id": "c96bbadf-6df0-4035-b445-912d2b98ea46",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "4.3. Interface Description\r\nThe vmem interfaces do three basic things: create and\r\ndestroy arenas to describe resources, allocate and free\r\nresources, and allow arenas to import new resources\r\ndynamically. This section describes the key concepts\r\nand the rationale behind them. Figure 4.3 (next page)\r\nprovides the complete vmem interface specification.\r\n4.3.1. Creating Arenas\r\nThe first thing we need is the ability to define a\r\nresource collection, or arena. An arena is simply a set\r\nof integers. Vmem arenas most often represent virtual\r\nmemory addresses (hence the name vmem), but in fact\r\nthey can represent any integer resource, from virtual\r\naddresses to minor device numbers to process IDs.\r\nThe integers in an arena can usually be described as a\r\nsingle contiguous range, or span, such as [100, 500),\r\nso we specify this initial span to vmem_create().\r\nFor discontiguous resources we can use vmem_add()\r\nto piece the arena together one span at a time.\r\n Example. To create an arena to represent the\r\nintegers in the range [100, 500) we can say:\r\nfoo = vmem_create(“foo”, 100, 400, ...);\r\n(Note: 100 is the start, 400 is the size.) If we want\r\nfoo to represent the integers [600, 800) as well, we\r\ncan add the span [600, 800) by using vmem_add():\r\nvmem_add(foo, 600, 200, VM_SLEEP);\r\nvmem_create() specifies the arena’s natural unit of\r\ncurrency, or quantum, which is typically either 1 (for\r\nsingle integers like process IDs) or PAGESIZE (for\r\nvirtual addresses). Vmem rounds all sizes to quantum\r\nmultiples and guarantees quantum−aligned allocations.\r\n4.3.2. Allocating and Freeing Resources\r\nThe primary interfaces to allocate and free resources\r\nare simple: vmem_alloc(vmp, size, vmflag)\r\nallocates a segment of size bytes from arena vmp,\r\nand vmem_free(vmp, addr, size) gives it back. \r\nWe also provide a vmem_xalloc() interface that can\r\nspecify common allocation constraints: alignment,\r\nphase (offset from the alignment), address range, and\r\nboundary−crossing restrictions (e.g. “don’t cross a\r\npage boundary”). vmem_xalloc() is useful for\r\nthings like kernel DMA code, which allocates kernel\r\nvirtual addresses using the phase and alignment\r\nconstraints to ensure correct cache coloring.\r\n Example. To allocate a 20−byte segment whose\r\naddress is 8 bytes away from a 64−byte boundary,\r\nand which lies in the range [200, 300), we can say:\r\naddr = vmem_xalloc(foo, 20, 64, 8, 0,\r\n200, 300, VM_SLEEP);\r\nIn this example addr will be 262: it is 8 bytes\r\naway from a 64−byte boundary (262 mod 64 = 8),\r\nand the segment [262, 282) lies within [200, 300).\r\nEach vmem_[x]alloc() can specify one of three\r\nallocation policies through its vmflag argument:\r\n VM_BESTFIT. Directs vmem to use the smallest\r\nfree segment that can satisfy the allocation. This\r\npolicy tends to minimize fragmentation of very\r\nsmall, precious resources.\r\n VM_INSTANTFIT. Directs vmem to provide a\r\ngood approximation to best−fit in guaranteed\r\nconstant time. This is the default allocation policy.\r\n VM_NEXTFIT. Directs vmem to use the next free\r\nsegment after the one previously allocated. This is\r\nuseful for things like process IDs, where we want\r\nto cycle through all the IDs before reusing them.\r\nWe also offer an arena−wide allocation policy called\r\nquantum caching. The idea is that most allocations\r\nare for just a few quanta (e.g. one or two pages of heap\r\nor one minor device number), so we employ high−\r\nperformance caching for each multiple of the quantum\r\nup to qcache_max, specified in vmem_create().\r\nWe make the caching threshold explicit so that each\r\narena can request the amount of caching appropriate\r\nfor the resource it manages. Quantum caches provide\r\nperfect−fit, very low latency, and linear scalability for\r\nthe most common allocation sizes (§4.4.4).\r\n4.3.3. Importing From Another Arena\r\nVmem allows one arena to import its resources from\r\nanother. vmem_create() specifies the source arena,\r\nand the functions to allocate and free from that source.\r\nThe arena imports new spans as needed, and gives\r\nthem back when all their segments have been freed.\r\nThe power of importing lies in the side effects of the\r\nimport functions, and is best understood by example.\r\nIn Solaris, the function segkmem_alloc() invokes\r\nvmem_alloc() to get a virtual address and then backs\r\nit with physical pages. Therefore, we can create an\r\narena of mapped pages by simply importing from an\r\narena of virtual addresses using segkmem_alloc()\r\nand segkmem_free(). Appendix A illustrates how\r\nvmem’s import mechanism can be used to create\r\ncomplex resources from simple building blocks.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/c96bbadf-6df0-4035-b445-912d2b98ea46.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=29f3391afd431f8891de0645dc0849fce37e98837794f434b4af2d5327e30912",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 716
      },
      {
        "segments": [
          {
            "segment_id": "c96bbadf-6df0-4035-b445-912d2b98ea46",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "4.3. Interface Description\r\nThe vmem interfaces do three basic things: create and\r\ndestroy arenas to describe resources, allocate and free\r\nresources, and allow arenas to import new resources\r\ndynamically. This section describes the key concepts\r\nand the rationale behind them. Figure 4.3 (next page)\r\nprovides the complete vmem interface specification.\r\n4.3.1. Creating Arenas\r\nThe first thing we need is the ability to define a\r\nresource collection, or arena. An arena is simply a set\r\nof integers. Vmem arenas most often represent virtual\r\nmemory addresses (hence the name vmem), but in fact\r\nthey can represent any integer resource, from virtual\r\naddresses to minor device numbers to process IDs.\r\nThe integers in an arena can usually be described as a\r\nsingle contiguous range, or span, such as [100, 500),\r\nso we specify this initial span to vmem_create().\r\nFor discontiguous resources we can use vmem_add()\r\nto piece the arena together one span at a time.\r\n Example. To create an arena to represent the\r\nintegers in the range [100, 500) we can say:\r\nfoo = vmem_create(“foo”, 100, 400, ...);\r\n(Note: 100 is the start, 400 is the size.) If we want\r\nfoo to represent the integers [600, 800) as well, we\r\ncan add the span [600, 800) by using vmem_add():\r\nvmem_add(foo, 600, 200, VM_SLEEP);\r\nvmem_create() specifies the arena’s natural unit of\r\ncurrency, or quantum, which is typically either 1 (for\r\nsingle integers like process IDs) or PAGESIZE (for\r\nvirtual addresses). Vmem rounds all sizes to quantum\r\nmultiples and guarantees quantum−aligned allocations.\r\n4.3.2. Allocating and Freeing Resources\r\nThe primary interfaces to allocate and free resources\r\nare simple: vmem_alloc(vmp, size, vmflag)\r\nallocates a segment of size bytes from arena vmp,\r\nand vmem_free(vmp, addr, size) gives it back. \r\nWe also provide a vmem_xalloc() interface that can\r\nspecify common allocation constraints: alignment,\r\nphase (offset from the alignment), address range, and\r\nboundary−crossing restrictions (e.g. “don’t cross a\r\npage boundary”). vmem_xalloc() is useful for\r\nthings like kernel DMA code, which allocates kernel\r\nvirtual addresses using the phase and alignment\r\nconstraints to ensure correct cache coloring.\r\n Example. To allocate a 20−byte segment whose\r\naddress is 8 bytes away from a 64−byte boundary,\r\nand which lies in the range [200, 300), we can say:\r\naddr = vmem_xalloc(foo, 20, 64, 8, 0,\r\n200, 300, VM_SLEEP);\r\nIn this example addr will be 262: it is 8 bytes\r\naway from a 64−byte boundary (262 mod 64 = 8),\r\nand the segment [262, 282) lies within [200, 300).\r\nEach vmem_[x]alloc() can specify one of three\r\nallocation policies through its vmflag argument:\r\n VM_BESTFIT. Directs vmem to use the smallest\r\nfree segment that can satisfy the allocation. This\r\npolicy tends to minimize fragmentation of very\r\nsmall, precious resources.\r\n VM_INSTANTFIT. Directs vmem to provide a\r\ngood approximation to best−fit in guaranteed\r\nconstant time. This is the default allocation policy.\r\n VM_NEXTFIT. Directs vmem to use the next free\r\nsegment after the one previously allocated. This is\r\nuseful for things like process IDs, where we want\r\nto cycle through all the IDs before reusing them.\r\nWe also offer an arena−wide allocation policy called\r\nquantum caching. The idea is that most allocations\r\nare for just a few quanta (e.g. one or two pages of heap\r\nor one minor device number), so we employ high−\r\nperformance caching for each multiple of the quantum\r\nup to qcache_max, specified in vmem_create().\r\nWe make the caching threshold explicit so that each\r\narena can request the amount of caching appropriate\r\nfor the resource it manages. Quantum caches provide\r\nperfect−fit, very low latency, and linear scalability for\r\nthe most common allocation sizes (§4.4.4).\r\n4.3.3. Importing From Another Arena\r\nVmem allows one arena to import its resources from\r\nanother. vmem_create() specifies the source arena,\r\nand the functions to allocate and free from that source.\r\nThe arena imports new spans as needed, and gives\r\nthem back when all their segments have been freed.\r\nThe power of importing lies in the side effects of the\r\nimport functions, and is best understood by example.\r\nIn Solaris, the function segkmem_alloc() invokes\r\nvmem_alloc() to get a virtual address and then backs\r\nit with physical pages. Therefore, we can create an\r\narena of mapped pages by simply importing from an\r\narena of virtual addresses using segkmem_alloc()\r\nand segkmem_free(). Appendix A illustrates how\r\nvmem’s import mechanism can be used to create\r\ncomplex resources from simple building blocks.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/c96bbadf-6df0-4035-b445-912d2b98ea46.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=29f3391afd431f8891de0645dc0849fce37e98837794f434b4af2d5327e30912",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 716
      },
      {
        "segments": [
          {
            "segment_id": "4cf1dc29-60d7-4d99-8550-d9bc093bf39d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 4.3: Vmem Interface Summary\r\nvmem_t *vmem_create(\r\nchar *name, /* descriptive name */\r\nvoid *base, /* start of initial span */\r\nsize_t size, /* size of initial span */\r\nsize_t quantum, /* unit of currency */\r\nvoid *(*afunc)(vmem_t *, size_t, int), /* import alloc function */\r\nvoid (*ffunc)(vmem_t *, void *, size_t), /* import free function */\r\nvmem_t *source, /* import source arena */\r\nsize_t qcache_max, /* maximum size to cache */\r\nint vmflag); /* VM_SLEEP or VM_NOSLEEP */\r\nCreates a vmem arena called name whose initial span is [base, base + size). The arena’s natural\r\nunit of currency is quantum, so vmem_alloc() guarantees quantum−aligned results. The arena may\r\nimport new spans by invoking afunc on source, and may return those spans by invoking ffunc on\r\nsource. Small allocations are common, so the arena provides high−performance caching for each\r\ninteger multiple of quantum up to qcache_max. vmflag is either VM_SLEEP or VM_NOSLEEP\r\ndepending on whether the caller is willing to wait for memory to create the arena. vmem_create()\r\nreturns an opaque pointer to the arena.\r\nvoid vmem_destroy(vmem_t *vmp);\r\nDestroys arena vmp.\r\nvoid *vmem_alloc(vmem_t *vmp, size_t size, int vmflag);\r\nAllocates size bytes from vmp. Returns the allocated address on success, NULL on failure.\r\nvmem_alloc() fails only if vmflag specifies VM_NOSLEEP and no resources are currently available.\r\nvmflag may also specify an allocation policy (VM_BESTFIT, VM_INSTANTFIT, or VM_NEXTFIT) as\r\ndescribed in §4.3.2. If no policy is specified the default is VM_INSTANTFIT, which provides a good\r\napproximation to best−fit in guaranteed constant time.\r\nvoid vmem_free(vmem_t *vmp, void *addr, size_t size);\r\nFrees size bytes at addr to arena vmp.\r\nvoid *vmem_xalloc(vmem_t *vmp, size_t size, size_t align, size_t phase,\r\nsize_t nocross, void *minaddr, void *maxaddr, int vmflag);\r\nAllocates size bytes at offset phase from an align boundary such that the resulting segment\r\n[addr, addr + size) is a subset of [minaddr, maxaddr) that does not straddle a nocross−\r\naligned boundary. vmflag is as above. One performance caveat: if either minaddr or maxaddr is\r\nnon−NULL, vmem may not be able to satisfy the allocation in constant time. If allocations within a\r\ngiven [minaddr, maxaddr) range are common it is more efficient to declare that range to be its own\r\narena and use unconstrained allocations on the new arena.\r\nvoid vmem_xfree(vmem_t *vmp, void *addr, size_t size);\r\nFrees size bytes at addr, where addr was a constrained allocation. vmem_xfree() must be used if\r\nthe original allocation was a vmem_xalloc() because both routines bypass the quantum caches.\r\nvoid *vmem_add(vmem_t *vmp, void *addr, size_t size, int vmflag);\r\nAdds the span [addr, addr + size) to arena vmp. Returns addr on success, NULL on failure.\r\nvmem_add() will fail only if vmflag is VM_NOSLEEP and no resources are currently available.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/4cf1dc29-60d7-4d99-8550-d9bc093bf39d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1b3cb6bcbe9d0bbecb9b711c592f47a37a6b848e3ba532757d569eeb2cf01f7c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 448
      },
      {
        "segments": [
          {
            "segment_id": "9b76691a-c4e7-44c7-bd4b-2207c0a89b69",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "4.4. Vmem Implementation\r\nIn this section we describe how vmem actually works.\r\nFigure 4.4 illustrates the overall structure of an arena.\r\n4.4.1. Keeping Track of Segments\r\nMost implementations of malloc() prepend a small\r\namount of space to each buffer to hold information for\r\nthe allocator. These boundary tags, invented by\r\nKnuth in 1962 [Knuth73], solve two major problems:\r\n They make it easy for free() to determine how\r\nlarge the buffer is, because malloc() can store\r\nthe size in the boundary tag.\r\n They make coalescing trivial. Boundary tags link\r\nall segments together in address order, so free()\r\ncan simply look both ways and coalesce if either\r\nneighbor is free.\r\nUnfortunately, resource allocators can’t use traditional\r\nboundary tags because the resource they’re managing\r\nmay not be memory (and therefore may not be able to\r\nhold information). In vmem we address this by using\r\nexternal boundary tags. For each segment in the arena\r\nwe allocate a boundary tag to manage it, as shown in\r\nFigure 4.4 below. (See Appendix A for a description\r\nof how we allocate the boundary tags themselves.)\r\nWe’ll see shortly that external boundary tags enable\r\nconstant−time performance.\r\n4.4.2. Allocating and Freeing Segments\r\nEach arena has a segment list that links all of its\r\nsegments in address order, as shown in Figure 4.4.\r\nEvery segment also belongs to either a freelist or an\r\nallocation hash chain, as described below. (The\r\narena’s segment list also includes span markers to\r\nkeep track of span boundaries, so we can easily tell\r\nwhen an imported span can be returned to its source.)\r\nWe keep all free segments on power−of−two freelists;\r\nthat is, freelist[n] contains all free segments\r\nwhose sizes are in the range [2n\r\n, 2n+1). To allocate a\r\nsegment we search the appropriate freelist for a\r\nsegment large enough to satisfy the allocation. This\r\napproach, called segregated fit, actually approximates\r\nbest−fit because any segment on the chosen freelist is\r\na good fit [Wilson95]. (Indeed, with power−of−two\r\nfreelists, a segregated fit is necessarily within 2x of a\r\nperfect fit.) Approximations to best−fit are appealing\r\nbecause they exhibit low fragmentation in practice for\r\na wide variety of workloads [Johnstone97].\r\nFigure 4.4: Structure of a Vmem Arena\r\nvmem_alloc() vectors allocations based on size: small allocations go to the quantum caches, larger ones to the segment list. In this\r\nfigure we’ve depicted an arena with a 1−page quantum and a 5−page qcache_max. Note that the “segment list” is, strictly speaking,\r\na list of boundary tags (“BT” below) that represent the segments. Boundary tags for allocated segments (white) are also linked into\r\nan allocated−segment hash table, and boundary tags for free segments (gray) are linked into size−segregated freelists (not shown).\r\n“Apparently, too few researchers realized the full\r\nsignificance of Knuth’s invention of boundary tags.”\r\nPaul R. Wilson et. al. in [Wilson95] Segment List Quantum Caches\r\nsize = 1 page\r\n16-page slabs\r\n1-page objects\r\nsize = 2 pages\r\n16-page slabs\r\n2-page objects\r\n. . .\r\n. . .\r\nsize = 5 pages\r\n16-page slabs\r\n5-page objects\r\nsize > 5 pages\r\nSegment\r\nAllocated\r\nSegment\r\nAllocated\r\nSegment\r\nFree\r\nSpan\r\nSegment\r\nFree\r\nSegment\r\nAllocated\r\nSpan\r\nBT BT BT BT BT BT BT\r\nVmem Source for Imported Resources",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/9b76691a-c4e7-44c7-bd4b-2207c0a89b69.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c13ece9c49869263588cd57eadd9e2931cce200678155d96c07a396500e665a8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 527
      },
      {
        "segments": [
          {
            "segment_id": "9b76691a-c4e7-44c7-bd4b-2207c0a89b69",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "4.4. Vmem Implementation\r\nIn this section we describe how vmem actually works.\r\nFigure 4.4 illustrates the overall structure of an arena.\r\n4.4.1. Keeping Track of Segments\r\nMost implementations of malloc() prepend a small\r\namount of space to each buffer to hold information for\r\nthe allocator. These boundary tags, invented by\r\nKnuth in 1962 [Knuth73], solve two major problems:\r\n They make it easy for free() to determine how\r\nlarge the buffer is, because malloc() can store\r\nthe size in the boundary tag.\r\n They make coalescing trivial. Boundary tags link\r\nall segments together in address order, so free()\r\ncan simply look both ways and coalesce if either\r\nneighbor is free.\r\nUnfortunately, resource allocators can’t use traditional\r\nboundary tags because the resource they’re managing\r\nmay not be memory (and therefore may not be able to\r\nhold information). In vmem we address this by using\r\nexternal boundary tags. For each segment in the arena\r\nwe allocate a boundary tag to manage it, as shown in\r\nFigure 4.4 below. (See Appendix A for a description\r\nof how we allocate the boundary tags themselves.)\r\nWe’ll see shortly that external boundary tags enable\r\nconstant−time performance.\r\n4.4.2. Allocating and Freeing Segments\r\nEach arena has a segment list that links all of its\r\nsegments in address order, as shown in Figure 4.4.\r\nEvery segment also belongs to either a freelist or an\r\nallocation hash chain, as described below. (The\r\narena’s segment list also includes span markers to\r\nkeep track of span boundaries, so we can easily tell\r\nwhen an imported span can be returned to its source.)\r\nWe keep all free segments on power−of−two freelists;\r\nthat is, freelist[n] contains all free segments\r\nwhose sizes are in the range [2n\r\n, 2n+1). To allocate a\r\nsegment we search the appropriate freelist for a\r\nsegment large enough to satisfy the allocation. This\r\napproach, called segregated fit, actually approximates\r\nbest−fit because any segment on the chosen freelist is\r\na good fit [Wilson95]. (Indeed, with power−of−two\r\nfreelists, a segregated fit is necessarily within 2x of a\r\nperfect fit.) Approximations to best−fit are appealing\r\nbecause they exhibit low fragmentation in practice for\r\na wide variety of workloads [Johnstone97].\r\nFigure 4.4: Structure of a Vmem Arena\r\nvmem_alloc() vectors allocations based on size: small allocations go to the quantum caches, larger ones to the segment list. In this\r\nfigure we’ve depicted an arena with a 1−page quantum and a 5−page qcache_max. Note that the “segment list” is, strictly speaking,\r\na list of boundary tags (“BT” below) that represent the segments. Boundary tags for allocated segments (white) are also linked into\r\nan allocated−segment hash table, and boundary tags for free segments (gray) are linked into size−segregated freelists (not shown).\r\n“Apparently, too few researchers realized the full\r\nsignificance of Knuth’s invention of boundary tags.”\r\nPaul R. Wilson et. al. in [Wilson95] Segment List Quantum Caches\r\nsize = 1 page\r\n16-page slabs\r\n1-page objects\r\nsize = 2 pages\r\n16-page slabs\r\n2-page objects\r\n. . .\r\n. . .\r\nsize = 5 pages\r\n16-page slabs\r\n5-page objects\r\nsize > 5 pages\r\nSegment\r\nAllocated\r\nSegment\r\nAllocated\r\nSegment\r\nFree\r\nSpan\r\nSegment\r\nFree\r\nSegment\r\nAllocated\r\nSpan\r\nBT BT BT BT BT BT BT\r\nVmem Source for Imported Resources",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/9b76691a-c4e7-44c7-bd4b-2207c0a89b69.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c13ece9c49869263588cd57eadd9e2931cce200678155d96c07a396500e665a8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 527
      },
      {
        "segments": [
          {
            "segment_id": "d1deaa9a-ed23-403e-985e-1683508937cb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 13,
            "page_width": 612,
            "page_height": 792,
            "content": "The algorithm for selecting a free segment depends on\r\nthe allocation policy specified in the flags to\r\nvmem_alloc() as follows; in all cases, assume that\r\nthe allocation size lies in the range [2n\r\n, 2n+1):\r\n VM_BESTFIT. Search for the smallest segment on\r\nfreelist[n] that can satisfy the allocation.\r\n VM_INSTANTFIT. If the size is exactly 2n, take\r\nthe first segment on freelist[n]. Otherwise,\r\ntake the first segment on freelist[n+1]. Any\r\nsegment on this freelist is necessarily large enough\r\nto satisfy the allocation, so we get constant−time\r\nperformance with a reasonably good fit.*\r\n VM_NEXTFIT. Ignore the freelists altogether and\r\nsearch the arena for the next free segment after the\r\none previously allocated.\r\nOnce we’ve selected a segment, we remove it from its\r\nfreelist. If the segment is not an exact fit we split the\r\nsegment, create a boundary tag for the remainder, and\r\nput the remainder on the appropriate freelist. We then\r\nadd our newly−allocated segment’s boundary tag to a\r\nhash table so vmem_free() can find it quickly.\r\nvmem_free() is straightforward: it looks up the\r\nsegment’s boundary tag in the allocated−segment hash\r\ntable, removes it from the hash table, tries to coalesce\r\nthe segment with its neighbors, and puts it on the\r\nappropriate freelist. All operations are constant−time.\r\nNote that the hash lookup also provides a cheap and\r\neffective sanity check: the freed address must be in the\r\nhash table, and the freed size must match the segment\r\nsize. This helps to catch bugs such as duplicate frees.\r\nThe key feature of the algorithm described above is\r\nthat its performance is independent of both transaction\r\nsize and arena fragmentation. Vmem appears to be\r\nthe first resource allocator that can perform allocations\r\nand frees of any size in guaranteed constant time.\r\n4.4.3. Locking Strategy\r\nFor simplicity, we protect each arena’s segment list,\r\nfreelists, and hash table with a global lock. We rely\r\non the fact that large allocations are relatively rare,\r\nand allow the arena’s quantum caches to provide\r\nlinear scalability for all the common allocation sizes.\r\nThis strategy is very effective in practice, as illustrated\r\nby the performance data in §4.5 and the allocation\r\nstatistics for a large Solaris 8 server in Appendix B.\r\n*We like instant−fit because it guarantees constant time performance,\r\nprovides low fragmentation in practice, and is easy to implement.\r\nThere are many other techniques for choosing a suitable free segment\r\nin reasonable (e.g. logarithmic) time, such as keeping all free\r\nsegments in a size−sorted tree; see [Wilson95] for a thorough survey.\r\nAny of these techniques could be used for a vmem implementation.\r\n4.4.4. Quantum Caching\r\nThe slab allocator can provide object caching for any\r\nvmem arena (§5.1), so vmem’s quantum caches are\r\nactually implemented as object caches. For each small\r\ninteger multiple of the arena’s quantum we create an\r\nobject cache to service requests of that size.\r\nvmem_alloc() and vmem_free() simply convert\r\neach small request (size <= qcache_max) into a\r\nkmem_cache_alloc() or kmem_cache_free() on\r\nthe appropriate cache, as illustrated in Figure 4.4.\r\nBecause it is based on object caching, quantum\r\ncaching provides very low latency and linear\r\nscalability for the most common allocation sizes.\r\n Example. Assume the arena shown in Figure 4.4.\r\nA 3−page allocation would proceed as follows:\r\nvmem_alloc(foo, 3 * PAGESIZE) would call\r\nkmem_cache_alloc(foo->vm_qcache[2]). In\r\nmost cases the cache’s magazine layer would\r\nsatisfy the allocation, and we would be done. If\r\nthe cache needed to create a new slab it would call\r\nvmem_alloc(foo, 16 * PAGESIZE), which\r\nwould be satisfied from the arena’s segment list.\r\nThe slab allocator would then divide its 16−page\r\nslab into five 3−page objects and use one of them\r\nto satisfy the original allocation.\r\nWhen we create an arena’s quantum caches we pass a\r\nflag to kmem_cache_create(), KMC_QCACHE, that\r\ndirects the slab allocator to use a particular slab size:\r\nthe next power of two above 3 * qcache_max. We\r\nuse this particular value for three reasons: (1) the slab\r\nsize must be larger than qcache_max to prevent\r\ninfinite recursion; (2) by numerical luck, this slab size\r\nprovides near−perfect slab packing (e.g. five 3−page\r\nobjects fill 15/16 of a 16−page slab); and (3) we’ll see\r\nbelow that using a common slab size for all quantum\r\ncaches helps to reduce overall arena fragmentation.\r\n4.4.5. Fragmentation\r\nFragmentation is the disintegration of a resource into\r\nunusably small, discontiguous segments. To see how\r\nthis can happen, imagine allocating a 1GB resource\r\none byte at a time, then freeing only the even−\r\nnumbered bytes. The arena would then have 500MB\r\nfree, yet it could not even satisfy a 2−byte allocation.\r\nWe observe that it is the combination of different\r\nallocation sizes and different segment lifetimes that\r\ncauses persistent fragmentation. If all allocations are\r\nthe same size, then any freed segment can obviously\r\nsatisfy another allocation of the same size. If all\r\nallocations are transient, the fragmentation is transient.\r\n“A waste is a terrible thing to mind.” − Anonymous",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/d1deaa9a-ed23-403e-985e-1683508937cb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=890bffef52633a354330c8644fb71daac485c971c67c33e7cce455d4f09fc31c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 814
      },
      {
        "segments": [
          {
            "segment_id": "d1deaa9a-ed23-403e-985e-1683508937cb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 13,
            "page_width": 612,
            "page_height": 792,
            "content": "The algorithm for selecting a free segment depends on\r\nthe allocation policy specified in the flags to\r\nvmem_alloc() as follows; in all cases, assume that\r\nthe allocation size lies in the range [2n\r\n, 2n+1):\r\n VM_BESTFIT. Search for the smallest segment on\r\nfreelist[n] that can satisfy the allocation.\r\n VM_INSTANTFIT. If the size is exactly 2n, take\r\nthe first segment on freelist[n]. Otherwise,\r\ntake the first segment on freelist[n+1]. Any\r\nsegment on this freelist is necessarily large enough\r\nto satisfy the allocation, so we get constant−time\r\nperformance with a reasonably good fit.*\r\n VM_NEXTFIT. Ignore the freelists altogether and\r\nsearch the arena for the next free segment after the\r\none previously allocated.\r\nOnce we’ve selected a segment, we remove it from its\r\nfreelist. If the segment is not an exact fit we split the\r\nsegment, create a boundary tag for the remainder, and\r\nput the remainder on the appropriate freelist. We then\r\nadd our newly−allocated segment’s boundary tag to a\r\nhash table so vmem_free() can find it quickly.\r\nvmem_free() is straightforward: it looks up the\r\nsegment’s boundary tag in the allocated−segment hash\r\ntable, removes it from the hash table, tries to coalesce\r\nthe segment with its neighbors, and puts it on the\r\nappropriate freelist. All operations are constant−time.\r\nNote that the hash lookup also provides a cheap and\r\neffective sanity check: the freed address must be in the\r\nhash table, and the freed size must match the segment\r\nsize. This helps to catch bugs such as duplicate frees.\r\nThe key feature of the algorithm described above is\r\nthat its performance is independent of both transaction\r\nsize and arena fragmentation. Vmem appears to be\r\nthe first resource allocator that can perform allocations\r\nand frees of any size in guaranteed constant time.\r\n4.4.3. Locking Strategy\r\nFor simplicity, we protect each arena’s segment list,\r\nfreelists, and hash table with a global lock. We rely\r\non the fact that large allocations are relatively rare,\r\nand allow the arena’s quantum caches to provide\r\nlinear scalability for all the common allocation sizes.\r\nThis strategy is very effective in practice, as illustrated\r\nby the performance data in §4.5 and the allocation\r\nstatistics for a large Solaris 8 server in Appendix B.\r\n*We like instant−fit because it guarantees constant time performance,\r\nprovides low fragmentation in practice, and is easy to implement.\r\nThere are many other techniques for choosing a suitable free segment\r\nin reasonable (e.g. logarithmic) time, such as keeping all free\r\nsegments in a size−sorted tree; see [Wilson95] for a thorough survey.\r\nAny of these techniques could be used for a vmem implementation.\r\n4.4.4. Quantum Caching\r\nThe slab allocator can provide object caching for any\r\nvmem arena (§5.1), so vmem’s quantum caches are\r\nactually implemented as object caches. For each small\r\ninteger multiple of the arena’s quantum we create an\r\nobject cache to service requests of that size.\r\nvmem_alloc() and vmem_free() simply convert\r\neach small request (size <= qcache_max) into a\r\nkmem_cache_alloc() or kmem_cache_free() on\r\nthe appropriate cache, as illustrated in Figure 4.4.\r\nBecause it is based on object caching, quantum\r\ncaching provides very low latency and linear\r\nscalability for the most common allocation sizes.\r\n Example. Assume the arena shown in Figure 4.4.\r\nA 3−page allocation would proceed as follows:\r\nvmem_alloc(foo, 3 * PAGESIZE) would call\r\nkmem_cache_alloc(foo->vm_qcache[2]). In\r\nmost cases the cache’s magazine layer would\r\nsatisfy the allocation, and we would be done. If\r\nthe cache needed to create a new slab it would call\r\nvmem_alloc(foo, 16 * PAGESIZE), which\r\nwould be satisfied from the arena’s segment list.\r\nThe slab allocator would then divide its 16−page\r\nslab into five 3−page objects and use one of them\r\nto satisfy the original allocation.\r\nWhen we create an arena’s quantum caches we pass a\r\nflag to kmem_cache_create(), KMC_QCACHE, that\r\ndirects the slab allocator to use a particular slab size:\r\nthe next power of two above 3 * qcache_max. We\r\nuse this particular value for three reasons: (1) the slab\r\nsize must be larger than qcache_max to prevent\r\ninfinite recursion; (2) by numerical luck, this slab size\r\nprovides near−perfect slab packing (e.g. five 3−page\r\nobjects fill 15/16 of a 16−page slab); and (3) we’ll see\r\nbelow that using a common slab size for all quantum\r\ncaches helps to reduce overall arena fragmentation.\r\n4.4.5. Fragmentation\r\nFragmentation is the disintegration of a resource into\r\nunusably small, discontiguous segments. To see how\r\nthis can happen, imagine allocating a 1GB resource\r\none byte at a time, then freeing only the even−\r\nnumbered bytes. The arena would then have 500MB\r\nfree, yet it could not even satisfy a 2−byte allocation.\r\nWe observe that it is the combination of different\r\nallocation sizes and different segment lifetimes that\r\ncauses persistent fragmentation. If all allocations are\r\nthe same size, then any freed segment can obviously\r\nsatisfy another allocation of the same size. If all\r\nallocations are transient, the fragmentation is transient.\r\n“A waste is a terrible thing to mind.” − Anonymous",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/d1deaa9a-ed23-403e-985e-1683508937cb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=890bffef52633a354330c8644fb71daac485c971c67c33e7cce455d4f09fc31c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 814
      },
      {
        "segments": [
          {
            "segment_id": "d3bd59f4-63d6-43f7-9bb6-f71b28d30a89",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 14,
            "page_width": 612,
            "page_height": 792,
            "content": "We have no control over segment lifetime, but\r\nquantum caching offers some control over allocation\r\nsize: namely, all quantum caches have the same slab\r\nsize, so most allocations from the arena’s segment list\r\noccur in slab−size chunks.\r\nAt first it may appear that all we’ve done is move the\r\nproblem: the segment list won’t fragment as much, but\r\nnow the quantum caches themselves can suffer\r\nfragmentation in the form of partially−used slabs. The\r\ncritical difference is that the free objects in a quantum\r\ncache are of a size that’s known to be useful, whereas\r\nthe segment list can disintegrate into useless pieces\r\nunder hostile workloads. Moreover, prior allocation is\r\na good predictor of future allocation [Weinstock88], so\r\nfree objects are likely to be used again.\r\nIt is impossible to prove that this helps,* but it seems\r\nto work well in practice. We have never had a report\r\nof severe fragmentation since vmem’s introduction\r\n(we had many such reports with the old resource map\r\nallocator), and Solaris systems often stay up for years.\r\n4.5. Performance\r\n4.5.1. Microbenchmark Performance\r\nWe’ve stated that vmem_alloc() and vmem_free()\r\nare constant−time operations regardless of arena\r\nfragmentation, whereas rmalloc() and rmfree()\r\nare linear−time. We measured alloc/free latency as a\r\nfunction of fragmentation to verify this.\r\nrmalloc() has a slight performance edge at very low\r\nfragmentation because the algorithm is so naïve. At\r\nzero fragmentation, vmem’s latency without quantum\r\ncaching was 1560ns, vs. 715ns for rmalloc().\r\nQuantum caching reduces vmem’s latency to just\r\n482ns, so for allocations that go to the quantum caches\r\n(the common case) vmem is faster than rmalloc()\r\neven at very low fragmentation.\r\n*In fact, it has been proven that “there is no reliable algorithm for\r\nensuring efficient memory usage, and none is possible.” [Wilson95]\r\n4.5.2. System−Level Performance\r\nVmem’s low latency and linear scaling remedied\r\nserious pathologies in the performance of kernel\r\nvirtual address allocation under rmalloc(), yielding\r\ndramatic improvements in system−level performance.\r\n LADDIS. Veritas reported a 50% improvement in\r\nLADDIS peak throughput with the new virtual\r\nmemory allocator [Taylor99].\r\n Web Service. On a large Starfire system running\r\n2700 Netscape servers under Softway’s Share II\r\nscheduler, vmem reduced system time from 60% to\r\n10%, roughly doubling system capacity [Swain98].\r\n I/O Bandwidth. An internal I/O benchmark on a\r\n64−CPU Starfire generated such heavy contention\r\non the old rmalloc() lock that the system was\r\nessentially useless. Contention was exacerbated by\r\nvery long hold times due to rmalloc()’s linear\r\nsearch of the increasingly fragmented kernel heap.\r\nlockstat(1M) (a Solaris utility that measures\r\nkernel lock contention) revealed that threads were\r\nspinning for an average of 48 milliseconds to\r\nacquire the rmalloc() lock, thus limiting I/O\r\nbandwidth to just 1000/48 = 21 I/O operations per\r\nsecond per CPU. With vmem the problem\r\ncompletely disappeared and performance improved\r\nby several orders of magnitude.\r\n4.6. Summary\r\nThe vmem interface supports both simple and highly\r\nconstrained allocations, and its importing mechanism\r\ncan build complex resources from simple components.\r\nThe interface is sufficiently general that we’ve been\r\nable to eliminate over 30 special−purpose allocators in\r\nSolaris since vmem’s introduction.\r\nThe vmem implementation has proven to be very fast\r\nand scalable, improving performance on system−level\r\nbenchmarks by 50% or more. It has also proven to be\r\nvery robust against fragmentation in practice.\r\nVmem’s instant−fit policy and external boundary tags\r\nappear to be new concepts. They guarantee constant−\r\ntime performance regardless of allocation size or arena\r\nfragmentation.\r\nVmem’s quantum caches provide very low latency and\r\nlinear scalability for the most common allocations.\r\nThey also present a particularly friendly workload to\r\nthe arena’s segment list, which helps to reduce overall\r\narena fragmentation.\r\nFigure 4.5.1: Latency vs. Fragmentation\r\nFragmentation (Discontiguous Free Segments)\r\n0 100 200 300 400 500 600 700 800 900 1000\r\nLatency (usec)\r\n0\r\n10\r\n20\r\n30 vmem\r\nrmalloc",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/d3bd59f4-63d6-43f7-9bb6-f71b28d30a89.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=591bd957e468b2fc42adae54709fc9af9f3c8b0729cfaa922408bb696a5a1b85",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 626
      },
      {
        "segments": [
          {
            "segment_id": "d3bd59f4-63d6-43f7-9bb6-f71b28d30a89",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 14,
            "page_width": 612,
            "page_height": 792,
            "content": "We have no control over segment lifetime, but\r\nquantum caching offers some control over allocation\r\nsize: namely, all quantum caches have the same slab\r\nsize, so most allocations from the arena’s segment list\r\noccur in slab−size chunks.\r\nAt first it may appear that all we’ve done is move the\r\nproblem: the segment list won’t fragment as much, but\r\nnow the quantum caches themselves can suffer\r\nfragmentation in the form of partially−used slabs. The\r\ncritical difference is that the free objects in a quantum\r\ncache are of a size that’s known to be useful, whereas\r\nthe segment list can disintegrate into useless pieces\r\nunder hostile workloads. Moreover, prior allocation is\r\na good predictor of future allocation [Weinstock88], so\r\nfree objects are likely to be used again.\r\nIt is impossible to prove that this helps,* but it seems\r\nto work well in practice. We have never had a report\r\nof severe fragmentation since vmem’s introduction\r\n(we had many such reports with the old resource map\r\nallocator), and Solaris systems often stay up for years.\r\n4.5. Performance\r\n4.5.1. Microbenchmark Performance\r\nWe’ve stated that vmem_alloc() and vmem_free()\r\nare constant−time operations regardless of arena\r\nfragmentation, whereas rmalloc() and rmfree()\r\nare linear−time. We measured alloc/free latency as a\r\nfunction of fragmentation to verify this.\r\nrmalloc() has a slight performance edge at very low\r\nfragmentation because the algorithm is so naïve. At\r\nzero fragmentation, vmem’s latency without quantum\r\ncaching was 1560ns, vs. 715ns for rmalloc().\r\nQuantum caching reduces vmem’s latency to just\r\n482ns, so for allocations that go to the quantum caches\r\n(the common case) vmem is faster than rmalloc()\r\neven at very low fragmentation.\r\n*In fact, it has been proven that “there is no reliable algorithm for\r\nensuring efficient memory usage, and none is possible.” [Wilson95]\r\n4.5.2. System−Level Performance\r\nVmem’s low latency and linear scaling remedied\r\nserious pathologies in the performance of kernel\r\nvirtual address allocation under rmalloc(), yielding\r\ndramatic improvements in system−level performance.\r\n LADDIS. Veritas reported a 50% improvement in\r\nLADDIS peak throughput with the new virtual\r\nmemory allocator [Taylor99].\r\n Web Service. On a large Starfire system running\r\n2700 Netscape servers under Softway’s Share II\r\nscheduler, vmem reduced system time from 60% to\r\n10%, roughly doubling system capacity [Swain98].\r\n I/O Bandwidth. An internal I/O benchmark on a\r\n64−CPU Starfire generated such heavy contention\r\non the old rmalloc() lock that the system was\r\nessentially useless. Contention was exacerbated by\r\nvery long hold times due to rmalloc()’s linear\r\nsearch of the increasingly fragmented kernel heap.\r\nlockstat(1M) (a Solaris utility that measures\r\nkernel lock contention) revealed that threads were\r\nspinning for an average of 48 milliseconds to\r\nacquire the rmalloc() lock, thus limiting I/O\r\nbandwidth to just 1000/48 = 21 I/O operations per\r\nsecond per CPU. With vmem the problem\r\ncompletely disappeared and performance improved\r\nby several orders of magnitude.\r\n4.6. Summary\r\nThe vmem interface supports both simple and highly\r\nconstrained allocations, and its importing mechanism\r\ncan build complex resources from simple components.\r\nThe interface is sufficiently general that we’ve been\r\nable to eliminate over 30 special−purpose allocators in\r\nSolaris since vmem’s introduction.\r\nThe vmem implementation has proven to be very fast\r\nand scalable, improving performance on system−level\r\nbenchmarks by 50% or more. It has also proven to be\r\nvery robust against fragmentation in practice.\r\nVmem’s instant−fit policy and external boundary tags\r\nappear to be new concepts. They guarantee constant−\r\ntime performance regardless of allocation size or arena\r\nfragmentation.\r\nVmem’s quantum caches provide very low latency and\r\nlinear scalability for the most common allocations.\r\nThey also present a particularly friendly workload to\r\nthe arena’s segment list, which helps to reduce overall\r\narena fragmentation.\r\nFigure 4.5.1: Latency vs. Fragmentation\r\nFragmentation (Discontiguous Free Segments)\r\n0 100 200 300 400 500 600 700 800 900 1000\r\nLatency (usec)\r\n0\r\n10\r\n20\r\n30 vmem\r\nrmalloc",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/d3bd59f4-63d6-43f7-9bb6-f71b28d30a89.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=591bd957e468b2fc42adae54709fc9af9f3c8b0729cfaa922408bb696a5a1b85",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 626
      },
      {
        "segments": [
          {
            "segment_id": "3c820a4d-243e-4b0b-9e08-d06f71774848",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 15,
            "page_width": 612,
            "page_height": 792,
            "content": "5. Core Slab Allocator Enhancements\r\nSections 3 and 4 described the magazine and vmem\r\nlayers, two new technologies above and below the slab\r\nlayer. In this section we describe two vmem−related\r\nenhancements to the slab allocator itself.\r\n5.1. Object Caching for Any Resource\r\nThe original slab allocator used rmalloc() to get\r\nkernel heap addresses for its slabs and invoked the VM\r\nsystem to back those addresses with physical pages.\r\nEvery object cache now uses a vmem arena as its slab\r\nsupplier. The slab allocator simply invokes\r\nvmem_alloc() and vmem_free() to create and\r\ndestroy slabs. It makes no assumptions about the\r\nnature of the resource it’s managing, so it can provide\r\nobject caching for any arena.* This feature is what\r\nmakes vmem’s high−performance quantum caching\r\npossible (§4.4.4). \r\n5.2. Reclaim Callbacks \r\nFor performance, the kernel caches things that aren’t\r\nstrictly needed. The DNLC (directory name lookup\r\ncache) improves pathname resolution performance, for\r\nexample, but most DNLC entries aren’t actually in use\r\nat any given moment. If the DNLC could be notified\r\nwhen the system was running low on memory, it could\r\nfree some of its entries to relieve memory pressure.\r\nWe support this by allowing clients to specify a\r\nreclaim callback to kmem_cache_create(). The\r\nallocator calls this function when the cache’s vmem\r\narena is running low on resources. The callback is\r\npurely advisory; what it actually does is entirely up to\r\nthe client. A typical action might be to give back\r\nsome fraction of the objects, or to free all objects that\r\nhaven’t been accessed in the last N seconds.\r\nThis capability allows clients like the DNLC, inode\r\ncache and NFS_READDIR cache to grow more or less\r\nunrestricted until the system runs low on memory, at\r\nwhich point they are asked to start giving some back.\r\nOne possible future enhancement would be to add an\r\nargument to the reclaim callback to indicate the\r\nnumber of bytes wanted, or the “level of desperation.”\r\nWe have not yet done so because simple callback\r\npolicies like “give back 10% each time I’m called”\r\nhave proven to be perfectly adequate in practice. \r\n*For caches backed by non−memory vmem arenas, the caller must\r\nspecify the KMC_NOTOUCH flag to kmem_cache_create() so\r\nthe allocator won’t try to use free buffers to hold its internal state.\r\n6. User−Level Memory Allocation:\r\n The libumem Library\r\nIt was relatively straightforward to transplant the\r\nmagazine, slab, and vmem technologies to user−level.\r\nWe created a library, libumem, that provides all the\r\nsame services. In this section we discuss the handful\r\nof porting issues that came up and compare libumem’s\r\nperformance to other user−level memory allocators.\r\nlibumem is still experimental as of this writing.\r\n6.1. Porting Issues\r\nThe allocation code (magazine, slab, and vmem) was\r\nessentially unchanged; the challenge was to find user−\r\nlevel replacements for the kernel functionality on\r\nwhich it relies, and to accommodate the limitations\r\nand interface requirements of user−level library code.\r\n CPU ID. The kernel uses the CPU ID, which can\r\nbe determined in just a few instructions, to index\r\ninto a cache’s cache_cpu[] array. There is no\r\nequivalent of CPU ID in the thread library; we\r\nneed one.** For the prototype we just hashed on the\r\nthread ID, which is available cheaply in libthread.\r\n Memory Pressure. In the kernel, the VM system\r\ninvokes kmem_reap() when system−wide free\r\nmemory runs low. There is no equivalent concept\r\nin userland. In libumem we check the depot\r\nworking set size whenever we access the depot and\r\nreturn any excess to the slab layer.\r\n Supporting malloc(3C) and free(3C). To\r\nimplement malloc() and free() we create a set\r\nof about 30 fixed−size object caches to handle\r\nsmall−to−medium malloc() requests. We use\r\nmalloc()’s size argument to index into a table\r\nto locate the nearest cache, e.g. malloc(350)\r\ngoes to the umem_alloc_384 cache. For larger\r\nallocations we use the VM system directly, i.e.\r\nsbrk(2) or mmap(2). We prepend an 8−byte\r\nboundary tag to each buffer so we can determine\r\nits size in free().\r\n Initialization. The cost of initializing the kernel\r\nmemory allocator is trivial compared to the cost of\r\nbooting, but the cost of initializing libumem is not\r\nentirely trivial compared to the cost of exec(2),\r\nprimarily because libumem must create the 30\r\nstandard caches that support malloc/free. We\r\ntherefore create these caches lazily (on demand).\r\n**Our game plan is to make the kernel and thread library cooperate,\r\nso that whenever the kernel dispatches a thread to a different CPU, it\r\nstores the new CPU ID in the user−level thread structure.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/3c820a4d-243e-4b0b-9e08-d06f71774848.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f0d0cf9db9af7ca552ef6021023827b1ff6030163ed650381f785c405e493d2e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 746
      },
      {
        "segments": [
          {
            "segment_id": "3c820a4d-243e-4b0b-9e08-d06f71774848",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 15,
            "page_width": 612,
            "page_height": 792,
            "content": "5. Core Slab Allocator Enhancements\r\nSections 3 and 4 described the magazine and vmem\r\nlayers, two new technologies above and below the slab\r\nlayer. In this section we describe two vmem−related\r\nenhancements to the slab allocator itself.\r\n5.1. Object Caching for Any Resource\r\nThe original slab allocator used rmalloc() to get\r\nkernel heap addresses for its slabs and invoked the VM\r\nsystem to back those addresses with physical pages.\r\nEvery object cache now uses a vmem arena as its slab\r\nsupplier. The slab allocator simply invokes\r\nvmem_alloc() and vmem_free() to create and\r\ndestroy slabs. It makes no assumptions about the\r\nnature of the resource it’s managing, so it can provide\r\nobject caching for any arena.* This feature is what\r\nmakes vmem’s high−performance quantum caching\r\npossible (§4.4.4). \r\n5.2. Reclaim Callbacks \r\nFor performance, the kernel caches things that aren’t\r\nstrictly needed. The DNLC (directory name lookup\r\ncache) improves pathname resolution performance, for\r\nexample, but most DNLC entries aren’t actually in use\r\nat any given moment. If the DNLC could be notified\r\nwhen the system was running low on memory, it could\r\nfree some of its entries to relieve memory pressure.\r\nWe support this by allowing clients to specify a\r\nreclaim callback to kmem_cache_create(). The\r\nallocator calls this function when the cache’s vmem\r\narena is running low on resources. The callback is\r\npurely advisory; what it actually does is entirely up to\r\nthe client. A typical action might be to give back\r\nsome fraction of the objects, or to free all objects that\r\nhaven’t been accessed in the last N seconds.\r\nThis capability allows clients like the DNLC, inode\r\ncache and NFS_READDIR cache to grow more or less\r\nunrestricted until the system runs low on memory, at\r\nwhich point they are asked to start giving some back.\r\nOne possible future enhancement would be to add an\r\nargument to the reclaim callback to indicate the\r\nnumber of bytes wanted, or the “level of desperation.”\r\nWe have not yet done so because simple callback\r\npolicies like “give back 10% each time I’m called”\r\nhave proven to be perfectly adequate in practice. \r\n*For caches backed by non−memory vmem arenas, the caller must\r\nspecify the KMC_NOTOUCH flag to kmem_cache_create() so\r\nthe allocator won’t try to use free buffers to hold its internal state.\r\n6. User−Level Memory Allocation:\r\n The libumem Library\r\nIt was relatively straightforward to transplant the\r\nmagazine, slab, and vmem technologies to user−level.\r\nWe created a library, libumem, that provides all the\r\nsame services. In this section we discuss the handful\r\nof porting issues that came up and compare libumem’s\r\nperformance to other user−level memory allocators.\r\nlibumem is still experimental as of this writing.\r\n6.1. Porting Issues\r\nThe allocation code (magazine, slab, and vmem) was\r\nessentially unchanged; the challenge was to find user−\r\nlevel replacements for the kernel functionality on\r\nwhich it relies, and to accommodate the limitations\r\nand interface requirements of user−level library code.\r\n CPU ID. The kernel uses the CPU ID, which can\r\nbe determined in just a few instructions, to index\r\ninto a cache’s cache_cpu[] array. There is no\r\nequivalent of CPU ID in the thread library; we\r\nneed one.** For the prototype we just hashed on the\r\nthread ID, which is available cheaply in libthread.\r\n Memory Pressure. In the kernel, the VM system\r\ninvokes kmem_reap() when system−wide free\r\nmemory runs low. There is no equivalent concept\r\nin userland. In libumem we check the depot\r\nworking set size whenever we access the depot and\r\nreturn any excess to the slab layer.\r\n Supporting malloc(3C) and free(3C). To\r\nimplement malloc() and free() we create a set\r\nof about 30 fixed−size object caches to handle\r\nsmall−to−medium malloc() requests. We use\r\nmalloc()’s size argument to index into a table\r\nto locate the nearest cache, e.g. malloc(350)\r\ngoes to the umem_alloc_384 cache. For larger\r\nallocations we use the VM system directly, i.e.\r\nsbrk(2) or mmap(2). We prepend an 8−byte\r\nboundary tag to each buffer so we can determine\r\nits size in free().\r\n Initialization. The cost of initializing the kernel\r\nmemory allocator is trivial compared to the cost of\r\nbooting, but the cost of initializing libumem is not\r\nentirely trivial compared to the cost of exec(2),\r\nprimarily because libumem must create the 30\r\nstandard caches that support malloc/free. We\r\ntherefore create these caches lazily (on demand).\r\n**Our game plan is to make the kernel and thread library cooperate,\r\nso that whenever the kernel dispatches a thread to a different CPU, it\r\nstores the new CPU ID in the user−level thread structure.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/3c820a4d-243e-4b0b-9e08-d06f71774848.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f0d0cf9db9af7ca552ef6021023827b1ff6030163ed650381f785c405e493d2e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 746
      },
      {
        "segments": [
          {
            "segment_id": "16a2bc69-6503-4f1d-aaa7-d66c3a0521f0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 16,
            "page_width": 612,
            "page_height": 792,
            "content": "6.2. Performance\r\nA complete analysis of user−level memory allocators\r\nis beyond the scope of this paper, so we compared\r\nlibumem only to the strongest competition:\r\n the Hoard allocator [Berger00], which appears to\r\nbe the current best−of−breed among scalable user−\r\nlevel memory allocators;\r\n ptmalloc [Gloger01], a widely used multithreaded\r\nmalloc used in the GNU C library;\r\n the Solaris mtmalloc library.\r\nWe also benchmarked the Solaris C library’s malloc\r\n[Sleator85] to establish a single−threaded baseline.\r\nDuring our measurements we found several serious\r\nscalability problems with the Solaris mtmalloc library.\r\nmtmalloc creates per−CPU power−of−two freelists for\r\nsizes up to 64K, but its algorithm for selecting a\r\nfreelist was simply round−robin; thus its workload was\r\nmerely fanned out, not made CPU−local. Moreover,\r\nthe round−robin index was itself a global variable, so\r\nfrequent increments by all CPUs caused severe\r\ncontention for its cache line. We also found that\r\nmtmalloc’s per−CPU data structures were not suitably\r\npadded and aligned to cache line boundaries to prevent\r\nfalse sharing, as discussed in §3.6.\r\nWe fixed mtmalloc to select a per−CPU freelist by\r\nthread ID hashing as in libumem, and we padded and\r\naligned its per−CPU data structures. These changes\r\nimproved the scalability of mtmalloc dramatically,\r\nmaking it competitive with Hoard and libumem.\r\nWe measured the allocators’ scalability on a 10−CPU\r\nE4000 using the methods described in §3.8. Figure\r\n6.2 shows that libc’s malloc and the original mtmalloc\r\nperform abysmally as the number of threads increases.\r\nptmalloc provides good scalability up to 8 CPUs, but\r\nappears not to scale beyond that. By contrast,\r\nlibumem, Hoard, and the fixed mtmalloc all show\r\nlinear scaling. Only the slopes differ, with libumem\r\nbeing the fastest.\r\nFigure 6.2: malloc/free Performance\r\nNote: the shaded area indicates data points where the number of threads exceeds the number of CPUs; all curves\r\nnecessarily flatten at that point. An allocator with linear scaling should be linear up to the shaded area, then flat.\r\n0\r\n0.5M\r\n1M\r\n1.5M\r\n2M\r\n2.5M\r\n3M\r\n1 2 3 4 5 6 7 8 9 10 11 12\r\nThroughput (alloc/free pairs per second)\r\nNumber of threads (10-CPU system)\r\nlibumem\r\nhoard\r\nmtmalloc (fixed)\r\nptmalloc\r\nmtmalloc (original)\r\nlibc",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/16a2bc69-6503-4f1d-aaa7-d66c3a0521f0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=277bb5618d81f21a97d6ce6329b8e231798cb2117d7e255c30f27cdc2ab9e2f4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 356
      },
      {
        "segments": [
          {
            "segment_id": "f54467a7-fa6e-4367-a8b6-2f1b39e71bcd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 17,
            "page_width": 612,
            "page_height": 792,
            "content": "7. Conclusions\r\nThe enduring lesson from our experience with the slab\r\nallocator is that it is essential to create excellent core\r\nservices. It may seem strange at first, but core\r\nservices are often the most neglected.\r\nPeople working on a particular performance problem\r\nsuch as web server performance typically focus on a\r\nspecific goal like better SPECweb99 numbers. If\r\nprofiling data suggests that a core system service is\r\none of the top five problems, our hypothetical\r\nSPECweb99 performance team is more likely to find a\r\nquick−and−dirty way to avoid that service than to\r\nembark on a major detour from their primary task and\r\nredesign the offending subsystem. This is how we\r\nended up with over 30 special−purpose allocators\r\nbefore the advent of vmem.\r\nSuch quick−and−dirty solutions, while adequate at the\r\ntime, do not advance operating system technology.\r\nQuite the opposite: they make the system more\r\ncomplex, less maintainable, and leave behind a mess\r\nof ticking time bombs that will eventually have to be\r\ndealt with. None of our 30 special−purpose allocators,\r\nfor example, had anything like a magazine layer; thus\r\nevery one of them was a scalability problem in\r\nwaiting. (In fact, some were no longer waiting.)\r\nBefore 1994, Solaris kernel engineers avoided the\r\nmemory allocator because it was known to be slow.\r\nNow, by contrast, our engineers actively seek ways to\r\nuse the allocator because it is known to be fast and\r\nscalable. They also know that the allocator provides\r\nextensive statistics and debugging support, which\r\nmakes whatever they’re doing that much easier.\r\nWe currently use the allocator to manage ordinary\r\nkernel memory, virtual memory, DMA, minor device\r\nnumbers, System V semaphores, thread stacks and task\r\nIDs. More creative uses are currently in the works,\r\nincluding using the allocator to manage pools of\r\nworker threads − the idea being that the depot working\r\nset provides an effective algorithm to manage the size\r\nof the thread pool. And in the near future, libumem\r\nwill bring all of this technology to user−level\r\napplications and libraries.\r\nWe’ve demonstrated that magazines and vmem have\r\nimproved performance on real−world system−level\r\nbenchmarks by 50% or more. But equally important,\r\nwe achieved these gains by investing in a core system\r\nservice (resource allocation) that many other project\r\nteams have built on. Investing in core services is\r\ncritical to maintaining and evolving a fast, reliable\r\noperating system.\r\nAcknowledgments\r\nWe would like to thank:\r\n Bruce Curtis, Denis Sheahan, Peter Swain, Randy\r\nTaylor, Sunay Tripathi, and Yufei Zhu for\r\nsystem−level performance measurements;\r\n Bryan Cantrill, Dan Price and Mike Shapiro for\r\ncreating an excellent suite of debugging tools for\r\nthe slab and vmem allocators, now available in\r\nSolaris 8 as part of mdb(1);\r\n Mohit Aron, Cathy Bonwick, Bryan Cantrill, Roger\r\nFaulkner, Dave Powell, Jonathan Shapiro, Mike\r\nShapiro, Bart Smaalders, Bill Sommerfeld, and\r\nMike Sullivan for many helpful comments on draft\r\nversions of the paper.\r\nReferences\r\nMagazines and vmem are part of Solaris 8. The\r\nsource is available for free download at www.sun.com.\r\nFor general background, [Wilson95] provides an\r\nextensive survey of memory allocation techniques. In\r\naddition, the references in [Berger00], [Bonwick94],\r\nand [Wilson95] list dozens of excellent papers on\r\nmemory allocation.\r\n[Berger00] Emery D. Berger, Kathryn S. McKinley,\r\nRobert D. Blumofe, Paul R. Wilson. Hoard: A\r\nScalable Memory Allocator for Multithreaded\r\nApplications. ASPLOS−IX, Cambridge, MA,\r\nNovember 2000. Available at http://www.hoard.org.\r\n[BIRD01] BIRD Programmer’s Documentation.\r\nAvailable at http://bird.network.cz.\r\n[Bonwick94] Jeff Bonwick. The Slab Allocator: An\r\nObject−Caching Kernel Memory Allocator. Summer\r\n1994 Usenix Conference, pp. 87−98. Available at\r\nhttp://www.usenix.org.\r\n[Bovet00] Daniel P. Bovet and Marco Cesati.\r\nUnderstanding the Linux Kernel. Prentice Hall, 2000.\r\n[Denning68] Peter J. Denning. The Working Set\r\nModel for Program Behaviour. CACM 11(5), 1968,\r\npp. 323−333.\r\n[FreeBSD01] The FreeBSD source code. Available at\r\nhttp://www.freebsd.org.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/f54467a7-fa6e-4367-a8b6-2f1b39e71bcd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d2b58d0492d1d45879261c86cd26c0018eaf1d6f3ec8ce555a4d0001829ce6e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 618
      },
      {
        "segments": [
          {
            "segment_id": "f54467a7-fa6e-4367-a8b6-2f1b39e71bcd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 17,
            "page_width": 612,
            "page_height": 792,
            "content": "7. Conclusions\r\nThe enduring lesson from our experience with the slab\r\nallocator is that it is essential to create excellent core\r\nservices. It may seem strange at first, but core\r\nservices are often the most neglected.\r\nPeople working on a particular performance problem\r\nsuch as web server performance typically focus on a\r\nspecific goal like better SPECweb99 numbers. If\r\nprofiling data suggests that a core system service is\r\none of the top five problems, our hypothetical\r\nSPECweb99 performance team is more likely to find a\r\nquick−and−dirty way to avoid that service than to\r\nembark on a major detour from their primary task and\r\nredesign the offending subsystem. This is how we\r\nended up with over 30 special−purpose allocators\r\nbefore the advent of vmem.\r\nSuch quick−and−dirty solutions, while adequate at the\r\ntime, do not advance operating system technology.\r\nQuite the opposite: they make the system more\r\ncomplex, less maintainable, and leave behind a mess\r\nof ticking time bombs that will eventually have to be\r\ndealt with. None of our 30 special−purpose allocators,\r\nfor example, had anything like a magazine layer; thus\r\nevery one of them was a scalability problem in\r\nwaiting. (In fact, some were no longer waiting.)\r\nBefore 1994, Solaris kernel engineers avoided the\r\nmemory allocator because it was known to be slow.\r\nNow, by contrast, our engineers actively seek ways to\r\nuse the allocator because it is known to be fast and\r\nscalable. They also know that the allocator provides\r\nextensive statistics and debugging support, which\r\nmakes whatever they’re doing that much easier.\r\nWe currently use the allocator to manage ordinary\r\nkernel memory, virtual memory, DMA, minor device\r\nnumbers, System V semaphores, thread stacks and task\r\nIDs. More creative uses are currently in the works,\r\nincluding using the allocator to manage pools of\r\nworker threads − the idea being that the depot working\r\nset provides an effective algorithm to manage the size\r\nof the thread pool. And in the near future, libumem\r\nwill bring all of this technology to user−level\r\napplications and libraries.\r\nWe’ve demonstrated that magazines and vmem have\r\nimproved performance on real−world system−level\r\nbenchmarks by 50% or more. But equally important,\r\nwe achieved these gains by investing in a core system\r\nservice (resource allocation) that many other project\r\nteams have built on. Investing in core services is\r\ncritical to maintaining and evolving a fast, reliable\r\noperating system.\r\nAcknowledgments\r\nWe would like to thank:\r\n Bruce Curtis, Denis Sheahan, Peter Swain, Randy\r\nTaylor, Sunay Tripathi, and Yufei Zhu for\r\nsystem−level performance measurements;\r\n Bryan Cantrill, Dan Price and Mike Shapiro for\r\ncreating an excellent suite of debugging tools for\r\nthe slab and vmem allocators, now available in\r\nSolaris 8 as part of mdb(1);\r\n Mohit Aron, Cathy Bonwick, Bryan Cantrill, Roger\r\nFaulkner, Dave Powell, Jonathan Shapiro, Mike\r\nShapiro, Bart Smaalders, Bill Sommerfeld, and\r\nMike Sullivan for many helpful comments on draft\r\nversions of the paper.\r\nReferences\r\nMagazines and vmem are part of Solaris 8. The\r\nsource is available for free download at www.sun.com.\r\nFor general background, [Wilson95] provides an\r\nextensive survey of memory allocation techniques. In\r\naddition, the references in [Berger00], [Bonwick94],\r\nand [Wilson95] list dozens of excellent papers on\r\nmemory allocation.\r\n[Berger00] Emery D. Berger, Kathryn S. McKinley,\r\nRobert D. Blumofe, Paul R. Wilson. Hoard: A\r\nScalable Memory Allocator for Multithreaded\r\nApplications. ASPLOS−IX, Cambridge, MA,\r\nNovember 2000. Available at http://www.hoard.org.\r\n[BIRD01] BIRD Programmer’s Documentation.\r\nAvailable at http://bird.network.cz.\r\n[Bonwick94] Jeff Bonwick. The Slab Allocator: An\r\nObject−Caching Kernel Memory Allocator. Summer\r\n1994 Usenix Conference, pp. 87−98. Available at\r\nhttp://www.usenix.org.\r\n[Bovet00] Daniel P. Bovet and Marco Cesati.\r\nUnderstanding the Linux Kernel. Prentice Hall, 2000.\r\n[Denning68] Peter J. Denning. The Working Set\r\nModel for Program Behaviour. CACM 11(5), 1968,\r\npp. 323−333.\r\n[FreeBSD01] The FreeBSD source code. Available at\r\nhttp://www.freebsd.org.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/f54467a7-fa6e-4367-a8b6-2f1b39e71bcd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d2b58d0492d1d45879261c86cd26c0018eaf1d6f3ec8ce555a4d0001829ce6e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 618
      },
      {
        "segments": [
          {
            "segment_id": "88b62dc8-d7ac-4a4b-94e5-e23297697a09",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 18,
            "page_width": 612,
            "page_height": 792,
            "content": "[Gloger01] Source code and documentation for\r\nptmalloc are available on Wolfram Gloger’s home\r\npage at http://www.malloc.de.\r\n[Johnstone97] Mark S. Johnstone and Paul R.\r\nWilson. The Memory Fragmentation Problem:\r\nSolved? ISMM’98 Proceedings of the ACM\r\nSIGPLAN International Symposium on Memory\r\nManagement, pp. 26−36. Available at\r\nftp://ftp.dcs.gla.ac.uk/pub/drastic/gc/wilson.ps.\r\n[Khanna92] Sandeep Khanna, Michael Sebree and\r\nJohn Zolnowski. Realtime Scheduling in SunOS 5.0.\r\nWinter 1992 USENIX Conference.\r\n[Knuth73] Donald Knuth. The Art of Computer\r\nProgramming: Fundamental Algorithms. Addison\r\nWesley, 1973.\r\n[Linux01] The Linux source code. Available at\r\nhttp://www.linux.org.\r\n[Mauro00] Jim Mauro and Richard McDougall.\r\nSolaris Internals: Core Kernel Architecture. Prentice\r\nHall, 2000.\r\n[McKenney93] Paul E. McKenney and Jack\r\nSlingwine. Efficient Kernel Memory Allocation on\r\nShared−Memory Multiprocessors. Proceedings of the\r\nWinter 1993 Usenix Conference, pp. 295−305.\r\nAvailable at http://www.usenix.org.\r\n[Nemesis01] The Nemesis source code. Available at\r\nhttp://nemesis.sourceforge.net.\r\n[NetBSD01] The NetBSD source code. Available at\r\nhttp://www.netbsd.org.\r\n[OpenBSD01] The OpenBSD source code. Available\r\nat http://www.openbsd.org.\r\n[Perl01] The Perl source code. Available at\r\nhttp://www.perl.org.\r\n[Shapiro01] Jonathan Shapiro, personal communi−\r\ncation. Information on the EROS operating system is\r\navailable at http://www.eros−os.org.\r\n[Sleator85] D. D. Sleator and R. E. Tarjan. Self−\r\nAdjusting Binary Trees. JACM 1985.\r\n[SPEC01] Standard Performance Evaluation\r\nCorporation. Available at http://www.spec.org.\r\n[Swain98] Peter Swain, Softway. Personal\r\ncommunication.\r\n[Taylor99] Randy Taylor, Veritas Software. Personal\r\ncommunication.\r\n[TPC01] Transaction Processing Council. Available\r\nat http://www.tpc.org.\r\n[Vahalia96] Uresh Vahalia. UNIX Internals: The New\r\nFrontiers. Prentice Hall, 1996.\r\n[Weinstock88] Charles B. Weinstock and William A.\r\nWulf. QuickFit: An Efficient Algorithm for Heap\r\nStorage Allocation. ACM SIGPLAN Notices, v.23,\r\nno. 10, pp. 141−144 (1988).\r\n[Wilson95] Paul R. Wilson, Mark S. Johnstone,\r\nMichael Neely, David Boles. Dynamic Storage\r\nAllocation: A Survey and Critical Review.\r\nProceedings of the International Workshop on\r\nMemory Management, September 1995. Available at\r\nhttp://citeseer.nj.nec.com/wilson95dynamic.html.\r\nAuthor Information\r\nJeff Bonwick (bonwick@eng.sun.com) is a Senior\r\nStaff Engineer at Sun Microsystems. He works\r\nprimarily on core kernel services (allocators, lock\r\nprimitives, timing, filesystems, VM, scalability) and\r\nhas created several system observability tools such as\r\nkstat(3K), mpstat(1M) and lockstat(1M).\r\nHe is currently leading the design and implementation\r\nof a new storage architecture for Solaris.\r\nJonathan Adams (jonathan−adams@ofb.net) is a\r\nsenior at the California Institute of Technology. He\r\ndeveloped libumem during his summer internship at\r\nSun.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/88b62dc8-d7ac-4a4b-94e5-e23297697a09.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e546c9d0da704c673c1b549880f9e955cfc8f0a58eb78807a18040ba8a83766c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 357
      },
      {
        "segments": [
          {
            "segment_id": "b2aa710e-c0dc-407f-a1b8-284fd6f403e7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 19,
            "page_width": 612,
            "page_height": 792,
            "content": "Appendix A: Composing Vmem Arenas and Object Caches\r\nIn this Appendix we describe all the key steps to get from system boot to creating a complex object cache.\r\nAt compile time we statically declare a few vmem arena structures and boundary tags to get us through boot.\r\nDuring boot, the first arena we create is the primordial heap_arena, which defines the kernel virtual address\r\nrange to use for the kernel heap:\r\nheap_arena = vmem_create(\"heap\",\r\nkernelheap, heapsize, /* base and size of kernel heap */\r\nPAGESIZE, /* unit of currency is one page */\r\nNULL, NULL, NULL, /* nothing to import from -- heap is primordial */\r\n0, /* no quantum caching needed */\r\nVM_SLEEP); /* OK to wait for memory to create arena */\r\nvmem_create(), seeing that we’re early in boot, uses one of the statically declared arenas to represent the\r\nheap, and uses statically declared boundary tags to represent the heap’s initial span. Once we have the heap\r\narena, we can create new boundary tags dynamically. For simplicity, we always allocate a whole page of\r\nboundary tags at a time: we select a page of heap, map it, divvy it up into boundary tags, use one of those\r\nboundary tags to represent the heap page we just allocated, and put the rest on the arena’s free boundary tag list.\r\nNext, we create kmem_va_arena as a subset of heap_arena to provide virtual address caching (via quantum\r\ncaching) for up to 8 pages. Quantum caching improves performance and helps to minimize heap fragmentation,\r\nas we saw in §4.4.5. kmem_va_arena uses vmem_alloc() and vmem_free() to import from heap_arena:\r\nkmem_va_arena = vmem_create(\"kmem_va\",\r\nNULL, 0, /* no initial span; we import everything */\r\nPAGESIZE, /* unit of currency is one page */\r\nvmem_alloc, /* import allocation function */\r\nvmem_free, /* import free function */\r\nheap_arena, /* import vmem source */\r\n8 * PAGESIZE, /* quantum caching for up to 8 pages */\r\nVM_SLEEP); /* OK to wait for memory to create arena */\r\nFinally, we create kmem_default_arena, the backing store for most object caches. Its import function,\r\nsegkmem_alloc(), invokes vmem_alloc() to get virtual addresses and then backs them with physical pages:\r\nkmem_default_arena = vmem_create(\"kmem_default\",\r\nNULL, 0, /* no initial span; we import everything */\r\nPAGESIZE, /* unit of currency is one page */\r\nsegkmem_alloc, /* import allocation function */\r\nsegkmem_free, /* import free function */\r\nkmem_va_arena, /* import vmem source */\r\n0, /* no quantum caching needed */\r\nVM_SLEEP); /* OK to wait for memory to create arena */\r\nAt this point we have a simple page−level allocator: to get three pages of mapped kernel heap, we could call\r\nvmem_alloc(kmem_default_arena, 3 * PAGESIZE, VM_SLEEP) directly. In fact, this is precisely how\r\nthe slab allocator gets memory for new slabs. Finally, the kernel’s various subsystems create their object\r\ncaches. For example, the UFS filesystem creates its inode cache:\r\ninode_cache = kmem_cache_create(\"ufs_inode_cache\",\r\nsizeof (struct inode), /* object size */\r\n0, /* use allocator's default alignment */\r\nufs_inode_cache_constructor, /* inode constructor */\r\nufs_inode_cache_destructor, /* inode destructor */\r\nufs_inode_cache_reclaim, /* inode reclaim */\r\nNULL, /* argument to above funcs */\r\nNULL, /* implies kmem_default_arena */\r\n0); /* no special flags */",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/b2aa710e-c0dc-407f-a1b8-284fd6f403e7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=374388f197e24ed33a5e21ff8d2556d18374ed4b1fe8b8372316e27e74486477",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 523
      },
      {
        "segments": [
          {
            "segment_id": "b2aa710e-c0dc-407f-a1b8-284fd6f403e7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 19,
            "page_width": 612,
            "page_height": 792,
            "content": "Appendix A: Composing Vmem Arenas and Object Caches\r\nIn this Appendix we describe all the key steps to get from system boot to creating a complex object cache.\r\nAt compile time we statically declare a few vmem arena structures and boundary tags to get us through boot.\r\nDuring boot, the first arena we create is the primordial heap_arena, which defines the kernel virtual address\r\nrange to use for the kernel heap:\r\nheap_arena = vmem_create(\"heap\",\r\nkernelheap, heapsize, /* base and size of kernel heap */\r\nPAGESIZE, /* unit of currency is one page */\r\nNULL, NULL, NULL, /* nothing to import from -- heap is primordial */\r\n0, /* no quantum caching needed */\r\nVM_SLEEP); /* OK to wait for memory to create arena */\r\nvmem_create(), seeing that we’re early in boot, uses one of the statically declared arenas to represent the\r\nheap, and uses statically declared boundary tags to represent the heap’s initial span. Once we have the heap\r\narena, we can create new boundary tags dynamically. For simplicity, we always allocate a whole page of\r\nboundary tags at a time: we select a page of heap, map it, divvy it up into boundary tags, use one of those\r\nboundary tags to represent the heap page we just allocated, and put the rest on the arena’s free boundary tag list.\r\nNext, we create kmem_va_arena as a subset of heap_arena to provide virtual address caching (via quantum\r\ncaching) for up to 8 pages. Quantum caching improves performance and helps to minimize heap fragmentation,\r\nas we saw in §4.4.5. kmem_va_arena uses vmem_alloc() and vmem_free() to import from heap_arena:\r\nkmem_va_arena = vmem_create(\"kmem_va\",\r\nNULL, 0, /* no initial span; we import everything */\r\nPAGESIZE, /* unit of currency is one page */\r\nvmem_alloc, /* import allocation function */\r\nvmem_free, /* import free function */\r\nheap_arena, /* import vmem source */\r\n8 * PAGESIZE, /* quantum caching for up to 8 pages */\r\nVM_SLEEP); /* OK to wait for memory to create arena */\r\nFinally, we create kmem_default_arena, the backing store for most object caches. Its import function,\r\nsegkmem_alloc(), invokes vmem_alloc() to get virtual addresses and then backs them with physical pages:\r\nkmem_default_arena = vmem_create(\"kmem_default\",\r\nNULL, 0, /* no initial span; we import everything */\r\nPAGESIZE, /* unit of currency is one page */\r\nsegkmem_alloc, /* import allocation function */\r\nsegkmem_free, /* import free function */\r\nkmem_va_arena, /* import vmem source */\r\n0, /* no quantum caching needed */\r\nVM_SLEEP); /* OK to wait for memory to create arena */\r\nAt this point we have a simple page−level allocator: to get three pages of mapped kernel heap, we could call\r\nvmem_alloc(kmem_default_arena, 3 * PAGESIZE, VM_SLEEP) directly. In fact, this is precisely how\r\nthe slab allocator gets memory for new slabs. Finally, the kernel’s various subsystems create their object\r\ncaches. For example, the UFS filesystem creates its inode cache:\r\ninode_cache = kmem_cache_create(\"ufs_inode_cache\",\r\nsizeof (struct inode), /* object size */\r\n0, /* use allocator's default alignment */\r\nufs_inode_cache_constructor, /* inode constructor */\r\nufs_inode_cache_destructor, /* inode destructor */\r\nufs_inode_cache_reclaim, /* inode reclaim */\r\nNULL, /* argument to above funcs */\r\nNULL, /* implies kmem_default_arena */\r\n0); /* no special flags */",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/b2aa710e-c0dc-407f-a1b8-284fd6f403e7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=374388f197e24ed33a5e21ff8d2556d18374ed4b1fe8b8372316e27e74486477",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 523
      },
      {
        "segments": [
          {
            "segment_id": "7bc44cce-65e0-4533-bf04-51989c2be374",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 20,
            "page_width": 612,
            "page_height": 792,
            "content": "Appendix B: Vmem Arenas and Object Caches in Solaris 8\r\n memory memory total\r\nvmem arena name in use imported allocs\r\n---------------------- --------- --------- ------\r\nheap 650231808 0 20569\r\n vmem_seg 9158656 9158656 1118\r\n vmem_vmem 128656 81920 81\r\n kmem_internal 28581888 28581888 4339\r\n kmem_cache 667392 974848 334\r\n kmem_log 1970976 1974272 6\r\n kmem_oversize 30067072 30400512 3616\r\n mod_sysfile 115 8192 4\r\n kmem_va 557580288 557580288 2494\r\n kmem_default 557137920 557137920 110966\r\n little_endian 0 0 0\r\n bp_map 18350080 18350080 7617\r\n ksyms 685077 761856 125\r\nheap32 1916928 0 58\r\n id32 16384 16384 2\r\n module_text 2325080 786432 120\r\n module_data 368762 1032192 165\r\n promplat 0 0 15\r\nsegkp 449314816 0 3749\r\ntaskid_space 3 0 4\r\nsbus0_dvma 3407872 0 14\r\n...\r\nsbus7_dvma 2097152 0 12\r\nip_minor 256 0 4\r\nptms_minor 1 0 1\r\n obj objs total\r\nobject cache name size in use allocs\r\n------------------------- ------ ------ ---------\r\nkmem_magazine_1 16 1923 3903\r\nkmem_magazine_3 32 6818 56014\r\nkmem_magazine_7 64 29898 37634\r\nkmem_magazine_15 128 26210 27237\r\nkmem_magazine_31 256 4662 8381\r\nkmem_magazine_47 384 4149 7003\r\nkmem_magazine_63 512 0 3018\r\nkmem_magazine_95 768 1841 3182\r\nkmem_magazine_143 1152 6655 6655\r\nkmem_slab_cache 56 29212 31594\r\nkmem_bufctl_cache 32 222752 249720\r\nkmem_va_8192 8192 67772 111495\r\nkmem_va_16384 16384 77 77\r\nkmem_va_24576 24576 28 29\r\nkmem_va_32768 32768 0 0\r\nkmem_va_40960 40960 0 0\r\nkmem_va_49152 49152 0 0\r\nkmem_va_57344 57344 0 0\r\nkmem_va_65536 65536 0 0\r\nkmem_alloc_8 8 51283 57609715\r\nkmem_alloc_16 16 4185 19065575\r\nkmem_alloc_24 24 2479 76864949\r\n...\r\nkmem_alloc_16384 16384 52 162\r\nstreams_mblk 64 128834 142921\r\nstreams_dblk_8 128 8 464076\r\nstreams_dblk_40 160 205 10722289\r\nstreams_dblk_72 192 302 201275\r\n...\r\nstreams_dblk_12136 12256 0 0\r\nstreams_dblk_esb 120 0 3\r\nid32_cache 8 1888 1888\r\nbp_map_16384 16384 0 6553071\r\nbp_map_32768 32768 0 2722\r\nbp_map_49152 49152 0 292\r\nbp_map_65536 65536 0 21\r\nbp_map_81920 81920 0 768\r\nbp_map_98304 98304 0 995\r\nbp_map_114688 114688 0 5\r\nbp_map_131072 131072 0 99\r\nsfmmuid_cache 48 35 7617426\r\nsfmmu8_cache 312 358161 389921\r\nsfmmu1_cache 88 126878 138258\r\nseg_cache 64 1098 134076345\r\nsegkp_8192 8192 0 0\r\nsegkp_16384 16384 79 79\r\nsegkp_24576 24576 722 845690\r\nsegkp_32768 32768 0 0\r\nsegkp_40960 40960 0 1213\r\nthread_cache 672 229 4027805\r\nlwp_cache 880 229 1260382\r\nturnstile_cache 64 749 3920308\r\ncred_cache 96 8 866335\r\ndnlc_space_cache 24 819 565894\r\nfile_cache 56 307 46876583\r\nqueue_cache 608 604 991955\r\nsyncq_cache 160 18 112\r\nas_cache 144 34 7727219\r\nanon_cache 48 4455 112122999\r\nanonmap_cache 56 676 60732684\r\nsegvn_cache 96 1096 121023992\r\nsnode_cache 256 379 1183612\r\nufs_inode_cache 480 23782 3156269\r\nsbus0_dvma_8192 8192 66 1180296\r\nsbus0_dvma_16384 16384 2 309600\r\nsbus0_dvma_24576 24576 2 13665\r\nsbus0_dvma_32768 32768 0 154246\r\nsbus0_dvma_40960 40960 0 0\r\nsbus0_dvma_49152 49152 0 0\r\nsbus0_dvma_57344 57344 0 0\r\nsbus0_dvma_65536 65536 0 0\r\nfas0_cache 256 26 21148\r\nfas1_cache 256 0 15\r\nfas2_cache 256 0 15\r\nfas3_cache 256 0 15\r\nsock_cache 432 45 234\r\nsock_unix_cache 432 0 8\r\nip_minor_1 1 116 549\r\nprocess_cache 2688 37 3987768\r\nfnode_cache 264 6 55\r\npipe_cache 496 8 545626\r\nauthkern_cache 72 0 312\r\nauthloopback_cache 72 0 232\r\nauthdes_cache_handle 72 0 0\r\nrnode_cache 656 3 15\r\nnfs_access_cache 40 2 20\r\nclient_handle_cache 32 4 4\r\npty_map 48 1 1\r\nThe data on this page was obtained by\r\nrunning the ::kmastat command under\r\nmdb(1) on a large Solaris 8 server. It was\r\nsubstantially trimmed to fit the page.\r\nThe (shortened) list of all vmem arenas\r\nappears below; the (shortened) list of all\r\nobject caches appears to the right. Shaded\r\nregions show the connection between vmem\r\narenas and their quantum caches. [Note:\r\nvmem names its quantum caches by\r\nappending the object size to the arena name,\r\ne.g. the 8K quantum cache for kmem_va is\r\nnamed kmem_va_8192.]\r\nArena names are indented in the table below\r\nto indicate their importing relationships. For\r\nexample, kmem_default imports virtual\r\naddresses from kmem_va, which in turn\r\nimports virtual addresses from heap.\r\nThe allocation statistics demonstrate the\r\nefficacy of quantum caching. At the time of\r\nthis snapshot there had been over a million\r\nallocations for sbus0_dvma (1.18 million\r\n8K allocations, as shown in the total\r\nallocation column for sbus0_dvma_8192;\r\n309,600 16K allocations, and so on). All of\r\nthis activity resulted in just 14 segment list\r\nallocations. Everything else was handled by\r\nthe quantum caches. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/7bc44cce-65e0-4533-bf04-51989c2be374.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f40c39f33ed61a4a75b49f94864eec428a33f5bc09735d5975371348ea6c7b9b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 657
      },
      {
        "segments": [
          {
            "segment_id": "7bc44cce-65e0-4533-bf04-51989c2be374",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 20,
            "page_width": 612,
            "page_height": 792,
            "content": "Appendix B: Vmem Arenas and Object Caches in Solaris 8\r\n memory memory total\r\nvmem arena name in use imported allocs\r\n---------------------- --------- --------- ------\r\nheap 650231808 0 20569\r\n vmem_seg 9158656 9158656 1118\r\n vmem_vmem 128656 81920 81\r\n kmem_internal 28581888 28581888 4339\r\n kmem_cache 667392 974848 334\r\n kmem_log 1970976 1974272 6\r\n kmem_oversize 30067072 30400512 3616\r\n mod_sysfile 115 8192 4\r\n kmem_va 557580288 557580288 2494\r\n kmem_default 557137920 557137920 110966\r\n little_endian 0 0 0\r\n bp_map 18350080 18350080 7617\r\n ksyms 685077 761856 125\r\nheap32 1916928 0 58\r\n id32 16384 16384 2\r\n module_text 2325080 786432 120\r\n module_data 368762 1032192 165\r\n promplat 0 0 15\r\nsegkp 449314816 0 3749\r\ntaskid_space 3 0 4\r\nsbus0_dvma 3407872 0 14\r\n...\r\nsbus7_dvma 2097152 0 12\r\nip_minor 256 0 4\r\nptms_minor 1 0 1\r\n obj objs total\r\nobject cache name size in use allocs\r\n------------------------- ------ ------ ---------\r\nkmem_magazine_1 16 1923 3903\r\nkmem_magazine_3 32 6818 56014\r\nkmem_magazine_7 64 29898 37634\r\nkmem_magazine_15 128 26210 27237\r\nkmem_magazine_31 256 4662 8381\r\nkmem_magazine_47 384 4149 7003\r\nkmem_magazine_63 512 0 3018\r\nkmem_magazine_95 768 1841 3182\r\nkmem_magazine_143 1152 6655 6655\r\nkmem_slab_cache 56 29212 31594\r\nkmem_bufctl_cache 32 222752 249720\r\nkmem_va_8192 8192 67772 111495\r\nkmem_va_16384 16384 77 77\r\nkmem_va_24576 24576 28 29\r\nkmem_va_32768 32768 0 0\r\nkmem_va_40960 40960 0 0\r\nkmem_va_49152 49152 0 0\r\nkmem_va_57344 57344 0 0\r\nkmem_va_65536 65536 0 0\r\nkmem_alloc_8 8 51283 57609715\r\nkmem_alloc_16 16 4185 19065575\r\nkmem_alloc_24 24 2479 76864949\r\n...\r\nkmem_alloc_16384 16384 52 162\r\nstreams_mblk 64 128834 142921\r\nstreams_dblk_8 128 8 464076\r\nstreams_dblk_40 160 205 10722289\r\nstreams_dblk_72 192 302 201275\r\n...\r\nstreams_dblk_12136 12256 0 0\r\nstreams_dblk_esb 120 0 3\r\nid32_cache 8 1888 1888\r\nbp_map_16384 16384 0 6553071\r\nbp_map_32768 32768 0 2722\r\nbp_map_49152 49152 0 292\r\nbp_map_65536 65536 0 21\r\nbp_map_81920 81920 0 768\r\nbp_map_98304 98304 0 995\r\nbp_map_114688 114688 0 5\r\nbp_map_131072 131072 0 99\r\nsfmmuid_cache 48 35 7617426\r\nsfmmu8_cache 312 358161 389921\r\nsfmmu1_cache 88 126878 138258\r\nseg_cache 64 1098 134076345\r\nsegkp_8192 8192 0 0\r\nsegkp_16384 16384 79 79\r\nsegkp_24576 24576 722 845690\r\nsegkp_32768 32768 0 0\r\nsegkp_40960 40960 0 1213\r\nthread_cache 672 229 4027805\r\nlwp_cache 880 229 1260382\r\nturnstile_cache 64 749 3920308\r\ncred_cache 96 8 866335\r\ndnlc_space_cache 24 819 565894\r\nfile_cache 56 307 46876583\r\nqueue_cache 608 604 991955\r\nsyncq_cache 160 18 112\r\nas_cache 144 34 7727219\r\nanon_cache 48 4455 112122999\r\nanonmap_cache 56 676 60732684\r\nsegvn_cache 96 1096 121023992\r\nsnode_cache 256 379 1183612\r\nufs_inode_cache 480 23782 3156269\r\nsbus0_dvma_8192 8192 66 1180296\r\nsbus0_dvma_16384 16384 2 309600\r\nsbus0_dvma_24576 24576 2 13665\r\nsbus0_dvma_32768 32768 0 154246\r\nsbus0_dvma_40960 40960 0 0\r\nsbus0_dvma_49152 49152 0 0\r\nsbus0_dvma_57344 57344 0 0\r\nsbus0_dvma_65536 65536 0 0\r\nfas0_cache 256 26 21148\r\nfas1_cache 256 0 15\r\nfas2_cache 256 0 15\r\nfas3_cache 256 0 15\r\nsock_cache 432 45 234\r\nsock_unix_cache 432 0 8\r\nip_minor_1 1 116 549\r\nprocess_cache 2688 37 3987768\r\nfnode_cache 264 6 55\r\npipe_cache 496 8 545626\r\nauthkern_cache 72 0 312\r\nauthloopback_cache 72 0 232\r\nauthdes_cache_handle 72 0 0\r\nrnode_cache 656 3 15\r\nnfs_access_cache 40 2 20\r\nclient_handle_cache 32 4 4\r\npty_map 48 1 1\r\nThe data on this page was obtained by\r\nrunning the ::kmastat command under\r\nmdb(1) on a large Solaris 8 server. It was\r\nsubstantially trimmed to fit the page.\r\nThe (shortened) list of all vmem arenas\r\nappears below; the (shortened) list of all\r\nobject caches appears to the right. Shaded\r\nregions show the connection between vmem\r\narenas and their quantum caches. [Note:\r\nvmem names its quantum caches by\r\nappending the object size to the arena name,\r\ne.g. the 8K quantum cache for kmem_va is\r\nnamed kmem_va_8192.]\r\nArena names are indented in the table below\r\nto indicate their importing relationships. For\r\nexample, kmem_default imports virtual\r\naddresses from kmem_va, which in turn\r\nimports virtual addresses from heap.\r\nThe allocation statistics demonstrate the\r\nefficacy of quantum caching. At the time of\r\nthis snapshot there had been over a million\r\nallocations for sbus0_dvma (1.18 million\r\n8K allocations, as shown in the total\r\nallocation column for sbus0_dvma_8192;\r\n309,600 16K allocations, and so on). All of\r\nthis activity resulted in just 14 segment list\r\nallocations. Everything else was handled by\r\nthe quantum caches. ",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/bf84d87b-1de9-435a-8297-0beb924e3c21/images/7bc44cce-65e0-4533-bf04-51989c2be374.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041904Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f40c39f33ed61a4a75b49f94864eec428a33f5bc09735d5975371348ea6c7b9b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 657
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "No response"
        }
      ]
    }
  }
}