{
  "file_name": "Lower Bound Techniques for Data Structures.pdf",
  "task_id": "8b39a6c6-0476-4ac9-b569-7da6a71c8ad7",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "77189e38-4077-4b6c-b401-0c4a5afe36e7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Lower Bound Techniques for Data Structures\r\nby\r\nMihai Pˇatra¸scu\r\nSubmitted to the Department of\r\nElectrical Engineering and Computer Science\r\nin partial fulfillment of the requirements for the degree of\r\nDoctor of Philosophy\r\nat the\r\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY\r\nSeptember 2008\r\n\rc Massachusetts Institute of Technology 2008. All rights reserved.\r\nAuthor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\r\nDepartment of\r\nElectrical Engineering and Computer Science\r\nAugust 8, 2008\r\nCertified by . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\r\nErik D. Demaine\r\nAssociate Professor\r\nThesis Supervisor\r\nAccepted by . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\r\nProfessor Terry P. Orlando\r\nChair, Department Committee on Graduate Students",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/77189e38-4077-4b6c-b401-0c4a5afe36e7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b94dcbf0f4cc14007ee2e64f69f00e7be67f0962302c4e705133cd9ec4512d2e",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "a16284a2-4e30-44d5-b734-d9c0f58b7e5b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/a16284a2-4e30-44d5-b734-d9c0f58b7e5b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3c682d6bc5df06f328f6e43c5b9cf05a09094f5529f412fa42111fb499277bba",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "040ab970-cf5a-449a-96b5-18ba8a921e8b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Lower Bound Techniques for Data Structures\r\nby\r\nMihai Pˇatra¸scu\r\nSubmitted to the Department of\r\nElectrical Engineering and Computer Science\r\non August 8, 2008, in partial fulfillment of the\r\nrequirements for the degree of\r\nDoctor of Philosophy\r\nAbstract\r\nWe describe new techniques for proving lower bounds on data-structure problems, with the\r\nfollowing broad consequences:\r\n• the first Ω(lg n) lower bound for any dynamic problem, improving on a bound that\r\nhad been standing since 1989;\r\n• for static data structures, the first separation between linear and polynomial space.\r\nSpecifically, for some problems that have constant query time when polynomial space\r\nis allowed, we can show Ω(lg n/ lg lg n) bounds when the space is O(n · polylog n).\r\nUsing these techniques, we analyze a variety of central data-structure problems, and\r\nobtain improved lower bounds for the following:\r\n• the partial-sums problem (a fundamental application of augmented binary search trees);\r\n• the predecessor problem (which is equivalent to IP lookup in Internet routers);\r\n• dynamic trees and dynamic connectivity;\r\n• orthogonal range stabbing.\r\n• orthogonal range counting, and orthogonal range reporting;\r\n• the partial match problem (searching with wild-cards);\r\n• (1 + \u000f)-approximate near neighbor on the hypercube;\r\n• approximate nearest neighbor in the `∞ metric.\r\nOur new techniques lead to surprisingly non-technical proofs. For several problems, we\r\nobtain simpler proofs for bounds that were already known.\r\nThesis Supervisor: Erik D. Demaine\r\nTitle: Associate Professor\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/040ab970-cf5a-449a-96b5-18ba8a921e8b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e72aaf1e09e39fc3357701998567320f1f99fccb1cdccba6b0c7683f06f5d8f8",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "be2275a8-024b-4bcd-980e-ca4a2380e0f5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "4",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/be2275a8-024b-4bcd-980e-ca4a2380e0f5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5305b9f2eeb091620695a6e47fdcd905a2d471ea0ebcac6bd0255097268d86bd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "557f9bb6-a8f7-40e8-b423-430ee53f4a1d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Acknowledgments\r\nThis thesis is based primarily on ten conference publications: [84] from SODA’04, [83] from\r\nSTOC’04, [87] from ICALP’05, [89] from STOC’06, [88] and [16] from FOCS’06, [90] from\r\nSODA’07, [81] from STOC’07, [13] and [82] from FOCS’08. It seems appropriate to sketch\r\nthe story of each one of these publications.\r\nIt all began with two papers I wrote in my first two undergraduate years at MIT, which\r\nappeared in SODA’04 and STOC’04, and were later merged in a single journal version [86].\r\nI owe a lot of thanks to Peter Bro Miltersen, whose survey of cell probe complexity [72] was\r\nmy crash course into the field. Without this survey, which did an excellent job of taking\r\nclueless readers to the important problems, a confused freshman would never have heard\r\nabout the field, nor the problems. And, quite likely, STOC would never have seen a paper\r\nproving information-theoretic lower bounds from an author who clearly did not know what\r\n“entropy” meant.\r\nMany thanks also go to Erik Demaine, who was my research advisor from that freshman\r\nyear until the end of this PhD thesis. Though it took years before we did any work together,\r\nhis unconditional support has indirectly made all my work possible. Erik’s willingness to pay\r\na freshman to do whatever he wanted was clearly not a mainstream idea, though in retrospect,\r\nit was an inspired one. Throughout the years, Erik’s understanding and tolerance for my\r\nunorthodox style, including in the creation of this thesis, have provided the best possible\r\nresearch environment for me.\r\nMy next step would have been far too great for me to take alone, but I was lucky enough\r\nto meet Mikkel Thorup, now a good friend and colleague. In early 2004 (if my memory\r\nserves me well, it was in January, during a meeting at SODA), we began thinking about\r\nthe predecessor problem. It took us quite some time to understand that what had been\r\nlabeled “optimal bounds” were not optimal, that proving an optimal bound would require a\r\nrevolution in static lower bounds (the first bound to beat communication complexity), and\r\nto finally find an idea to break this barrier. This 2-year research journey consumed us, and\r\nI would certainly have given up along the way, had it not been for Mikkel’s contagious and\r\nconstant optimism, constant influx of ideas, and the many beers that we had together.\r\nThis research journey remained one of the most memorable in my career, though, unfor\u0002tunately, the ending was underwhelming. Our STOC 2006 [89] paper went largely unnoticed,\r\nwith maybe 10 people attending the talk, and no special issue invitation. I consoled myself\r\nwith Mikkel’s explanation that ours had been paper with too many new ideas to be digested\r\nsoon after publication.\r\nMikkel’s theory got some support through our next joint paper. I proposed that we look\r\nat some lower bounds via the so-called richness method for hard problems like partial match\r\nor nearest neighbor. After 2 years of predecessor lower bounds, it was a simple exercise to\r\nobtain better lower bound by richness; in fact, we felt like we were simplifying our technique\r\nfor beating communication complexity, in order to make it work here. Despite my opinion\r\nthat this paper was too simple to be interesting, Mikkel convinced me to submit it to FOCS.\r\nSure enough, the paper was accepted to FOCS’06 [88] with raving reviews and a special issue\r\ninvitation. One is reminded to listen to senior people once in while.\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/557f9bb6-a8f7-40e8-b423-430ee53f4a1d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8b5c4c536e0d5cd4d0a4f4e707e7b7f333e61dec75b29933efa0b9d6d0034485",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 578
      },
      {
        "segments": [
          {
            "segment_id": "557f9bb6-a8f7-40e8-b423-430ee53f4a1d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Acknowledgments\r\nThis thesis is based primarily on ten conference publications: [84] from SODA’04, [83] from\r\nSTOC’04, [87] from ICALP’05, [89] from STOC’06, [88] and [16] from FOCS’06, [90] from\r\nSODA’07, [81] from STOC’07, [13] and [82] from FOCS’08. It seems appropriate to sketch\r\nthe story of each one of these publications.\r\nIt all began with two papers I wrote in my first two undergraduate years at MIT, which\r\nappeared in SODA’04 and STOC’04, and were later merged in a single journal version [86].\r\nI owe a lot of thanks to Peter Bro Miltersen, whose survey of cell probe complexity [72] was\r\nmy crash course into the field. Without this survey, which did an excellent job of taking\r\nclueless readers to the important problems, a confused freshman would never have heard\r\nabout the field, nor the problems. And, quite likely, STOC would never have seen a paper\r\nproving information-theoretic lower bounds from an author who clearly did not know what\r\n“entropy” meant.\r\nMany thanks also go to Erik Demaine, who was my research advisor from that freshman\r\nyear until the end of this PhD thesis. Though it took years before we did any work together,\r\nhis unconditional support has indirectly made all my work possible. Erik’s willingness to pay\r\na freshman to do whatever he wanted was clearly not a mainstream idea, though in retrospect,\r\nit was an inspired one. Throughout the years, Erik’s understanding and tolerance for my\r\nunorthodox style, including in the creation of this thesis, have provided the best possible\r\nresearch environment for me.\r\nMy next step would have been far too great for me to take alone, but I was lucky enough\r\nto meet Mikkel Thorup, now a good friend and colleague. In early 2004 (if my memory\r\nserves me well, it was in January, during a meeting at SODA), we began thinking about\r\nthe predecessor problem. It took us quite some time to understand that what had been\r\nlabeled “optimal bounds” were not optimal, that proving an optimal bound would require a\r\nrevolution in static lower bounds (the first bound to beat communication complexity), and\r\nto finally find an idea to break this barrier. This 2-year research journey consumed us, and\r\nI would certainly have given up along the way, had it not been for Mikkel’s contagious and\r\nconstant optimism, constant influx of ideas, and the many beers that we had together.\r\nThis research journey remained one of the most memorable in my career, though, unfor\u0002tunately, the ending was underwhelming. Our STOC 2006 [89] paper went largely unnoticed,\r\nwith maybe 10 people attending the talk, and no special issue invitation. I consoled myself\r\nwith Mikkel’s explanation that ours had been paper with too many new ideas to be digested\r\nsoon after publication.\r\nMikkel’s theory got some support through our next joint paper. I proposed that we look\r\nat some lower bounds via the so-called richness method for hard problems like partial match\r\nor nearest neighbor. After 2 years of predecessor lower bounds, it was a simple exercise to\r\nobtain better lower bound by richness; in fact, we felt like we were simplifying our technique\r\nfor beating communication complexity, in order to make it work here. Despite my opinion\r\nthat this paper was too simple to be interesting, Mikkel convinced me to submit it to FOCS.\r\nSure enough, the paper was accepted to FOCS’06 [88] with raving reviews and a special issue\r\ninvitation. One is reminded to listen to senior people once in while.\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/557f9bb6-a8f7-40e8-b423-430ee53f4a1d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8b5c4c536e0d5cd4d0a4f4e707e7b7f333e61dec75b29933efa0b9d6d0034485",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 578
      },
      {
        "segments": [
          {
            "segment_id": "5c3050b7-6a4e-4c27-b637-521f913569df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "After my initial interaction with Mikkel, I had begun to understand the field, and I was\r\nable to make progress on several fronts at the same time. Since my first paper in 2003, I\r\nkept working on a very silly dynamic problem: prove a lower bound for partial sums in the\r\nbit-probe model. It seemed doubtful that anyone would care, but the problem was so clean\r\nand simple, that I couldn’t let go. One day, as I was sitting in an MIT library (I was still\r\nan undergraduate and didn’t have an office), I discovered something entirely unexpected.\r\nYou see, before my first paper proving an Ω(lg n) dynamic lower bound, there had been\r\njust one technique for proving dynamic bounds, dating back to Fredman and Saks in 1989.\r\nEverybody had tried, and failed, to prove a logarithmic bound by this technique. And there\r\nI was, seeing a clear and simple proof of an Ω(lg n) bound by this classic technique. This\r\nwas a great surprise to me, and it had very interesting consequences, including a new bound\r\nfor my silly little problem, as well as a new record lower bound in the bit-probe model. With\r\nthe gods clearly on my side (Miltersen was on the PC), this paper [87] got the Best Student\r\nPaper award at ICALP.\r\nMy work with Mikkel continued with a randomized lower bound for predecessor search\r\n(our first bound only applied to deterministic algorithms). We had the moral obligation\r\nto “finish” the problem, as Mikkel put it. This took quite some work, but in the end, we\r\nsucceeded, and the paper [90] appeared in SODA’07.\r\nAt that time, I also wrote my first paper with Alex Andoni, an old and very good friend,\r\nwith whom I would later share an apartment and an office. Surprisingly, it took us until\r\n2006 to get our first paper, and we have only written one other paper since then, despite\r\nour very frequent research discussions over beer, or while walking home. These two papers\r\nare a severe underestimate of the entertaining research discussions that we have had. I owe\r\nAlex a great deal of thanks for the amount of understanding of high dimensional geometry\r\nthat he has passed on to me, and, above all, for being a highly supportive friend.\r\nOur first paper was, to some extent, a lucky accident. After a visit to my uncle in\r\nPhiladelphia, I was stuck on a long bus ride back to MIT, when Alex called to say that\r\nhe had some intuition about why (1 + ε)-approximate nearest neighbor should be hard. As\r\nalways, intuition is hard to convey, but I understood at least that he wanted to think in very\r\nhigh dimensions, and let the points of the database be at constant pairwise distance. Luckily,\r\non that bus ride I was thinking of lower bounds for lopsided set disjointness (a problem left\r\nopen by the seminal paper of Miltersen et al. [73] from STOC’95). It didn’t take long after\r\npassing to realize the connection, and I was back on the phone with Alex explaining how his\r\nconstruction can be turned in a reduction from lopsided set disjointness to nearest neighbor.\r\nBack at MIT, the proof obviously got Piotr Indyk very excited. We later merged with\r\nanother result of his, yielding a FOCS’06 paper [16]. Like in the case of Alex, the influence\r\nthat Piotr has had on my thinking is not conveyed by our number of joint papers (we only\r\nhave one). Nonetheless, my interaction with him has been both very enjoyable and very\r\nuseful, and I am very grateful for his help.\r\nMy second paper with Alex Andoni was [13] from FOCS’08. This began in spring 2007\r\nas a series of discussions between me and Piotr Indyk about approximate near neighbor in\r\n`∞, discussions which got me very excited about the problem, but didn’t really lead to any\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/5c3050b7-6a4e-4c27-b637-521f913569df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11dde4c8c5fdf89f2c9bf58df6588491d8bea272c6bdb6c209cfef6c19d8781d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 649
      },
      {
        "segments": [
          {
            "segment_id": "5c3050b7-6a4e-4c27-b637-521f913569df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "After my initial interaction with Mikkel, I had begun to understand the field, and I was\r\nable to make progress on several fronts at the same time. Since my first paper in 2003, I\r\nkept working on a very silly dynamic problem: prove a lower bound for partial sums in the\r\nbit-probe model. It seemed doubtful that anyone would care, but the problem was so clean\r\nand simple, that I couldn’t let go. One day, as I was sitting in an MIT library (I was still\r\nan undergraduate and didn’t have an office), I discovered something entirely unexpected.\r\nYou see, before my first paper proving an Ω(lg n) dynamic lower bound, there had been\r\njust one technique for proving dynamic bounds, dating back to Fredman and Saks in 1989.\r\nEverybody had tried, and failed, to prove a logarithmic bound by this technique. And there\r\nI was, seeing a clear and simple proof of an Ω(lg n) bound by this classic technique. This\r\nwas a great surprise to me, and it had very interesting consequences, including a new bound\r\nfor my silly little problem, as well as a new record lower bound in the bit-probe model. With\r\nthe gods clearly on my side (Miltersen was on the PC), this paper [87] got the Best Student\r\nPaper award at ICALP.\r\nMy work with Mikkel continued with a randomized lower bound for predecessor search\r\n(our first bound only applied to deterministic algorithms). We had the moral obligation\r\nto “finish” the problem, as Mikkel put it. This took quite some work, but in the end, we\r\nsucceeded, and the paper [90] appeared in SODA’07.\r\nAt that time, I also wrote my first paper with Alex Andoni, an old and very good friend,\r\nwith whom I would later share an apartment and an office. Surprisingly, it took us until\r\n2006 to get our first paper, and we have only written one other paper since then, despite\r\nour very frequent research discussions over beer, or while walking home. These two papers\r\nare a severe underestimate of the entertaining research discussions that we have had. I owe\r\nAlex a great deal of thanks for the amount of understanding of high dimensional geometry\r\nthat he has passed on to me, and, above all, for being a highly supportive friend.\r\nOur first paper was, to some extent, a lucky accident. After a visit to my uncle in\r\nPhiladelphia, I was stuck on a long bus ride back to MIT, when Alex called to say that\r\nhe had some intuition about why (1 + ε)-approximate nearest neighbor should be hard. As\r\nalways, intuition is hard to convey, but I understood at least that he wanted to think in very\r\nhigh dimensions, and let the points of the database be at constant pairwise distance. Luckily,\r\non that bus ride I was thinking of lower bounds for lopsided set disjointness (a problem left\r\nopen by the seminal paper of Miltersen et al. [73] from STOC’95). It didn’t take long after\r\npassing to realize the connection, and I was back on the phone with Alex explaining how his\r\nconstruction can be turned in a reduction from lopsided set disjointness to nearest neighbor.\r\nBack at MIT, the proof obviously got Piotr Indyk very excited. We later merged with\r\nanother result of his, yielding a FOCS’06 paper [16]. Like in the case of Alex, the influence\r\nthat Piotr has had on my thinking is not conveyed by our number of joint papers (we only\r\nhave one). Nonetheless, my interaction with him has been both very enjoyable and very\r\nuseful, and I am very grateful for his help.\r\nMy second paper with Alex Andoni was [13] from FOCS’08. This began in spring 2007\r\nas a series of discussions between me and Piotr Indyk about approximate near neighbor in\r\n`∞, discussions which got me very excited about the problem, but didn’t really lead to any\r\n6",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/5c3050b7-6a4e-4c27-b637-521f913569df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11dde4c8c5fdf89f2c9bf58df6588491d8bea272c6bdb6c209cfef6c19d8781d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 649
      },
      {
        "segments": [
          {
            "segment_id": "7f061767-177c-43bf-8ac8-d417a9bbaa4a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "solution. By the summer of 2007, I had made up my mind that we had to seek an asymmetric\r\ncommunication lower bound. That summer, both I and Alex were interns at IBM Almaden,\r\nand I convinced him to join on long walks on the beautiful hills at Almaden, and discuss\r\nthis problem. Painfully slowly, we developed an information-theoretic understanding of the\r\nbest previous upper bound, and an idea about how the lower bound should be proved.\r\nUnfortunately, our plan for the lower bound led us to a rather nontrivial isoperimetric\r\ninequality, which we tried to prove for several weeks in the fall of 2007. Our proof strategy\r\nseemed to work, more or less, but it led to a very ugly case analysis, so we decided to\r\noutsource the theorem to a mathematician. We chose none other than our third housemate,\r\nDorian Croitoru, who came back in a few weeks with a nicer proof (if not lacking in case\r\nanalysis).\r\nMy next paper on the list was my (single author) paper [81] from STOC’07. Ever since\r\nwe broke the communication barrier with Mikkel Thorup, it had been clear to me that the\r\nmost important and most exciting implication had to be range query problems, where there\r\nis a huge gap between space O(n\r\n2\r\n) and space O(n polylog n). Extending the ideas to these\r\nproblems was less than obvious, and took quite a bit of time to find the right approach, but\r\neventually I ended up with a proof of an Ω(lg n/ lg lg n) lower bound for range counting in 2\r\ndimensions.\r\nIn FOCS’08 I wrote follow-up to this, the (single author) paper [82], which I found very\r\nsurprising in its techniques. That paper is perhaps the coronation of the work in this thesis,\r\nshowing that proofs for many interesting lower bounds (both static and dynamic) can be\r\nobtained by simple reductions from lopsided set disjointness.\r\nThe most surprising lower bound in that paper is perhaps the one for 4-dimensional\r\nrange reporting. Around 2004–2005, I got motivated to work on range reporting, by talking\r\nto Christian Mortensen, then a student at ITU Copenhagen. Christian had some nice results\r\nfor the 2-dimensional case, and I was interested in proving that the 3-dimensional case is\r\nhard. Unfortunately, my proofs always seemed to break down one way or another. Christian\r\neventually left for industry, and the problem slipped from my view.\r\nIn 2007, I got a very good explanation for why my lower bound had failed: Yakov\r\nNekrich [79] showed in SoCG’07 a surprising O((lg lg n)\r\n2\r\n) upper bound for 3 dimensions\r\n(making it “easy”). It had never occurred to me that a better upper bound could exist,\r\nwhich in some sense served to remind me why lower bounds are important. I was very\r\nexcited by this development, and immediately started wondering whether the 4-dimensional\r\nwould also collapse to near-constant time. My bet was that it wouldn’t, but what kind of\r\nstructure could prove 4 dimensions hard, if 3 dimensions were easy?\r\nYakov Nekrich and Marek Karpinski invited me for a one-week visit to Bonn in October\r\n2007, which prompted a discussion about the problem, but not much progress towards a\r\nlower bound. After my return to MIT, the ideas became increasingly more clear, and by the\r\nend of the fall I had an almost complete proof, which nonetheless seemed “boring” (technical)\r\nand unsatisfying.\r\nIn the spring of 2008, I was trying to submit 4 papers to FOCS, during my job interview\r\nseason. Needless to say, this was a challenging experience. I had an interview at Google\r\n7",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7f061767-177c-43bf-8ac8-d417a9bbaa4a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9fe552d30808cb754d8ac43a988b54e7b041498e67c9a88ffa963aefcd2b9704",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 599
      },
      {
        "segments": [
          {
            "segment_id": "7f061767-177c-43bf-8ac8-d417a9bbaa4a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "solution. By the summer of 2007, I had made up my mind that we had to seek an asymmetric\r\ncommunication lower bound. That summer, both I and Alex were interns at IBM Almaden,\r\nand I convinced him to join on long walks on the beautiful hills at Almaden, and discuss\r\nthis problem. Painfully slowly, we developed an information-theoretic understanding of the\r\nbest previous upper bound, and an idea about how the lower bound should be proved.\r\nUnfortunately, our plan for the lower bound led us to a rather nontrivial isoperimetric\r\ninequality, which we tried to prove for several weeks in the fall of 2007. Our proof strategy\r\nseemed to work, more or less, but it led to a very ugly case analysis, so we decided to\r\noutsource the theorem to a mathematician. We chose none other than our third housemate,\r\nDorian Croitoru, who came back in a few weeks with a nicer proof (if not lacking in case\r\nanalysis).\r\nMy next paper on the list was my (single author) paper [81] from STOC’07. Ever since\r\nwe broke the communication barrier with Mikkel Thorup, it had been clear to me that the\r\nmost important and most exciting implication had to be range query problems, where there\r\nis a huge gap between space O(n\r\n2\r\n) and space O(n polylog n). Extending the ideas to these\r\nproblems was less than obvious, and took quite a bit of time to find the right approach, but\r\neventually I ended up with a proof of an Ω(lg n/ lg lg n) lower bound for range counting in 2\r\ndimensions.\r\nIn FOCS’08 I wrote follow-up to this, the (single author) paper [82], which I found very\r\nsurprising in its techniques. That paper is perhaps the coronation of the work in this thesis,\r\nshowing that proofs for many interesting lower bounds (both static and dynamic) can be\r\nobtained by simple reductions from lopsided set disjointness.\r\nThe most surprising lower bound in that paper is perhaps the one for 4-dimensional\r\nrange reporting. Around 2004–2005, I got motivated to work on range reporting, by talking\r\nto Christian Mortensen, then a student at ITU Copenhagen. Christian had some nice results\r\nfor the 2-dimensional case, and I was interested in proving that the 3-dimensional case is\r\nhard. Unfortunately, my proofs always seemed to break down one way or another. Christian\r\neventually left for industry, and the problem slipped from my view.\r\nIn 2007, I got a very good explanation for why my lower bound had failed: Yakov\r\nNekrich [79] showed in SoCG’07 a surprising O((lg lg n)\r\n2\r\n) upper bound for 3 dimensions\r\n(making it “easy”). It had never occurred to me that a better upper bound could exist,\r\nwhich in some sense served to remind me why lower bounds are important. I was very\r\nexcited by this development, and immediately started wondering whether the 4-dimensional\r\nwould also collapse to near-constant time. My bet was that it wouldn’t, but what kind of\r\nstructure could prove 4 dimensions hard, if 3 dimensions were easy?\r\nYakov Nekrich and Marek Karpinski invited me for a one-week visit to Bonn in October\r\n2007, which prompted a discussion about the problem, but not much progress towards a\r\nlower bound. After my return to MIT, the ideas became increasingly more clear, and by the\r\nend of the fall I had an almost complete proof, which nonetheless seemed “boring” (technical)\r\nand unsatisfying.\r\nIn the spring of 2008, I was trying to submit 4 papers to FOCS, during my job interview\r\nseason. Needless to say, this was a challenging experience. I had an interview at Google\r\n7",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7f061767-177c-43bf-8ac8-d417a9bbaa4a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9fe552d30808cb754d8ac43a988b54e7b041498e67c9a88ffa963aefcd2b9704",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 599
      },
      {
        "segments": [
          {
            "segment_id": "a4be9eaf-ea08-4163-b0a3-31ba79c5027a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "scheduled two days before the deadline, and I made an entirely unrealistic plan to go to New\r\nYork with the 5am train, after working through the night. By 5am, I had a serious headache\r\nand the beginning of a cold, and I had to postpone the interview at the last minute. However,\r\nthis proved to be a very auspicious incident. As I was lying on an MIT couch the next day\r\ntrying to recover, I had an entirely unexpected revelation: the 4-dimensional lower bound\r\ncould be proved by a series of reductions from lopsided set disjointness! This got developed\r\ninto the set of reductions in [82], and submitted to the conference. I owe my sincere apologies\r\nto the Google researchers for messing their schedules with my unrealistic planning. But in\r\nthe end, I’m glad things hapenned this way, and the incident was clearly good for science.\r\n8",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/a4be9eaf-ea08-4163-b0a3-31ba79c5027a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d07a90cd9a9b6d84f5d487cafaa48cf252e438d6d13084bc7007e0c8d28a6940",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 147
      },
      {
        "segments": [
          {
            "segment_id": "2b0a8395-999f-4eb0-bee3-c3393808b166",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "Contents\r\n1 Introduction 13\r\n1.1 What is the Cell-Probe Model? . . . . . . . . . . . . . . . . . . . . . . . . . 13\r\n1.1.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\r\n1.1.2 Predictive Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\r\n1.2 Overview of Cell-Probe Lower Bounds . . . . . . . . . . . . . . . . . . . . . 15\r\n1.2.1 Dynamic Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n1.2.2 Round Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n1.2.3 Richness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\r\n1.3 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\r\n1.3.1 The Epoch Barrier in Dynamic Lower Bounds . . . . . . . . . . . . . 18\r\n1.3.2 The Logarithmic Barrier in Bit-Probe Complexity . . . . . . . . . . . 18\r\n1.3.3 The Communication Barrier in Static Complexity . . . . . . . . . . . 19\r\n1.3.4 Richness Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . 20\r\n1.3.5 Lower Bounds for Range Queries . . . . . . . . . . . . . . . . . . . . 22\r\n1.3.6 Simple Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\r\n2 Catalog of Problems 25\r\n2.1 Predecessor Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\r\n2.1.1 Flavors of Integer Search . . . . . . . . . . . . . . . . . . . . . . . . . 25\r\n2.1.2 Previous Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 26\r\n2.1.3 Previous Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . 27\r\n2.1.4 Our Optimal Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\r\n2.2 Dynamic Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\r\n2.2.1 Maintaining Partial Sums . . . . . . . . . . . . . . . . . . . . . . . . 29\r\n2.2.2 Previous Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . 30\r\n2.2.3 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\r\n2.2.4 Related Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\r\n2.3 Range Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\r\n2.3.1 Orthogonal Range Counting . . . . . . . . . . . . . . . . . . . . . . . 34\r\n2.3.2 Range Reporting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\r\n2.3.3 Orthogonal Stabbing . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\r\n2.4 Problems in High Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . 39\r\n2.4.1 Partial Match . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/2b0a8395-999f-4eb0-bee3-c3393808b166.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=21286bd2c64a4c217b4f10b80514a91d3521e92cf79350c25b7d9d6af4aadc07",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 953
      },
      {
        "segments": [
          {
            "segment_id": "2b0a8395-999f-4eb0-bee3-c3393808b166",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "Contents\r\n1 Introduction 13\r\n1.1 What is the Cell-Probe Model? . . . . . . . . . . . . . . . . . . . . . . . . . 13\r\n1.1.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\r\n1.1.2 Predictive Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\r\n1.2 Overview of Cell-Probe Lower Bounds . . . . . . . . . . . . . . . . . . . . . 15\r\n1.2.1 Dynamic Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n1.2.2 Round Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n1.2.3 Richness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\r\n1.3 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\r\n1.3.1 The Epoch Barrier in Dynamic Lower Bounds . . . . . . . . . . . . . 18\r\n1.3.2 The Logarithmic Barrier in Bit-Probe Complexity . . . . . . . . . . . 18\r\n1.3.3 The Communication Barrier in Static Complexity . . . . . . . . . . . 19\r\n1.3.4 Richness Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . 20\r\n1.3.5 Lower Bounds for Range Queries . . . . . . . . . . . . . . . . . . . . 22\r\n1.3.6 Simple Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\r\n2 Catalog of Problems 25\r\n2.1 Predecessor Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\r\n2.1.1 Flavors of Integer Search . . . . . . . . . . . . . . . . . . . . . . . . . 25\r\n2.1.2 Previous Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 26\r\n2.1.3 Previous Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . 27\r\n2.1.4 Our Optimal Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\r\n2.2 Dynamic Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\r\n2.2.1 Maintaining Partial Sums . . . . . . . . . . . . . . . . . . . . . . . . 29\r\n2.2.2 Previous Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . 30\r\n2.2.3 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\r\n2.2.4 Related Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\r\n2.3 Range Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\r\n2.3.1 Orthogonal Range Counting . . . . . . . . . . . . . . . . . . . . . . . 34\r\n2.3.2 Range Reporting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\r\n2.3.3 Orthogonal Stabbing . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\r\n2.4 Problems in High Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . 39\r\n2.4.1 Partial Match . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\r\n9",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/2b0a8395-999f-4eb0-bee3-c3393808b166.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=21286bd2c64a4c217b4f10b80514a91d3521e92cf79350c25b7d9d6af4aadc07",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 953
      },
      {
        "segments": [
          {
            "segment_id": "b343f914-01e2-42bc-9601-20a073b3ff63",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "2.4.2 Near Neighbor Search in L1, L2 . . . . . . . . . . . . . . . . . . . . . 41\r\n2.4.3 Near Neighbor Search in L-infinity . . . . . . . . . . . . . . . . . . . 44\r\n3 Dynamic Omega(log n) Bounds 47\r\n3.1 Partial Sums: The Hard Instance . . . . . . . . . . . . . . . . . . . . . . . . 47\r\n3.2 Information Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\r\n3.3 Interleaves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\n3.4 A Tree For The Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . 50\r\n3.5 The Bit-Reversal Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . 52\r\n3.6 Dynamic Connectivity: The Hard Instance . . . . . . . . . . . . . . . . . . . 53\r\n3.7 The Main Trick: Nondeterminism . . . . . . . . . . . . . . . . . . . . . . . . 55\r\n3.8 Proof of the Nondeterministic Bound . . . . . . . . . . . . . . . . . . . . . . 55\r\n3.9 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\r\n4 Epoch-Based Lower Bounds 59\r\n4.1 Trade-offs and Higher Word Sizes . . . . . . . . . . . . . . . . . . . . . . . . 60\r\n4.2 Bit-Probe Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\r\n4.3 Lower Bounds for Partial Sums . . . . . . . . . . . . . . . . . . . . . . . . . 63\r\n4.3.1 Formal Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\r\n4.3.2 Bounding Probes into an Epoch . . . . . . . . . . . . . . . . . . . . . 65\r\n4.3.3 Deriving the Trade-offs of Theorem 4.1 . . . . . . . . . . . . . . . . . 67\r\n4.3.4 Proof of Lemma 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\r\n5 Communication Complexity 71\r\n5.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\r\n5.1.1 Set Disjointness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\r\n5.1.2 Complexity Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\r\n5.2 Richness Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\r\n5.2.1 Rectangles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\r\n5.2.2 Richness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\r\n5.2.3 Application to Indexing . . . . . . . . . . . . . . . . . . . . . . . . . 74\r\n5.3 Direct Sum for Richness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\r\n5.3.1 A Direct Sum of Indexing Problems . . . . . . . . . . . . . . . . . . . 75\r\n5.3.2 Proof of Theorem 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\r\n5.4 Randomized Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\r\n5.4.1 Warm-Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\r\n5.4.2 A Strong Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . 79\r\n5.4.3 Direct Sum for Randomized Richness . . . . . . . . . . . . . . . . . . 80\r\n5.4.4 Proof of Lemma 5.17 . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\r\n5.5 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\r\n10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b343f914-01e2-42bc-9601-20a073b3ff63.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d562db99935bfb8ba9ba495c4806577b9b67e2b4690cd5cd5df35fe85d85f984",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1110
      },
      {
        "segments": [
          {
            "segment_id": "b343f914-01e2-42bc-9601-20a073b3ff63",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "2.4.2 Near Neighbor Search in L1, L2 . . . . . . . . . . . . . . . . . . . . . 41\r\n2.4.3 Near Neighbor Search in L-infinity . . . . . . . . . . . . . . . . . . . 44\r\n3 Dynamic Omega(log n) Bounds 47\r\n3.1 Partial Sums: The Hard Instance . . . . . . . . . . . . . . . . . . . . . . . . 47\r\n3.2 Information Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\r\n3.3 Interleaves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\n3.4 A Tree For The Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . 50\r\n3.5 The Bit-Reversal Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . 52\r\n3.6 Dynamic Connectivity: The Hard Instance . . . . . . . . . . . . . . . . . . . 53\r\n3.7 The Main Trick: Nondeterminism . . . . . . . . . . . . . . . . . . . . . . . . 55\r\n3.8 Proof of the Nondeterministic Bound . . . . . . . . . . . . . . . . . . . . . . 55\r\n3.9 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\r\n4 Epoch-Based Lower Bounds 59\r\n4.1 Trade-offs and Higher Word Sizes . . . . . . . . . . . . . . . . . . . . . . . . 60\r\n4.2 Bit-Probe Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\r\n4.3 Lower Bounds for Partial Sums . . . . . . . . . . . . . . . . . . . . . . . . . 63\r\n4.3.1 Formal Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\r\n4.3.2 Bounding Probes into an Epoch . . . . . . . . . . . . . . . . . . . . . 65\r\n4.3.3 Deriving the Trade-offs of Theorem 4.1 . . . . . . . . . . . . . . . . . 67\r\n4.3.4 Proof of Lemma 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\r\n5 Communication Complexity 71\r\n5.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\r\n5.1.1 Set Disjointness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\r\n5.1.2 Complexity Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\r\n5.2 Richness Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\r\n5.2.1 Rectangles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\r\n5.2.2 Richness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\r\n5.2.3 Application to Indexing . . . . . . . . . . . . . . . . . . . . . . . . . 74\r\n5.3 Direct Sum for Richness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\r\n5.3.1 A Direct Sum of Indexing Problems . . . . . . . . . . . . . . . . . . . 75\r\n5.3.2 Proof of Theorem 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\r\n5.4 Randomized Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\r\n5.4.1 Warm-Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\r\n5.4.2 A Strong Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . 79\r\n5.4.3 Direct Sum for Randomized Richness . . . . . . . . . . . . . . . . . . 80\r\n5.4.4 Proof of Lemma 5.17 . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\r\n5.5 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\r\n10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b343f914-01e2-42bc-9601-20a073b3ff63.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d562db99935bfb8ba9ba495c4806577b9b67e2b4690cd5cd5df35fe85d85f984",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1110
      },
      {
        "segments": [
          {
            "segment_id": "40c8041b-1446-4853-92c6-3e6f7e2922f4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "6 Static Lower Bounds 85\r\n6.1 Partial Match . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\r\n6.2 Approximate Near Neighbor . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\r\n6.3 Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\r\n6.4 Near-Linear Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\r\n7 Range Query Problems 93\r\n7.1 The Butterfly Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\r\n7.1.1 Reachability Oracles to Stabbing . . . . . . . . . . . . . . . . . . . . 94\r\n7.1.2 The Structure of Dynamic Problems . . . . . . . . . . . . . . . . . . 95\r\n7.2 Adding Structure to Set Disjointness . . . . . . . . . . . . . . . . . . . . . . 98\r\n7.2.1 Randomized Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\r\n7.3 Set Disjointness to Reachability Oracles . . . . . . . . . . . . . . . . . . . . 99\r\n8 Near Neighbor Search in `∞ 103\r\n8.1 Review of Indyk’s Upper Bound . . . . . . . . . . . . . . . . . . . . . . . . . 104\r\n8.2 Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\r\n8.3 An Isoperimetric Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r\n8.4 Expansion in One Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . 110\r\n9 Predecessor Search 113\r\n9.1 Data Structures Using Linear Space . . . . . . . . . . . . . . . . . . . . . . . 115\r\n9.1.1 Equivalence to Longest Common Prefix . . . . . . . . . . . . . . . . . 115\r\n9.1.2 The Data Structure of van Emde Boas . . . . . . . . . . . . . . . . . 116\r\n9.1.3 Fusion Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\r\n9.2 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\r\n9.2.1 The Cell-Probe Elimination Lemma . . . . . . . . . . . . . . . . . . . 119\r\n9.2.2 Setup for the Predecessor Problem . . . . . . . . . . . . . . . . . . . 120\r\n9.2.3 Deriving the Trade-Offs . . . . . . . . . . . . . . . . . . . . . . . . . 121\r\n9.3 Proof of Cell-Probe Elimination . . . . . . . . . . . . . . . . . . . . . . . . . 124\r\n9.3.1 The Solution to the Direct Sum Problem . . . . . . . . . . . . . . . . 125\r\n9.3.2 Proof of Lemma 9.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\r\n11",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/40c8041b-1446-4853-92c6-3e6f7e2922f4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=76b428f95ba2beb7457f5aa5b5290baf9448428b8e28f65b67cb1a30ec8a0cd3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 803
      },
      {
        "segments": [
          {
            "segment_id": "40c8041b-1446-4853-92c6-3e6f7e2922f4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "6 Static Lower Bounds 85\r\n6.1 Partial Match . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\r\n6.2 Approximate Near Neighbor . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\r\n6.3 Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\r\n6.4 Near-Linear Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\r\n7 Range Query Problems 93\r\n7.1 The Butterfly Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\r\n7.1.1 Reachability Oracles to Stabbing . . . . . . . . . . . . . . . . . . . . 94\r\n7.1.2 The Structure of Dynamic Problems . . . . . . . . . . . . . . . . . . 95\r\n7.2 Adding Structure to Set Disjointness . . . . . . . . . . . . . . . . . . . . . . 98\r\n7.2.1 Randomized Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\r\n7.3 Set Disjointness to Reachability Oracles . . . . . . . . . . . . . . . . . . . . 99\r\n8 Near Neighbor Search in `∞ 103\r\n8.1 Review of Indyk’s Upper Bound . . . . . . . . . . . . . . . . . . . . . . . . . 104\r\n8.2 Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\r\n8.3 An Isoperimetric Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r\n8.4 Expansion in One Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . 110\r\n9 Predecessor Search 113\r\n9.1 Data Structures Using Linear Space . . . . . . . . . . . . . . . . . . . . . . . 115\r\n9.1.1 Equivalence to Longest Common Prefix . . . . . . . . . . . . . . . . . 115\r\n9.1.2 The Data Structure of van Emde Boas . . . . . . . . . . . . . . . . . 116\r\n9.1.3 Fusion Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\r\n9.2 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\r\n9.2.1 The Cell-Probe Elimination Lemma . . . . . . . . . . . . . . . . . . . 119\r\n9.2.2 Setup for the Predecessor Problem . . . . . . . . . . . . . . . . . . . 120\r\n9.2.3 Deriving the Trade-Offs . . . . . . . . . . . . . . . . . . . . . . . . . 121\r\n9.3 Proof of Cell-Probe Elimination . . . . . . . . . . . . . . . . . . . . . . . . . 124\r\n9.3.1 The Solution to the Direct Sum Problem . . . . . . . . . . . . . . . . 125\r\n9.3.2 Proof of Lemma 9.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\r\n11",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/40c8041b-1446-4853-92c6-3e6f7e2922f4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=76b428f95ba2beb7457f5aa5b5290baf9448428b8e28f65b67cb1a30ec8a0cd3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 803
      },
      {
        "segments": [
          {
            "segment_id": "c67733ab-a1bc-4564-ac81-d1e6f7d80c37",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "12",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/c67733ab-a1bc-4564-ac81-d1e6f7d80c37.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=50e937452a1b914829b237f5dc5481676c2e1ddad12f05ffd34e35df4025f28a",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "e825b220-9cf6-46b4-ad88-79737f716bf0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 13,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 1\r\nIntroduction\r\n1.1 What is the Cell-Probe Model?\r\nPerhaps the best way to understand the cell-probe model is as the “von Neumann bottle\u0002neck,” a term coined by John Backus in his 1977 Turing Award lecture. In the von Neumann\r\narchitecture, which is pervasive in today’s computers, the memory and the CPU are isolated\r\ncomponents of the computer, communicating through a simple interface. At sufficiently high\r\nlevel of abstraction, this interface allows just two functions: reading and writing the atoms of\r\nmemory (be they words, cache lines, or pages). In Backus’ words, “programming is basically\r\nplanning and detailing the enormous traffic of words through the von Neumann bottleneck.”\r\nFormally, let the memory be an array of cells, each having w bits. The data structure is\r\nallowed to occupy S consecutive cells of this memory, called “the space.” The queries, and,\r\nfor dynamic data structures, the updates, are implemented through algorithms that read\r\nand write cells. The running time of an operation is just the number of cell probes, i.e. the\r\nnumber of reads and writes executed. All computation based on cells that have been read is\r\nfree.\r\nFormally, the cell-probe model is a “non-uniform” model of computation. For a static\r\ndata structure the memory representation is an arbitrary function of the input database.\r\nThe query and update algorithms may maintain unbounded state, which is reset between\r\noperations. The query/update algorithms start with a state that stores the parameters of\r\nthe operation; any information about the data structure must be learned through cell probes.\r\nThe query and updates algorithms are arbitrary functions that specify the next read and\r\nwrite based on the current state, and specify how the state changes when a value is read.\r\n1.1.1 Examples\r\nLet us now exemplify with two fundamental data structure problems that we want to study.\r\nIn Chapter 2, we introduce a larger collection of problems that the thesis is preoccupied\r\nwith.\r\nIn the partial sums problem, the goal is to maintain an array A[1 . . n] of words (w-bit\r\nintegers), initialized to zeroes, under the following operations:\r\n13",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e825b220-9cf6-46b4-ad88-79737f716bf0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3499a861fa9913d31893f7979c6261156d274fd01ddd4985da1c72862f6cc65d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 346
      },
      {
        "segments": [
          {
            "segment_id": "e305bc02-2725-4ba3-941a-6a69fa93ff77",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 14,
            "page_width": 612,
            "page_height": 792,
            "content": "update(k, ∆): modify A[k] ← ∆.\r\nsum(k): returns the partial sum Pk\r\ni=1 A[i].\r\nThe following are three very simple solutions to the problem:\r\n• store that array A in plain form. The update requires 1 cell probe (update the\r\ncorresponding value), while the query requires up to n cell probes to add values\r\nA[1] + · · · + A[k].\r\n• maintain an array S[1 . . n], where S[k] = Pk\r\ni=1 A[i]. The update requires O(n) cell\r\nprobes to recompute all sums, while the query runs in constant time.\r\n• maintain an augmented binary search tree with the array element in the leaves, where\r\neach internal node stores the sum of the leaves in its subtree. Both the query time and\r\nthe update time are O(log n).\r\nDynamic problems are characterized by a trade-off between the update time tu and the\r\nquery time tq. Our goal is to establish the optimal trade-off, by describing a data structure\r\nand a proof of a matching lower bound.\r\nIn most discussion in the literature, one is interested in the “running time per operation”,\r\ni.e. a single value max{tu, tq} which describes the complexity of both operations. In this view,\r\nwe will say that the partial sums problem admits an upper bound of O(lg n), and we will\r\nseek an Ω(lg n) lower bound. Effectively, we want to prove that binary search trees are the\r\noptimal solution for this problem.\r\nTo give an example of a static problem, consider predecessor search. The goal is to\r\npreprocess a set A of n integers (of w bits each), and answer the following query efficiently:\r\npredecessor(x): find max{y ∈ A | y < A}.\r\nThis problem can be solved with space O(n), and query time O(lg n): simply store the\r\narray in sorted order, and solve the query by binary search. Static problems can always be\r\nsolved by complete tabulation: storing the answers to all possible queries. For predecessor\r\nsearch, this gives space O(2w), and constant query time. Later, we will see better upper\r\nbounds for this problem.\r\nStatic problems are characterized by a trade-off between the space S and the query time\r\nt, which we want to understand via matching upper and lower bounds. In the literature,\r\none is often interested in the query time achievable with linear, or near-linear space S =\r\nO(n polylog n).\r\n1.1.2 Predictive Power\r\nFor maximal portability, upper bounds for data structures are often designed in the word\r\nRAM model of computation, which considers any operation from the C programming lan\u0002guage as “constant time.” Sometimes, however, it makes sense to consider variations of the\r\nmodel to allow a closer alignment of the theoretical prediction and practical performance.\r\nFor example, one might augment the model with a new operation available on a family of\r\nCPUs. Alternatively, some operations, like division or multiplication, may be too slow on a\r\n14",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e305bc02-2725-4ba3-941a-6a69fa93ff77.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e5944cf036ab8b17776e978a584365e6d3ff8375b0374c9a4b466326969c1f72",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 480
      },
      {
        "segments": [
          {
            "segment_id": "9338bee2-fb30-42db-a498-8c3b67e3e366",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 15,
            "page_width": 612,
            "page_height": 792,
            "content": "time\r\nepoch:\r\n# updates:\r\n# bits written:\r\nu\r\n0\r\nr\r\n0\r\ntuw\r\nu u\r\n1\r\nr\r\n1\r\nrtuw\r\nu u u u\r\n2\r\nr\r\n2\r\nr\r\n2\r\ntuw\r\nu u u u u u u u\r\n3\r\nr\r\n3\r\nr\r\n3\r\ntuw\r\nu u u u u u u u u u u u u u u u\r\n4\r\nr\r\n4\r\nr\r\n4\r\ntuw\r\n?\r\nquery\r\ninfo about epoch 3: 0 (r\r\n0 + r1 + r2\r\n)tuw < 2r\r\n2\r\ntuw bits\r\nFigure 1-1: Epochs grow exponentially at a rate of r, looking back from the query. Not\r\nenough information about epoch 3 is recorded outside it, so the query needs to probe a cell\r\nfrom epoch 3 with constant probability.\r\nparticular CPU, and one may exclude them from the set of constant-time operations while\r\ndesigning a new data structure.\r\nBy contrast, it has been accepted for at least two decades that the “holy grail” of lower\r\nbounds is to show results in the cell-probe model. Since the model ignores details particular\r\nto the CPU (which operations are available in the instruction set), a lower bound will apply\r\nto any computer that fits the von Neumann model. Furthermore, the lower bound holds for\r\nany implementation in a high-level programming language (such as C), since these languages\r\nenforce a separation between memory and the processing unit.\r\nTo obtain realistic lower bounds, it is standard to assume that the cells have w = Ω(lg n)\r\nbits. This allows pointers and indices to be manipulated in constant time, as is the case\r\non modern computers and programming languages. To model external memory (or CPU\r\ncaches), one can simply let w ≈ B lg n, where B is the size of a page or cache line.\r\n1.2 Overview of Cell-Probe Lower Bounds\r\nThe cell-probe model was introduced three decades ago, by Yao [106] in FOCS’78. Unfor\u0002tunately, his initial lower bounds were not particularly strong, and, as it often happens in\r\nComputer Science, the field had to wait for a second compelling paper before it took off.\r\nIn the case of cell-probe complexity, this came not as a second paper, but as a pair of\r\nalmost simultaneous papers appearing two decades ago. In 1988, Ajtai [3] proved a lower\r\nbound for a static problem: predecessor search. In STOC’89, Fredman and Saks [51] proved\r\nthe first lower bounds for dynamic problems, specifically for the partial sums problem and\r\nfor union-find.\r\nStatic and dynamic lower bounds continued as two largely independent threads of re\u0002search, with the sets of authors working on the two fields being almost disjoint. Dynamic\r\nlower bounds relied on the ad hoc chronogram technique of Fredman and Saks, while static\r\nlower bounds were based on communication complexity, a well-studied field of complexity\r\ntheory.\r\n15",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9338bee2-fb30-42db-a498-8c3b67e3e366.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5367413f7194b689708d9cd771c44f840f16d199f4e13dc8fa198a2053eae22f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 461
      },
      {
        "segments": [
          {
            "segment_id": "7f0cca47-3ef7-4c0b-8b7e-2715dc77de11",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 16,
            "page_width": 612,
            "page_height": 792,
            "content": "1.2.1 Dynamic Bounds\r\nWe first sketch the idea behind the technique of Fredman and Saks [51]; see Figure 1-1.\r\nThe proof begins by generating a sequence of random updates, ended by exactly one query.\r\nLooking back in time from the query, we partition the updates into exponentially growing\r\nepochs: for a certain r, epoch i contains the r\r\ni updates immediately before epoch i − 1.\r\nThe goal is to prove that for each i, the query needs to read a cell written in epoch i with\r\nconstant probability. Then, by linearity of expectation over all epochs, we can bound the\r\nexpected query time to tq = Ω(logr n).\r\nObserve that information about epoch i cannot be reflected in earlier epochs (those\r\noccurred back in time). On the other hand, the latest i − 1 epochs contain less than 2 · r\r\ni−1\r\nupdates. Assume the cell-probe complexity of each update is bounded by tu. Then, during\r\nthe latest i − 1 epochs, only 2 r\r\ni−1\r\ntuw bits are written. Setting r = C · tuw for a sufficiently\r\nlarge constant C, we have r\r\ni \u001d 2 ri−1\r\ntuw, so less than one bit of information is reported\r\nabout each update in epoch i. Assume that, in the particular computation problem that\r\nwe want to lower bound, a random query is forced to learn information about a random\r\nupdate from each epoch. Then, the query must read a cell from epoch i with constant\r\nprobability, because complete information about the needed update is not available outside\r\nthe epoch. We have a lower bound of tq = Ω(logr n) = Ω(lg n/ lg(wtu)). For the natural case\r\nw = Θ(lg n), we have max{tu, tq} = Ω(lg n/ lg lg n).\r\nAll subsequent papers [22, 70, 74, 59, 8, 49, 5, 9, 58] on dynamic lower bounds used this\r\nepoch-based approach of Fredman and Saks (sometimes called the “chronogram technique”).\r\nPerhaps the most influential in this line of research was that the paper by Alstrup,\r\nHusfeldt, and Rauhe [8] from FOCS’98. They proved a bound for the so-called marked\r\nancestor problem, and showed many reductions from this single bound to various interesting\r\ndata-structure problems.\r\nIn STOC’99, Alstrup, Ben-Amram, and Rauhe [5] showed optimal trade-offs for the\r\nunion-find problem, improving on the original bound of Fredman and Saks [51].\r\n1.2.2 Round Elimination\r\nThe seminal paper of Ajtai [3] turned out to have a bug invalidating the results, though\r\nthis did not stop it from being one of the most influential papers in the field. In STOC’94,\r\nMiltersen [71] corrected Ajtai’s error, and derived a new set of results for the predecessor\r\nproblem. More importantly, he recasted the proof as a lower bound for asymmetric commu\u0002nication complexity.\r\nThe idea here is to consider a communication game in which Alice holds a query and Bob\r\nholds a database. The goal of the players is to answer the query on the database, through\r\ncommunication. A cell-probe algorithm immediately gives a communication protocol: in\r\neach round, Alice sends lg S bits (an address), and Bob replies with w bits (the contents\r\nof the memory location). Thus, a lower bound on the communication complexity implies a\r\nlower bound on the cell-probe complexity.\r\nThe technique for proving a communication lower bound, implicit in the work of Ajtai [3]\r\n16",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7f0cca47-3ef7-4c0b-8b7e-2715dc77de11.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=08e7851495dd5e610e3101d0fb22fef42234fb55d2641c5fb129c024de34a577",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 553
      },
      {
        "segments": [
          {
            "segment_id": "7f0cca47-3ef7-4c0b-8b7e-2715dc77de11",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 16,
            "page_width": 612,
            "page_height": 792,
            "content": "1.2.1 Dynamic Bounds\r\nWe first sketch the idea behind the technique of Fredman and Saks [51]; see Figure 1-1.\r\nThe proof begins by generating a sequence of random updates, ended by exactly one query.\r\nLooking back in time from the query, we partition the updates into exponentially growing\r\nepochs: for a certain r, epoch i contains the r\r\ni updates immediately before epoch i − 1.\r\nThe goal is to prove that for each i, the query needs to read a cell written in epoch i with\r\nconstant probability. Then, by linearity of expectation over all epochs, we can bound the\r\nexpected query time to tq = Ω(logr n).\r\nObserve that information about epoch i cannot be reflected in earlier epochs (those\r\noccurred back in time). On the other hand, the latest i − 1 epochs contain less than 2 · r\r\ni−1\r\nupdates. Assume the cell-probe complexity of each update is bounded by tu. Then, during\r\nthe latest i − 1 epochs, only 2 r\r\ni−1\r\ntuw bits are written. Setting r = C · tuw for a sufficiently\r\nlarge constant C, we have r\r\ni \u001d 2 ri−1\r\ntuw, so less than one bit of information is reported\r\nabout each update in epoch i. Assume that, in the particular computation problem that\r\nwe want to lower bound, a random query is forced to learn information about a random\r\nupdate from each epoch. Then, the query must read a cell from epoch i with constant\r\nprobability, because complete information about the needed update is not available outside\r\nthe epoch. We have a lower bound of tq = Ω(logr n) = Ω(lg n/ lg(wtu)). For the natural case\r\nw = Θ(lg n), we have max{tu, tq} = Ω(lg n/ lg lg n).\r\nAll subsequent papers [22, 70, 74, 59, 8, 49, 5, 9, 58] on dynamic lower bounds used this\r\nepoch-based approach of Fredman and Saks (sometimes called the “chronogram technique”).\r\nPerhaps the most influential in this line of research was that the paper by Alstrup,\r\nHusfeldt, and Rauhe [8] from FOCS’98. They proved a bound for the so-called marked\r\nancestor problem, and showed many reductions from this single bound to various interesting\r\ndata-structure problems.\r\nIn STOC’99, Alstrup, Ben-Amram, and Rauhe [5] showed optimal trade-offs for the\r\nunion-find problem, improving on the original bound of Fredman and Saks [51].\r\n1.2.2 Round Elimination\r\nThe seminal paper of Ajtai [3] turned out to have a bug invalidating the results, though\r\nthis did not stop it from being one of the most influential papers in the field. In STOC’94,\r\nMiltersen [71] corrected Ajtai’s error, and derived a new set of results for the predecessor\r\nproblem. More importantly, he recasted the proof as a lower bound for asymmetric commu\u0002nication complexity.\r\nThe idea here is to consider a communication game in which Alice holds a query and Bob\r\nholds a database. The goal of the players is to answer the query on the database, through\r\ncommunication. A cell-probe algorithm immediately gives a communication protocol: in\r\neach round, Alice sends lg S bits (an address), and Bob replies with w bits (the contents\r\nof the memory location). Thus, a lower bound on the communication complexity implies a\r\nlower bound on the cell-probe complexity.\r\nThe technique for proving a communication lower bound, implicit in the work of Ajtai [3]\r\n16",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7f0cca47-3ef7-4c0b-8b7e-2715dc77de11.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=08e7851495dd5e610e3101d0fb22fef42234fb55d2641c5fb129c024de34a577",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 553
      },
      {
        "segments": [
          {
            "segment_id": "d832aacb-4ad5-4e64-b161-c8916f5db99f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 17,
            "page_width": 612,
            "page_height": 792,
            "content": "and Miltersen [71], was made explicit in STOC’95 by Miltersen, Nisan, Safra, and Wigder\u0002son [73]. This technique is based on the round elimination lemma, a result that shows how to\r\neliminate a message in the communication protocol, at the cost of reducing the problem size.\r\nIf all messages can be eliminated and the final problem size is nontrivial (superconstant),\r\nthe protocol cannot be correct.\r\nFurther work on the predecessor problem included the PhD thesis of Bing Xiao [105], the\r\npaper of Beame and Fich from STOC’99 [21], and the paper of Sen and Venkatesh [95]. These\r\nauthors obtained better lower bounds for predecessor search, by showing tighter versions of\r\nround elimination.\r\nAnother application of round elimination was described by Chakrabarti, Chazelle, Gum,\r\nand Lvov [26] in STOC’99. They considered the d-dimensional approximate nearest neighbor\r\nproblem, with constant approximation and polynomial space, and proved a time lower bound\r\nof Ω(lg lg d/ lg lg lg d). This initial bound was deterministic, but Chakrabarti and Regev [27]\r\nproved a randomized bound via a new variation of the round elimination lemma.\r\n1.2.3 Richness\r\nBesides explicitly defining the round elimination concept, Miltersen et al. [73] also introduced\r\na second technique for proving asymmetric lower bounds for communication complexity.\r\nThis technique, called the richness method, could prove bounds of the form: either Alice\r\ncommunicates a bits, or Bob communicates b bits (in total over all messages). By converting\r\na data structure with cell-probe complexity t into a communication protocol, Alice will\r\ncommunicate tlg S bits, and Bob will communicate t · w bits. Thus, we obtain the lower\r\nbound t ≥ min{\r\na\r\nlg S\r\n,\r\nb\r\nw\r\n}. Typically, the b values in such proofs are extremely large, and the\r\nminimum is given by the first term. Thus, the lower bound is t ≥ a/ lg S, or equivalently\r\nS ≥ 2\r\na/t\r\n.\r\nThe richness method is usually used for “hard” problems, like nearest neighbor in high\r\ndimensions, where we want to show a large (superpolynomial) lower bound on the space.\r\nThe bounds we can prove, having form S ≥ 2\r\na/t, are usually very interesting for constant\r\nquery time, but they degrade very quickly with larger t.\r\nThe problems that were analyzed via the richness method were:\r\n• partial match (searching with wildcards), by Borodin, Ostrovsky, and Rabani [25] in\r\nSTOC’99, and by Jayram, Khot, Kumar, and Rabani [64] in STOC’03.\r\n• exact near neighbor search, by Barkol and Rabani [20] in STOC’00. Note that there\r\nexists a reduction from partial match to exact near neighbor. However, the bounds for\r\npartial match were not optimal, and Barkol and Rabani chose to attack near neighbor\r\ndirectly.\r\n• deterministic approximate near neighbor, by Liu [69].\r\n17",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/d832aacb-4ad5-4e64-b161-c8916f5db99f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=81520e0bc95948d943ca07515f5a9b7d962ab52353543e1ed24ee3b5278a9706",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 452
      },
      {
        "segments": [
          {
            "segment_id": "3ba12985-ca95-4336-9101-74478bb08ded",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 18,
            "page_width": 612,
            "page_height": 792,
            "content": "1.3 Our Contributions\r\nIn our work, we attack all the fronts of cell-probe lower bounds described above.\r\n1.3.1 The Epoch Barrier in Dynamic Lower Bounds\r\nAs the natural applications of the chronogram technique were being exhausted, the main bar\u0002rier in dynamic lower bounds became the chronogram technique itself. By design, the epochs\r\nhave to grow at a rate of at least Ω(tu), which means that the large trade-off that can be\r\nshown is tq = Ω(lg n/ lg tu), making the largest possible bound max{tu, tq} = Ω(lg n/ lg lg n).\r\nThis creates a rather unfortunate gap in our understanding, since upper bounds of O(lg n)\r\nare in abundance: such bounds are obtained via binary search trees, which are probably the\r\nsingle most useful idea in dynamic data structures. For example, the well-studied partial\r\nsums problem had an Ω(lg n/ lg lg n) lower bound from the initial paper of Fredman and\r\nSaks [51], and this bound could not be improved to show that binary search trees are optimal.\r\nIn 1999, Miltersen [72] surveyed the field of cell-probe complexity, and proposed sev\u0002eral challenges for future research. Two of his three “big challenges” asked to prove an\r\nω(lg n/ lg lg n) lower bound for any dynamic problem in the cell-probe model, respectively\r\nan ω(lg n) lower bound for any problem in the bit-probe model (see §1.3.2). Of the remaining\r\nfour challenges, one asked to prove Ω(lg n) for partial sums.\r\nThese three challenges are solved in our work, and this thesis. In joint work with Erik\r\nDemaine appearing in SODA’04 [84], we introduced a new technique for dynamic lower\r\nbounds, which could prove an Ω(lg n) lower bound for the partial sums problem. Though\r\nthis was an important 15-year old open problem, the solution turned out to be extremely\r\nclean. The entire proof is some 3 pages of text, and includes essentially no calculation.\r\nIn STOC’04 [83], we augmented our technique through some new ideas, and obtained\r\nΩ(lg n) lower bounds for more problems, including an instance of list manipulation, dynamic\r\nconnectivity, and dynamic trees. These two publications were merged into a single journal\r\npaper [86].\r\nThe logarithmic lower bounds are described in Chapter 3. These may constitute the\r\neasiest example of a data-structural lower bound, and can be presented to a large audience.\r\nIndeed, our proof has been taught in a course by Jeff Erickson at the University of Illinois at\r\nUrbana and Champaign, and in a course co-designed by myself and Erik Demaine at MIT.\r\n1.3.2 The Logarithmic Barrier in Bit-Probe Complexity\r\nThe bit-probe model of computation is an instantiation of the cell-probe model with cells of\r\nw = 1 bit. While this lacks the realism of the cell-probe model with w = Ω(lg n) bits per\r\ncell, it is a clean theoretical model that has been studied often.\r\nIn this model, the best dynamic lower bound was Ω(lg n), shown by Miltersen [74]. In\r\nhis survey of cell-probe complexity [72], Miltersen lists obtaining an ω(lg n) bound as one of\r\nthe three “big challenges” for future research.\r\n18",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/3ba12985-ca95-4336-9101-74478bb08ded.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9541a9e1317209c848483710467f705d829a754c70b17ee86161e7d814f9fce9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "fbce8459-b374-4e25-b4f4-0dd92a0e9090",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 19,
            "page_width": 612,
            "page_height": 792,
            "content": "We address this challenge in our joint work with Tarnit¸ˇa [87] appearing in ICALP’05.\r\nOur proof is based on a surprising discovery: we present a subtle improvement to the classic\r\nchronogram technique of Fredman and Saks [51], which, in particular, allows it to prove\r\nlogarithmic lower bounds in the cell-probe model. Given that the chronogram technique was\r\nthe only known approach for dynamic lower bounds before our work in 2004, it is surprising\r\nto find that the solution to the desired logarithmic bound has always been this close.\r\nTo summarize our idea, remember that in the old epoch argument, the information\r\nrevealed by epochs 1, . . . , i − 1 about epoch i was bounded by the number of cells written in\r\nthese epochs. We will now switch to a simple alternative bound: the number of cells read\r\nduring epochs 1, . . . , i−1 and written during epoch i. This doesn’t immediately help, since all\r\ncell reads from epoch i−1 could read data from epoch i, making these two bounds identical.\r\nHowever, one can randomize the epoch construction, by inserting the query after a random\r\nnumber of updates. This randomization “smoothes” out the distribution of epochs from\r\nwhich cells are read, i.e. a query reads O(tq/ logr n) cells from every epoch, in expectation over\r\nthe randomness in the epoch construction. Then, the O(r\r\ni−1\r\n) updates in epochs 1, . . . , i − 1\r\nonly read O(r\r\ni−1\r\n· tu/ logr n) cells from epoch i. This is not enough information if r \u001d\r\ntu/ logr n = Θ(tu/tq), which implies tq = Ω(logr n) = Ω(lg n/ lg tu\r\ntq\r\n). From this, it follows\r\nthat max{tu, tq} = Ω(lg n).\r\nOur improved epoch technique has an immediate advantage: it is the first technique that\r\ncan prove a bound for the regime tu < tq. In particular, we obtain a tight update/query\r\ntrade-off for the partial sums problem, whereas previous approaches could only attack the\r\ncase tu > tq. Furthermore, by carefully using the new lower bound in the regime tu < tq,\r\nwe obtain an Ω(\r\nlg n\r\nlg lg n\r\n)\r\n2\r\n\u0001\r\nlower bound in the bit-prove model. This offers the highest known\r\nbound in the bit-probe model, and answers Miltersen’s challenge.\r\nIntuitively, performance in the bit-probe model should typically be slower by a lg n factor\r\ncompared to the cell-probe model. However, our Ω(lg e 2 n) bound in the bit-probe world is\r\nfar from an echo of an Ω(lg e n) bound in the cell-probe world. Indeed, Ω( lg n\r\nlg lg n\r\n) bounds in the\r\ncell-probe model have been known since 1989, but the bit-probe record has remained just\r\nthe slightly higher Ω(lg n). In fact, our bound is the first to show a quasi-optimal Ω(lg e n)\r\nseparation between bit-probe complexity and the cell-probe complexity, for superconstant\r\ncell-probe complexity.\r\nOur improved epoch construction, as well as our bit-probe lower bounds, are presented\r\nin Chapter 4.\r\n1.3.3 The Communication Barrier in Static Complexity\r\nAll bounds for static data structures were shown by transforming the cell-probe algorithm\r\ninto a communication protocol, and proving a communication lower bound for the problem\r\nat hand, either by round elimination, or by the richness method.\r\nIntuitively, we do not expect this relation between cell-probe and communication com\u0002plexity to be tight. In the communication model, Bob can remember past communication,\r\nand may answer new queries based on this. Needless to say, if Bob is just a table of cells, he\r\n19",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/fbce8459-b374-4e25-b4f4-0dd92a0e9090.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=03e544fb0e24e466dda8388482fe63292f4a6c74b7c7fd1d9e18f0c82bbdd9bf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 590
      },
      {
        "segments": [
          {
            "segment_id": "fbce8459-b374-4e25-b4f4-0dd92a0e9090",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 19,
            "page_width": 612,
            "page_height": 792,
            "content": "We address this challenge in our joint work with Tarnit¸ˇa [87] appearing in ICALP’05.\r\nOur proof is based on a surprising discovery: we present a subtle improvement to the classic\r\nchronogram technique of Fredman and Saks [51], which, in particular, allows it to prove\r\nlogarithmic lower bounds in the cell-probe model. Given that the chronogram technique was\r\nthe only known approach for dynamic lower bounds before our work in 2004, it is surprising\r\nto find that the solution to the desired logarithmic bound has always been this close.\r\nTo summarize our idea, remember that in the old epoch argument, the information\r\nrevealed by epochs 1, . . . , i − 1 about epoch i was bounded by the number of cells written in\r\nthese epochs. We will now switch to a simple alternative bound: the number of cells read\r\nduring epochs 1, . . . , i−1 and written during epoch i. This doesn’t immediately help, since all\r\ncell reads from epoch i−1 could read data from epoch i, making these two bounds identical.\r\nHowever, one can randomize the epoch construction, by inserting the query after a random\r\nnumber of updates. This randomization “smoothes” out the distribution of epochs from\r\nwhich cells are read, i.e. a query reads O(tq/ logr n) cells from every epoch, in expectation over\r\nthe randomness in the epoch construction. Then, the O(r\r\ni−1\r\n) updates in epochs 1, . . . , i − 1\r\nonly read O(r\r\ni−1\r\n· tu/ logr n) cells from epoch i. This is not enough information if r \u001d\r\ntu/ logr n = Θ(tu/tq), which implies tq = Ω(logr n) = Ω(lg n/ lg tu\r\ntq\r\n). From this, it follows\r\nthat max{tu, tq} = Ω(lg n).\r\nOur improved epoch technique has an immediate advantage: it is the first technique that\r\ncan prove a bound for the regime tu < tq. In particular, we obtain a tight update/query\r\ntrade-off for the partial sums problem, whereas previous approaches could only attack the\r\ncase tu > tq. Furthermore, by carefully using the new lower bound in the regime tu < tq,\r\nwe obtain an Ω(\r\nlg n\r\nlg lg n\r\n)\r\n2\r\n\u0001\r\nlower bound in the bit-prove model. This offers the highest known\r\nbound in the bit-probe model, and answers Miltersen’s challenge.\r\nIntuitively, performance in the bit-probe model should typically be slower by a lg n factor\r\ncompared to the cell-probe model. However, our Ω(lg e 2 n) bound in the bit-probe world is\r\nfar from an echo of an Ω(lg e n) bound in the cell-probe world. Indeed, Ω( lg n\r\nlg lg n\r\n) bounds in the\r\ncell-probe model have been known since 1989, but the bit-probe record has remained just\r\nthe slightly higher Ω(lg n). In fact, our bound is the first to show a quasi-optimal Ω(lg e n)\r\nseparation between bit-probe complexity and the cell-probe complexity, for superconstant\r\ncell-probe complexity.\r\nOur improved epoch construction, as well as our bit-probe lower bounds, are presented\r\nin Chapter 4.\r\n1.3.3 The Communication Barrier in Static Complexity\r\nAll bounds for static data structures were shown by transforming the cell-probe algorithm\r\ninto a communication protocol, and proving a communication lower bound for the problem\r\nat hand, either by round elimination, or by the richness method.\r\nIntuitively, we do not expect this relation between cell-probe and communication com\u0002plexity to be tight. In the communication model, Bob can remember past communication,\r\nand may answer new queries based on this. Needless to say, if Bob is just a table of cells, he\r\n19",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/fbce8459-b374-4e25-b4f4-0dd92a0e9090.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=03e544fb0e24e466dda8388482fe63292f4a6c74b7c7fd1d9e18f0c82bbdd9bf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 590
      },
      {
        "segments": [
          {
            "segment_id": "a79fa027-584d-49cf-bf5f-e7a78daada87",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 20,
            "page_width": 612,
            "page_height": 792,
            "content": "cannot “remember” anything, and his responses must be a function of Alice’s last message\r\n(i.e. the address being probed).\r\nThe most serious consequence of the reduction to communication complexity is that the\r\nlower bounds are insensitive to polynomial changes in the space S. For instance, going from\r\nspace S = O(n\r\n3\r\n) to space S = O(n) only translates into a change in Alice’s message size by\r\na factor of 3. In the communication game model, this will increase the number of rounds\r\nby at most 3, since Alice can break a longer message into three separate messages, and Bob\r\ncan remember the intermediate parts.\r\nUnfortunately, this means that communication complexity cannot be used to separate\r\ndata structures of polynomial space, versus data structures of linear space. By contrast, for\r\nmany natural data-structure problems, the most interesting behavior occurs close to linear\r\nspace. In fact, when we consider applications to large data bases, the difference between\r\nspace O(n\r\n3\r\n) and space O(n polylog n) is essentially the difference between interesting and\r\nuninteresting solutions.\r\nIn our joint work with Mikkel Thorup [89] appearing in STOC’06, we provided the first\r\nseparation between data structures of polynomial size, and data structures of linear size. In\r\nfact, our technique could prove an asymptotic separation between space Θ(n\r\n1+ε\r\n) and space\r\nn\r\n1+o(1), for any constant ε > 0.\r\nOur idea was to consider a communication game in which Alice holds k independent\r\nqueries to the same database, for large k. At each step of the cell-probe algorithm, these\r\nqueries need to probe a subset of at most k cells from the space of S cells. Thus, Alice\r\nneeds to send O(lg \r\nS\r\nk\r\n\u0001\r\n) = O(k lg S\r\nk\r\n) bits to describe the memory addresses. For k close\r\nto n, and S = n\r\n1+o(1), this is asymptotically smaller than k lg S. This means that the\r\n“amortized” complexity per query is o(lg S), and, if our lower bound per query is as high\r\nas for a single query, we obtain a better cell-probe lower bound. A result in communication\r\ncomplexity showing that the complexity of k independent problems increases by a factor of\r\nΩ(k) compared to one problem is called a direct sum result.\r\nIn our paper [89], we showed a direct sum result for the round elimination lemma. This\r\nimplied an optimal lower bound for predecessor search, finally closing this well-studied prob\u0002lem. Since predecessor search admits better upper bounds with space O(n\r\n2\r\n) than with space\r\nO(n polylog n), a tight bound could not have been obtained prior to our technique breaking\r\nthe communication barrier.\r\nFor our direct sum version of round elimination, and our improved predecessor bounds,\r\nthe reader is referred to Chapter 9.\r\n1.3.4 Richness Lower Bounds\r\nA systematic improvement. A broad impact of our work on the richness method was\r\nto show that the cell-probe consequence of richness lower bounds had been systematically\r\nunderestimated: any richness lower bound implies better cell-probe lower bounds than what\r\nwas previously thought, when the space is n\r\n1+o(1). In other words, we can take any lower\r\nbound shown by the richness method, and obtain a better lower bound for small-space data\r\nstructures by black-box use of the old result.\r\n20",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/a79fa027-584d-49cf-bf5f-e7a78daada87.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=94d90f64182f47125517b4cdb40d5251493e9e2e79a6528ae2df62f1d2d6ec51",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 543
      },
      {
        "segments": [
          {
            "segment_id": "a79fa027-584d-49cf-bf5f-e7a78daada87",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 20,
            "page_width": 612,
            "page_height": 792,
            "content": "cannot “remember” anything, and his responses must be a function of Alice’s last message\r\n(i.e. the address being probed).\r\nThe most serious consequence of the reduction to communication complexity is that the\r\nlower bounds are insensitive to polynomial changes in the space S. For instance, going from\r\nspace S = O(n\r\n3\r\n) to space S = O(n) only translates into a change in Alice’s message size by\r\na factor of 3. In the communication game model, this will increase the number of rounds\r\nby at most 3, since Alice can break a longer message into three separate messages, and Bob\r\ncan remember the intermediate parts.\r\nUnfortunately, this means that communication complexity cannot be used to separate\r\ndata structures of polynomial space, versus data structures of linear space. By contrast, for\r\nmany natural data-structure problems, the most interesting behavior occurs close to linear\r\nspace. In fact, when we consider applications to large data bases, the difference between\r\nspace O(n\r\n3\r\n) and space O(n polylog n) is essentially the difference between interesting and\r\nuninteresting solutions.\r\nIn our joint work with Mikkel Thorup [89] appearing in STOC’06, we provided the first\r\nseparation between data structures of polynomial size, and data structures of linear size. In\r\nfact, our technique could prove an asymptotic separation between space Θ(n\r\n1+ε\r\n) and space\r\nn\r\n1+o(1), for any constant ε > 0.\r\nOur idea was to consider a communication game in which Alice holds k independent\r\nqueries to the same database, for large k. At each step of the cell-probe algorithm, these\r\nqueries need to probe a subset of at most k cells from the space of S cells. Thus, Alice\r\nneeds to send O(lg \r\nS\r\nk\r\n\u0001\r\n) = O(k lg S\r\nk\r\n) bits to describe the memory addresses. For k close\r\nto n, and S = n\r\n1+o(1), this is asymptotically smaller than k lg S. This means that the\r\n“amortized” complexity per query is o(lg S), and, if our lower bound per query is as high\r\nas for a single query, we obtain a better cell-probe lower bound. A result in communication\r\ncomplexity showing that the complexity of k independent problems increases by a factor of\r\nΩ(k) compared to one problem is called a direct sum result.\r\nIn our paper [89], we showed a direct sum result for the round elimination lemma. This\r\nimplied an optimal lower bound for predecessor search, finally closing this well-studied prob\u0002lem. Since predecessor search admits better upper bounds with space O(n\r\n2\r\n) than with space\r\nO(n polylog n), a tight bound could not have been obtained prior to our technique breaking\r\nthe communication barrier.\r\nFor our direct sum version of round elimination, and our improved predecessor bounds,\r\nthe reader is referred to Chapter 9.\r\n1.3.4 Richness Lower Bounds\r\nA systematic improvement. A broad impact of our work on the richness method was\r\nto show that the cell-probe consequence of richness lower bounds had been systematically\r\nunderestimated: any richness lower bound implies better cell-probe lower bounds than what\r\nwas previously thought, when the space is n\r\n1+o(1). In other words, we can take any lower\r\nbound shown by the richness method, and obtain a better lower bound for small-space data\r\nstructures by black-box use of the old result.\r\n20",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/a79fa027-584d-49cf-bf5f-e7a78daada87.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=94d90f64182f47125517b4cdb40d5251493e9e2e79a6528ae2df62f1d2d6ec51",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 543
      },
      {
        "segments": [
          {
            "segment_id": "353ddf9c-76eb-41f2-a746-c75ad6c623ff",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 21,
            "page_width": 612,
            "page_height": 792,
            "content": "Remember that proving Alice must communicate Ω(a) bits essentially implies a cell-probe\r\nlower bound of t = Ω(a/ lg S). By our trick for beating communication complexity, we will\r\nconsider k = O(n/ poly(w)) queries in parallel, which means that Alice will communicate\r\nO(t · k lg S\r\nk\r\n) = O(tk lg Sw\r\nn\r\n) bits in total.\r\nOur novel technical result is a direct sum theorem for the richness measure: for any\r\nproblem f that has an Ω(a) communication lower bound by richness, k independent copies of\r\nf will have a lower bound of Ω(k·a). This implies that t·k lg Sw\r\nn = Ω(k·a), so t = Ω(a/ lg Swn\r\n).\r\nOur new bounds is better than the classic t = Ω(a/ lg S), for space n\r\n1+o(1). In particular, for\r\nthe most important case of S = O(n polylog n) and w = O(polylog n), our improved bound\r\nis better by Θ(lg n/ lg lg n).\r\nOur direct sums theorems for richness appeared in joint work with Mikkel Thorup [88]\r\nat FOCS’06, and are described in Chapter 5.\r\nSpecific problems. Our work has also extended the reach of the richness method, proving\r\nlower bound for several interesting problem.\r\nOne such problem is lopsided set disjointness (LSD), in which Alice and Bob receive\r\nsets S and T (|S| \u001c |T|), and they want to determine whether S ∩ T = ∅. In their\r\nseminal work from STOC’95, Miltersen et al. [73] proved a deterministic lower bound for\r\nlopsided set disjointness, and left the randomized case as an “interesting open problem.”\r\nThe first randomized bounds were provided in our joint work with Alexandr Andoni and\r\nPiotr Indyk [16], appearing in FOCS’06, and strengthened in our (single-author) paper [82]\r\nappearing in FOCS’08. Some of these lower bounds are described in Chapter 5.\r\nTraditionally, LSD was studied for its fundamental appeal, not because of data-structural\r\napplications. However, in [82] we showed a simple reduction from LSD to partial match.\r\nDespite the work of [73, 25, 64], the best bound for partial match was suboptimal, showing\r\nthat Alice must communicate Ω(d/ lg n) bits, where d is the dimension. Our reduction,\r\ncoupled with our LSD result, imply an optimal bound of Ω(d) on Alice’s communication,\r\nclosing this issue. The proof of our reduction appears in Chapter 6.\r\nSince partial match reduces to exact near neighbor, we also obtain an optimal communi\u0002cation bound for that problem. This supersedes the work of [20], who had a more complicated\r\nproof of a tight lower bound for exact near neighbor.\r\nTurning our attention to (1+ε)-approximate nearest neighbor, we find the existing lower\r\nbounds unsatisfactory. The upper bounds for this problem use n\r\nO(1/ε2)\r\nspace, by dimension\u0002ality reduction. On the other hand, the lower bounds of [26, 27] allowed constant ε and\r\npolynomial space, proving a tight time lower bound of Ω(lg lg d/ lg lg lg d) in d dimensions.\r\nHowever, from a practical perspective, the main issue with the upper bound is the prohibitive\r\nspace usage, not the doubly logarithmic query time. To address this concern, our paper with\r\nAndoni and Indyk [16] proved a communication lower bound in which Alice’s communication\r\nis Ω( 1\r\nε\r\n2\r\nlg n) bits. This shows that the space must be n\r\nΩ(1/(tε2)), for query time t. Our lower\r\nbound is once more by reduction from LSD, and can be found in Chapter 6.\r\nFinally, we consider the near neighbor problem in the `∞ metric, in which Indyk [61] had\r\nshown an exotic trade-off between approximation and space, obtaining O(lg lg d) approxi\u000221",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/353ddf9c-76eb-41f2-a746-c75ad6c623ff.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d0a111285d95ea49788eecfc8c04f282fd5b2d457eef8e71aaeae2053b7e6ecb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 594
      },
      {
        "segments": [
          {
            "segment_id": "353ddf9c-76eb-41f2-a746-c75ad6c623ff",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 21,
            "page_width": 612,
            "page_height": 792,
            "content": "Remember that proving Alice must communicate Ω(a) bits essentially implies a cell-probe\r\nlower bound of t = Ω(a/ lg S). By our trick for beating communication complexity, we will\r\nconsider k = O(n/ poly(w)) queries in parallel, which means that Alice will communicate\r\nO(t · k lg S\r\nk\r\n) = O(tk lg Sw\r\nn\r\n) bits in total.\r\nOur novel technical result is a direct sum theorem for the richness measure: for any\r\nproblem f that has an Ω(a) communication lower bound by richness, k independent copies of\r\nf will have a lower bound of Ω(k·a). This implies that t·k lg Sw\r\nn = Ω(k·a), so t = Ω(a/ lg Swn\r\n).\r\nOur new bounds is better than the classic t = Ω(a/ lg S), for space n\r\n1+o(1). In particular, for\r\nthe most important case of S = O(n polylog n) and w = O(polylog n), our improved bound\r\nis better by Θ(lg n/ lg lg n).\r\nOur direct sums theorems for richness appeared in joint work with Mikkel Thorup [88]\r\nat FOCS’06, and are described in Chapter 5.\r\nSpecific problems. Our work has also extended the reach of the richness method, proving\r\nlower bound for several interesting problem.\r\nOne such problem is lopsided set disjointness (LSD), in which Alice and Bob receive\r\nsets S and T (|S| \u001c |T|), and they want to determine whether S ∩ T = ∅. In their\r\nseminal work from STOC’95, Miltersen et al. [73] proved a deterministic lower bound for\r\nlopsided set disjointness, and left the randomized case as an “interesting open problem.”\r\nThe first randomized bounds were provided in our joint work with Alexandr Andoni and\r\nPiotr Indyk [16], appearing in FOCS’06, and strengthened in our (single-author) paper [82]\r\nappearing in FOCS’08. Some of these lower bounds are described in Chapter 5.\r\nTraditionally, LSD was studied for its fundamental appeal, not because of data-structural\r\napplications. However, in [82] we showed a simple reduction from LSD to partial match.\r\nDespite the work of [73, 25, 64], the best bound for partial match was suboptimal, showing\r\nthat Alice must communicate Ω(d/ lg n) bits, where d is the dimension. Our reduction,\r\ncoupled with our LSD result, imply an optimal bound of Ω(d) on Alice’s communication,\r\nclosing this issue. The proof of our reduction appears in Chapter 6.\r\nSince partial match reduces to exact near neighbor, we also obtain an optimal communi\u0002cation bound for that problem. This supersedes the work of [20], who had a more complicated\r\nproof of a tight lower bound for exact near neighbor.\r\nTurning our attention to (1+ε)-approximate nearest neighbor, we find the existing lower\r\nbounds unsatisfactory. The upper bounds for this problem use n\r\nO(1/ε2)\r\nspace, by dimension\u0002ality reduction. On the other hand, the lower bounds of [26, 27] allowed constant ε and\r\npolynomial space, proving a tight time lower bound of Ω(lg lg d/ lg lg lg d) in d dimensions.\r\nHowever, from a practical perspective, the main issue with the upper bound is the prohibitive\r\nspace usage, not the doubly logarithmic query time. To address this concern, our paper with\r\nAndoni and Indyk [16] proved a communication lower bound in which Alice’s communication\r\nis Ω( 1\r\nε\r\n2\r\nlg n) bits. This shows that the space must be n\r\nΩ(1/(tε2)), for query time t. Our lower\r\nbound is once more by reduction from LSD, and can be found in Chapter 6.\r\nFinally, we consider the near neighbor problem in the `∞ metric, in which Indyk [61] had\r\nshown an exotic trade-off between approximation and space, obtaining O(lg lg d) approxi\u000221",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/353ddf9c-76eb-41f2-a746-c75ad6c623ff.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d0a111285d95ea49788eecfc8c04f282fd5b2d457eef8e71aaeae2053b7e6ecb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 594
      },
      {
        "segments": [
          {
            "segment_id": "37077d4a-511c-494b-ac58-957e6458cb76",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 22,
            "page_width": 612,
            "page_height": 792,
            "content": "mation for any polynomial space. In our joint work with Andoni and Croitoru [13], we gave\r\na very appealing reformulation of Indyk’s algorithm in information-theoretic terms, which\r\nmade his space/approximation trade-off more natural. Then, we described a richness lower\r\nbound for the problem, showing that this space/approximation trade-off is optimal. These\r\nresults can be found in Chapter 8.\r\n1.3.5 Lower Bounds for Range Queries\r\nBy far, the most exciting consequence of our technique for surpassing the communication\r\nbarrier is that it opens the door to tight bounds for range queries. Such queries are some\r\nof the most natural examples of what computers might be queried for, and any statement\r\nabout their importance is probably superfluous. The introductory lecture of any database\r\ncourse is virtually certain to contain an example like “find employees with a salary between\r\n70000 and 95000, who have been hired between 1998 and 2001.”\r\nOrthogonal range queries can be solved in constant time if polynomial space is allowed,\r\nby simply precomputing all answers. Thus, any understanding of these problems hinges on\r\nour technique to obtain better lower bounds for space O(n polylog n).\r\nIn my papers [81, 82], I prove optimal bounds for range counting in 2 dimensions, stabbing\r\nin 2 dimensions, and range reporting in 4 dimensions. Surprisingly, our lower bounds are\r\nonce more by reduction from LSD.\r\nRange counting problems have traditionally been studied in algebraic models. In the\r\ngroup and semigroup models, each point has an associated weight from an arbitrary commu\u0002tative (semi)group and the “counting” query asks for the sum of the weights of the points in\r\nthe range. The data structure can only manipulate weights through the black-box addition\r\noperator (and, in the group model, subtraction).\r\nThe semigroup model has allowed for every strong lower bounds, and nearly optimal\r\nresults are known in all dimensions. By contrast, as soon as we allow subtraction (the group\r\nmodel), the lower bounds are very weak, and we did not even have a good bound for 2\r\ndimensions. This rift between our understanding of the semigroup and stronger models has\r\na deeper conceptual explanation. In general, semigroup lower bounds hide arguments of a\r\nvery geometric flavor. If a value is included in a sum, it can never be taken out again (no\r\nsubtraction is available), and this immediately places a geometric restriction on the set of\r\nranges that could use the cell. Thus, semigroup lower bounds are essentially bounds on\r\na certain kind of “decomposability” of geometric shapes. On the other hand, bounds in\r\nthe group or cell-probe model require a different, information theoretic understanding of\r\nthe problem. In the group model, the sum stored in a cell does not tell us anything in\r\nisolation, as terms could easily be subtracted out. The lower bound must now find certain\r\nbottlenecks in the manipulation of information that make the problem hard. Essentially, the\r\ndifference in the type of reasoning behind semigroup lower bounds and group/cell-probe lower\r\nbounds is parallel to the difference between “understanding geometry” and “understanding\r\ncomputation”. Since we have been vastly more successful at the former, it should not come\r\nas a surprise that progress outside the semigroup model has been extremely slow.\r\nProving good bounds outside the restrictive semigroup model has long been recognized as\r\n22",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/37077d4a-511c-494b-ac58-957e6458cb76.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a35535dbe218e28a87dc17b6e8b6d3198fd01ce2cb9d1d9bb71785a13a94ff68",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 542
      },
      {
        "segments": [
          {
            "segment_id": "37077d4a-511c-494b-ac58-957e6458cb76",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 22,
            "page_width": 612,
            "page_height": 792,
            "content": "mation for any polynomial space. In our joint work with Andoni and Croitoru [13], we gave\r\na very appealing reformulation of Indyk’s algorithm in information-theoretic terms, which\r\nmade his space/approximation trade-off more natural. Then, we described a richness lower\r\nbound for the problem, showing that this space/approximation trade-off is optimal. These\r\nresults can be found in Chapter 8.\r\n1.3.5 Lower Bounds for Range Queries\r\nBy far, the most exciting consequence of our technique for surpassing the communication\r\nbarrier is that it opens the door to tight bounds for range queries. Such queries are some\r\nof the most natural examples of what computers might be queried for, and any statement\r\nabout their importance is probably superfluous. The introductory lecture of any database\r\ncourse is virtually certain to contain an example like “find employees with a salary between\r\n70000 and 95000, who have been hired between 1998 and 2001.”\r\nOrthogonal range queries can be solved in constant time if polynomial space is allowed,\r\nby simply precomputing all answers. Thus, any understanding of these problems hinges on\r\nour technique to obtain better lower bounds for space O(n polylog n).\r\nIn my papers [81, 82], I prove optimal bounds for range counting in 2 dimensions, stabbing\r\nin 2 dimensions, and range reporting in 4 dimensions. Surprisingly, our lower bounds are\r\nonce more by reduction from LSD.\r\nRange counting problems have traditionally been studied in algebraic models. In the\r\ngroup and semigroup models, each point has an associated weight from an arbitrary commu\u0002tative (semi)group and the “counting” query asks for the sum of the weights of the points in\r\nthe range. The data structure can only manipulate weights through the black-box addition\r\noperator (and, in the group model, subtraction).\r\nThe semigroup model has allowed for every strong lower bounds, and nearly optimal\r\nresults are known in all dimensions. By contrast, as soon as we allow subtraction (the group\r\nmodel), the lower bounds are very weak, and we did not even have a good bound for 2\r\ndimensions. This rift between our understanding of the semigroup and stronger models has\r\na deeper conceptual explanation. In general, semigroup lower bounds hide arguments of a\r\nvery geometric flavor. If a value is included in a sum, it can never be taken out again (no\r\nsubtraction is available), and this immediately places a geometric restriction on the set of\r\nranges that could use the cell. Thus, semigroup lower bounds are essentially bounds on\r\na certain kind of “decomposability” of geometric shapes. On the other hand, bounds in\r\nthe group or cell-probe model require a different, information theoretic understanding of\r\nthe problem. In the group model, the sum stored in a cell does not tell us anything in\r\nisolation, as terms could easily be subtracted out. The lower bound must now find certain\r\nbottlenecks in the manipulation of information that make the problem hard. Essentially, the\r\ndifference in the type of reasoning behind semigroup lower bounds and group/cell-probe lower\r\nbounds is parallel to the difference between “understanding geometry” and “understanding\r\ncomputation”. Since we have been vastly more successful at the former, it should not come\r\nas a surprise that progress outside the semigroup model has been extremely slow.\r\nProving good bounds outside the restrictive semigroup model has long been recognized as\r\n22",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/37077d4a-511c-494b-ac58-957e6458cb76.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a35535dbe218e28a87dc17b6e8b6d3198fd01ce2cb9d1d9bb71785a13a94ff68",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 542
      },
      {
        "segments": [
          {
            "segment_id": "39b35918-0983-474d-ae4a-caece2e75c9d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 23,
            "page_width": 612,
            "page_height": 792,
            "content": "lopsided set disjointness [82, 16, 73]\r\nreachability oracles\r\nin the butterfly\r\nloses lg lg n\r\npartial match\r\n[82, 88, 64, 25, 73]\r\n(1 + ε)-ANN\r\nin `1, `2\r\n[16, 27, 26]\r\ndyn. marked\r\nancestor [8]\r\n2D stabbing 3-ANN in `∞\r\n[13, 61]\r\nNN in `1, `2\r\n[82, 88, 20, 25]\r\nworst-case\r\nunion-find\r\n[51, 5]\r\ndyn. 1D\r\nstabbing\r\npartial sums\r\n[87, 86, 58, 22, 51]\r\n4D range\r\nreporting\r\n[82]\r\n2D range\r\ncounting\r\n[82, 81]\r\ndyn. NN\r\nin 2D [9]\r\ndyn. 2D range\r\nreporting\r\ndyn. graphs\r\n[86, 87, 74, 49, 59]\r\nFigure 1-2: Dashed lines indicate reductions that were already known, while solid lines\r\nindicate novel reductions. For problems in bold, the results of this thesis are stronger than\r\nwhat was previously known. Citations indicate work on lower bounds.\r\nan important challenge. As early as 1982, Fredman [48] asked for better bounds in the group\r\nmodel for dynamic range counting. In FOCS’86, Chazelle [31] echoed this, and also asked\r\nabout the static case. Our results answer these old challenges, at least in the 2-dimensional\r\ncase.\r\n1.3.6 Simple Proofs\r\nAn important contribution of our work is to show clean, simple proofs of lower bounds, in\r\nan area of Theoretical Computer Science that is often dominated by exhausting technical\r\ndetails.\r\nPerhaps the best illustration of this contribution is in Figure 1-2, which shows the lower\r\nbounds that can be obtained by reduction from lopsided set disjointness. (The reader unfa\u0002miliar with the problems can consult Chapter 2.)\r\nThis reduction tree is able to unify a large fraction of the known results in dynamic\r\ndata structures and static data structures (both in the case of large space, and for near\u0002linear space). In many cases, our proofs by reduction considerably simplify the previous\r\nproofs. Putting all the work in a single lower bound has exciting consequences from the\r\nteaching perspective: instead of presenting many different lower bounds (even “simple” ones\r\nare seldom light on technical detail!), we can now present many interesting results through\r\nclean algorithmic reductions, which are easier to grasp.\r\nLooking at the reduction tree, it may seem hard to imagine a formal connection between\r\nlower bounds for such different problems, in such different settings. Much of the magic of our\r\nresults lies in considering a somewhat artificial problem, which nonetheless gives the right\r\n23",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/39b35918-0983-474d-ae4a-caece2e75c9d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=287cf684e112379232eb39479cca4c5237596b0eba811382c10140c74b301a96",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c1534d7f-20ba-42b0-8138-820d8e8ab675",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 24,
            "page_width": 612,
            "page_height": 792,
            "content": "link between the problems: reachability queries in butterfly graphs. Once we decide to use\r\nthis middle ground, it is not hard to give reductions to and from set disjointness, dynamic\r\nmarked ancestor, and static 4-dimensional range reporting. Each of these reductions is\r\nnatural, but the combination is no less surprising.\r\nThese reductions are described in Chapters 6 and 7.\r\n24",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/c1534d7f-20ba-42b0-8138-820d8e8ab675.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e91d1156281908bf5666b9637e228043732eddb1f7cb7e2e6f6f99096afc0ef4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 440
      },
      {
        "segments": [
          {
            "segment_id": "ef7a47e6-2ee2-4ffa-9fc5-c2817f57287c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 25,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 2\r\nCatalog of Problems\r\nThis section introduces the various data-structure problems that we consider throughout\r\nthis thesis. It is hoped that the discussion of each topic is sufficiently self-contained, that\r\nthe reader can quickly jump to his or her topic of interest.\r\n2.1 Predecessor Search\r\n2.1.1 Flavors of Integer Search\r\nBinary search is certainly one of the most fundamental ideas in computer science — but\r\nwhat do we use it for? Given a set S with |S| = n values from some ordered universe U, the\r\nfollowing are natural and often-asked queries the can be solved by binary search:\r\nexact search: given some x ∈ U, is x ∈ S?\r\npredecessor search: given some x ∈ U, find max{y ∈ S | y < S}, i.e. the predecessor of\r\nx in the set S.\r\n1D range reporting: given some interval [x, y], report all/one of the points in S ∩ [x, y].\r\nExact search is ubiquitous in computer science, while range reporting is a natural and\r\nimportant database query. Though at first sight predecessor search may seem less natural,\r\nit may in fact be the most widely used of the three. Some of its many applications include:\r\n• IP-lookup, perhaps the most important application, is the basic query that must be\r\nanswered by every Internet router when forwarding a packet (which probably makes\r\nit the most executed algorithmic problem in the world). The problem is equivalent to\r\npredecessor search [44].\r\n• to make data structures persistent, each cell is replaced with a list of updates. Deciding\r\nwhich version to use is a predecessor problem.\r\n• orthogonal range queries start by converting any universe U into “rank space” {1, . . . , n}.\r\nThe dependence on the universe becomes an additive term in the running time.\r\n• the succinct incarnation of predecessor search is known as the “rank/select problem,”\r\nand it underlies most succinct data structures.\r\n25",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/ef7a47e6-2ee2-4ffa-9fc5-c2817f57287c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=95ace5ab09840a993367e80c6d8019790fde88810556fb7f9ddf6cfdf19c5d9b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 318
      },
      {
        "segments": [
          {
            "segment_id": "20dd51fc-deb2-4faf-8c7b-66db77c37352",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 26,
            "page_width": 612,
            "page_height": 792,
            "content": "• (1 + ε)-approximate near neighbor in any constant dimension (with the normal Eu\u0002clidean metric) is equivalent to predecessor search [28].\r\n• in the emergency planning version of dynamic connectivity, a query is equivalent to\r\npredecessor search; see our work with Mikkel Thorup [85].\r\nIt is possible to study search problems in an arbitrary universe U endowed with a com\u0002parison operation (the so called comparison model), in which case the Θ(lg n) running time\r\nof binary search is easily seen to be optimal. However, this “optimality” of binary search is\r\nmisleading, given that actual computers represent data in some well-specified formats with\r\nbounded precision.\r\nThe most natural universe is U = {0, . . . , 2\r\nw − 1}, capturing the assumption that values\r\nare arbitrary w-bit integers that fit in a machine word. The standard floating point repre\u0002sentation (IEEE 754, 1985) was designed so that two w-bit floating point values compare the\r\nsame as two w-bit integers. Thus, any algorithm for search queries that applies to integers\r\ncarries over to floating point numbers. Furthermore, most algorithms apply naturally to\r\nstrings, which can be interpreted as numbers of high, variable precision.\r\nBy far, the best known use of bounded precision is hashing. In fact, when one thinks\r\nof exact search queries, the first solution that comes to mind is probably hash tables, not\r\nbinary search. Hash tables provide an optimal solution to this problem, at least if we accept\r\nrandomization in the construction. Consider the following theorem due to Fredman, Koml´os,\r\nand Szemer´edi [50]:\r\nTheorem 2.1. There exists a dictionary data structure using O(n) words of memory that\r\nanswers exact search queries deterministically in O(1) time. The data structure can be\r\nconstructed by a randomized algorithm in O(n) time with high probability.\r\nDietzfelbinger and Meyer auf der Heide [39] give a dynamic dictionary that implements\r\nqueries deterministically in constant time, while the randomized updates run in constant\r\ntime with high probability.\r\nBy contrast, constant time is not possible for predecessor search. The problem has been\r\nstudied intensely in the past 20 years, and it was only in our recent work with Mikkel\r\nThorup [89, 90] that the optimal bounds were understood. We discuss the history of this\r\nproblem in the next section.\r\nOne-dimensional range reporting remains the least understood of the three problems. In\r\nour work with Christian Mortensen and Rasmus Pagh [76], we have shown surprising upper\r\nbounds for dynamic range reporting in one dimension, with a query time that is exponentially\r\nfaster than the optimal query time for predecessor search. However, there is currently no\r\nlower bound for the problem, and we do not know whether it requires superconstant time\r\nper operation.\r\n2.1.2 Previous Upper Bounds\r\nThe famous data structure of van Emde Boas [101] from FOCS’75 can solve predecessor\r\nsearch in O(lg w) = O(lg lg U) time, using linear space [103]. The main idea of this data\r\n26",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/20dd51fc-deb2-4faf-8c7b-66db77c37352.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fc03525c7b3611fbe7c9d8c9f79b79dcde1488592cbff709e7b230884f9fd7b3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 483
      },
      {
        "segments": [
          {
            "segment_id": "351e61fd-1f55-4b8c-87ac-fb12e7de64de",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 27,
            "page_width": 612,
            "page_height": 792,
            "content": "structure is to binary search for the longest common prefix between the query and a value\r\nin the database.\r\nIt is interesting to note that the solution of van Emde Boas remains very relevant in\r\nmodern times. IP look-up has received considerable attention in the networking community,\r\nsince a very fast solution is needed for the router to keep up with the connection speed.\r\nResearch [44, 36, 102, 1] on software implementations of IP look-up has rediscovered the van\r\nEmde Boas solution, and engineered it into very efficient practical solutions.\r\nIn external memory with pages of B words, predecessor search can be solved by B-trees\r\nin time O(logB n). In STOC’90, Fredman and Willard [52] introduced fusion trees, which\r\nuse an ingenious packing of multiple values into a single word to simulate a page of size\r\nB = w\r\nε\r\n. Fusion trees solve predecessor search in linear space and O(logw n) query time.\r\nSince w = Ω(lg n), the search time is always O(lg n/ lg lg n), i.e. fusion trees are always\r\nasymptotically better than binary search. In fact, taking the best of fusion trees and van\r\nEmde Boas yields a search time of O(min{\r\nlg n\r\nlg w\r\n, lg w}) ≤ O(\r\n√\r\nlg n). Variations on fusion trees\r\ninclude [53, 104, 11, 10], though the O(logw n) query time is not improved.\r\nIn 1999, Beame and Fich [21] found a theoretical improvement to van Emde Boas’ data\r\nstructure, bringing the search time down to O(\r\nlg w\r\nlg lg w\r\n). Combined with fusion trees, this gave\r\nthem a bound of O(min {\r\nlg n\r\nlg w\r\n,\r\nlg w\r\nlg lg w\r\n}) ≤ O(\r\nq lg n\r\nlg lg n\r\n). Unfortunately, the new data structure\r\nof Beame and Fich uses O(n\r\n2\r\n) space, and their main open problems asked whether the space\r\ncould be improved to (near) linear.\r\nThe exponential trees of Andersson and Thorup [12] are an intriguing construction that\r\nuses a predecessor structure for n\r\nγ “splitters,” and recurses in each bucket of O(n1−γ\r\n) el\u0002ements found between pairs of splitters. Given any predecessor structure with polynomial\r\nspace and query time tq ≥ lgε n, this idea can improve it in a black-box fashion to use O(n)\r\nspace and O(tq) query time. Unfortunately, applied to the construction of Beame and Fich,\r\nexponential trees cannot reduce the space to linear without increasing the query to O(lg w).\r\nNonetheless, exponential trees seem to have generated optimism that the van Emde Boas\r\nbound can be improved.\r\nOur lower bounds, which are joint work with Mikkel Thorup [89, 90] and appear Chap\u0002ter 9, refute this possibility and answer the question of Beame and Fich in the negative. We\r\nshow that for near-linear space, such as space n · lgO(1) n, the best running time is essentially\r\nthe minimum of the two classic solutions: fusion trees and van Emde Boas.\r\n2.1.3 Previous Lower Bounds\r\nAjtai [3] was the first to prove a superconstant lower bound. His results, with a correction\r\nby Miltersen [71], show that for polynomial space, there exists n as a function of w making\r\nthe query time Ω(√\r\nlg w), and likewise there exists w a function of n making the query\r\ncomplexity Ω(√3lg n).\r\nMiltersen et al. [73] revisited Ajtai’s proof, extending it to randomized algorithms. More\r\nimportantly, they captured the essence of the proof in an independent round elimination\r\nlemma, which is an important tool for proving lower bounds in asymmetric communication.\r\n27",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/351e61fd-1f55-4b8c-87ac-fb12e7de64de.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5fe3287d29bf4e200d723a9f641549a7dca3339b0db891aa42866073848a8e50",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 584
      },
      {
        "segments": [
          {
            "segment_id": "351e61fd-1f55-4b8c-87ac-fb12e7de64de",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 27,
            "page_width": 612,
            "page_height": 792,
            "content": "structure is to binary search for the longest common prefix between the query and a value\r\nin the database.\r\nIt is interesting to note that the solution of van Emde Boas remains very relevant in\r\nmodern times. IP look-up has received considerable attention in the networking community,\r\nsince a very fast solution is needed for the router to keep up with the connection speed.\r\nResearch [44, 36, 102, 1] on software implementations of IP look-up has rediscovered the van\r\nEmde Boas solution, and engineered it into very efficient practical solutions.\r\nIn external memory with pages of B words, predecessor search can be solved by B-trees\r\nin time O(logB n). In STOC’90, Fredman and Willard [52] introduced fusion trees, which\r\nuse an ingenious packing of multiple values into a single word to simulate a page of size\r\nB = w\r\nε\r\n. Fusion trees solve predecessor search in linear space and O(logw n) query time.\r\nSince w = Ω(lg n), the search time is always O(lg n/ lg lg n), i.e. fusion trees are always\r\nasymptotically better than binary search. In fact, taking the best of fusion trees and van\r\nEmde Boas yields a search time of O(min{\r\nlg n\r\nlg w\r\n, lg w}) ≤ O(\r\n√\r\nlg n). Variations on fusion trees\r\ninclude [53, 104, 11, 10], though the O(logw n) query time is not improved.\r\nIn 1999, Beame and Fich [21] found a theoretical improvement to van Emde Boas’ data\r\nstructure, bringing the search time down to O(\r\nlg w\r\nlg lg w\r\n). Combined with fusion trees, this gave\r\nthem a bound of O(min {\r\nlg n\r\nlg w\r\n,\r\nlg w\r\nlg lg w\r\n}) ≤ O(\r\nq lg n\r\nlg lg n\r\n). Unfortunately, the new data structure\r\nof Beame and Fich uses O(n\r\n2\r\n) space, and their main open problems asked whether the space\r\ncould be improved to (near) linear.\r\nThe exponential trees of Andersson and Thorup [12] are an intriguing construction that\r\nuses a predecessor structure for n\r\nγ “splitters,” and recurses in each bucket of O(n1−γ\r\n) el\u0002ements found between pairs of splitters. Given any predecessor structure with polynomial\r\nspace and query time tq ≥ lgε n, this idea can improve it in a black-box fashion to use O(n)\r\nspace and O(tq) query time. Unfortunately, applied to the construction of Beame and Fich,\r\nexponential trees cannot reduce the space to linear without increasing the query to O(lg w).\r\nNonetheless, exponential trees seem to have generated optimism that the van Emde Boas\r\nbound can be improved.\r\nOur lower bounds, which are joint work with Mikkel Thorup [89, 90] and appear Chap\u0002ter 9, refute this possibility and answer the question of Beame and Fich in the negative. We\r\nshow that for near-linear space, such as space n · lgO(1) n, the best running time is essentially\r\nthe minimum of the two classic solutions: fusion trees and van Emde Boas.\r\n2.1.3 Previous Lower Bounds\r\nAjtai [3] was the first to prove a superconstant lower bound. His results, with a correction\r\nby Miltersen [71], show that for polynomial space, there exists n as a function of w making\r\nthe query time Ω(√\r\nlg w), and likewise there exists w a function of n making the query\r\ncomplexity Ω(√3lg n).\r\nMiltersen et al. [73] revisited Ajtai’s proof, extending it to randomized algorithms. More\r\nimportantly, they captured the essence of the proof in an independent round elimination\r\nlemma, which is an important tool for proving lower bounds in asymmetric communication.\r\n27",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/351e61fd-1f55-4b8c-87ac-fb12e7de64de.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5fe3287d29bf4e200d723a9f641549a7dca3339b0db891aa42866073848a8e50",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 584
      },
      {
        "segments": [
          {
            "segment_id": "e09498d0-df48-49ef-873b-8863b3b58c19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 28,
            "page_width": 612,
            "page_height": 792,
            "content": "Beame and Fich [21] improved Ajtai’s lower bounds to Ω( lg w\r\nlg lg w\r\n) and Ω(q lg n\r\nlg lg n\r\n) respec\u0002tively. Sen and Venkatesh [95] later gave an improved round elimination lemma, which\r\nextended the lower bounds of Beame and Fich to randomized algorithms.\r\nChakrabarti and Regev [27] introduced an alternative to round elimination, the message\r\ncompression lemma. As we showed in [89], this lemma can be used to derive an optimal\r\nspace/time trade-off when the space is S ≥ n\r\n1+ε\r\n. In this case, the optimal query time turns\r\nout to be Θmin \blogw n, lg w\r\nlg lg S\r\n\t\u0001.\r\nOur result seems to have gone against the standard thinking in the community. Sen\r\nand Venkatesh [95] asked whether message compression is really needed for the result of\r\nChakrabarti and Regev [27], or it could be replaced by standard round elimination. By\r\ncontrast, our result shows that message compression is essential even for classic predecessor\r\nlower bounds.\r\nIt is interesting to note that the lower bounds for predecessor search hold, by reductions,\r\nfor all applications mentioned in the previous section. To facilitate these reductions, the\r\nlower bounds are in fact shown for the colored predecessor problem: the values in S are\r\ncolored red or blue, and the query only needs to return the color of the predecessor.\r\n2.1.4 Our Optimal Bounds\r\nOur results from [89, 90] give an optimal trade-off between the query time and the space S.\r\nLetting a = lg S·w\r\nn\r\n, the query time is, up to constant factors:\r\nmin\r\n\r\n\r\n\r\nlogw n\r\nlg w−lg n\r\na\r\nlg w\r\na\r\nlg(\r\na\r\nlg n\r\n· lg w\r\na )\r\nlg w\r\na\r\nlg(lg w\r\na\r\n/ lg lg n\r\na )\r\n(2.1)\r\nThe upper bounds are achieved by a deterministic query algorithm. For any space S, the\r\ndata structure can be constructed in time O(S) with high probability, starting from a sorted\r\nlist of integers. In addition, our data structure supports efficient dynamic updates: if tq is the\r\nquery time, the (randomized) update time is O(tq +\r\nS\r\nn\r\n) with high probability. Thus, besides\r\nlocating the element through one predecessor query, updates change a minimal fraction of\r\nthe data structure.\r\nIn the external memory model with pages of B words, an additional term of logB n is\r\nadded to (2.1). Thus, our result shows that it is always optimal to either use the standard\r\nB-trees, or ignore external memory completely, and use the best word RAM strategy.\r\nFor space S = n · poly(w lg n) and w ≥ (1 + ε) lg n, the trade-off is simplified to:\r\nmin \blogw n, lg w\r\n\u000e\r\nlg w\r\nlg lg n\r\n\t\r\n(2.2)\r\n28",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e09498d0-df48-49ef-873b-8863b3b58c19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7b4c4add8ec5915b091cadf513c9e66d5be171ad18134c845ce604b029a2f456",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 456
      },
      {
        "segments": [
          {
            "segment_id": "6c7fe4e0-7204-4b55-9dbc-a9f8035077b0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 29,
            "page_width": 612,
            "page_height": 792,
            "content": "The first branch corresponds to fusion trees. The second branch demonstrates that van Emde\r\nBoas cannot be improved in general (for all word sizes), though a very small improvement\r\ncan be made for word size w > (lg n)\r\nω(1). This improvement is described in our paper with\r\nMikkel Thorup [89].\r\nOur optimal lower bound required a significant conceptual development in the field.\r\nPreviously, all lower bounds were shown via communication complexity, which fundamentally\r\ncannot prove a separation between data structures of polynomial size and data structures\r\nof linear size (see Chapter 1). This separation is needed to understand predecessor search,\r\nsince Beame and Fich beat the van Emde Boas bound using quadratic space.\r\nThe key idea is to analyze many queries simultaneously, and prove a direct-sum version\r\nof the round elimination lemma. As opposed to previous proofs, our results cannot afford to\r\nincrease the distributional error probability. Thus, a second conceptual idea is to consider a\r\nstronger model for the induction on cell probes: in our model, the algorithm is allowed to\r\nreject a large fraction of the queries before starting to make probes.\r\nThis thesis. Since this thesis focuses of lower bounds, we omit a discussion of the upper\r\nbounds. Furthermore, we want to avoid the calculations involved in deriving the entire trade\u0002off of (2.1), and keep the thesis focused on the techniques. Thus, we will only prove the lower\r\nbound for the case w = Θ(lg n), which is enough to demonstrate the optimality of van Emde\r\nBoas, and a separation between linear and polynomial space. The reader interested in the\r\ndetails of the entire trade-off calculation is referred to our publication [89].\r\nIn Chapter 9, we first review fusion trees and the van Emde Boas data structure from an\r\ninformation-theoretic perspective, building intuition for our lower bound. Then, we prove\r\nour direct-sum round elimination for multiple queries, which implies the full trade-off (2.1)\r\n(though, we only include the calculation for w = 3 lg n).\r\n2.2 Dynamic Problems\r\n2.2.1 Maintaining Partial Sums\r\nThis problem, often described as “maintaining a dynamic histogram,” asks to maintain an\r\narray A[1 . . n] of integers, subject to:\r\nupdate(k, ∆): modify A[k] ← ∆, where ∆ is an arbitrary w-bit integer.\r\nsum(k): returns the “partial sum” Pk\r\ni=1 A[i].\r\nThis problem is a prototypical application of augmented binary search trees (BSTs), which\r\ngive an O(lg n) upper bound. The idea is to consider a fixed balanced binary tree with n\r\nleaves, which represent the n values of the array. Each internal node is augmented with a\r\nvalue equal to the sum of the values in its children (equivalently, the sum of all leaves in the\r\nnode’s subtree).\r\nA large body of work in lower bounds has been dedicated to understanding the partial\r\nsums problem, and in fact, it is by far the best studied dynamic problem. This should\r\n29",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/6c7fe4e0-7204-4b55-9dbc-a9f8035077b0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f95dd4fce125309d94a32981e25a05acab21f7ea2fd41a05db2405a1fef353ff",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 479
      },
      {
        "segments": [
          {
            "segment_id": "0b6914c4-dac6-4260-bff9-c0b983e0d7f3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 30,
            "page_width": 612,
            "page_height": 792,
            "content": "not come as a surprise, since augmented BSTs are a fundamental technique in dynamic\r\ndata structures, and the partial sums problem likely offers the cleanest setting in which this\r\ntechnique can be studied.\r\n2.2.2 Previous Lower Bounds\r\nThe partial-sums problem has been a favorite target for new lower bound ideas since the\r\ndawn of data structures. Early efforts concentrated on algebraic models of computation. In\r\nthe semigroup or group models, the elements of the array come from a black-box (semi)group.\r\nThe algorithm can only manipulate the ∆ inputs through additions and, in the group model,\r\nsubtractions; all other computations in terms of the indices touched by the operations are\r\nfree.\r\nIn the semigroup model, Fredman [47] gave a tight Ω(lg n) bound. Since additive inverses\r\ndo not exist in the semigroup model, an update A[i] ← ∆ invalidates all memory cells\r\nstoring sums containing the old value of A[i]. If updates have the form A[i] ← A[i] + ∆,\r\nYao [107] proved a lower bound of Ω(lg n/ lg lg n). Finally, in FOCS’93, Hampapuram and\r\nFredman [54] proved an Ω(lg n) lower bound for this version of the problem.\r\nIn the group model, a tight bound (including the lead constant) was given by Fredman [48]\r\nfor the restricted class of “oblivious” algorithms, whose behavior can be described by matrix\r\nmultiplication. In STOC’89, Fredman and Saks [51] gave an Ω(lg n/ lg lg n) bound for the\r\ngeneral case, which remained the best known before our work.\r\nIn fact, the results of Fredman and Saks [51] also contained the first dynamic lower\r\nbounds in the cell-probe model. Their lower bound trade-off between the update time tu\r\nand query time tq states that tq = Ω(lg n\r\n\u000e\r\nlg(w+tu+lg n)), which implies that max{tu, tq} =\r\nΩ(lg n\r\n\u000e\r\nlg(w + lg n)).\r\nIn FOCS’91, Ben-Amram and Galil [22] reproved the lower bounds of Fredman and Saks\r\nin a more formalized framework, centered around the concepts of problem and output vari\u0002ability. Using these ideas, they showed [23] Ω(lg n/ lg lg n) lower bounds in more complicated\r\nalgebraic models (allowing multiplication, etc).\r\nFredman and Henzinger [49] reproved the lower bounds of [51] for a more restricted\r\nversion of partial sums, which allowed them to construct reductions to dynamic connectivity,\r\ndynamic planarity testing, and other graph problems. Husfeldt, Rauhe, Skyum [59] also\r\nexplored variations of the lower bound allowing reduction to dynamic graph problems.\r\nHusfeldt and Rauhe [58] gave the first technical improvement after [51], by proving that\r\nthe lower bounds hold even for nondeterministic query algorithms. A stronger improvement\r\nwas given in FOCS’98 by Alstrup, Husfeldt, and Rauhe [8], who prove that tq lg tu = Ω(lg n).\r\nWhile this bound cannot improve max{tu, tq} = Ω(lg n/ lg lg n), it at least implies tq =\r\nΩ(lg n) when the update time is constant.\r\nThe partial sums problem has also been considered in the bit-probe model of computation,\r\nwhere each A[i] is a bit. Fredman [48] showed a bound of Ω(lg n/ lg lg n) for this case. The\r\nhighest bound for any dynamic problem in the bit-probe model was Ω(lg n), due to Miltersen\r\net al. [74].\r\n30",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/0b6914c4-dac6-4260-bff9-c0b983e0d7f3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3e91d3eb60fd943d6bf46b935553971f2df66853a3ab127a6c9249c4de5bc246",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 529
      },
      {
        "segments": [
          {
            "segment_id": "0b6914c4-dac6-4260-bff9-c0b983e0d7f3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 30,
            "page_width": 612,
            "page_height": 792,
            "content": "not come as a surprise, since augmented BSTs are a fundamental technique in dynamic\r\ndata structures, and the partial sums problem likely offers the cleanest setting in which this\r\ntechnique can be studied.\r\n2.2.2 Previous Lower Bounds\r\nThe partial-sums problem has been a favorite target for new lower bound ideas since the\r\ndawn of data structures. Early efforts concentrated on algebraic models of computation. In\r\nthe semigroup or group models, the elements of the array come from a black-box (semi)group.\r\nThe algorithm can only manipulate the ∆ inputs through additions and, in the group model,\r\nsubtractions; all other computations in terms of the indices touched by the operations are\r\nfree.\r\nIn the semigroup model, Fredman [47] gave a tight Ω(lg n) bound. Since additive inverses\r\ndo not exist in the semigroup model, an update A[i] ← ∆ invalidates all memory cells\r\nstoring sums containing the old value of A[i]. If updates have the form A[i] ← A[i] + ∆,\r\nYao [107] proved a lower bound of Ω(lg n/ lg lg n). Finally, in FOCS’93, Hampapuram and\r\nFredman [54] proved an Ω(lg n) lower bound for this version of the problem.\r\nIn the group model, a tight bound (including the lead constant) was given by Fredman [48]\r\nfor the restricted class of “oblivious” algorithms, whose behavior can be described by matrix\r\nmultiplication. In STOC’89, Fredman and Saks [51] gave an Ω(lg n/ lg lg n) bound for the\r\ngeneral case, which remained the best known before our work.\r\nIn fact, the results of Fredman and Saks [51] also contained the first dynamic lower\r\nbounds in the cell-probe model. Their lower bound trade-off between the update time tu\r\nand query time tq states that tq = Ω(lg n\r\n\u000e\r\nlg(w+tu+lg n)), which implies that max{tu, tq} =\r\nΩ(lg n\r\n\u000e\r\nlg(w + lg n)).\r\nIn FOCS’91, Ben-Amram and Galil [22] reproved the lower bounds of Fredman and Saks\r\nin a more formalized framework, centered around the concepts of problem and output vari\u0002ability. Using these ideas, they showed [23] Ω(lg n/ lg lg n) lower bounds in more complicated\r\nalgebraic models (allowing multiplication, etc).\r\nFredman and Henzinger [49] reproved the lower bounds of [51] for a more restricted\r\nversion of partial sums, which allowed them to construct reductions to dynamic connectivity,\r\ndynamic planarity testing, and other graph problems. Husfeldt, Rauhe, Skyum [59] also\r\nexplored variations of the lower bound allowing reduction to dynamic graph problems.\r\nHusfeldt and Rauhe [58] gave the first technical improvement after [51], by proving that\r\nthe lower bounds hold even for nondeterministic query algorithms. A stronger improvement\r\nwas given in FOCS’98 by Alstrup, Husfeldt, and Rauhe [8], who prove that tq lg tu = Ω(lg n).\r\nWhile this bound cannot improve max{tu, tq} = Ω(lg n/ lg lg n), it at least implies tq =\r\nΩ(lg n) when the update time is constant.\r\nThe partial sums problem has also been considered in the bit-probe model of computation,\r\nwhere each A[i] is a bit. Fredman [48] showed a bound of Ω(lg n/ lg lg n) for this case. The\r\nhighest bound for any dynamic problem in the bit-probe model was Ω(lg n), due to Miltersen\r\net al. [74].\r\n30",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/0b6914c4-dac6-4260-bff9-c0b983e0d7f3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3e91d3eb60fd943d6bf46b935553971f2df66853a3ab127a6c9249c4de5bc246",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 529
      },
      {
        "segments": [
          {
            "segment_id": "7089050e-0048-4b6d-b75c-0db3d46b277e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 31,
            "page_width": 612,
            "page_height": 792,
            "content": "In 1999, Miltersen [72] surveyed the field of cell-probe complexity, and proposed sev\u0002eral challenges for future research. Two of his three “big challenges” asked to prove an\r\nω(lg n/ lg lg n) lower bound for any dynamic problem in the cell-probe model, respectively\r\nan ω(lg n) lower bound for any problem in the bit-probe model. Of the remaining four\r\nchallenges, one asked to prove Ω(lg n) for partial sums, at least in the group model of com\u0002putation.\r\n2.2.3 Our Results\r\nIn our work with Erik Demaine [86, 87], we broke these barriers in cell-probe and bit-probe\r\ncomplexity, and showed optimal bounds for the partial sums problem.\r\nIn Chapter 3, we describe a very simple proof of an Ω(lg n) lower bound for partial sums,\r\nfinally showing that the natural upper bound of binary search trees is optimal. The cleanness\r\nof this approach (the proof is roughly 3 pages of text, involving no calculation) stands in\r\nsharp contrast to the 15 years that the problem remained open.\r\nIn Chapter 4, we prove a versatile trade-off for partial sums, via a subtle variation to\r\nthe classic chronogram technique of Fredman and Saks [51]. This trade-off is optimal in\r\nthe cell-probe model, in particular reproving the Ω(lg n) lower bound. It is surprising to\r\nfind that the answer to Miltersen’s big challenges consists of a small variation of what was\r\nalready know.\r\nOur trade-off also allows us to show an Ω(lg n/ lg lg lg n) bound for partial sums in the\r\nbit-probe model, improving on Fredman’s 1982 result [48]. Finally, it allows us to show an\r\nΩ\r\n lg n\r\nlg lg n\r\n\u00012\u0001\r\nlower bound in the bit-probe model, for dynamic connectivity. This gives a new\r\nrecord for lower bounds in the bit-probe model, addressing Miltersen’s challenge to show an\r\nω(lg n) bound.\r\nIntuitively, performance in the bit-probe model should typically be slower by a lg n factor\r\ncompared to the cell-probe model. However, our Ω(lg e 2 n) bound in the bit-probe world is\r\nfar from an echo of an Ω(lg e n) bound in the cell-probe world. Indeed, Ω( lg n\r\nlg lg n\r\n) bounds in the\r\ncell-probe model have been known since 1989, but the bit-probe record has remained just\r\nthe slightly higher Ω(lg n). In fact, our bound is the first to show a quasi-optimal Ω(lg e n)\r\nseparation between bit-probe complexity and the cell-probe complexity, for superconstant\r\ncell-probe complexity.\r\n2.2.4 Related Problems\r\nList manipulation. In practice, one often seeks a cross between linked lists and array:\r\nmaintaing a collection of items under typical linked-list operations, with the additional in\u0002dexing operation (that is typical of array). We can consider the following natural operations,\r\nignoring all details of what constitutes a “record” (all manipulation is done through pointers\r\nto black-box records):\r\ninsert(p, r): insert record r immediately after record p in its list.\r\ndelete(r): delete record r from its list.\r\n31",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7089050e-0048-4b6d-b75c-0db3d46b277e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fb90a2250799efea006fb17eccc3109c6f0baabf31665ca0d9ca61dd372d88ad",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 483
      },
      {
        "segments": [
          {
            "segment_id": "7b402279-9650-4b97-8ee0-93372582b1a2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 32,
            "page_width": 612,
            "page_height": 792,
            "content": "link(h1, h2): concatenate two lists, identified by their head records h1 and h2.\r\ncut(h, p): split the list with head at h into two lists, the second beginning from record p.\r\nindex(k, h): return a pointer to the k-th value in the list with header at h.\r\nrank(p): return the rank (position) of record p in its list.\r\nfind(p): return the head of the list that contains record p.\r\nAll these operations, and many variants thereof, can be solved in O(lg n) time per oper\u0002ation using augmented binary search trees. For insert/delete updates, the binary search\r\ntree algorithm must be able to maintain balance in a dynamic tree. Such algorithms abound\r\n(consider red-black trees, AVL trees, splay trees, etc). For link/cut, the tree algorithm\r\nmust support split and merge operations. Many dynamic trees support these operations\r\nin O(lg n) time, including 2-3-4 trees, AVL trees, splay trees, etc.\r\nIt turns out that there is a complexity gap between list manipulation with link/cut\r\noperations, and list manipulation with insert/delete. In the former case, updates can\r\nmake large topological changes to the lists, while in the latter, the updates only have a very\r\nlocal record-wise effect.\r\nIf the set of operations includes link, cut, and any of the three queries (including\r\nthe minimalist find), we can show an Ω(lg n) lower bound for list manipulation. Thus,\r\naugmented binary search trees give an optimal solution. The lower bound is described in\r\nChapter 3, and needs a few interesting ideas beyond the partial-sums lower bound.\r\nIf the set of updates is restricted to insert and delete (with all queries allowed),\r\nthe problem admits an O(lg n/ lg lg n) solution, as proved by Dietz [38]. This restricted\r\ncase of list manipulation can be shown equivalent to the partial sums problem, in which\r\nevery array element is A[i] ∈ {0, 1}. The original lower bound of Fredman and Saks [51],\r\nas well as our trade-off from Chapter 4 show a tight Ω(lg n/ lg lg n) bound for restricted\r\npartial sums problem. By reduction, this implies a tight bound for list manipulation with\r\ninsert/delete.\r\nDynamic connectivity. Dynamic graph problems ask to maintain a graph under various\r\noperations (typically edge insertions and deletions), and queries for typical graph properties\r\n(connectivity of two vertices, MST, shortest path, etc). This has been a very active area of\r\nresearch in the past two decades, and many surprising results have appeared.\r\nDynamic connectivity is the most basic dynamic graph problem. It asks to maintain an\r\nundirected graph with a fixed set of n vertices subject to the following operations:\r\ninsert(u, v): insert an edge (u, v) into the graph.\r\ndelete(u, v): delete the edge (u, v) from the graph.\r\nconnected(u, v): test whether u and v lie in the same connected component.\r\nIf the graph is always a forest, Sleator and Tarjan’s classic “dynamic trees” [97] achieve an\r\nO(lg n) upper bound. A simpler solution is given by Euler tour trees [55], which show that\r\nthe problem is equivalent to list manipulation. If we maintain the Euler tour of each tree as\r\na list, then an insert or delete can be translated to O(1) link/cut operations, and the\r\nquery can be solved by comparing the results of two find queries.\r\n32",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7b402279-9650-4b97-8ee0-93372582b1a2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=826e1e883d17b7b9be9ea0082fc11901273069c542850cfcd038ca7f043c1c92",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 541
      },
      {
        "segments": [
          {
            "segment_id": "7b402279-9650-4b97-8ee0-93372582b1a2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 32,
            "page_width": 612,
            "page_height": 792,
            "content": "link(h1, h2): concatenate two lists, identified by their head records h1 and h2.\r\ncut(h, p): split the list with head at h into two lists, the second beginning from record p.\r\nindex(k, h): return a pointer to the k-th value in the list with header at h.\r\nrank(p): return the rank (position) of record p in its list.\r\nfind(p): return the head of the list that contains record p.\r\nAll these operations, and many variants thereof, can be solved in O(lg n) time per oper\u0002ation using augmented binary search trees. For insert/delete updates, the binary search\r\ntree algorithm must be able to maintain balance in a dynamic tree. Such algorithms abound\r\n(consider red-black trees, AVL trees, splay trees, etc). For link/cut, the tree algorithm\r\nmust support split and merge operations. Many dynamic trees support these operations\r\nin O(lg n) time, including 2-3-4 trees, AVL trees, splay trees, etc.\r\nIt turns out that there is a complexity gap between list manipulation with link/cut\r\noperations, and list manipulation with insert/delete. In the former case, updates can\r\nmake large topological changes to the lists, while in the latter, the updates only have a very\r\nlocal record-wise effect.\r\nIf the set of operations includes link, cut, and any of the three queries (including\r\nthe minimalist find), we can show an Ω(lg n) lower bound for list manipulation. Thus,\r\naugmented binary search trees give an optimal solution. The lower bound is described in\r\nChapter 3, and needs a few interesting ideas beyond the partial-sums lower bound.\r\nIf the set of updates is restricted to insert and delete (with all queries allowed),\r\nthe problem admits an O(lg n/ lg lg n) solution, as proved by Dietz [38]. This restricted\r\ncase of list manipulation can be shown equivalent to the partial sums problem, in which\r\nevery array element is A[i] ∈ {0, 1}. The original lower bound of Fredman and Saks [51],\r\nas well as our trade-off from Chapter 4 show a tight Ω(lg n/ lg lg n) bound for restricted\r\npartial sums problem. By reduction, this implies a tight bound for list manipulation with\r\ninsert/delete.\r\nDynamic connectivity. Dynamic graph problems ask to maintain a graph under various\r\noperations (typically edge insertions and deletions), and queries for typical graph properties\r\n(connectivity of two vertices, MST, shortest path, etc). This has been a very active area of\r\nresearch in the past two decades, and many surprising results have appeared.\r\nDynamic connectivity is the most basic dynamic graph problem. It asks to maintain an\r\nundirected graph with a fixed set of n vertices subject to the following operations:\r\ninsert(u, v): insert an edge (u, v) into the graph.\r\ndelete(u, v): delete the edge (u, v) from the graph.\r\nconnected(u, v): test whether u and v lie in the same connected component.\r\nIf the graph is always a forest, Sleator and Tarjan’s classic “dynamic trees” [97] achieve an\r\nO(lg n) upper bound. A simpler solution is given by Euler tour trees [55], which show that\r\nthe problem is equivalent to list manipulation. If we maintain the Euler tour of each tree as\r\na list, then an insert or delete can be translated to O(1) link/cut operations, and the\r\nquery can be solved by comparing the results of two find queries.\r\n32",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7b402279-9650-4b97-8ee0-93372582b1a2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=826e1e883d17b7b9be9ea0082fc11901273069c542850cfcd038ca7f043c1c92",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 541
      },
      {
        "segments": [
          {
            "segment_id": "b07c1b94-a00c-41ec-98d3-d16dd2e29f31",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 33,
            "page_width": 612,
            "page_height": 792,
            "content": "For general graphs, the first to achieve polylogarithmic time per operation were Henzinger\r\nand King [55], who gave an O(lg3 n) running time, using randomization and amortization.\r\nHenzinger and Thorup [56] improve the bound to O(lg2 n). Holm, de Lichtenberg, and\r\nThorup [57] gave a simple deterministic solution with the same amortized running time,\r\nO(lg2 n). The best known result is by Thorup [99], achieving very close to logarithmic time:\r\nO(lg n · (lg lg n)\r\n3\r\n). For plane graphs, Eppstein et al. [40] gave an O(lg n) upper bound.\r\nOur results from Chapter 3 imply an Ω(lg n) lower bound for dynamic connectivity, even\r\nin forests and plane graphs. It is easy to show by reductions that our bounds hold for many\r\ndynamic graph problems, including dynamic MST, dynamic planarity testing, etc.\r\n2.3 Range Queries\r\nRange-query problems include some of the most natural and fundamental problems in com\u0002putational geometry and databases. The goal is to represent a set of n objects in d dimen\u0002sions, such that certain queries about objects that intersect a given range can be answered\r\nefficiently. In this line of research, the dimension d is a small constant (2, 3, 4, . . . ), and\r\nconstants in the O-notation may depend on d.\r\nUsual choices for the query include counting the number of points in the range, reporting\r\nall points in the range, and existential queries (test whether the range contains any point,\r\nor is empty).\r\nBy far, the most common instantiation of the problem is when the n objects are points,\r\nand the range is an axis-parallel rectangle [a1, b1] × · · · × [ad, bd]. This problem is usually\r\ntermed orthogonal range queries, though the choice is so common that talking about range\r\nqueries without further qualification usually means orthogonal range queries. Such queries\r\nare some of the most natural examples of what computers might be queried for. The intro\u0002ductory lecture of any database course is virtually certain to contain an example like “find\r\nemployees with a salary between 70000 and 95000, who have been hired between 1998 and\r\n2001.”\r\nAn important special case of orthogonal range queries consists of dominance queries,\r\nwhere query rectangles have the form [0, b1] × · · · × [0, bd]. One may also consider ranges of\r\na more geometric flavor, including half-spaces, simplices, balls etc.\r\nA second common instantiation of range queries are the stabbing problems, where that\r\nquery asks for the objects stabbed by a point. Typically, the objects are axis-aligned boxes.\r\nRange queries have been studied intensively, and an overview is well beyond the scope of\r\nthis work. Instead, we refer the reader to a survey by Agarwal [2]. Below, we discuss mainly\r\nthe known lower bounds, as well as our new results.\r\nGeneral flavor. The general flavor of all upper bound results is given by that standard use\r\nof range trees. This idea can raise a d-dimensional solution to a solution in d+ 1 dimensions,\r\npaying a factor of roughly O(lg n) in time and space. It is generally believed that this\r\ncost for each additional the dimension is optimal. Unfortunately, we cannot prove optimal\r\nlower bounds for large d, since current lower bound techniques cannot show superlogarithmic\r\n33",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b07c1b94-a00c-41ec-98d3-d16dd2e29f31.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=30e8b78ae6f68f2185b1c67531ddd6f9cc276ec4b680fcdb3c18c5e69b5a5aee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 538
      },
      {
        "segments": [
          {
            "segment_id": "b07c1b94-a00c-41ec-98d3-d16dd2e29f31",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 33,
            "page_width": 612,
            "page_height": 792,
            "content": "For general graphs, the first to achieve polylogarithmic time per operation were Henzinger\r\nand King [55], who gave an O(lg3 n) running time, using randomization and amortization.\r\nHenzinger and Thorup [56] improve the bound to O(lg2 n). Holm, de Lichtenberg, and\r\nThorup [57] gave a simple deterministic solution with the same amortized running time,\r\nO(lg2 n). The best known result is by Thorup [99], achieving very close to logarithmic time:\r\nO(lg n · (lg lg n)\r\n3\r\n). For plane graphs, Eppstein et al. [40] gave an O(lg n) upper bound.\r\nOur results from Chapter 3 imply an Ω(lg n) lower bound for dynamic connectivity, even\r\nin forests and plane graphs. It is easy to show by reductions that our bounds hold for many\r\ndynamic graph problems, including dynamic MST, dynamic planarity testing, etc.\r\n2.3 Range Queries\r\nRange-query problems include some of the most natural and fundamental problems in com\u0002putational geometry and databases. The goal is to represent a set of n objects in d dimen\u0002sions, such that certain queries about objects that intersect a given range can be answered\r\nefficiently. In this line of research, the dimension d is a small constant (2, 3, 4, . . . ), and\r\nconstants in the O-notation may depend on d.\r\nUsual choices for the query include counting the number of points in the range, reporting\r\nall points in the range, and existential queries (test whether the range contains any point,\r\nor is empty).\r\nBy far, the most common instantiation of the problem is when the n objects are points,\r\nand the range is an axis-parallel rectangle [a1, b1] × · · · × [ad, bd]. This problem is usually\r\ntermed orthogonal range queries, though the choice is so common that talking about range\r\nqueries without further qualification usually means orthogonal range queries. Such queries\r\nare some of the most natural examples of what computers might be queried for. The intro\u0002ductory lecture of any database course is virtually certain to contain an example like “find\r\nemployees with a salary between 70000 and 95000, who have been hired between 1998 and\r\n2001.”\r\nAn important special case of orthogonal range queries consists of dominance queries,\r\nwhere query rectangles have the form [0, b1] × · · · × [0, bd]. One may also consider ranges of\r\na more geometric flavor, including half-spaces, simplices, balls etc.\r\nA second common instantiation of range queries are the stabbing problems, where that\r\nquery asks for the objects stabbed by a point. Typically, the objects are axis-aligned boxes.\r\nRange queries have been studied intensively, and an overview is well beyond the scope of\r\nthis work. Instead, we refer the reader to a survey by Agarwal [2]. Below, we discuss mainly\r\nthe known lower bounds, as well as our new results.\r\nGeneral flavor. The general flavor of all upper bound results is given by that standard use\r\nof range trees. This idea can raise a d-dimensional solution to a solution in d+ 1 dimensions,\r\npaying a factor of roughly O(lg n) in time and space. It is generally believed that this\r\ncost for each additional the dimension is optimal. Unfortunately, we cannot prove optimal\r\nlower bounds for large d, since current lower bound techniques cannot show superlogarithmic\r\n33",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b07c1b94-a00c-41ec-98d3-d16dd2e29f31.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=30e8b78ae6f68f2185b1c67531ddd6f9cc276ec4b680fcdb3c18c5e69b5a5aee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 538
      },
      {
        "segments": [
          {
            "segment_id": "18166484-94f3-4485-942c-4909eea4e195",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 34,
            "page_width": 612,
            "page_height": 792,
            "content": "bounds in the cell-probe model. Then, it remains to ask about optimal bounds for small\r\ndimension.\r\nAnother artifact of our slow progress on lower bounds is that we cannot separate space\r\nS from, say, space S · lg n in the cell-probe model. Thus, while it would be interesting to\r\nshow that the space needs to grow with the dimension, all work has concentrated on showing\r\ngrowth in the time bound. For static lower bounds, one just assumes that the space is an\r\narbitrary O(n · polylog n).\r\nThere is also a question about the universe of the coordinate values. Using a predecessor\r\nstructure for each of the d coordinates, it is possible to reduce all coordinates to rank space,\r\n{1, . . . , n}. Thus, the bounds only depend additively on the original universe.\r\nOn the other hand, colored predecessor search can be reduced to essentially any range\r\nquery, including dominance range reporting in 2 dimensions. Thus, the complexity of range\r\nqueries must depend additively on the predecessor complexity. Since the predecessor bound\r\nis well understood (see §2.1), we will eliminate this noise from the bounds, and assume all\r\ncoordinate values are originally in the range {1, . . . , n}. In almost all cases, the predecessor\r\nbound is a asymptotically smaller, so the additional term is inconsequential anyway.\r\n2.3.1 Orthogonal Range Counting\r\nAs mentioned before, these queries ask to count the points inside a range [a1, b1]×· · ·×[ad, bd].\r\nAs with the partial sums problem, range counting has been traditionally studied in algebraic\r\nmodels. In the group and semigroup models, each point has an associated weight from\r\nan arbitrary commutative (semi)group and the “counting” query asks for the sum of the\r\nweights of the points in the range. The data structure can only manipulate weights through\r\nthe black-box addition (and, in the group model, subtraction), and must work for any choice\r\nof the (semi)group. The running time of a query is the number of algebraic operations\r\nperformed. Any other computation, i.e. planning algebraic operations based on coordinates,\r\nis free.\r\nThe semigroup model has allowed for every strong lower bounds. As early as 1981, Fred\u0002man [47] showed that dynamic range reporting (with insert and delete operations) has a\r\nlower bound of Ω(lgd n) in the semigroup model. The delete operation is particularly con\u0002venient for lower bound in the semigroup model, because any memory cell storing a sum that\r\ncontains the deleted element in now entirely useless (there is no way to subtract the deleted\r\nelement). With only inserts allowed, Yao [107] showed a lower bound of Ω(lg n/ lg lg n)\r\nfor dimension d = 1. This was significantly strengthened by Chazelle [31], who showed\r\na dynamic lower bound of Ω(lg n/ lg lg n)\r\nd\r\n\u0001\r\nin d dimensions, and a static lower bound of\r\nΩ\r\n\r\n(lg n/ lg lg n)\r\nd−1\r\n\u0001\r\n.\r\nIn recent years, different types of nonorthogonal ranges were analyzed in the semigroup\r\nmodel, and very strong lower bounds were shown. See, for instance, the survey of Agarwal [2].\r\nBy contrast, the lower bounds in the group model have remained quite weak. Not only\r\ndo known lower bounds fail to grow appropriately with the dimension, but we cannot even\r\nget satisfactory bounds in, say, 2 dimensions. No static lower bound had been proved before\r\nour work, perhaps with the exception of a result by Chazelle [32]. This result states that for\r\n34",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/18166484-94f3-4485-942c-4909eea4e195.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=36e2aa92d39a32765c456b77d5afe348ee5265091ca1de67d27e43f07fe79c3a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 571
      },
      {
        "segments": [
          {
            "segment_id": "18166484-94f3-4485-942c-4909eea4e195",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 34,
            "page_width": 612,
            "page_height": 792,
            "content": "bounds in the cell-probe model. Then, it remains to ask about optimal bounds for small\r\ndimension.\r\nAnother artifact of our slow progress on lower bounds is that we cannot separate space\r\nS from, say, space S · lg n in the cell-probe model. Thus, while it would be interesting to\r\nshow that the space needs to grow with the dimension, all work has concentrated on showing\r\ngrowth in the time bound. For static lower bounds, one just assumes that the space is an\r\narbitrary O(n · polylog n).\r\nThere is also a question about the universe of the coordinate values. Using a predecessor\r\nstructure for each of the d coordinates, it is possible to reduce all coordinates to rank space,\r\n{1, . . . , n}. Thus, the bounds only depend additively on the original universe.\r\nOn the other hand, colored predecessor search can be reduced to essentially any range\r\nquery, including dominance range reporting in 2 dimensions. Thus, the complexity of range\r\nqueries must depend additively on the predecessor complexity. Since the predecessor bound\r\nis well understood (see §2.1), we will eliminate this noise from the bounds, and assume all\r\ncoordinate values are originally in the range {1, . . . , n}. In almost all cases, the predecessor\r\nbound is a asymptotically smaller, so the additional term is inconsequential anyway.\r\n2.3.1 Orthogonal Range Counting\r\nAs mentioned before, these queries ask to count the points inside a range [a1, b1]×· · ·×[ad, bd].\r\nAs with the partial sums problem, range counting has been traditionally studied in algebraic\r\nmodels. In the group and semigroup models, each point has an associated weight from\r\nan arbitrary commutative (semi)group and the “counting” query asks for the sum of the\r\nweights of the points in the range. The data structure can only manipulate weights through\r\nthe black-box addition (and, in the group model, subtraction), and must work for any choice\r\nof the (semi)group. The running time of a query is the number of algebraic operations\r\nperformed. Any other computation, i.e. planning algebraic operations based on coordinates,\r\nis free.\r\nThe semigroup model has allowed for every strong lower bounds. As early as 1981, Fred\u0002man [47] showed that dynamic range reporting (with insert and delete operations) has a\r\nlower bound of Ω(lgd n) in the semigroup model. The delete operation is particularly con\u0002venient for lower bound in the semigroup model, because any memory cell storing a sum that\r\ncontains the deleted element in now entirely useless (there is no way to subtract the deleted\r\nelement). With only inserts allowed, Yao [107] showed a lower bound of Ω(lg n/ lg lg n)\r\nfor dimension d = 1. This was significantly strengthened by Chazelle [31], who showed\r\na dynamic lower bound of Ω(lg n/ lg lg n)\r\nd\r\n\u0001\r\nin d dimensions, and a static lower bound of\r\nΩ\r\n\r\n(lg n/ lg lg n)\r\nd−1\r\n\u0001\r\n.\r\nIn recent years, different types of nonorthogonal ranges were analyzed in the semigroup\r\nmodel, and very strong lower bounds were shown. See, for instance, the survey of Agarwal [2].\r\nBy contrast, the lower bounds in the group model have remained quite weak. Not only\r\ndo known lower bounds fail to grow appropriately with the dimension, but we cannot even\r\nget satisfactory bounds in, say, 2 dimensions. No static lower bound had been proved before\r\nour work, perhaps with the exception of a result by Chazelle [32]. This result states that for\r\n34",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/18166484-94f3-4485-942c-4909eea4e195.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=36e2aa92d39a32765c456b77d5afe348ee5265091ca1de67d27e43f07fe79c3a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 571
      },
      {
        "segments": [
          {
            "segment_id": "3fedd742-397f-4d5c-a1c2-9e3c144c1b12",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 35,
            "page_width": 612,
            "page_height": 792,
            "content": "n input points and n query ranges in 2D, the offline problem takes Ω(n lg lg n) time. This\r\nimplies that any static data structure that can be constructed in o(n lg lg n) time, requires\r\nquery time Ω(lg lg n) — a result that is exponentially weaker than the upper bound.\r\nThe group/semigroup gap. Why is there such a rift between our lower-bound abilities\r\nin the semigroup and stronger models? In general, semigroup lower bounds hide arguments\r\nof a very geometric flavor. To see why, note than when a value is included in a sum, it can\r\nnever be taken out again (no subtraction is available). In particular, if a the sum stored in\r\none cell includes an input point, this immediately places a geometric restriction on the set\r\nof ranges that could use the cell (the range must include the input point). Thus, semigroup\r\nlower bounds are essentially bounds on a certain kind of “decomposability” of geometric\r\nshapes.\r\nOn the other hand, bounds in the group or cell-probe model require a different, informa\u0002tion theoretic understanding of the problem. In the group model, the sum stored in a cell\r\ndoes not tell us anything, as terms could easily be subtracted out. The lower bound must\r\nnow find certain bottlenecks in the manipulation of information that make the problem hard.\r\nOn the bright side, when such bottlenecks were found, it was generally possible to use them\r\nboth for group-model lower bounds (arguing about dimensionality in vector spaces), and for\r\ncell-probe lower bounds (arguing about entropy).\r\nPhilosophically speaking, the difference in the type of reasoning behind semigroup lower\r\nbounds and group/cell-probe lower bounds is parallel to the difference between “understand\u0002ing geometry” and “understanding computation”. Since we have been vastly more successful\r\nat the former, it should not come as a surprise that progress outside the semigroup model\r\nhas been extremely slow.\r\nAs one might expect, proving good bounds outside the restrictive semigroup model has\r\nbeen recognized as an important challenge for a long time. As early as 1982, Fredman [48]\r\nasked for better bounds in the group model for dynamic range counting. In FOCS’86,\r\nChazelle [31] echoed this, and also asked about the static case.\r\nOur results. We address the challenges of Fredman and Chazelle, and obtain an almost\r\nperfect understanding the range counting in 2 dimensions. See Table 2.1 for a summary of\r\nthe known and new results.\r\nThe upper bounds have been stated in a variety of sources; see e.g. [31]. The following\r\ntheorems give formal statements for our lower bounds. We note that the trade-offs we obtain\r\nare very similar to the ones known in the semigroup model. The space/time trade-offs are\r\nknown to be tight for space Ω(n lg1+ε n). The query/time trade-off is tight for update time\r\ntu = Ω(lg2+ε n). Lower bounds are shown for a fixed set of input points in [n]\r\n2\r\n, and dominance\r\nqueries.\r\nTheorem 2.2. In the group model, a static data structure of size n·σ must take Ω( lg n\r\nlg σ+lg lg n\r\n)\r\nexpected time for dominance counting queries.\r\nTheorem 2.3. In the cell-probe model with w-bit cells, a deterministic static data structure\r\nof size n · σ must take Ω( lg n\r\nlg σ+lg w\r\n) time for dominance counting queries.\r\n35",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/3fedd742-397f-4d5c-a1c2-9e3c144c1b12.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a731d2daeb5f023e01bb3a9cce1c4140090d194429e719e5165147b5beb6de63",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "3fedd742-397f-4d5c-a1c2-9e3c144c1b12",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 35,
            "page_width": 612,
            "page_height": 792,
            "content": "n input points and n query ranges in 2D, the offline problem takes Ω(n lg lg n) time. This\r\nimplies that any static data structure that can be constructed in o(n lg lg n) time, requires\r\nquery time Ω(lg lg n) — a result that is exponentially weaker than the upper bound.\r\nThe group/semigroup gap. Why is there such a rift between our lower-bound abilities\r\nin the semigroup and stronger models? In general, semigroup lower bounds hide arguments\r\nof a very geometric flavor. To see why, note than when a value is included in a sum, it can\r\nnever be taken out again (no subtraction is available). In particular, if a the sum stored in\r\none cell includes an input point, this immediately places a geometric restriction on the set\r\nof ranges that could use the cell (the range must include the input point). Thus, semigroup\r\nlower bounds are essentially bounds on a certain kind of “decomposability” of geometric\r\nshapes.\r\nOn the other hand, bounds in the group or cell-probe model require a different, informa\u0002tion theoretic understanding of the problem. In the group model, the sum stored in a cell\r\ndoes not tell us anything, as terms could easily be subtracted out. The lower bound must\r\nnow find certain bottlenecks in the manipulation of information that make the problem hard.\r\nOn the bright side, when such bottlenecks were found, it was generally possible to use them\r\nboth for group-model lower bounds (arguing about dimensionality in vector spaces), and for\r\ncell-probe lower bounds (arguing about entropy).\r\nPhilosophically speaking, the difference in the type of reasoning behind semigroup lower\r\nbounds and group/cell-probe lower bounds is parallel to the difference between “understand\u0002ing geometry” and “understanding computation”. Since we have been vastly more successful\r\nat the former, it should not come as a surprise that progress outside the semigroup model\r\nhas been extremely slow.\r\nAs one might expect, proving good bounds outside the restrictive semigroup model has\r\nbeen recognized as an important challenge for a long time. As early as 1982, Fredman [48]\r\nasked for better bounds in the group model for dynamic range counting. In FOCS’86,\r\nChazelle [31] echoed this, and also asked about the static case.\r\nOur results. We address the challenges of Fredman and Chazelle, and obtain an almost\r\nperfect understanding the range counting in 2 dimensions. See Table 2.1 for a summary of\r\nthe known and new results.\r\nThe upper bounds have been stated in a variety of sources; see e.g. [31]. The following\r\ntheorems give formal statements for our lower bounds. We note that the trade-offs we obtain\r\nare very similar to the ones known in the semigroup model. The space/time trade-offs are\r\nknown to be tight for space Ω(n lg1+ε n). The query/time trade-off is tight for update time\r\ntu = Ω(lg2+ε n). Lower bounds are shown for a fixed set of input points in [n]\r\n2\r\n, and dominance\r\nqueries.\r\nTheorem 2.2. In the group model, a static data structure of size n·σ must take Ω( lg n\r\nlg σ+lg lg n\r\n)\r\nexpected time for dominance counting queries.\r\nTheorem 2.3. In the cell-probe model with w-bit cells, a deterministic static data structure\r\nof size n · σ must take Ω( lg n\r\nlg σ+lg w\r\n) time for dominance counting queries.\r\n35",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/3fedd742-397f-4d5c-a1c2-9e3c144c1b12.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a731d2daeb5f023e01bb3a9cce1c4140090d194429e719e5165147b5beb6de63",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "68ff9fdc-64d3-4c1d-8604-9f7181a63e51",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 36,
            "page_width": 612,
            "page_height": 792,
            "content": "Problem Model Lower Bounds Dimension\r\nstatic semigroup Ω\r\n\r\n(lg n/ lg lg n)\r\nd−1\r\n\u0001\r\n[31]\r\ngroup Ω(lg lg n) ? [32] d = 2\r\nO\r\n\r\n(\r\nlg n\r\nlg lg n\r\n)\r\nd−1\r\n\u0001\r\nΩ(lg n/ lg lg n) new d = 2\r\ncell-probe Ω(lg lg n) [90] d = 2\r\nΩ(lg n/ lg lg n) new d = 2\r\ndynamic semigroup, with delete Ω(lgd n) [47]\r\nsemigroup, no delete Ω(lg n/ lg lg n) [107] d = 1\r\nO(lgd n) Ω\r\n\r\n(lg n/ lg lg n)\r\nd\r\n\u0001\r\n[31]\r\nΩ(lg n) [54] d = 1\r\ngroup Ω(lg n/ lg lg n) [51] d = 1\r\nΩ(lg n) [86] d = 1\r\nΩ\r\n\r\n(lg n/ lg lg n)\r\n2\r\n\u0001\r\nnew d = 2\r\nTable 2.1: Old and new results for orthogonal range counting. The upper bound for static\r\ndata structures assumes O(n polylog n) space. (?) The bound of [32], starred, says that\r\nfor n input points and n queries, the offline problem takes Ω(n lg lg n) time.\r\nTheorem 2.4. In the group model, a dynamic data structure which supports updates in\r\nexpected time tu requires tq = Ω(\r\nlg n\r\nlg tu+lg lg n\r\n)\r\n2\r\n\u0001\r\nexpected time for dominance counting queries.\r\nUnfortunately, our techniques cannot obtain better bounds in higher dimensions. De\u0002pending on mood, the reader may view this as a breakthrough (e.g. providing the first\r\nconvincingly superconstant bounds for the static case), or as a lucky discovery that moves\r\nthe borderline of the big open problem from d = 2 to d = 3. We believe there is truth in\r\nboth views.\r\nAnother unfortunate limitation is that we cannot obtain an Ω(lg e 2 n) bound in the cell\u0002probe model.\r\n2.3.2 Range Reporting\r\nAs mentioned before, range reporting is the problem of reporting the points in a box [a1, b1]×\r\n· · ·×[ad, bd]. Reporting k points must take Ω(k) time, so, to avoid this technicality, our lower\r\nbounds only deal with the existential range reporting problem (report whether there exists\r\nany point in the range).\r\nRange reporting has enjoyed some attention recently. In FOCS’00, Alstrup, Brodal, and\r\nRauhe [7] showed how to solve static 2D range reporting in O(lg lg n) time and almost linear\r\nspace.\r\nDynamic range reporting in 2D has a lower bound of Ω(lg n/ lg lg n), as shown by Alstrup,\r\nHusfeldt, and Rauhe [8] in FOCS’98. Their proof goes via the marked ancestor problem,\r\nand dynamic stabbing in 1D; see §2.3.3. Mortensen [75] showed how to obtain a tight\r\nO(lg n/ lg lg n) upper bound in SODA’03.\r\nNote that a (near-)constant running time for static 2D stabbing is natural in retrospect,\r\nsince dominance reporting in 2D can be solved in constant time by the famous range mini\u000236",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/68ff9fdc-64d3-4c1d-8604-9f7181a63e51.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fae2c3950a2c3c810a9dc17005fce5687487c2d8101f11e61b1ad433a28056a0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 461
      },
      {
        "segments": [
          {
            "segment_id": "d4dc2ea4-d9ec-4994-8374-7d1522d1b009",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 37,
            "page_width": 612,
            "page_height": 792,
            "content": "mum query (RMQ) results [24]. However, until recently, it seemed safe to conjecture that 3D\r\nrange reporting would require Ω(lg e n) query time for space O(n · polylog n) — that is, that\r\nthere is no special trick to play in 3D, and one just has to recurse to range trees to lift the\r\n2D solution into 3D. This is even more reasonable a conjecture, given that the dynamic 2D\r\nproblem has a lower bound of Ω(lg n/ lg lg n). Traditionally, dynamic d-dimensional problems\r\ntended to behave like static problems in d + 1 dimensions.\r\nHowever, this conjecture was refuted by a recent result of Nekrich [79] from SoCG’07. It\r\nwas shown that 3D range reporting can be done in doubly-logarithmic query time, specifically\r\ntq = O(lg2lg n). Without threatening the belief that ultimately the bounds should grow\r\nby Θ(lg n/ lg lg n) per dimension, this positive result raised the intriguing question whether\r\nfurther dimensions might also collapse to nearly constant time before this exponential growth\r\nbegins.\r\nFour dimensions. In Chapter 7, we show that the 4th dimension will not see a similar\r\nimprovement:\r\nTheorem 2.5. A data structure for range reporting in 4 dimensions using space n · σ in the\r\ncell probe model with w-bit cells, requires query time Ω( lg n\r\nlg(w+σ)\r\n).\r\nFor the main case w = O(lg n) and S = O(n · polylog n), the query time must be\r\nΩ(lg n/ lg lg n). This is almost tight, since the result of Nekrich implies an upper bound of\r\nO(lg n lg lg n).\r\nReachability oracles in butterfly graphs. The natural question that our result stirs\r\nis: why would 4 dimensions be hard, if 3 dimensions turned out to be easy? The question\r\nhas a simple, but fascinating answer: reachability oracles in butterfly graphs.\r\nThe following problem appears very hard: preprocess a sparse directed graph in less than\r\nn\r\n2\r\nspace, such that reachability queries (can u be reached from v?) are answered efficiently.\r\nThe problem seems to belong to folklore, and we are not aware of any nontrivial positive\r\nresults. By contrast, for undirected graphs, many oracles are known.\r\nWe show the first lower bound supporting the apparent difficulty of the problem:\r\nTheorem 2.6. A reachability oracle using space nσ in the cell probe model with w-bit cells,\r\nrequires query time Ω( lg n\r\nlg(σ+w)\r\n).\r\nThe bound holds even if the graph is a subgraph of a butterfly graph, and in fact it is\r\ntight for this special case. If constant time is desired, our bounds shows that the space needs\r\nto be n\r\n1+Ω(1). This stands in contrast to undirected graphs, for which connectivity oracles\r\nare easy to implement with O(n) space and O(1) query time. Note however, that our lower\r\nbound is still very far from the conjectured hardness of the problem.\r\nAfter showing this result, we give a reduction from reachability on butterfly graphs to\r\nstatic 4D range reporting, which proves the logarithmic complexity gap between 3 and 4\r\ndimensions. These results are describe in Chapter 7.\r\n37",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/d4dc2ea4-d9ec-4994-8374-7d1522d1b009.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=10210a830236cda8b8f934be82ab5b3a0e0229e4b99eb9d9bc7836a8c86d6ccf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "893a51ee-dc75-41b0-b588-7ae61d9823e6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 38,
            "page_width": 612,
            "page_height": 792,
            "content": "Reporting in 2 dimensions. As mentioned already, in FOCS’00, Alstrup, Brodal, and\r\nRauhe [7] showed how to solve static 2D range reporting in O(lg lg n) time and almost linear\r\nspace. This raises the question whether the query time can be reduced to constant. In\r\nthe case of dominance queries, it can indeed by made O(1) using range minimum queries.\r\nHowever, in Chapter 9, we show that:\r\nTheorem 2.7. A data structure for 2-dimensional range reporting in rank space [n]\r\n2\r\n, using\r\nmemory O(n · polylog n) in the cell-probe model with cells of O(lg n) cells, requires Ω(lg lg n)\r\nquery time.\r\nTo the best of our knowledge, this is the first separation between dominance queries and\r\nthe general case. Our lower bound, in fact, hinges on some fairly deep developments in lower\r\nbounds for predecessor search: it relies on our lower bounds for the direct sum of predecessor\r\nproblems.\r\n2.3.3 Orthogonal Stabbing\r\nA dual of range queries is stabbing: preprocess a set of n boxes of the form [a1, b1] × · · · ×\r\n[ad, bd], such that we can quickly find the box(es) containing a query point.\r\nStabbing is a very important form of classification queries. For instance, network routers\r\nhave rules (access control lists) applying to packets coming from some IP range, and heading\r\nto another IP range. A query is needed for every packet passing through the router, making\r\nthis a critical problem. This application has motivated the following theoretically-minded\r\npapers [100, 44, 17, 41], as well as a significant body of practically-minded ones.\r\nAnother important application of stabbing is method dispatching, in experimental object\r\noriented languages that (unlike, say, Java and C++) allow dynamic dispatching on more\r\narguments than the class. This application has motivated the following theoretically-minded\r\npapers [78, 6, 45, 46], as well as a number of practically-minded ones.\r\nPrevious results. Static stabbing in one dimension can be solved easily by predecessor\r\nsearch (after locating the query among the interval end-points, you can determined the\r\nstabbed interval in constant time). For the sake of a lower bound, one can also obtain the\r\ninverse reduction: colored predecessor search reduces to stabbing (add an interval for each\r\nred point, and its next blue point; the query stabs an interval iff its predecessor is red).\r\nDynamic stabbing in one dimension is as hard as the marked ancestor problem of Alstrup,\r\nHusfeldt, and Rauhe [8] in FOCS’98. In this problem, we are to maintain a complete tree of\r\ndegree b and depth d, in which vertices have a mark bit. The updates may mark or unmark\r\na vertex. The query is given a leaf v, and must determine whether the path from the root\r\nto v contains any marked node.\r\nMarked ancestor reduces to dynamic stabbing in 1D, by associating each vertex with an\r\ninterval extending from the leftmost to the rightmost leaf in its subtree. Marking a node\r\nadds the interval to the set, and unmarking removes it. Then, an ancestor of a leaf is marked\r\niff the leaf stabs an interval currently in the set.\r\n38",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/893a51ee-dc75-41b0-b588-7ae61d9823e6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6703208f3537f6192eed0c625fda54d0d83e35c55de79bf06c0b90fef4c976e6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "893a51ee-dc75-41b0-b588-7ae61d9823e6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 38,
            "page_width": 612,
            "page_height": 792,
            "content": "Reporting in 2 dimensions. As mentioned already, in FOCS’00, Alstrup, Brodal, and\r\nRauhe [7] showed how to solve static 2D range reporting in O(lg lg n) time and almost linear\r\nspace. This raises the question whether the query time can be reduced to constant. In\r\nthe case of dominance queries, it can indeed by made O(1) using range minimum queries.\r\nHowever, in Chapter 9, we show that:\r\nTheorem 2.7. A data structure for 2-dimensional range reporting in rank space [n]\r\n2\r\n, using\r\nmemory O(n · polylog n) in the cell-probe model with cells of O(lg n) cells, requires Ω(lg lg n)\r\nquery time.\r\nTo the best of our knowledge, this is the first separation between dominance queries and\r\nthe general case. Our lower bound, in fact, hinges on some fairly deep developments in lower\r\nbounds for predecessor search: it relies on our lower bounds for the direct sum of predecessor\r\nproblems.\r\n2.3.3 Orthogonal Stabbing\r\nA dual of range queries is stabbing: preprocess a set of n boxes of the form [a1, b1] × · · · ×\r\n[ad, bd], such that we can quickly find the box(es) containing a query point.\r\nStabbing is a very important form of classification queries. For instance, network routers\r\nhave rules (access control lists) applying to packets coming from some IP range, and heading\r\nto another IP range. A query is needed for every packet passing through the router, making\r\nthis a critical problem. This application has motivated the following theoretically-minded\r\npapers [100, 44, 17, 41], as well as a significant body of practically-minded ones.\r\nAnother important application of stabbing is method dispatching, in experimental object\r\noriented languages that (unlike, say, Java and C++) allow dynamic dispatching on more\r\narguments than the class. This application has motivated the following theoretically-minded\r\npapers [78, 6, 45, 46], as well as a number of practically-minded ones.\r\nPrevious results. Static stabbing in one dimension can be solved easily by predecessor\r\nsearch (after locating the query among the interval end-points, you can determined the\r\nstabbed interval in constant time). For the sake of a lower bound, one can also obtain the\r\ninverse reduction: colored predecessor search reduces to stabbing (add an interval for each\r\nred point, and its next blue point; the query stabs an interval iff its predecessor is red).\r\nDynamic stabbing in one dimension is as hard as the marked ancestor problem of Alstrup,\r\nHusfeldt, and Rauhe [8] in FOCS’98. In this problem, we are to maintain a complete tree of\r\ndegree b and depth d, in which vertices have a mark bit. The updates may mark or unmark\r\na vertex. The query is given a leaf v, and must determine whether the path from the root\r\nto v contains any marked node.\r\nMarked ancestor reduces to dynamic stabbing in 1D, by associating each vertex with an\r\ninterval extending from the leftmost to the rightmost leaf in its subtree. Marking a node\r\nadds the interval to the set, and unmarking removes it. Then, an ancestor of a leaf is marked\r\niff the leaf stabs an interval currently in the set.\r\n38",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/893a51ee-dc75-41b0-b588-7ae61d9823e6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6703208f3537f6192eed0c625fda54d0d83e35c55de79bf06c0b90fef4c976e6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "bcad0f2d-6a46-4366-a123-158e1f574684",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 39,
            "page_width": 612,
            "page_height": 792,
            "content": "Alstrup et al. [8] showed a lower bound trade-off for the marked ancestor problem, stating\r\nthat tq = Ω(lg n/ lg(w + tu)). In particular, max{tq, tu} = Ω(lg n/ lg lg n) for word size\r\nw = O(polylog n), and this bound carries over to dynamic range stabbing in 1D. A tight\r\nupper bound for range stabbing is obtained by Thorup [100] in STOC’03.\r\nOur results. We show the first superconstant lower bounds for static stabbing in 2-\r\ndimensions, which is the application relevant to routers, and arguably the most frequently\r\nneeded case of multimethod dispatching.\r\nTheorem 2.8. A data structure for orthogonal range stabbing in 2 dimensions using space\r\nn · σ in the cell probe model with w-bit cells, requires query time Ω( lg n\r\nlg(σw)\r\n).\r\nOur bound is tight, matching the upper bound of [30]. Our bound holds by reduction\r\nfrom reachability oracles in butterfly graphs; see §2.3.2.\r\nReductions. It is easy to see that stabbing in d dimensions reduces to range reporting\r\nin 2d dimensions, since boxes can be expressed as 2d-dimensional points. Thus, our lower\r\nbound for 2D stabbing immediately implies our lower bound for 4D reporting.\r\nExistential range stabbing in 2D also reduces to (weighted) range counting in 2D by the\r\nfollowing neat trick. We replace a rectangle [a1, b1] × [a2, b2] by 4 points: (a1, b1) and (a2, b2)\r\nwith weight +1, and (a1, b2) and (a2, b1) with weight −1. To test whether (q1, q2) stabs a\r\nrectangle, query the sum in the range [0, q1] × [0, q2]. If the query lies inside a rectangle, the\r\nlower-left corner contributes +1 to count. If the query point is outside, the corners cancel\r\nout.\r\nOur lower bounds for stabbing can in fact guarantee that the query never stabs more\r\nthan one rectangle. Then, in the above reduction, it suffices to count points mod 2. Thus,\r\nwe obtain a lower bound even for the unweighted range counting problem.\r\n2.4 Problems in High Dimensions\r\n2.4.1 Partial Match\r\nFormally, the problem asks to preprocess a data base of n strings in {0, 1}\r\nd\r\n, and support the\r\nfollowing query: given a pattern in {0, 1, ?}\r\nd\r\n, determine whether any string in the database\r\nmatches this pattern (where ? can match anything).\r\nSince it is defined on the hypercube, the partial match problem is only interesting in very\r\nhigh dimensions. In fact, partial match is a stylized version of many practically important\r\nqueries in high dimensions:\r\nRange queries: Preprocess a set of n points in R\r\nd\r\nto answer orthogonal range queries, such\r\nas reporting points in the range [a1, b1] × · · · × [ad, bd]. Partial match can be seen as\r\na dominance query on the hypercube: double the dimension and apply the following\r\ntransformation: 0 7→ 01; 1 7→ 10; ? 7→ 11.\r\n39",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/bcad0f2d-6a46-4366-a123-158e1f574684.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3c08c406456b4049180ebd5b737f33df7dc8c5d065d71da36e7d27b48b6e7bd6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 477
      },
      {
        "segments": [
          {
            "segment_id": "2105d42f-e82b-4e60-a93a-1c9872789398",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 40,
            "page_width": 612,
            "page_height": 792,
            "content": "Near neighbor in `∞: A natural example of an orthogonal range in high dimensions is the\r\nhypercube, which has the nice feature that dimensions are treated symmetrically. This\r\ndefines the near neighbor problem in `∞; see §2.4.3 for a discussion of this problem.\r\nIntersection queries: Preprocess a family of sets F = {S1, . . . , Sd}, where Si ⊆ [n], to\r\nanswer the query: given a subcollection of F, is the intersection of those sets empty?\r\nThis is one of the main queries in search engines: the set Siis the list of documents\r\ncontaining word i, and the query asks for a document containing some set of words.\r\nSearching with wild-cards: Preprocess a collection of strings over an alphabet Σ, to\r\nsearch for query strings in Σ ∪ {?}, where ? is a wild card matching any letter. Partial\r\nmatch is the case Σ = {0, 1}.\r\nIt should be noted that these problems only degenerate into the partial match problem\r\nin high dimensions; in low dimensions, other considerations are more important. See, for\r\nexample, the ample literature of range queries, discussed in §2.3.\r\nThe “low-dimensional” version of searching with wild-cards puts a bound k on the num\u0002ber of ?’s in the string. This problem is considerably less explored than range reporting.\r\nHowever, a recent break-through result of Cole, Gottlieb and Lewenstein [33] achieved results\r\nreminiscent of range reporting: they use space O(n lgk n) and time O(lgk n · lg lg n), for any\r\nconstant k.\r\nProblem status. The first upper bounds for partial match was obtained by Rivest [93],\r\nwho showed that the trivial 2dspace can be slightly improved when d ≤ 2 lg n. Charikar,\r\nIndyk, and Panigrahy [29] showed that query time O(n/2\r\nτ\r\n) can be achieved with space\r\nn·2\r\nO(d lg2 d/√\r\nτ / lg n)\r\n. This is currently the best known theoretical result, though many heuristic\r\nsolutions are used in practical applications.\r\nIt is generally conjectured that the problem suffers from the curse of dimensionality. A\r\nfairly plausible conjecture seems to be that there is no constant ε > 0, such that query time\r\nO(n\r\n1−ε\r\n) can be supported with space poly(m) · 2\r\nO(d\r\n1−ε\r\n)\r\n.\r\nPartial match has been investigated in the asymmetric communication model (where Alice\r\nholds the query and Bob holds the database), in the hope of obtaining evidence for this curse\r\nof dimensionality. In STOC’95, Miltersen et al. [73] could show an Ω(√\r\nlg d) cell-probe lower\r\nbound via round elimination. In STOC’99, Borodin, Ostrovsky, and Rabani [25] used the\r\nrichness method to show that either the querier sends a = Ω(lg d · lg n) bits, or the database\r\nsends b = Ω(n\r\n1−ε\r\n) bits, for any ε > 0. In STOC’03, Jayram, Khot, Kumar, and Rabani [64]\r\nsignificantly strengthened the bound to prove that either Alice sends a = Ω(d/ lg n) bits, or\r\nBob sends b = Ω(n\r\n1−ε\r\n) bits.\r\nOur results. In our work [82], and Chapter 6, we give a reduction from lopsided set\r\ndisjointness to partial match, showing that:\r\nTheorem 2.9. Let Alice hold a string in {0, 1, ?}\r\nd\r\n, and Bob hold n strings in {0, 1}\r\nd\r\n. In\r\nany bounded-error protocol answering the partial match query, either Alice sends Ω(d) bits\r\nor Bob sends Ω(n\r\n1−ε\r\n) bits, for any constant ε > 0.\r\n40",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/2105d42f-e82b-4e60-a93a-1c9872789398.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=04a944c822438a042281aa603b93300ee9b9cdcad025cfce9ddd617651147311",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 565
      },
      {
        "segments": [
          {
            "segment_id": "2105d42f-e82b-4e60-a93a-1c9872789398",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 40,
            "page_width": 612,
            "page_height": 792,
            "content": "Near neighbor in `∞: A natural example of an orthogonal range in high dimensions is the\r\nhypercube, which has the nice feature that dimensions are treated symmetrically. This\r\ndefines the near neighbor problem in `∞; see §2.4.3 for a discussion of this problem.\r\nIntersection queries: Preprocess a family of sets F = {S1, . . . , Sd}, where Si ⊆ [n], to\r\nanswer the query: given a subcollection of F, is the intersection of those sets empty?\r\nThis is one of the main queries in search engines: the set Siis the list of documents\r\ncontaining word i, and the query asks for a document containing some set of words.\r\nSearching with wild-cards: Preprocess a collection of strings over an alphabet Σ, to\r\nsearch for query strings in Σ ∪ {?}, where ? is a wild card matching any letter. Partial\r\nmatch is the case Σ = {0, 1}.\r\nIt should be noted that these problems only degenerate into the partial match problem\r\nin high dimensions; in low dimensions, other considerations are more important. See, for\r\nexample, the ample literature of range queries, discussed in §2.3.\r\nThe “low-dimensional” version of searching with wild-cards puts a bound k on the num\u0002ber of ?’s in the string. This problem is considerably less explored than range reporting.\r\nHowever, a recent break-through result of Cole, Gottlieb and Lewenstein [33] achieved results\r\nreminiscent of range reporting: they use space O(n lgk n) and time O(lgk n · lg lg n), for any\r\nconstant k.\r\nProblem status. The first upper bounds for partial match was obtained by Rivest [93],\r\nwho showed that the trivial 2dspace can be slightly improved when d ≤ 2 lg n. Charikar,\r\nIndyk, and Panigrahy [29] showed that query time O(n/2\r\nτ\r\n) can be achieved with space\r\nn·2\r\nO(d lg2 d/√\r\nτ / lg n)\r\n. This is currently the best known theoretical result, though many heuristic\r\nsolutions are used in practical applications.\r\nIt is generally conjectured that the problem suffers from the curse of dimensionality. A\r\nfairly plausible conjecture seems to be that there is no constant ε > 0, such that query time\r\nO(n\r\n1−ε\r\n) can be supported with space poly(m) · 2\r\nO(d\r\n1−ε\r\n)\r\n.\r\nPartial match has been investigated in the asymmetric communication model (where Alice\r\nholds the query and Bob holds the database), in the hope of obtaining evidence for this curse\r\nof dimensionality. In STOC’95, Miltersen et al. [73] could show an Ω(√\r\nlg d) cell-probe lower\r\nbound via round elimination. In STOC’99, Borodin, Ostrovsky, and Rabani [25] used the\r\nrichness method to show that either the querier sends a = Ω(lg d · lg n) bits, or the database\r\nsends b = Ω(n\r\n1−ε\r\n) bits, for any ε > 0. In STOC’03, Jayram, Khot, Kumar, and Rabani [64]\r\nsignificantly strengthened the bound to prove that either Alice sends a = Ω(d/ lg n) bits, or\r\nBob sends b = Ω(n\r\n1−ε\r\n) bits.\r\nOur results. In our work [82], and Chapter 6, we give a reduction from lopsided set\r\ndisjointness to partial match, showing that:\r\nTheorem 2.9. Let Alice hold a string in {0, 1, ?}\r\nd\r\n, and Bob hold n strings in {0, 1}\r\nd\r\n. In\r\nany bounded-error protocol answering the partial match query, either Alice sends Ω(d) bits\r\nor Bob sends Ω(n\r\n1−ε\r\n) bits, for any constant ε > 0.\r\n40",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/2105d42f-e82b-4e60-a93a-1c9872789398.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=04a944c822438a042281aa603b93300ee9b9cdcad025cfce9ddd617651147311",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 565
      },
      {
        "segments": [
          {
            "segment_id": "4b7836f7-7c74-4823-a708-6d6fdff7ec0b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 41,
            "page_width": 612,
            "page_height": 792,
            "content": "This improves the best previous bound for Alice’s communication from Ω(d/ lg n) to the\r\noptimal Ω(d).\r\nOur reduction is a simple exercise, and it seems surprising that the connection was not\r\nestablished before. Notably, Barkol and Rabani [20] gave a difficult lower bound for exact\r\nnear neighbor in the Hamming cube, showing a = Ω(d) and b = Ω(n\r\n1/8−ε\r\n), though it was\r\nwell known that partial match reduces to exact near neighbor. This suggests that partial\r\nmatch was viewed as a “nasty” problem. Our result greatly simplifies their proof, as well as\r\nimproving Bob’s communication to Ω(n\r\n1−ε\r\n).\r\nTheorem 2.9 implies that a decision tree for the partial match problem with predicate size\r\nO(n\r\nε\r\n) must either have size 2Ω(d), or depth Ω(n\r\n1−2ε/d). Thus, the course of dimensionality\r\nis true for decision trees in a very strong form.\r\nBy the standard relation between asymmetric communication and cell-probe complexity,\r\nTheorem 2.9 also implies that a data structure with query time t must use space 2Ω(d/t),\r\nassuming the word size is O(n\r\n1−ε/t). We normally assume the word size to be O(d), or\r\nmaybe d\r\nO(1), making the condition w = O(n1−ε/t) essentially trivial. As usual with such\r\nbounds, the cell-probe result is optimal for constant query time, but degrades quickly with\r\nt.\r\nOur direct sum results for richness (see Chapter 6) slightly strengthen the implication of\r\nTheorem 2.9 to t = Ω(d/ lg S·w\r\nn\r\n). This implies, for example, a time lower bound of Ω(d/ lg w)\r\nfor space S = O(n · poly(d lg n)), which is of course very far from the upper bounds.\r\n2.4.2 Near Neighbor Search in `1, `2\r\nNearest neighbor search (NNS) is the problem of preprocessing a collection of n points,\r\nsuch that we can quickly find the database point closest to a given query point. This is a\r\nkey algorithmic problem arising in several areas such as data compression, databases and\r\ndata mining, information retrieval, image and video databases, machine learning, pattern\r\nrecognition, statistics and data analysis.\r\nThe most natural examples of spaces in which NNS can be defined are the `\r\nd\r\np norms,\r\ndenoting the space <\r\nd\r\nendowed with the distance kx−ykp =\r\n\u0010Pd\r\ni=1 |xi − yi\r\n|\r\np\r\n\u00111/p\r\n. Significant\r\nattention has been devoted to NNS in the Euclidean norm `2 and the Manhattan norm `1.\r\nWe refer the reader to surveys in [14, 96, 94]. Another very important, but less understood\r\nspace is `∞, which we discuss in §2.4.3.\r\nExact NNS is conjectured to suffer from a “curse of dimensionality,” which makes the\r\nbounds grow exponentially with the dimension. For example, two natural solutions for NNS\r\nare:\r\n• answer queries by a linear scan, using linear space and linear query time.\r\n• construct the d-dimensional Voronoi diagram, which uses space n\r\nΘ(d)\r\n, but allows query\r\ntime poly(d lg n).\r\nThe conjectured curse of dimensionality states that the transition between these two types\r\nof bounds is sharp: it is impossible to achieve O(n\r\n1−ε\r\n) query time with space n\r\no(d)\r\n.\r\n41",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/4b7836f7-7c74-4823-a708-6d6fdff7ec0b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d123a16efac7c5ae6a0afbee2c4d063afa5276dbeb49888cb20183b512143042",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 513
      },
      {
        "segments": [
          {
            "segment_id": "4b7836f7-7c74-4823-a708-6d6fdff7ec0b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 41,
            "page_width": 612,
            "page_height": 792,
            "content": "This improves the best previous bound for Alice’s communication from Ω(d/ lg n) to the\r\noptimal Ω(d).\r\nOur reduction is a simple exercise, and it seems surprising that the connection was not\r\nestablished before. Notably, Barkol and Rabani [20] gave a difficult lower bound for exact\r\nnear neighbor in the Hamming cube, showing a = Ω(d) and b = Ω(n\r\n1/8−ε\r\n), though it was\r\nwell known that partial match reduces to exact near neighbor. This suggests that partial\r\nmatch was viewed as a “nasty” problem. Our result greatly simplifies their proof, as well as\r\nimproving Bob’s communication to Ω(n\r\n1−ε\r\n).\r\nTheorem 2.9 implies that a decision tree for the partial match problem with predicate size\r\nO(n\r\nε\r\n) must either have size 2Ω(d), or depth Ω(n\r\n1−2ε/d). Thus, the course of dimensionality\r\nis true for decision trees in a very strong form.\r\nBy the standard relation between asymmetric communication and cell-probe complexity,\r\nTheorem 2.9 also implies that a data structure with query time t must use space 2Ω(d/t),\r\nassuming the word size is O(n\r\n1−ε/t). We normally assume the word size to be O(d), or\r\nmaybe d\r\nO(1), making the condition w = O(n1−ε/t) essentially trivial. As usual with such\r\nbounds, the cell-probe result is optimal for constant query time, but degrades quickly with\r\nt.\r\nOur direct sum results for richness (see Chapter 6) slightly strengthen the implication of\r\nTheorem 2.9 to t = Ω(d/ lg S·w\r\nn\r\n). This implies, for example, a time lower bound of Ω(d/ lg w)\r\nfor space S = O(n · poly(d lg n)), which is of course very far from the upper bounds.\r\n2.4.2 Near Neighbor Search in `1, `2\r\nNearest neighbor search (NNS) is the problem of preprocessing a collection of n points,\r\nsuch that we can quickly find the database point closest to a given query point. This is a\r\nkey algorithmic problem arising in several areas such as data compression, databases and\r\ndata mining, information retrieval, image and video databases, machine learning, pattern\r\nrecognition, statistics and data analysis.\r\nThe most natural examples of spaces in which NNS can be defined are the `\r\nd\r\np norms,\r\ndenoting the space <\r\nd\r\nendowed with the distance kx−ykp =\r\n\u0010Pd\r\ni=1 |xi − yi\r\n|\r\np\r\n\u00111/p\r\n. Significant\r\nattention has been devoted to NNS in the Euclidean norm `2 and the Manhattan norm `1.\r\nWe refer the reader to surveys in [14, 96, 94]. Another very important, but less understood\r\nspace is `∞, which we discuss in §2.4.3.\r\nExact NNS is conjectured to suffer from a “curse of dimensionality,” which makes the\r\nbounds grow exponentially with the dimension. For example, two natural solutions for NNS\r\nare:\r\n• answer queries by a linear scan, using linear space and linear query time.\r\n• construct the d-dimensional Voronoi diagram, which uses space n\r\nΘ(d)\r\n, but allows query\r\ntime poly(d lg n).\r\nThe conjectured curse of dimensionality states that the transition between these two types\r\nof bounds is sharp: it is impossible to achieve O(n\r\n1−ε\r\n) query time with space n\r\no(d)\r\n.\r\n41",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/4b7836f7-7c74-4823-a708-6d6fdff7ec0b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d123a16efac7c5ae6a0afbee2c4d063afa5276dbeb49888cb20183b512143042",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 513
      },
      {
        "segments": [
          {
            "segment_id": "a6be9d97-88e9-47ac-9801-4894fc7ced72",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 42,
            "page_width": 612,
            "page_height": 792,
            "content": "Though many heuristic solutions have been designed for practical instances, from a the\u0002oretical perspective, the curse of dimensionality has only been overcome by allowing approx\u0002imation. In the c-approximate nearest neighbor problem, the goal is to return a point which\r\nis at most a factor c farther than the nearest neighbor.\r\nA very clean setup for NNS is the Hamming cube {0, 1}\r\nd\r\n. In this case, kp − qkH =\r\nkp − qk1 =\r\n\r\nkp − qk2\r\n\u00012\r\n, so a c approximation for `2 is equivalent to a c\r\n2 approximation for\r\n`1. Essentially, it suffices to consider the problem in this particular case, since there exist\r\nefficient embeddings of other spaces into the Hamming cube.\r\nHardness of Exact NNS\r\nPerhaps the strongest evidence for the curse of dimensionality of NNS is a simple reduction\r\nfrom partial match (see §2.4.1). If the query contains k wildcards, and we map translate\r\nthese wildcards into a coordinate value of 1\r\n2\r\n, the nearest distance between the query and\r\nan integral point is k\r\n2\r\nis `1 and pk/4 in `2. If the query is matched by a database string,\r\nthe string is precisely at this minimum distance. Otherwise, the nearest neighbor is farther\r\naway.\r\nBefore our work, communication lower bounds for partial match were not optimal: [64]\r\nonly bounded the communication of the querier by Ω(d/ lg n). In STOC’00, Barkol and\r\nRabani [20] circumvented the partial match problem, and considered exact NNS directly.\r\nThey considered the problem in the Hamming cube, and showed that either Alice must\r\ncommunicate a = Ω(d) bits, or Bob must send b = Ω(n\r\n1/8−δ\r\n) bits, for any δ > 0. Our result\r\nfor partial match (Theorem 2.9) supersedes this result: Bob’s communication is improved to\r\nΩ(n\r\n1−δ\r\n), and our proof is considerably simpler.\r\nUpper Bounds for Approximate NNS\r\nSolutions to the approximate nearest neighbor problem often go via the c-approximate near\r\nneighbor problem. In this problem, a data structure must be constructed for a given set of\r\npoints S, approximation c, and radius r. Given a point q, the near-neighbor query returns:\r\n• a point p ∈ S at distance kp − qk ≤ c · r; or\r\n• No, if all points p ∈ S are at distance kp − qk > r.\r\nNote that there is an overlap between the two cases. In the decision version of this\r\nproblem, the query returns:\r\n• Yes, if there is a point p ∈ S at distance kq − pk ≤ r.\r\n• No, if all points p ∈ S are at distance kq − pk > c · r.\r\n• an unspecified value otherwise.\r\nOne can use approximate near neighbor to solve approximate nearest neighbor by con\u0002structing near-neighbor structures for all r = c\r\ni\r\n, and doing a binary search for the correct r\r\nat query time.\r\n42",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/a6be9d97-88e9-47ac-9801-4894fc7ced72.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bce00fd96d43c716f691542b844f95cd3a29ad9bcbf6770a21c5b08fb5614bd3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 479
      },
      {
        "segments": [
          {
            "segment_id": "e7fc61b1-a999-4e81-9d5c-7671eee00dc7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 43,
            "page_width": 612,
            "page_height": 792,
            "content": "Solutions for approximate NNS attack two ranges of parameters: approximation 1 + ε,\r\nand “large” approximation c (for instance, c = 2).\r\nIn the 1 + ε regime, the landmark papers of Kushilevitz, Ostrovsky, and Rabani [67]\r\nand Indyk and Motwani [63] solved the near neighbor problem with space n\r\nO(1/ε2) and very\r\nefficient query time. These results demonstrate that exponential dependence on the dimen\u0002sion can be overcome for any constant approximation, though, of course, the space bound is\r\nonly interesting from a theoretical perspective. In some sense, the curse of dimensionality is\r\nreplaced by a “curse of approximation.”\r\nThe main idea behind these data structures is the dimensionality reduction method. This\r\ncan be exemplified by the Johnson-Lindenstrauss lemma, which states for any fixed points\r\np, q in arbitrary dimension, a projection π on a “random” hyperplane of dimension O(\r\n1\r\nε\r\n2\r\nlg 1\r\nδ\r\n)\r\nsatisfies (ignoring normalization):\r\nPr \u0002(1 − ε)kp − qk2 ≤ kπ(p) − π(q)k2 ≤ (1 + ε)kp − qk2\r\n\u0003\r\n≤ δ\r\nProofs are given for various notions of a “random hyperplane”.\r\nThus, the database can be projected to a space of dimension d\r\n0 = O(\r\n1\r\nε\r\n2\r\nlg n), and the\r\ndistance from the query to the nearest neighbor does not change by more than 1+ε with high\r\nprobability. After reducing the problem to low dimensions, one can essentially use complete\r\ntabulation to achieve space exponential in the dimension. The idea is to store “all” points\r\nin <\r\nd\r\n0\r\nwithin distance 1 from some p ∈ S. To do this, we impose a cubic grid on <\r\nd\r\n0\r\n, with\r\neach cell having diameter \u000f. It can be shown [63] that each unit ball in <\r\nd\r\n0\r\ntouches at most\r\n(1/\u000f)\r\nO(d\r\n0\r\n) grid cells. Therefore, we can afford to store all such cells within the given space\r\nbound.\r\nTo answer a query, we simply check if a grid cell containing the query point has been\r\nstored. The query algorithm is extremely efficient: it only needs to apply the projection to\r\nthe query, and then look up the resulting point. Thus, the cell-probe complexity is constant.\r\nThe paper of Indyk and Motwani [63] also initiated the study of c-approximate near\r\nneighbor, for “large” c. They introduced locality sensitive hashing (LSH), a technique used\r\nin all subsequent work in this regime. By constructing a family of LSH functions with\r\nparameter ρ, one can obtain a data structure with space Oe(n\r\n1+ρ\r\n) and query time Oe(n\r\nρ\r\n).\r\nThey constructed such a family with ρ = 1/c for the `1 and `2 metric, obtaining a near\u0002neighbor data structures with space Oe(n\r\n1+1/c) and query time Oe(n1/c).\r\nPanigrahy [80] showed how to use LSH functions to obtain data structures with very\r\nefficient query time or near-linear space. In particular, he obtained query time poly(d lg n)\r\nwith space n\r\n1+O(ρ) = n1+O(1/c)\r\n, as well as space Oe(n) with query time n\r\nO(ρ) = nO(1/c)\r\n.\r\nMotwani, Naor, and Panigrahy [77] showed a geometric lower bound on the parameter\r\nof LSH functions: in `1, ρ = Ω(1/c), while in `2, ρ = Ω(1/c2). Of course, this does not rule\r\nout data structures using other techniques.\r\nDatar, Immorlica, Indyk, and Mirrokni [34] improved the LSH upper bound for `2 to\r\nsome ρ < 1\r\nc\r\n. Finally, Andoni and Indyk [14] showed ρ =\r\n1\r\nc\r\n2 + o(1), asymptotically matching\r\nthe lower bound.\r\n43",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e7fc61b1-a999-4e81-9d5c-7671eee00dc7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e7afdad47e5e4f06e06b393e786e250687bdbe09035593cff33545c451b48137",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 576
      },
      {
        "segments": [
          {
            "segment_id": "e7fc61b1-a999-4e81-9d5c-7671eee00dc7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 43,
            "page_width": 612,
            "page_height": 792,
            "content": "Solutions for approximate NNS attack two ranges of parameters: approximation 1 + ε,\r\nand “large” approximation c (for instance, c = 2).\r\nIn the 1 + ε regime, the landmark papers of Kushilevitz, Ostrovsky, and Rabani [67]\r\nand Indyk and Motwani [63] solved the near neighbor problem with space n\r\nO(1/ε2) and very\r\nefficient query time. These results demonstrate that exponential dependence on the dimen\u0002sion can be overcome for any constant approximation, though, of course, the space bound is\r\nonly interesting from a theoretical perspective. In some sense, the curse of dimensionality is\r\nreplaced by a “curse of approximation.”\r\nThe main idea behind these data structures is the dimensionality reduction method. This\r\ncan be exemplified by the Johnson-Lindenstrauss lemma, which states for any fixed points\r\np, q in arbitrary dimension, a projection π on a “random” hyperplane of dimension O(\r\n1\r\nε\r\n2\r\nlg 1\r\nδ\r\n)\r\nsatisfies (ignoring normalization):\r\nPr \u0002(1 − ε)kp − qk2 ≤ kπ(p) − π(q)k2 ≤ (1 + ε)kp − qk2\r\n\u0003\r\n≤ δ\r\nProofs are given for various notions of a “random hyperplane”.\r\nThus, the database can be projected to a space of dimension d\r\n0 = O(\r\n1\r\nε\r\n2\r\nlg n), and the\r\ndistance from the query to the nearest neighbor does not change by more than 1+ε with high\r\nprobability. After reducing the problem to low dimensions, one can essentially use complete\r\ntabulation to achieve space exponential in the dimension. The idea is to store “all” points\r\nin <\r\nd\r\n0\r\nwithin distance 1 from some p ∈ S. To do this, we impose a cubic grid on <\r\nd\r\n0\r\n, with\r\neach cell having diameter \u000f. It can be shown [63] that each unit ball in <\r\nd\r\n0\r\ntouches at most\r\n(1/\u000f)\r\nO(d\r\n0\r\n) grid cells. Therefore, we can afford to store all such cells within the given space\r\nbound.\r\nTo answer a query, we simply check if a grid cell containing the query point has been\r\nstored. The query algorithm is extremely efficient: it only needs to apply the projection to\r\nthe query, and then look up the resulting point. Thus, the cell-probe complexity is constant.\r\nThe paper of Indyk and Motwani [63] also initiated the study of c-approximate near\r\nneighbor, for “large” c. They introduced locality sensitive hashing (LSH), a technique used\r\nin all subsequent work in this regime. By constructing a family of LSH functions with\r\nparameter ρ, one can obtain a data structure with space Oe(n\r\n1+ρ\r\n) and query time Oe(n\r\nρ\r\n).\r\nThey constructed such a family with ρ = 1/c for the `1 and `2 metric, obtaining a near\u0002neighbor data structures with space Oe(n\r\n1+1/c) and query time Oe(n1/c).\r\nPanigrahy [80] showed how to use LSH functions to obtain data structures with very\r\nefficient query time or near-linear space. In particular, he obtained query time poly(d lg n)\r\nwith space n\r\n1+O(ρ) = n1+O(1/c)\r\n, as well as space Oe(n) with query time n\r\nO(ρ) = nO(1/c)\r\n.\r\nMotwani, Naor, and Panigrahy [77] showed a geometric lower bound on the parameter\r\nof LSH functions: in `1, ρ = Ω(1/c), while in `2, ρ = Ω(1/c2). Of course, this does not rule\r\nout data structures using other techniques.\r\nDatar, Immorlica, Indyk, and Mirrokni [34] improved the LSH upper bound for `2 to\r\nsome ρ < 1\r\nc\r\n. Finally, Andoni and Indyk [14] showed ρ =\r\n1\r\nc\r\n2 + o(1), asymptotically matching\r\nthe lower bound.\r\n43",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e7fc61b1-a999-4e81-9d5c-7671eee00dc7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e7afdad47e5e4f06e06b393e786e250687bdbe09035593cff33545c451b48137",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 576
      },
      {
        "segments": [
          {
            "segment_id": "eea1773c-ae21-4eb3-ab0d-82d36fdb6d90",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 44,
            "page_width": 612,
            "page_height": 792,
            "content": "Unlike the regime of 1 +ε approximations, work in the regime of high approximation has\r\nproduced practical data structures (see, for example, the popular E2LSH package of Andoni\r\nand Indyk). Usually, the solution for approximate near neighbor is used as a heuristic filter:\r\nthe query scans all points at distance at most c · r from the query (hoping they are few), and\r\nfinds the true nearest neighbor.\r\nLower Bounds for Approximate NNS\r\nPrior to our work, lower bounds for approximate NNS have focused on the difference be\u0002tween the near-neighbor and the nearest-neighbor problems. If we want constant 1 + ε\r\napproximation and we accept polynomial space as “efficient,” then [67, 63] solve the near\r\nneighbor problem optimally, with constant query time. However, in the Hamming cube, the\r\nnearest neighbor problem will have an O(lg lg d) query time: the query must binary search\r\nfor i ≤ log1+ε d, such that the distance to the nearest neighbor is between (1 + ε)\r\ni and\r\n(1 + ε)\r\ni+1. In STOC’99, Chakrabarti, Chazelle, Gum, and Lvov [26] used round elimination\r\nto show the first lower bound for approximate nearest neighbor. In FOCS’04, Chakrabarti\r\nand Regev [27] slightly improved the upper bound to O(lg lg d/ lg lg lg d), and showed a\r\nmatching lower bound.\r\nIn our joint work with Alex Andoni and Piotr Indyk [16], we turned to the main cause\r\nof impracticality for 1 + ε approximation: the prohibitive space usage. We considered ap\u0002proximate near neighbor in the usual asymmetric communication setting, and showed:\r\nTheorem 2.10. Let Alice receive a query q ∈ {0, 1}\r\nd and Bob receive a database S of n\r\npoints from {0, 1}\r\nd\r\n, where d = ( 1\r\nε\r\nlg n)\r\nO(1). In any randomized protocol solving the (1 + ε)-\r\napproximate near neighbor problem, either Alice sends Ω( 1\r\nε\r\n2\r\nlg n) bits, or Bob sends Ω(n\r\n1−δ\r\n)\r\nbits, for any δ > 0.\r\nThis theorem is shown by reduction from lopsided set disjointness; a proof can be found\r\nin Chapter 6. As usual, this lower bound implies that for data structures with constant cell\u0002probe complexity, and for decision trees, the space must be n\r\nΩ(1/ε2)\r\n. Thus, dimensionality\r\nreduction method gives an optimal (but prohibitive) space bound.\r\nVery recently, Talwar, Panigrahy, and Wieder [98] considered the space consumption in\r\nthe regime of high approximation. They showed that a data structure with constant query\r\ntime requires space n\r\n1+Ω(1/c)\r\nin the `1 case, and n\r\n1+Ω(1/c2)\r\nin the `2 case.\r\nFinally, Liu [69] considered the case of approximate NNS when randomization is disal\u0002lowed. For any constant approximation, he showed that in a deterministic protocol, either\r\nthe querier sends a = Ω(d) bits, or the database sends b = Ω(nd) bits. This suggests that, in\r\nabsence of randomization, approximate NNS might also suffer from a curse of dimensionality.\r\n2.4.3 Near Neighbor Search in `∞\r\nThe `∞ space is dictated by a very natural metric that measures the maximum distance over\r\ncoordinates: kx−yk∞ = maxd\r\ni=1 |xi−yi\r\n|. Though recent years have seen a significant increase\r\n44",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/eea1773c-ae21-4eb3-ab0d-82d36fdb6d90.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5829b347563146297a68cf03351043058ce08d9ae20e46d2eb1cc6519dd905d0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 516
      },
      {
        "segments": [
          {
            "segment_id": "eea1773c-ae21-4eb3-ab0d-82d36fdb6d90",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 44,
            "page_width": 612,
            "page_height": 792,
            "content": "Unlike the regime of 1 +ε approximations, work in the regime of high approximation has\r\nproduced practical data structures (see, for example, the popular E2LSH package of Andoni\r\nand Indyk). Usually, the solution for approximate near neighbor is used as a heuristic filter:\r\nthe query scans all points at distance at most c · r from the query (hoping they are few), and\r\nfinds the true nearest neighbor.\r\nLower Bounds for Approximate NNS\r\nPrior to our work, lower bounds for approximate NNS have focused on the difference be\u0002tween the near-neighbor and the nearest-neighbor problems. If we want constant 1 + ε\r\napproximation and we accept polynomial space as “efficient,” then [67, 63] solve the near\r\nneighbor problem optimally, with constant query time. However, in the Hamming cube, the\r\nnearest neighbor problem will have an O(lg lg d) query time: the query must binary search\r\nfor i ≤ log1+ε d, such that the distance to the nearest neighbor is between (1 + ε)\r\ni and\r\n(1 + ε)\r\ni+1. In STOC’99, Chakrabarti, Chazelle, Gum, and Lvov [26] used round elimination\r\nto show the first lower bound for approximate nearest neighbor. In FOCS’04, Chakrabarti\r\nand Regev [27] slightly improved the upper bound to O(lg lg d/ lg lg lg d), and showed a\r\nmatching lower bound.\r\nIn our joint work with Alex Andoni and Piotr Indyk [16], we turned to the main cause\r\nof impracticality for 1 + ε approximation: the prohibitive space usage. We considered ap\u0002proximate near neighbor in the usual asymmetric communication setting, and showed:\r\nTheorem 2.10. Let Alice receive a query q ∈ {0, 1}\r\nd and Bob receive a database S of n\r\npoints from {0, 1}\r\nd\r\n, where d = ( 1\r\nε\r\nlg n)\r\nO(1). In any randomized protocol solving the (1 + ε)-\r\napproximate near neighbor problem, either Alice sends Ω( 1\r\nε\r\n2\r\nlg n) bits, or Bob sends Ω(n\r\n1−δ\r\n)\r\nbits, for any δ > 0.\r\nThis theorem is shown by reduction from lopsided set disjointness; a proof can be found\r\nin Chapter 6. As usual, this lower bound implies that for data structures with constant cell\u0002probe complexity, and for decision trees, the space must be n\r\nΩ(1/ε2)\r\n. Thus, dimensionality\r\nreduction method gives an optimal (but prohibitive) space bound.\r\nVery recently, Talwar, Panigrahy, and Wieder [98] considered the space consumption in\r\nthe regime of high approximation. They showed that a data structure with constant query\r\ntime requires space n\r\n1+Ω(1/c)\r\nin the `1 case, and n\r\n1+Ω(1/c2)\r\nin the `2 case.\r\nFinally, Liu [69] considered the case of approximate NNS when randomization is disal\u0002lowed. For any constant approximation, he showed that in a deterministic protocol, either\r\nthe querier sends a = Ω(d) bits, or the database sends b = Ω(nd) bits. This suggests that, in\r\nabsence of randomization, approximate NNS might also suffer from a curse of dimensionality.\r\n2.4.3 Near Neighbor Search in `∞\r\nThe `∞ space is dictated by a very natural metric that measures the maximum distance over\r\ncoordinates: kx−yk∞ = maxd\r\ni=1 |xi−yi\r\n|. Though recent years have seen a significant increase\r\n44",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/eea1773c-ae21-4eb3-ab0d-82d36fdb6d90.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5829b347563146297a68cf03351043058ce08d9ae20e46d2eb1cc6519dd905d0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 516
      },
      {
        "segments": [
          {
            "segment_id": "af122e48-f35b-4ec1-a50b-83b416741b3a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 45,
            "page_width": 612,
            "page_height": 792,
            "content": "in our understanding of high-dimensional nearest neighbor search in `1 and `2 spaces, `∞\r\nremains the odd-man out, with a much less understood, and intriguingly different structure.\r\nIn fact, there is precisely one data structure for with provable theoretical guarantees\r\nfor NNS in `∞. In FOCS’98, Indyk [61] proved the following unorthodox result: there is\r\na data structure (in fact, a decision tree) of size O(n\r\nρ\r\n), for any ρ > 1, which achieves ap\u0002proximation 4dlogρ\r\nlog 4de + 1 for NNS in the d-dimensional `∞ metric. The query time is\r\nO(d · polylog n). For 3-approximation, Indyk can achieve space n\r\nlog d+1. Note that in the im\u0002portant regime of polynomial space, Indyk’s algorithm achieves an uncommon approximation\r\nfactor of O(log log d).\r\nPartial match can be reduced to 3-approximate near neighbor in `∞, by applying the\r\nfollowing transformation to each coordinate of the query: 0 7→ −1\r\n2\r\n; ? 7→ 1\r\n2\r\n; 1 7→ 3\r\n2\r\n. Thus, it\r\nis likely that efficient solutions are only possible for approximation c ≥ 3.\r\nApplications of `∞\r\nFor some applications, especially when coordinates are rather heterogeneous, `∞ may be a\r\nnatural choice for a similarity metric. If the features represented by coordinates are hard\r\nto relate, it is hard to add up their differences numerically, in the sense of `1 or `2 (the\r\n“comparing apples to oranges” phenomenon). One popular proposal is to convert each\r\ncoordinate to rank space, and use the maximum rank difference as an indication of similarity.\r\nSee for example [42].\r\nHowever, the most compelling reasons for studying `∞ are extroverted, stemming from\r\nits importance in a theoretical understanding of other problems. For example, many NNS\r\nproblems under various metrics have been reduced to NNS under `∞ via embeddings (maps\r\nthat preserve distances up to some distortion). A well-known result of Matouˇsek states that\r\nany metric on n points can be embedded into `∞ with dimension d = O(cn1/c log n) and\r\ndistortion 2c − 1. In particular, if we allow dimension n, the embedding can be isometric\r\n(no distortion). Of course, this general guarantee on the dimension is too high for many\r\napplications, but it suggests that `∞ is a very good target space for trying to embed some\r\nparticular metric more efficiently.\r\nEarly embeddings into `∞ with interesting dimension included various results for Haus\u0002dorff metrics [43], embedding tree metrics into dimension O(log n) [68], and planar graphs\r\nmetrics into dimension O(log n) [66] (improving over [91]).\r\nMore recently, embeddings have been found into generalizations of `∞, namely product\r\nspaces. For a metric M, the max-product over k copies of M is the space Mk with the\r\ndistance function d∞,M(x, y) = maxk\r\ni=1 dM(xi\r\n, yi), where x, y ∈ Mk. Indyk [62] has extended\r\nhis original NNS algorithm from `∞ to max-product spaces, thus an algorithm for the Frechet\r\nmetric.\r\nIn current research, it was shown by Andoni, Indyk, and Krauthgamer [15] that algo\u0002rithms for max-product spaces yield (via a detour through sum-product spaces) interesting\r\nupper bounds for the Ulam metric and the Earth-Mover Distance. By embedding these met\u0002rics into (iterated) sum-products, one can achieve approximations that are provably smaller\r\nthan the best possible embeddings into `1 or `2.\r\n45",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/af122e48-f35b-4ec1-a50b-83b416741b3a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eb4d0553ca0077e30716f49e8df6797cf20067ced21b5ec5c847fac7b372eca8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "af122e48-f35b-4ec1-a50b-83b416741b3a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 45,
            "page_width": 612,
            "page_height": 792,
            "content": "in our understanding of high-dimensional nearest neighbor search in `1 and `2 spaces, `∞\r\nremains the odd-man out, with a much less understood, and intriguingly different structure.\r\nIn fact, there is precisely one data structure for with provable theoretical guarantees\r\nfor NNS in `∞. In FOCS’98, Indyk [61] proved the following unorthodox result: there is\r\na data structure (in fact, a decision tree) of size O(n\r\nρ\r\n), for any ρ > 1, which achieves ap\u0002proximation 4dlogρ\r\nlog 4de + 1 for NNS in the d-dimensional `∞ metric. The query time is\r\nO(d · polylog n). For 3-approximation, Indyk can achieve space n\r\nlog d+1. Note that in the im\u0002portant regime of polynomial space, Indyk’s algorithm achieves an uncommon approximation\r\nfactor of O(log log d).\r\nPartial match can be reduced to 3-approximate near neighbor in `∞, by applying the\r\nfollowing transformation to each coordinate of the query: 0 7→ −1\r\n2\r\n; ? 7→ 1\r\n2\r\n; 1 7→ 3\r\n2\r\n. Thus, it\r\nis likely that efficient solutions are only possible for approximation c ≥ 3.\r\nApplications of `∞\r\nFor some applications, especially when coordinates are rather heterogeneous, `∞ may be a\r\nnatural choice for a similarity metric. If the features represented by coordinates are hard\r\nto relate, it is hard to add up their differences numerically, in the sense of `1 or `2 (the\r\n“comparing apples to oranges” phenomenon). One popular proposal is to convert each\r\ncoordinate to rank space, and use the maximum rank difference as an indication of similarity.\r\nSee for example [42].\r\nHowever, the most compelling reasons for studying `∞ are extroverted, stemming from\r\nits importance in a theoretical understanding of other problems. For example, many NNS\r\nproblems under various metrics have been reduced to NNS under `∞ via embeddings (maps\r\nthat preserve distances up to some distortion). A well-known result of Matouˇsek states that\r\nany metric on n points can be embedded into `∞ with dimension d = O(cn1/c log n) and\r\ndistortion 2c − 1. In particular, if we allow dimension n, the embedding can be isometric\r\n(no distortion). Of course, this general guarantee on the dimension is too high for many\r\napplications, but it suggests that `∞ is a very good target space for trying to embed some\r\nparticular metric more efficiently.\r\nEarly embeddings into `∞ with interesting dimension included various results for Haus\u0002dorff metrics [43], embedding tree metrics into dimension O(log n) [68], and planar graphs\r\nmetrics into dimension O(log n) [66] (improving over [91]).\r\nMore recently, embeddings have been found into generalizations of `∞, namely product\r\nspaces. For a metric M, the max-product over k copies of M is the space Mk with the\r\ndistance function d∞,M(x, y) = maxk\r\ni=1 dM(xi\r\n, yi), where x, y ∈ Mk. Indyk [62] has extended\r\nhis original NNS algorithm from `∞ to max-product spaces, thus an algorithm for the Frechet\r\nmetric.\r\nIn current research, it was shown by Andoni, Indyk, and Krauthgamer [15] that algo\u0002rithms for max-product spaces yield (via a detour through sum-product spaces) interesting\r\nupper bounds for the Ulam metric and the Earth-Mover Distance. By embedding these met\u0002rics into (iterated) sum-products, one can achieve approximations that are provably smaller\r\nthan the best possible embeddings into `1 or `2.\r\n45",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/af122e48-f35b-4ec1-a50b-83b416741b3a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eb4d0553ca0077e30716f49e8df6797cf20067ced21b5ec5c847fac7b372eca8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "d029740d-fa22-4b81-a1f9-d5a04e41e9d9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 46,
            "page_width": 612,
            "page_height": 792,
            "content": "Thus, the bottleneck in some of the best current algorithms for Frechet, Ulam and EMD\r\nmetrics is the `∞ metric. In particular, Indyk’s unusual log-logarithmic approximation for\r\npolynomial space carries over to these metrics.\r\nOur Results\r\nThe unusual structure of `∞ NNS (as evidenced by an uncommon approximation result)\r\nand the current developments leading to interesting applications plead for a better under\u0002standing of the problem. The reduction from partial match to NNS with approximation\r\nbetter than 3 does not explain the most interesting feature of the problem, namely the\r\nspace/approximation trade-off. It also leaves open the most interesting possibility: a con\u0002stant factor approximation with polynomial space. (It appears, in fact, that researchers were\r\noptimistic about such an upper bound being achievable [60].)\r\nIn our joint work with Alex Andoni and Dorian Croitoru [13], presented in Chapter 8,\r\nwe show the following lower bound for the asymmetric communication complexity of c\u0002approximate NNS in `∞:\r\nTheorem 2.11. Assume Alice holds a point q, and Bob holds a database D of n points in\r\nd dimensions. Fix δ > 0, and assume d satisfies Ω(lg1+δ n) ≤ d ≤ o(n). For any ρ > 10,\r\ndefine c = Θ(O(logρlog2 d) > 3.\r\nIn any deterministic protocol solving the c-approximate near neighbor problem in `∞,\r\neither Alice sends Ω(ρ lg n) bits, or Bob sends Ω(n\r\n1−δ\r\n) bits.\r\nOur lower bound stems from a new information-theoretic understanding of Indyk’s algo\u0002rithm, also presented in Chapter 8. We feel that this conceptual change allows for a cleared\r\nexplanation of the algorithm, which may be of independent interest.\r\nNote that our lower bound is tight in the communication model, by Indyk’s algorithm.\r\nAs usual, the communication bound implies that for data structures with constant cell-probe\r\ncomplexity, as well as for decision trees of depth O(n\r\n1−δ\r\n), the space must be n\r\nΩ(ρ)\r\n. Since\r\nIndyk’s result is a deterministic decision tree with depth O(d · polylog n), we obtain an\r\noptimal trade-off between space and approximation, at least in the decision tree model.\r\nThis suggests that Indyk’s unusual space/approximation trade-off, in particular the\r\nΘ(lg lg d) approximation with polynomial space, is in fact inherent to NNS in `∞.\r\n46",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/d029740d-fa22-4b81-a1f9-d5a04e41e9d9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2a78be361c406e91edf0f740bcece3850cc36f79fecd6b4a4b08bf4094ace3a7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 364
      },
      {
        "segments": [
          {
            "segment_id": "e034a499-586d-467d-b594-dfea76e2f4f7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 47,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 3\r\nDynamic Ω(lg n) Bounds\r\nIn this chapter, we prove our first lower bounds: we show that the partial sums and dynamic\r\nconnectivity problems require Ω(lg n) time per operation. We introduce the proof technique\r\nusing the partial sums problem; dynamic connectivity requires two additional tricks, de\u0002scribed in §3.6 and §3.7. The proofs are surprisingly simple and clean, in contrast to the\r\nfact that proving any Ω(lg n) bound was a well-known open problem for 15 years before our\r\npaper [86].\r\n3.1 Partial Sums: The Hard Instance\r\nIt will pay to consider the partial sums problem in a more abstract setting, namely over\r\nan arbitrary group G. Remember that the problem asked to maintain an array A[1 . . n],\r\ninitialized to zeroes (the group identity), under the following operations:\r\nupdate(k, ∆): modify A[k] ← ∆, where ∆ ∈ G.\r\nsum(k): returns the partial sum Pk\r\ni=1 A[i].\r\nOur proof works for any choice of G. In the cell-probe model with w-bit cells, the most\r\nnatural choice of G is Z/2\r\nwZ, i.e. integer arithmetic modulo 2w. In this case, the argument\r\n∆ of an update is a machine word. Letting δ = lg |G|, our proof will show that any data\r\nstructure requires an average running time of Ω( δ\r\nw\r\n·n lg n) to execute a sequence of n updates\r\nand n queries chosen from a particular distribution. If δ = w, we obtain an amortized Ω(lg n)\r\nbound per operation.\r\nThe hard instance is described by a permutation π of size n, and a sequence h∆1, . . . , ∆ni ∈\r\nGn. Each ∆iis chosen independently and uniformly at random from G; we defer the choice\r\nof π until later. For t from 1 to n, the hard instance issues two operations: the query\r\nsum(π(t)), followed by update(π(t), ∆t). We call t the “time,” saying, for instance, that\r\nsum(π(t)) occurs at time t.\r\nA very useful visualization of an instance is as a two-dimensional chart, with time on one\r\naxis, and the index in A on the other axis. The answer to a query sum(π(t)) is the sum of\r\nthe update points in the rectangle [0, t] × [0, π(t)]; these are the updates which have already\r\noccurred, and affect indices relevant to the partial sum. See Figure 3-1 (a).\r\n47",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e034a499-586d-467d-b594-dfea76e2f4f7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=80b888c8a8a8b677b928da007d14a798ac6536133abf7ba59fe0e627a4e51170",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 386
      },
      {
        "segments": [
          {
            "segment_id": "e9b33ca2-b9cf-4593-b0c4-34f1996ad46c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 48,
            "page_width": 612,
            "page_height": 792,
            "content": "(a)\r\ntime\r\nrank update(1, ∆1)\r\nsum(1)\r\nupdate(5, ∆2)\r\nsum(5)\r\nupdate(3, ∆3)\r\nsum(3)\r\nupdate(7, ∆4)\r\nsum(7)\r\nupdate(2, ∆5)\r\nsum(2)\r\nupdate(6, ∆6)\r\nsum(6)\r\nupdate(4, ∆7)\r\nsum(4)\r\nupdate(8, ∆8)\r\nsum(8)\r\n(b)\r\ntime\r\nt0\r\nt1\r\nt2\r\nld R1, Mem[34]\r\nst R2, Mem[41]\r\nst R3, Mem[34]\r\nst R3, Mem[41]\r\nst R9, Mem[74]\r\nld R7, Mem[34]\r\nst R1, Mem[41]\r\nst R2, Mem[21]\r\nld R1, Mem[34]\r\nld R2, Mem[74]\r\nst R5, Mem[21]\r\nld R4, Mem[41]\r\nld R1, Mem[34]\r\nst R5, Mem[34]\r\nFigure 3-1: (a) An instance of the partial sums problem. The query sum(6) occurring at\r\ntime 6 has the answer ∆1+∆2+∆3+∆5. (b) The execution of a hypothetical cell-probe\r\nalgorithm. IT(t0, t1, t2) consists of cells 41 and 74.\r\n3.2 Information Transfer\r\nLet t0 < t1 < t2, where t0 and t2 are valid time values, and t1 is non-integral to avoid ties.\r\nThese time stamps define two adjacent intervals of operations: the time intervals [t0, t1] and\r\n[t1, t2]; we will be preoccupied by the interaction between these time intervals. Since the\r\nalgorithm cannot maintain state between operations, such interaction can only be caused by\r\nthe algorithm writing a cell during the first interval and reading it during the second.\r\nDefinition 3.1. The information transfer IT(t0, t1, t2) is the set of memory locations which:\r\n• were read at a time tr ∈ [t1, t2].\r\n• were written at a time tw ∈ [t0, t1], and not overwritten during [tw + 1, tr].\r\nThe definition is illustrated in Figure 3-1 (b). Observe that the information transfer is a\r\nfunction of the algorithm, the permutation π, and the sequence ∆.\r\nFor now, let us concentrate on bounding |IT(t0, t1, t2)|, ignoring the question of how\r\nthis might be useful. Intuitively, any dependence of the queries from [t1, t2] on updates\r\nfrom the interval [t0, t1] must come from the information in the cells IT(t0, t1, t2). Indeed,\r\nIT(t0, t1, t2) captures the only possible information flow between the intervals: an update\r\nhappening during [t0, t1] cannot be reflected in a cell written before time t0.\r\nLet us formalize this intuition. We break the random sequence \n∆1, . . . , ∆n\r\n\u000b\r\ninto the\r\nsequence ∆[t0,t1] =\r\n\n\r\n∆t0, . . . , ∆bt1c\r\n\u000b\r\n, and ∆?containing all other values. The values in ∆?\r\nare uninteresting to our analysis, so fix them to some arbitrary ∆?. Let At be the answer\r\nof the query sum(π(t)) at time t. We write A[t1,t2] =\r\n\n\r\nAdt1e, . . . , At2\r\n\u000b\r\nfor the answers to the\r\nqueries in the second interval.\r\nIn information theoretic terms, the observation that all dependence of the interval [t1, t2]\r\non the interval [t0, t1] is captured by the information transfer, can be reformulated as saying\r\n48",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e9b33ca2-b9cf-4593-b0c4-34f1996ad46c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=42dcf98faa2ca26d083021fbe23e78565fa992d75e3fdc5bfbaa1f7cd9798ceb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 450
      },
      {
        "segments": [
          {
            "segment_id": "027cdfdb-0851-410a-bb4a-5a1df558fbf4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 49,
            "page_width": 612,
            "page_height": 792,
            "content": "that the entropy of the observable outputs of interval [t1, t2] (i.e., the query results A[t1,t2])\r\nis bounded by the information transfer:\r\nLemma 3.2. H\r\n\r\nA[t1,t2]\r\n\f\r\n\f ∆\r\n? = ∆?\r\n\u0001\r\n≤ w + 2w · E\r\n\u0002\r\n|IT(t0, t1, t2)|\r\n\f\r\n\f ∆\r\n? = ∆?\r\n\u0003\r\n.\r\nProof. The bound follows by proposing an encoding for A[t1,t2], since the entropy is upper\r\nbounded by the average length of any encoding. Our encoding is essentially the information\r\ntransfer; formally, it stores:\r\n• first, the cardinality |IT(t0, t1, t2)|, in order to make the encoding prefix free.\r\n• the address of each cell; an address is at most w bits in our model.\r\n• the contents of each cell at time t1, which takes w bits per cell.\r\nThe average length of the encoding is w + 2w · E\r\n\u0002\r\n|IT(t0, t1, t2)|\r\n\f\r\n\f ∆? = ∆?\r\n\u0003\r\nbits, as needed.\r\nTo finish the proof, we must show that the information transfer actually encodes A[t1,t2]; that\r\nis, we must give a decoding algorithm that recovers A[t1,t2]from IT(t0, t1, t2).\r\nOur decoding algorithm begins by simulating the data structure during the time period\r\n[1, t0 − 1]; this is possible because ∆?is fixed, so all operations before time t0 are known. It\r\nthen skips the time period [t0, t1], and simulates the data structure again during the time\r\nperiod [t1, t2]. Of course, simulating the time period [t1, t2] recovers the answers A[t1,t2], which\r\nis what we wanted to do.\r\nTo see why it is possible to simulate [t1, t2], consider a read instruction executed by a\r\ndata structure operation during [t1, t2]. Depending on the time tw when the cell was last\r\nwritten, we have the following cases:\r\ntw > t1: We can recognize this case by maintaining a list of memory locations written during\r\nthe simulation; the data is immediately available.\r\nt0 < tw < t1: We can recognize this case by examining the set of addresses in the encoding;\r\nthe cell contents can be read from the encoding.\r\ntw < t0: This is the default case, if the cell doesn’t satisfy the previous conditions. The\r\ncontents of the cell is determined from the state of the memory upon finishing the first\r\nsimulation up to time t0 − 1.\r\n3.3 Interleaves\r\nIn the previous section, we showed an upper bound on the dependence of [t1, t2] on [t0, t1];\r\nwe now aim to give a lower bound. Refer to the example in Figure 3-2 (a). The information\r\nthat the queries in [t1, t2] need to know about the updates in [t0, t1] is the sequence \n∆6, ∆6+\r\n∆3 + ∆4, ∆6 + ∆3 + ∆4 + ∆5\r\n\u000b\r\n. Equivalently, the queries need to know \n∆6, ∆3 + ∆4, ∆5\r\n\u000b\r\n,\r\nwhich are three independent random variables, uniformly distributed in the group G.\r\nThis required information comes from interleaves between the update indices in [t0, t1],\r\non the one hand, and the query indices in [t1, t2], on the other. See Figure 3-2 (b).\r\nDefinition 3.3. If one sorts the set {π(t0), . . . , π(t2)}, the interleave number IL(t0, t1, t2)\r\nis defined as the number of transitions between a value π(i) with i < t1, and a consecutive\r\nvalue π(j) with j > t1.\r\n49",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/027cdfdb-0851-410a-bb4a-5a1df558fbf4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4e806e22ace6bd724bf251310b5872c35bca0df28ea5f438eef94bc38c735da8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "027cdfdb-0851-410a-bb4a-5a1df558fbf4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 49,
            "page_width": 612,
            "page_height": 792,
            "content": "that the entropy of the observable outputs of interval [t1, t2] (i.e., the query results A[t1,t2])\r\nis bounded by the information transfer:\r\nLemma 3.2. H\r\n\r\nA[t1,t2]\r\n\f\r\n\f ∆\r\n? = ∆?\r\n\u0001\r\n≤ w + 2w · E\r\n\u0002\r\n|IT(t0, t1, t2)|\r\n\f\r\n\f ∆\r\n? = ∆?\r\n\u0003\r\n.\r\nProof. The bound follows by proposing an encoding for A[t1,t2], since the entropy is upper\r\nbounded by the average length of any encoding. Our encoding is essentially the information\r\ntransfer; formally, it stores:\r\n• first, the cardinality |IT(t0, t1, t2)|, in order to make the encoding prefix free.\r\n• the address of each cell; an address is at most w bits in our model.\r\n• the contents of each cell at time t1, which takes w bits per cell.\r\nThe average length of the encoding is w + 2w · E\r\n\u0002\r\n|IT(t0, t1, t2)|\r\n\f\r\n\f ∆? = ∆?\r\n\u0003\r\nbits, as needed.\r\nTo finish the proof, we must show that the information transfer actually encodes A[t1,t2]; that\r\nis, we must give a decoding algorithm that recovers A[t1,t2]from IT(t0, t1, t2).\r\nOur decoding algorithm begins by simulating the data structure during the time period\r\n[1, t0 − 1]; this is possible because ∆?is fixed, so all operations before time t0 are known. It\r\nthen skips the time period [t0, t1], and simulates the data structure again during the time\r\nperiod [t1, t2]. Of course, simulating the time period [t1, t2] recovers the answers A[t1,t2], which\r\nis what we wanted to do.\r\nTo see why it is possible to simulate [t1, t2], consider a read instruction executed by a\r\ndata structure operation during [t1, t2]. Depending on the time tw when the cell was last\r\nwritten, we have the following cases:\r\ntw > t1: We can recognize this case by maintaining a list of memory locations written during\r\nthe simulation; the data is immediately available.\r\nt0 < tw < t1: We can recognize this case by examining the set of addresses in the encoding;\r\nthe cell contents can be read from the encoding.\r\ntw < t0: This is the default case, if the cell doesn’t satisfy the previous conditions. The\r\ncontents of the cell is determined from the state of the memory upon finishing the first\r\nsimulation up to time t0 − 1.\r\n3.3 Interleaves\r\nIn the previous section, we showed an upper bound on the dependence of [t1, t2] on [t0, t1];\r\nwe now aim to give a lower bound. Refer to the example in Figure 3-2 (a). The information\r\nthat the queries in [t1, t2] need to know about the updates in [t0, t1] is the sequence \n∆6, ∆6+\r\n∆3 + ∆4, ∆6 + ∆3 + ∆4 + ∆5\r\n\u000b\r\n. Equivalently, the queries need to know \n∆6, ∆3 + ∆4, ∆5\r\n\u000b\r\n,\r\nwhich are three independent random variables, uniformly distributed in the group G.\r\nThis required information comes from interleaves between the update indices in [t0, t1],\r\non the one hand, and the query indices in [t1, t2], on the other. See Figure 3-2 (b).\r\nDefinition 3.3. If one sorts the set {π(t0), . . . , π(t2)}, the interleave number IL(t0, t1, t2)\r\nis defined as the number of transitions between a value π(i) with i < t1, and a consecutive\r\nvalue π(j) with j > t1.\r\n49",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/027cdfdb-0851-410a-bb4a-5a1df558fbf4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4e806e22ace6bd724bf251310b5872c35bca0df28ea5f438eef94bc38c735da8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "1b9e7374-b06a-4f67-af3f-0a6c7d930560",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 50,
            "page_width": 612,
            "page_height": 792,
            "content": "(a)\r\nt0\r\nt1\r\nt2\r\n∆1 ∆2\r\n∆3\r\n∆4\r\n∆5\r\n∆6\r\n∆7\r\n∆8\r\n∆9 ∆10\r\n∆11\r\n∆12\r\n(b)\r\nt0\r\nt1\r\nt2\r\nFigure 3-2: (a) The vertical lines describe the information that the queries in [t1, t2] from\r\nthe updates in [t0, t1]. (b) The interleave number IL(t0, t1, t2) is the number of down arrows\r\ncrossing t1, where arrows indicate left-to-right order.\r\nThe interleave number is only a function of π. Figure 3-2 suggests that interleaves\r\nbetween two intervals cause a large dependence of the queries A[t1,t2] on the updates ∆[t1,t2],\r\ni.e. A[t1,t2] has large conditional entropy, even if all updates outside ∆[t1,t2] are fixed:\r\nLemma 3.4. H\r\n\r\nA[t1,t2]\r\n\f\r\n\f ∆\r\n? = ∆?\r\n\u0001\r\n= δ · IL(t0, t1, t2).\r\nProof. Each answer in A[t1,t2]is a sum of some random variables from ∆[t0,t1], plus a constant\r\nthat depends on the fixed ∆?. Consider the indices L = {π(t0), . . . , π(bt1c)} from the first\r\ninterval, and R = {π(dt1e), . . . , π(t2)} from the second interval. Relabel the indices of R as\r\nr1 < r2 < · · · and consider these ri’s in order:\r\n• If L ∩ [ri−1, ri] = ∅, the answer to sum(ri) is the same as for sum(ri−1), except for a\r\ndifferent constant term. The answer to sum(ri) contributes nothing to the entropy.\r\n• Otherwise, the answer to sum(ri) is a random variable independent of all previous\r\nanswers, due to the addition of random ∆’s to indices L ∩ [ri−1, ri]. This random\r\nvariable is uniformly distributed in G, so it contributes δ bits of entropy.\r\nComparing Lemmas 3.4 and 3.2, we see that E\r\n\u0002\r\n|IT(t0, t1, t2)|\r\n\f\r\n\f ∆? = ∆?\r\n\u0003\r\n≥\r\nδ\r\n2w\r\n·\r\nIL(t0, t1, t2) − 1 for any fixed ∆?. By taking expectation over ∆?, we have:\r\nCorollary 3.5. For any fixed π, t0 < t1 < t2, and any algorithm solving the partial sums\r\nproblem, we have E∆\r\n\u0002\r\n|IT(t0, t1, t2)|\r\n\u0003\r\n≥\r\nδ\r\n2w\r\n· IL(t0, t1, t2) − 1.\r\n3.4 A Tree For The Lower Bound\r\nThe final step of the algorithm is to consider the information transfer between many pairs\r\nof intervals, and piece together the lower bounds from Corollary 3.5 into one lower bound\r\n50",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/1b9e7374-b06a-4f67-af3f-0a6c7d930560.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ee5f1f15342b22a8abb9d6f697f0695f7275b9729b912d6eb2587d41b4e3c778",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 375
      },
      {
        "segments": [
          {
            "segment_id": "ef926f98-679b-41f0-bace-f75f184cf1ff",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 51,
            "page_width": 612,
            "page_height": 792,
            "content": "(a) t0\r\nt1\r\nt2\r\n(b)\r\nt0\r\nt1\r\nt2\r\nFigure 3-3: The bit-reversal permutation of size n = 16, and the lower-bound tree over the\r\ntime axis. Each node has a maximal possible interleave: e.g. 2 in (a), and 4 in (b).\r\nfor the total running time of the data structure. The main trick for putting together these\r\nlower bounds is to consider a lower-bound tree T : an arbitrary binary tree with n leaves,\r\nwhere each leaf denotes a time unit (a query and update pair). In other words, T is built\r\n“over the time axis,” as in Figure 3-3.\r\nFor each internal node v of T , we consider the time interval [t0, t1] spanning the left\r\nsubtree, and the interval [t1, t2] spanning the right subtree. We then define:\r\n• the information transfer through the node: IT(v) = |IT(t0, t1, t2)|. Essentially, IT(v)\r\ncounts the cells written in the left subtree of v, and read in the right subtree.\r\n• the interleave at the node: IL(v) = IL(t0, t1, t2).\r\nTheorem 3.6. For any algorithm and fixed π, the expected running time of the algorithm\r\nover a random sequence ∆ is at least δ\r\n2w\r\nP\r\nv∈T IL(v) − n.\r\nProof. First, observe that on any problem instance (any fixed ∆), the number of read instruc\u0002tions executed by the algorithm is at least P\r\nv∈T IT(v). Indeed, for each read instruction,\r\nlet tr be the time it is executed, and tw ≤ tr be the time when the cell was last written.\r\nIf tr = tw, we can ignore this trivial read. Otherwise, this read instruction appears in the\r\ninformation transfer through exactly one node: the lowest common ancestor of tw and tr.\r\nThus, P\r\nv\r\nIT(v) never double-counts a read instruction.\r\nNow we apply Corollary 3.5 to each node, concluding that for each v, E∆[IT(v)] ≥\r\nδ\r\n2w\r\n· IL(v) − 1. Thus, the total expected running time is at least δ\r\n2w\r\nP\r\nv\r\nIL(v) − (n − 1). It\r\nis important to note that each lower bound for |IT(v)| applies to the expected value under\r\nthe same distribution (a uniformly random sequence ∆). Thus we may sum up these lower\r\nbounds to get a lower bound on the entire running time, using linearity of expectation.\r\nTo complete our lower bound, it remains to design an access sequence π that has high\r\ntotal interleave, P\r\nv∈T IL(v) = Ω(n lg n), for some lower-bound tree T . From now on, assume\r\nn is a power of 2, and let T be a perfect binary tree.\r\n51",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/ef926f98-679b-41f0-bace-f75f184cf1ff.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=80cb15785bade5152ad3b289e17aba3557bdbe961794871fce3951f86d5c2099",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 430
      },
      {
        "segments": [
          {
            "segment_id": "b275e20b-4ad6-4949-82d8-3c94ff6f684f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 52,
            "page_width": 612,
            "page_height": 792,
            "content": "Claim 3.7. If π is a uniformly random permutation, Eπ\r\n\u0002P\r\nv∈T IL(v)\r\n\u0003\r\n= Ω(n lg n).\r\nProof. Consider a node v with 2k leaves in its subtree, and let S be the set of indices touched\r\nin v’s subtree, i.e. S = {π(t0), . . . , π(t2)}. The interleave at v is the number of down arrows\r\ncrossing from the left subtree to the right subtree, when S is sorted; see Figure 3-2 (b) and\r\nFigure 3-3. For two indices j1 < j2 that are consecutive in S, the probability that j1 is\r\ntouched in the left subtree, and j2 is touched in the right subtree will be k\r\n2k\r\n·\r\nk\r\n2k−1 >\r\n1\r\n4\r\n. By\r\nlinearity of expectation over the 2k−1 arrows, Eπ[IL(v)] = (2k−1)·\r\nk\r\n2k\r\n·\r\nk\r\n2k−1 =\r\nk\r\n2\r\n. Summing\r\nup over all internal nodes v gives Eπ\r\n\u0002P\r\nv\r\nIL(v)\r\n\u0003\r\n=\r\n1\r\n4\r\nn log2 n.\r\nThus, any algorithm requires Ω( δ\r\nw\r\n·n lg n) cell probes in expectation on problem instances\r\ngiven by random ∆ and random π. This shows our Ω(lg n) amortized lower bound for δ = w.\r\n3.5 The Bit-Reversal Permutation\r\nAn interesting alternative to choosing π randomly, is to design a worst-case π that maximizes\r\nthe total interleave P\r\nv∈T IL(v). We construct π recursively. Assume π\r\n0\r\nis the worst-case\r\npermutation of size n. Then, we shuffle two copies of π\r\n0\r\nto get a permutation π of size 2n.\r\nFormally:\r\nπ =\r\n\n\r\n2π\r\n0\r\n(1) − 1, · · · , 2π\r\n0\r\n(n) − 1, 2π\r\n0\r\n(1), · · · , 2π\r\n0\r\n(n)\r\n\u000b\r\nThe two halves interleave perfectly, giving an interleave at the root equal to n. The order\r\nin each half of π is the same as π\r\n0\r\n. Thus, by the recursive construction, each node with\r\nP\r\n2k leaves in its subtree has a perfect interleave of k. Summing over all internal nodes,\r\nv\r\nIL(v) = 1\r\n2\r\nn log2 n. Refer to Figure 3-3 for an example with n = 16, and an illustration\r\nof the perfect interleave at each node.\r\nThe permutation that we have just constructed is the rather famous bit-reversal permu\u0002tation. Subtracting 1 from every index, we get a permutation of the elements {0, . . . , n − 1}\r\nwhich is easy to describe: the value π(i) is the number formed by reversing the lg n bits of\r\ni. To see this connection, consider the recursive definition of π: the first half of the values\r\n(most significant bit of i is zero) are even (least significant bit of π(i) is zero); the second\r\nhalf (most significant bit of i is one) are odd (least significant bit of π(i) is one). Recursively,\r\nall bits of i except the most significant one appear in π\r\n0\r\nin reverse order.\r\nDuality of upper and lower bounds. An important theme of this thesis is the idea that\r\na good lower bound should be a natural dual of the best upper bound. The standard upper\r\nbound for the partial-sums problem is a balanced binary search tree with the array A[1 . . n]\r\nin its leaves. Every internal node is augmented to store the sum of all leaves in its subtree.\r\nAn update recomputes all values on the leaf-to-root path, while a query sums left children\r\nhanging from the root-to-leaf path.\r\nThinking from a lower bound perspective, we can ask when an update (a cell write) to\r\nsome node v is “actually useful.” If v is a left child, the value it stores is used for any\r\nquery that lies in the subtree of its right sibling. A write to v is useful if a query to the\r\nsibling’s subtree occurs before another update recomputes v’s value. In other words, a write\r\n52",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b275e20b-4ad6-4949-82d8-3c94ff6f684f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=57a31bfb2b94647fac9829034d7b454c0e569abf4bfb037fd861f9144b1342c8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 641
      },
      {
        "segments": [
          {
            "segment_id": "b275e20b-4ad6-4949-82d8-3c94ff6f684f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 52,
            "page_width": 612,
            "page_height": 792,
            "content": "Claim 3.7. If π is a uniformly random permutation, Eπ\r\n\u0002P\r\nv∈T IL(v)\r\n\u0003\r\n= Ω(n lg n).\r\nProof. Consider a node v with 2k leaves in its subtree, and let S be the set of indices touched\r\nin v’s subtree, i.e. S = {π(t0), . . . , π(t2)}. The interleave at v is the number of down arrows\r\ncrossing from the left subtree to the right subtree, when S is sorted; see Figure 3-2 (b) and\r\nFigure 3-3. For two indices j1 < j2 that are consecutive in S, the probability that j1 is\r\ntouched in the left subtree, and j2 is touched in the right subtree will be k\r\n2k\r\n·\r\nk\r\n2k−1 >\r\n1\r\n4\r\n. By\r\nlinearity of expectation over the 2k−1 arrows, Eπ[IL(v)] = (2k−1)·\r\nk\r\n2k\r\n·\r\nk\r\n2k−1 =\r\nk\r\n2\r\n. Summing\r\nup over all internal nodes v gives Eπ\r\n\u0002P\r\nv\r\nIL(v)\r\n\u0003\r\n=\r\n1\r\n4\r\nn log2 n.\r\nThus, any algorithm requires Ω( δ\r\nw\r\n·n lg n) cell probes in expectation on problem instances\r\ngiven by random ∆ and random π. This shows our Ω(lg n) amortized lower bound for δ = w.\r\n3.5 The Bit-Reversal Permutation\r\nAn interesting alternative to choosing π randomly, is to design a worst-case π that maximizes\r\nthe total interleave P\r\nv∈T IL(v). We construct π recursively. Assume π\r\n0\r\nis the worst-case\r\npermutation of size n. Then, we shuffle two copies of π\r\n0\r\nto get a permutation π of size 2n.\r\nFormally:\r\nπ =\r\n\n\r\n2π\r\n0\r\n(1) − 1, · · · , 2π\r\n0\r\n(n) − 1, 2π\r\n0\r\n(1), · · · , 2π\r\n0\r\n(n)\r\n\u000b\r\nThe two halves interleave perfectly, giving an interleave at the root equal to n. The order\r\nin each half of π is the same as π\r\n0\r\n. Thus, by the recursive construction, each node with\r\nP\r\n2k leaves in its subtree has a perfect interleave of k. Summing over all internal nodes,\r\nv\r\nIL(v) = 1\r\n2\r\nn log2 n. Refer to Figure 3-3 for an example with n = 16, and an illustration\r\nof the perfect interleave at each node.\r\nThe permutation that we have just constructed is the rather famous bit-reversal permu\u0002tation. Subtracting 1 from every index, we get a permutation of the elements {0, . . . , n − 1}\r\nwhich is easy to describe: the value π(i) is the number formed by reversing the lg n bits of\r\ni. To see this connection, consider the recursive definition of π: the first half of the values\r\n(most significant bit of i is zero) are even (least significant bit of π(i) is zero); the second\r\nhalf (most significant bit of i is one) are odd (least significant bit of π(i) is one). Recursively,\r\nall bits of i except the most significant one appear in π\r\n0\r\nin reverse order.\r\nDuality of upper and lower bounds. An important theme of this thesis is the idea that\r\na good lower bound should be a natural dual of the best upper bound. The standard upper\r\nbound for the partial-sums problem is a balanced binary search tree with the array A[1 . . n]\r\nin its leaves. Every internal node is augmented to store the sum of all leaves in its subtree.\r\nAn update recomputes all values on the leaf-to-root path, while a query sums left children\r\nhanging from the root-to-leaf path.\r\nThinking from a lower bound perspective, we can ask when an update (a cell write) to\r\nsome node v is “actually useful.” If v is a left child, the value it stores is used for any\r\nquery that lies in the subtree of its right sibling. A write to v is useful if a query to the\r\nsibling’s subtree occurs before another update recomputes v’s value. In other words, a write\r\n52",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b275e20b-4ad6-4949-82d8-3c94ff6f684f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=57a31bfb2b94647fac9829034d7b454c0e569abf4bfb037fd861f9144b1342c8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 641
      },
      {
        "segments": [
          {
            "segment_id": "f30f2a4b-a8df-42f3-820b-cc06ed4eefe7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 53,
            "page_width": 612,
            "page_height": 792,
            "content": "time\r\nA[1] A[2] A[3] A[4] A[5] A[6] A[7] A[8] A[9] A[10]A[11]A[12]A[13]A[14]A[15]A[16]\r\nv\r\nupdates write v\r\nqueries read v\r\nFigure 3-4: An augmented binary search tree solving a partial-sums instance. The writes to\r\nsome node v are “useful” when interleaves occur in the time-sorted order.\r\ninstruction is useful whenever there is an interleave between v’s left and right subtrees,\r\nsorting operations by time. See Figure 3-4.\r\nIntuitively, the amount of “useful work” that the algorithm does is the sum of the in\u0002terleaves at every node of the binary search tree. To maximize this sum, we can use a\r\nbit-reversal permutation again. Note the bit-reversal permutation is equal to its own inverse\r\n(reversing the bits twice gives the identity); in other words, the permutation is invariant un\u0002der 90-degree rotations. Thus, the lower-bound tree sitting on the time axis counts exactly\r\nthe same interleaves that generate work in the upper bound.\r\n3.6 Dynamic Connectivity: The Hard Instance\r\nWe now switch gears to dynamic connectivity. Remember that this problem asks to maintain\r\nan undirected graph with a fixed set V of vertices, subject to the following operations:\r\ninsert(u, v): insert an edge (u, v) into the graph.\r\ndelete(u, v): delete the edge (u, v) from the graph.\r\nconnected(u, v): test whether u and v lie in the same connected component.\r\nWe aim to show an amortized lower bound of Ω(lg |V |) per operation.\r\nIt turns out that the problem can be dressed up as an instance of the partial sums\r\nproblem. Let n =\r\n√\r\nV − 1, and consider the partial sums problem over an array A[1 . . n],\r\nwhere each element comes from the group G = S√\r\nV\r\n, the permutation group on √V elements.\r\nWe consider a graph whose vertices form an integer grid of size √\r\nV by √V ; see Figure 3-5.\r\n53",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f30f2a4b-a8df-42f3-820b-cc06ed4eefe7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=493025b2f86ab7caf11e9b2c65b02760d4a44045ec0b59615bf6edddd950043f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 307
      },
      {
        "segments": [
          {
            "segment_id": "bcf63e50-1103-498f-b2b0-9eacd61b4b75",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 54,
            "page_width": 612,
            "page_height": 792,
            "content": "A[1] A[2] A[3] A[4] A[5] A[6]\r\nFigure 3-5: Our hard instance of dynamic connectivity implements the partial-sums problem\r\nover the group S√\r\nV\r\n.\r\nEdges only connect vertices from adjacent columns; the edges between column i and i + 1\r\ndescribe the permutation A[i] from the partial sums problem.\r\nIn other words, the graph is a disjoint union of √\r\nV paths. Each path stretches from\r\ncolumn 1 to column √\r\nV , and the paths are permuted arbitrarily between columns. This has\r\na strong partial-sums flavor: a node at coordinates (1, y1) on the first column is connected to\r\na node (k, y2) on the kth column, if and only if the partial sum permutation A[1]◦· · ·◦A[k−1]\r\nhas y1 going to y2.\r\nGiven our choice of the group G, we have δ = lg (\r\n√\r\nV )!\u0001= Θ√V · lg V\r\n\u0001\r\n. For dy\u0002namic connectivity, we concentrate on the natural word size w = Θ(lg V ), so this group is\r\nrepresented by Θ(√\r\nV ) memory words. Even though these huge group elements may seem\r\nworrisome (compared to the previous setting where each A[i] was a word), notice that nothing\r\nin our proof depended on the relation between δ and w. Our lower bound still holds, and it\r\nimplies that a partial-sums operation requires Ω( δ\r\nw\r\n·lg n) = Ω(\r\n√\r\nV lg V\r\nlg V\r\n·lg √\r\nV ) = Ω(√V ·lg V )\r\ncell probes on average.\r\nObserve that a partial sums update can be implemented by O(\r\n√\r\nV ) dynamic connec\u0002tivity updates: when some A[i] changes, we run √\r\nV delete’s of the old edges between\r\ncolumns i and i + 1, followed by √V insert’s of the new edges. If we could implement sum\r\nusing O(\r\n√\r\nV ) connected queries, we would deduce that the dynamic connectivity problem\r\nrequires a running time of Ω(\r\n√\r\nV ·lg V\r\n√\r\nV\r\n) = Ω(lg V ) per operation.\r\nUnfortunately, it is not clear how to implement sum through few connected queries,\r\nsince connectivity queries have boolean output, whereas sum needs to return a permutation\r\nwith Θ√V ·lg V\r\n\u0001\r\nbits of entropy. To deal with this issue, we introduce a conceptual change\r\nto the partial sums problem, considering a different type of query:\r\nverify-sum(k, σ): test whether sum(k) = σ.\r\nThis query is easy to implement via √\r\nV connectivity queries: for i = 1 to √V , these\r\nqueries test whether the point (1, i) from the first column is connected to point (k, σ(k)) from\r\nthe kth column. This runs a pointwise test of the permutation equality σ = A[1]◦· · ·◦A[k−1].\r\nBelow, we extend our lower bound to partial sums with verify-sum queries:\r\n54",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/bcf63e50-1103-498f-b2b0-9eacd61b4b75.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ed85be3a60f2eab53a04b12695517ccd512d0367fe945013c4e3797bfae57f20",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 458
      },
      {
        "segments": [
          {
            "segment_id": "48bfd821-93e8-469c-82d4-e205129536c2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 55,
            "page_width": 612,
            "page_height": 792,
            "content": "Theorem 3.8. In the cell-probe model with w-bit cells, any data structure requires Ω( δ\r\nw\r\nn lg n)\r\nexpected time to support a sequence of n update and n verify-sum operations, drawn from\r\na certain probability distribution.\r\nBy our construction, this immediately implies that, in the cell-probe model with cells of\r\nΘ(lg V ) bits, dynamic connectivity requires Ω(lg V ) time per operation.\r\n3.7 The Main Trick: Nondeterminism\r\nOur lower bound technique thus far depends crucially on the query answers having high\r\nentropy, which lower bounds the information transfer, by Lemma 3.2. High entropy is natural\r\nfor sum queries, but impossible for verify-sum. To deal with this problem, we augment\r\nour model of computation with nondeterminism, and argue that sum and verify-sum are\r\nequivalent in this stronger model.\r\nTechnically, a nondeterministic cell-probe algorithm is defined as follows. In the be\u0002ginning of each query, an arbitrary number of threads are created. Each thread proceeds\r\nindependently according to the following rules:\r\n1. first, the thread may read some cells.\r\n2. the thread decides whether the accept or reject. Exactly one thread must accept.\r\n3. the accepting thread may now write some cells, and must output the answer.\r\nAn alternative view of our model is that an all-powerful prover reveals the query answer,\r\nand then probes a minimal set of cells sufficient to certify that the answer is correct. We\r\ndefine the running time of the query as the number of cell reads and writes executed by the\r\naccepting1thread.\r\nA deterministic algorithm for verify-sum running in time t immediately implies a non\u0002deterministic algorithm for sum, also running in time t. The algorithm for sum starts by\r\nguessing the correct sum (trying all possibilities in separate threads), and verifying the guess\r\nusing verify-sum. If the guess was wrong, the thread rejects; otherwise, it returns the\r\ncorrect answer.\r\n3.8 Proof of the Nondeterministic Bound\r\nWe will now show that the lower bound for partial sums holds even for nondeterministic\r\ndata structures, implying the same bound for verify-sum queries. The only missing part\r\nof the proof is a new version of Lemma 3.2, which bounds the entropy of (nondeterministic)\r\nqueries in terms of the information transfer.\r\nRemember that in Definition 3.1, we let the information transfer IT(t0, t1, t2) be the set\r\nof cells that were: (1) read at a time tr ∈ [t1, t2]; and (2) written at a time tw ∈ [t0, t1], and\r\n1There is no need to consider rejecting threads here. If we have a bound t on the running time of the\r\naccepting thread, the algorithm may immediately reject after running for time t + 1, since it knows it must\r\nbe running a rejecting thread.\r\n55",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/48bfd821-93e8-469c-82d4-e205129536c2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b5392af4a038b1803ab0f59138584dc2cec620457d41e7b8c8b5e2223a8771ad",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 448
      },
      {
        "segments": [
          {
            "segment_id": "b5d842ab-8edb-4366-aeaa-c09e35aee7c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 56,
            "page_width": 612,
            "page_height": 792,
            "content": "not overwritten during [tw + 1, tr]. Condition 2. remains well defined for nondeterministic\r\nalgorithms, since only the unique accepting thread may write memory cells. For condition\r\n1., we will only look at the reads made by the accepting threads, and ignore the rejecting\r\nthreads.\r\nMore formally, let the accepting execution of a problem instance be the sequence of cell\r\nreads and writes executed by the updates and the accepting threads of each query. Define:\r\nW(t0, t1) : the set of cells written in the accepting execution during time interval [t0, t1].\r\nR(t0, t1) : the set of cells read in the accepting execution during time interval [t0, t1], which\r\nhad last been written at some time tw < t0.\r\nThen, IT(t0, t1, t2) = W(t0, t1) ∩ R(t1, t2). Our replacement for Lemma 3.2 states that:\r\nLemma 3.9. H\r\n\r\nA[t1,t2]\r\n\f\r\n\f ∆\r\n? = ∆?\r\n\u0001\r\n≤ O\r\n\r\nE\r\n\u0002\r\nw·|IT(t0, t1, t2)| + |R(t0, t2)| + |W(t1, t2)|\r\n\f\r\n\f ∆\r\n? =\r\n∆?\r\n\u0003\u0001\r\nNote that this lemma is weaker than the original Lemma 3.2 due to the additional terms\r\ndepending on W(t0, t1) and R(t1, t2). However, these terms are fairly small, adding O(1) bits\r\nof entropy per cell, as opposed to O(w) bits for each cell in the information transfer. This\r\nproperty will prove crucial.\r\nBefore we prove the lemma, we redo the analysis of §3.4, showing that we obtain the\r\nsame bounds for nondeterministic data structures. As before, we consider a lower-bound\r\ntree T , whose n leaves represent time units (query and update pairs). For each internal\r\nnode v of T , let [t0, t1] span the left subtree, and [t1, t2] span the right subtree. We then\r\ndefine IT(v) = |IT(t0, t1, t2)|, W(v) = |W(t0, t1)|, and R(v) = |R(t1, t2)|.\r\nLet T be the total running time of the data structure on a particular instance. As before,\r\nobserve that each cell read in the accepting execution is counted in exactly one IT(v), at\r\nthe lowest common ancestor of the read and write times. Thus, T ≥\r\nP\r\nv∈T IT(v).\r\nFor each node, we compare Lemmas 3.9 and 3.4 to obtain a lower bound in terms of the\r\ninterleave at the node:\r\nE[w · IT(v) + W(v) + R(v)] = Ω(δ · IL(v)) (3.1)\r\nNote that summing up R(v) +W(v) over the nodes on a single level of the tree gives at most\r\nT, because each instruction is counted in at most one node. Thus, summing (3.1) over all\r\nv ∈ T yields: E[w·T + T ·depth(T )] = Ω(δ\r\nP\r\nv\r\nIL(v)). By using the bit-reversal permutation\r\nand letting T be a perfect binary tree, we have P\r\nv\r\nIL(v) = Ω(n lg n), and depth(T ) = lg n.\r\nSince w = Ω(lg n) in our model, the lower bound becomes E[2w·T] = Ω(δ ·n lg n), as desired.\r\nProof of Lemma 3.9. The proof is an encoding argument similar to Lemma 3.2, with one\r\nadditional complication: during decoding, we do not know which thread will accept, and we\r\nmust simulate all of them. Note, however, that the cells read by the rejecting threads are\r\nnot included in the information transfer, and thus we cannot afford to include them in the\r\nencoding. But without these cells, it is not clear how to decode correctly: when simulating\r\na rejecting thread, we may think incorrectly that a cell was not written during [t0, t1]. If we\r\n56",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b5d842ab-8edb-4366-aeaa-c09e35aee7c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b3768234f0a92b1e9c2c3b0c30045b8a74869ea254b44e1f783a10b8c533f492",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 573
      },
      {
        "segments": [
          {
            "segment_id": "b5d842ab-8edb-4366-aeaa-c09e35aee7c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 56,
            "page_width": 612,
            "page_height": 792,
            "content": "not overwritten during [tw + 1, tr]. Condition 2. remains well defined for nondeterministic\r\nalgorithms, since only the unique accepting thread may write memory cells. For condition\r\n1., we will only look at the reads made by the accepting threads, and ignore the rejecting\r\nthreads.\r\nMore formally, let the accepting execution of a problem instance be the sequence of cell\r\nreads and writes executed by the updates and the accepting threads of each query. Define:\r\nW(t0, t1) : the set of cells written in the accepting execution during time interval [t0, t1].\r\nR(t0, t1) : the set of cells read in the accepting execution during time interval [t0, t1], which\r\nhad last been written at some time tw < t0.\r\nThen, IT(t0, t1, t2) = W(t0, t1) ∩ R(t1, t2). Our replacement for Lemma 3.2 states that:\r\nLemma 3.9. H\r\n\r\nA[t1,t2]\r\n\f\r\n\f ∆\r\n? = ∆?\r\n\u0001\r\n≤ O\r\n\r\nE\r\n\u0002\r\nw·|IT(t0, t1, t2)| + |R(t0, t2)| + |W(t1, t2)|\r\n\f\r\n\f ∆\r\n? =\r\n∆?\r\n\u0003\u0001\r\nNote that this lemma is weaker than the original Lemma 3.2 due to the additional terms\r\ndepending on W(t0, t1) and R(t1, t2). However, these terms are fairly small, adding O(1) bits\r\nof entropy per cell, as opposed to O(w) bits for each cell in the information transfer. This\r\nproperty will prove crucial.\r\nBefore we prove the lemma, we redo the analysis of §3.4, showing that we obtain the\r\nsame bounds for nondeterministic data structures. As before, we consider a lower-bound\r\ntree T , whose n leaves represent time units (query and update pairs). For each internal\r\nnode v of T , let [t0, t1] span the left subtree, and [t1, t2] span the right subtree. We then\r\ndefine IT(v) = |IT(t0, t1, t2)|, W(v) = |W(t0, t1)|, and R(v) = |R(t1, t2)|.\r\nLet T be the total running time of the data structure on a particular instance. As before,\r\nobserve that each cell read in the accepting execution is counted in exactly one IT(v), at\r\nthe lowest common ancestor of the read and write times. Thus, T ≥\r\nP\r\nv∈T IT(v).\r\nFor each node, we compare Lemmas 3.9 and 3.4 to obtain a lower bound in terms of the\r\ninterleave at the node:\r\nE[w · IT(v) + W(v) + R(v)] = Ω(δ · IL(v)) (3.1)\r\nNote that summing up R(v) +W(v) over the nodes on a single level of the tree gives at most\r\nT, because each instruction is counted in at most one node. Thus, summing (3.1) over all\r\nv ∈ T yields: E[w·T + T ·depth(T )] = Ω(δ\r\nP\r\nv\r\nIL(v)). By using the bit-reversal permutation\r\nand letting T be a perfect binary tree, we have P\r\nv\r\nIL(v) = Ω(n lg n), and depth(T ) = lg n.\r\nSince w = Ω(lg n) in our model, the lower bound becomes E[2w·T] = Ω(δ ·n lg n), as desired.\r\nProof of Lemma 3.9. The proof is an encoding argument similar to Lemma 3.2, with one\r\nadditional complication: during decoding, we do not know which thread will accept, and we\r\nmust simulate all of them. Note, however, that the cells read by the rejecting threads are\r\nnot included in the information transfer, and thus we cannot afford to include them in the\r\nencoding. But without these cells, it is not clear how to decode correctly: when simulating\r\na rejecting thread, we may think incorrectly that a cell was not written during [t0, t1]. If we\r\n56",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b5d842ab-8edb-4366-aeaa-c09e35aee7c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b3768234f0a92b1e9c2c3b0c30045b8a74869ea254b44e1f783a10b8c533f492",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 573
      },
      {
        "segments": [
          {
            "segment_id": "6e49e576-f6bf-4fd7-b446-61030d131df1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 57,
            "page_width": 612,
            "page_height": 792,
            "content": "give the algorithm a stale version of the cell (from before time t0), a rejecting thread might\r\nnow turn into an accepting thread, giving us an incorrect answer.\r\nTo fix this, our encoding will contain two components:\r\nC1: for each cell in IT(t0, t1, t2), store the cell address, and the contents at time t1.\r\nC2: a dictionary for cells in W(t0, t1) ∪ R(t1, t2)\r\n\u0001\r\n\\ IT(t0, t1, t2), with one bit of associated\r\ninformation: “W” if the cell comes from W(t0, t1), and “R” if it comes from R(t1, t2).\r\nComponent C2 will allow us to stop the execution of rejecting threads that try to read\r\na “dangerous” cell: a cell written in [t0, t1], but which is not in the information transfer\r\n(and thus, its contents in unknown). The presence of C2 in the encoding accounts for a\r\ncovert information transfer: the fact that a cell was not written during [t0, t1] is a type of\r\ninformation that the algorithm can learn during [t1, t2].\r\nThe immediate concern is that the dictionary of C2 is too large. Indeed, a dictionary\r\nstoring a set S from some universe U, with some r-bit data associated to each element,\r\nrequires at least lg \r\n|U|\r\n|S|\r\n\u0001\r\n+|S|· r bits of space. Assuming that the space of cells is [2w], C2 will\r\nuse roughly (|W(t0, t1)|+|R(t1, t2)|)·w bits of space, an unacceptable bound that dominates\r\nthe information transfer.\r\nWe address this concern by pulling an interesting rabbit out of the hat: a retrieval-only\r\ndictionary (also known as a “Bloomier filter”). The idea is that the membership (is some\r\nx ∈ S?) and retrieval (return the data associated with some x ∈ S) functionalities of a\r\ndictionary don’t need to be bundled together. If we only need the retrieval query and never\r\nrun membership tests, we do not actually need to store the set S, and we can avoid the lower\r\nbound of lg \r\n|U|\r\n|S|\r\n\u0001\r\nbits:\r\nLemma 3.10. Consider a set S from a universe U, where each element of S has r bits of\r\nassociated data. We can construct a data structure occupying O(|S| · r + lg lg |U|) bits of\r\nmemory that answers the following query:\r\nretrieve(x) : if x ∈ S, return x’s associated data; if x /∈ S, return an arbitrary value.\r\nProof. To the reader familiar with the field, this is a simple application of perfect hash\r\nfunctions. However, for the sake of completeness, we choose to include a simple proof based\r\non the the probabilistic method.\r\nLet n = |S|, u = |U|. Consider a hash function h : U → [2n]. If the function is injective\r\non S, we can use an array with 2n locations of r bits each, storing the data associated to\r\neach x ∈ S at h(x). For retrieval, injectivity of S guarantees that the answer is correct\r\nwhenever x ∈ S.\r\nThere are \r\n2n\r\nn\r\n\u0001\r\n· n! ·(2n)\r\nu−n\r\nchoices of h that are injective on S, out of (2n)\r\nu possibilities.\r\nThus, if h is chosen uniformly at random, it works for any fixed S with probability \r\n2n\r\nn\r\n\u0001\r\n·\r\nn!\r\n\u000e\r\n(2n)\r\nn ≥ 2−O(n)\r\n. Pick a family H of 2O(n)· lg \r\nu\r\nn\r\n\u0001\r\nindependently random h. For any fixed\r\nS, the probability that no h ∈ H is injective on S is (1−\r\n1\r\n2O(n) )\r\n|H| = exp \r\nΘ\r\n\r\nlg \r\nu\r\nn\r\n\u0001\u0001\u0001 < 1/\r\nu\r\nn\r\n\u0001\r\n.\r\nBy a union bound over all \r\nu\r\nn\r\n\u0001\r\nchoices of S, there exists a family H such that for any S,\r\nthere exists h ∈ H injective on S.\r\nSince our lemma does not promise anything about the time efficiency of the dictionary,\r\nwe can simply construct H by iterating over all possibilities. The space will be 2n · r bits for\r\n57",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/6e49e576-f6bf-4fd7-b446-61030d131df1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d3fbba05bb4989194362c535918d9c706015034df3d0bc51adada1e9a508f051",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 645
      },
      {
        "segments": [
          {
            "segment_id": "6e49e576-f6bf-4fd7-b446-61030d131df1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 57,
            "page_width": 612,
            "page_height": 792,
            "content": "give the algorithm a stale version of the cell (from before time t0), a rejecting thread might\r\nnow turn into an accepting thread, giving us an incorrect answer.\r\nTo fix this, our encoding will contain two components:\r\nC1: for each cell in IT(t0, t1, t2), store the cell address, and the contents at time t1.\r\nC2: a dictionary for cells in W(t0, t1) ∪ R(t1, t2)\r\n\u0001\r\n\\ IT(t0, t1, t2), with one bit of associated\r\ninformation: “W” if the cell comes from W(t0, t1), and “R” if it comes from R(t1, t2).\r\nComponent C2 will allow us to stop the execution of rejecting threads that try to read\r\na “dangerous” cell: a cell written in [t0, t1], but which is not in the information transfer\r\n(and thus, its contents in unknown). The presence of C2 in the encoding accounts for a\r\ncovert information transfer: the fact that a cell was not written during [t0, t1] is a type of\r\ninformation that the algorithm can learn during [t1, t2].\r\nThe immediate concern is that the dictionary of C2 is too large. Indeed, a dictionary\r\nstoring a set S from some universe U, with some r-bit data associated to each element,\r\nrequires at least lg \r\n|U|\r\n|S|\r\n\u0001\r\n+|S|· r bits of space. Assuming that the space of cells is [2w], C2 will\r\nuse roughly (|W(t0, t1)|+|R(t1, t2)|)·w bits of space, an unacceptable bound that dominates\r\nthe information transfer.\r\nWe address this concern by pulling an interesting rabbit out of the hat: a retrieval-only\r\ndictionary (also known as a “Bloomier filter”). The idea is that the membership (is some\r\nx ∈ S?) and retrieval (return the data associated with some x ∈ S) functionalities of a\r\ndictionary don’t need to be bundled together. If we only need the retrieval query and never\r\nrun membership tests, we do not actually need to store the set S, and we can avoid the lower\r\nbound of lg \r\n|U|\r\n|S|\r\n\u0001\r\nbits:\r\nLemma 3.10. Consider a set S from a universe U, where each element of S has r bits of\r\nassociated data. We can construct a data structure occupying O(|S| · r + lg lg |U|) bits of\r\nmemory that answers the following query:\r\nretrieve(x) : if x ∈ S, return x’s associated data; if x /∈ S, return an arbitrary value.\r\nProof. To the reader familiar with the field, this is a simple application of perfect hash\r\nfunctions. However, for the sake of completeness, we choose to include a simple proof based\r\non the the probabilistic method.\r\nLet n = |S|, u = |U|. Consider a hash function h : U → [2n]. If the function is injective\r\non S, we can use an array with 2n locations of r bits each, storing the data associated to\r\neach x ∈ S at h(x). For retrieval, injectivity of S guarantees that the answer is correct\r\nwhenever x ∈ S.\r\nThere are \r\n2n\r\nn\r\n\u0001\r\n· n! ·(2n)\r\nu−n\r\nchoices of h that are injective on S, out of (2n)\r\nu possibilities.\r\nThus, if h is chosen uniformly at random, it works for any fixed S with probability \r\n2n\r\nn\r\n\u0001\r\n·\r\nn!\r\n\u000e\r\n(2n)\r\nn ≥ 2−O(n)\r\n. Pick a family H of 2O(n)· lg \r\nu\r\nn\r\n\u0001\r\nindependently random h. For any fixed\r\nS, the probability that no h ∈ H is injective on S is (1−\r\n1\r\n2O(n) )\r\n|H| = exp \r\nΘ\r\n\r\nlg \r\nu\r\nn\r\n\u0001\u0001\u0001 < 1/\r\nu\r\nn\r\n\u0001\r\n.\r\nBy a union bound over all \r\nu\r\nn\r\n\u0001\r\nchoices of S, there exists a family H such that for any S,\r\nthere exists h ∈ H injective on S.\r\nSince our lemma does not promise anything about the time efficiency of the dictionary,\r\nwe can simply construct H by iterating over all possibilities. The space will be 2n · r bits for\r\n57",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/6e49e576-f6bf-4fd7-b446-61030d131df1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d3fbba05bb4989194362c535918d9c706015034df3d0bc51adada1e9a508f051",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 645
      },
      {
        "segments": [
          {
            "segment_id": "687c3cad-4e48-40fa-b4e6-24324dfd26f4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 58,
            "page_width": 612,
            "page_height": 792,
            "content": "the array of values, plus lg |H| = O(n + lg lg u) bits to specify a hash function from H that\r\nis injective on S.\r\nWe will implement C2 using a retrieval-only dictionary, requiring O\r\n\r\n|W(t0, t1)|+|R(t1, t2)|\r\n\u0001\r\nbits of space. Component C1 requires O(w)·|IT(t0, t1, t2)| bits. It only remains to show that\r\nthe query answers A[t1,t2] can be recovered from this encoding, thus giving an upper bound\r\non the entropy of the answers.\r\nTo recover the answers A[t1,t2], we simulate the execution of the data structure during\r\n[t1, t2]. Updates, which do not use nondeterminism, are simulated as in Lemma 3.2. For a\r\nquery happening at time t ∈ [t1, t2], we simulate all possible threads. A cell read by one of\r\nthese threads falls into one of the following cases:\r\nW(t1, t): We can recognize this case by maintaining a list of memory locations written during\r\nthe simulation; the contents of the cell is immediately available.\r\nIT(t0, t1, t2): We can recognize this case by examining the addresses in C1; the cell contents\r\ncan be read from the encoding.\r\nW(t0, t1) \\ IT(t0, t1, t2): We can recognize this case by querying the dictionary C2. If the\r\nretrieval query returns “W,” we know that the answer cannot be “R” (the correct\r\nanswer may be “W,” or the cell may be outside the dictionary set, in which case an\r\narbitrary value is returned). But if the answer cannot be “R,” the cell cannot be in\r\nR(t1, t2). Thus, this thread is certainly not the accepting thread, and the simulation\r\nmay reject immediately.\r\nW(1, t0 − 1) \\ W(t0, t): This is the default case, if the cell doesn’t satisfy previous conditions.\r\nThe contents of the cell is known, because the operations before time t0 are fixed, as\r\npart of ∆?.\r\nIt should be noted that C2 allows us to handle an arbitrary number of rejecting threads.\r\nAll such threads are either simulated correctly until they reject, or the simulation rejects\r\nearlier, when the algorithm tries to read a cell in W(t0, t1) \\ IT(t0, t1, t2).\r\n3.9 Bibliographical Notes\r\nIn our paper [86], we generalize the argument presented here to prove lower bound trade-offs\r\nbetween the update time tu and the query time tq. We omit these proofs from the current\r\nthesis, since our improved epoch arguments from the next chapter will yield slightly better\r\ntrade-offs than the ones obtained in [86].\r\nDictionaries supporting only retrieval have found another beautiful application to the\r\nrange reporting problem in one dimension. See our paper [76] for the most recent work on\r\n1-dimensional range reporting. Dynamic dictionaries with retrieval were investigated in our\r\npaper [37], which gives tight upper and lower bounds.\r\n58",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/687c3cad-4e48-40fa-b4e6-24324dfd26f4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c2909d82d1bf33f5b1d0b057f45744f8361746fcfff77d8ba1e92a461b9de5fa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 454
      },
      {
        "segments": [
          {
            "segment_id": "3e6d3415-6691-4968-8b04-3c35e05196b1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 59,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 4\r\nEpoch-Based Lower Bounds\r\nThis chapter presents a subtle improvement to the classic chronogram technique of Fredman\r\nand Saks [51], which enables it to prove logarithmic lower bounds in the cell-probe model..\r\nTo fully appreciate this development, one must remember that the chronogram technique\r\nwas the only known approach for proving dynamic lower bounds from 1989 until our work\r\nin 2004 [86]. At the same time, obtaining a logarithmic bound in the cell-probe model was\r\nviewed as one of the most important problems in data-structure lower bounds. It is now\r\nquite surprising to find that the answer has always been this close.\r\nFormally, our result is the following:\r\nTheorem 4.1. Consider an implementation of the partial-sums problem over a group G\r\nwith |G| ≥ 2\r\nδ\r\n, in the cell-probe model with b-bit cells. Let tu denote the expected amortized\r\nrunning time of an update, and tq the expected running time of a query. Then, in the average\r\ncase of an input distribution, the following lower bounds hold:\r\ntq lg \u0012\r\ntu\r\nlg n\r\n·\r\nb + lg lg n\r\nδ\r\n\u0013\r\n= Ω \u0012\u0018 δ\r\nb + lg lg n\r\n\u0019\r\n· lg n\r\n\u0013\r\ntu lg \u0012\r\ntq\r\nlg n\r\n/\r\n\u0018\r\nδ\r\nb + lg lg n\r\n\u0019\u0013 = Ω \r\nδ\r\nb + lg(tq/d\r\nδ\r\nb\r\ne)\r\n· lg n\r\n!\r\nNote that the theorem does not assume δ ≤ b, so it also gives interesting results in the\r\nbit-prove model, where group element are larger than a single cell. Another strengthening\r\nof the chronogram technique apparent in this theorem is that it is now possible to derive\r\nlower bound trade-offs in the regime of fast updates and slow queries. This implies almost\r\nmatching the bounds achieved by buffer trees, which constitute one of the most important\r\ntools for external-memory algorithms.\r\nBefore we prove the theorem, we first apply it in some interesting interesting setups, and\r\ncompare with the best previously known results.\r\n59",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/3e6d3415-6691-4968-8b04-3c35e05196b1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8cea16e788dbb758e984698ae8b2adcf4a1a01bff15c75f0a82f1cb24a02d727",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 328
      },
      {
        "segments": [
          {
            "segment_id": "1e773772-a27f-4fff-bcdd-292e76ac4b91",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 60,
            "page_width": 612,
            "page_height": 792,
            "content": "4.1 Trade-offs and Higher Word Sizes\r\nAssuming b = Ω(lg n) and δ ≤ b, our bounds simplify to:\r\ntq\r\n\u0012\r\nlg tu\r\nlg n\r\n+ lg b\r\nδ\r\n\u0013\r\n= Ω(lg n) tu lg tq\r\nlg n\r\n= Ω \u0012\r\nδ\r\nb\r\n· lg n\r\n\u0013\r\nThe first trade-off, which is optimal [86], represents a strengthening of the normal trade\u0002offs obtained by the chronogram technique. Note in particular that our trade-off implies\r\nmax{tu, tq} = Ω(lg n), which had been a major open problem since [51].\r\nThe second trade-off for fast updates is fundamentally new; all previous technique are\r\nhelpless in the regime tq ≥ tu.\r\nBuffer trees [18] are a general algorithmic paradigm for obtaining fast updates, given a\r\nhigher cell size. For our problem, this yields a cell-probe upper bound of tu = O(\r\nl\r\nδ+lg n\r\nb\r\n· lgtq/ lg n n\r\nm\r\n),\r\nfor any tq = Ω(lg n). Thus, we obtain tight bounds when δ = Ω(lg n). (Note that in the\r\ncell-probe model, we have a trivial lower bound of tu ≥ 1, matching the ceiling in the upper\r\nbound.)\r\nTo appreciate these bounds in a natural setup, let us consider the external memory model,\r\nwhich is the main motivation for looking at a higher cell size. In this model, the unit for\r\nmemory access is a page, which is modeled by a cell in the cell-probe model. A page contains\r\nB words, which are generally assumed to have Ω(lg n) bits. The model also provides for a\r\ncache, a set of cells which the algorithm can access at zero cost. We assume that the cache\r\nis not preserved between operations (algorithmic literature is ambivalent in this regard).\r\nThis matches the assumption of the cell-probe model, where each operation can only learn\r\ninformation by probing the memory. Note that the nonuniformity in the cell-probe model\r\nallows unbounded internal state for an operation, so any restriction on the size of the cache\r\ncannot be captured by cell-probe lower bounds.\r\nUnder the natural assumption that δ matches the size of the word, we see that our lower\r\nbound becomes tu = Ω( 1\r\nB\r\nlgtq/ lg n n). Buffer trees offer a matching upper bound, if the update\r\nalgorithm is afforded a cache of Ω(tq/ lg n) pages. As mentioned before, we cannot expect\r\ncell-probe lower bounds to be sensitive to cache size.\r\n4.2 Bit-Probe Complexity\r\nThe bit-probe model is an instantiation of the cell-probe model with one-bit cells. Bit-probe\r\ncomplexity can be considered a fundamental measure of computation. Though a cell size of\r\nΘ(lg n) bits is more realistic when comparing to real-world computers, the bit-probe measure\r\nposses an appeal of its own, due to the exceedingly clean mathematical setup.\r\nSetting b = δ = 1, which is the most natural interpretation of partial sums in the bit\u000260",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/1e773772-a27f-4fff-bcdd-292e76ac4b91.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7c5fa95d89cf3efcf4f2ef06af4228cbdaa9748890358b129425676a2bfe11c0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 474
      },
      {
        "segments": [
          {
            "segment_id": "b581bd54-b185-42f1-a693-2cec5eb0ac57",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 61,
            "page_width": 612,
            "page_height": 792,
            "content": "probe model, our lower bounds simplify to:\r\ntq lg \u0012\r\ntu\r\nlg n/ lg lg n\r\n\u0013\r\n= Ω(lg n) tu · lg \u0012\r\ntq\r\nlg n\r\n\u0013\r\n· lg tq = Ω(lg n)\r\nThe folklore solution to the problem achieves the following trade-offs:\r\ntq lg tu\r\nlg n\r\n= Ω(lg n) tu · lg tq\r\nlg n\r\n= Ω(lg n)\r\nIt can be seen that our lower bounds come close, but do not exactly match the upper bounds.\r\nIn the most interesting point of balanced running times, the upper bound is max{tu, tq} =\r\nO(lg n), while our lower bound implies max{tu, tq} = Ω( lg n\r\nlg lg lg n\r\n). Thus, our lower bound is\r\noff by just a triply logarithmic factor.\r\nPreviously, the best known lower bound was max{tu, tq} = Ω( lg n\r\nlg lg n\r\n) achieved by Fredman\r\nin 1982 [48]. This was by a reduction to the greater-than problem, which Fredman introduced\r\nspecifically for this purpose. As we showed in [87], there is an O(\r\nlg n\r\nlg lg n\r\n) upper bound for this\r\nproblem, so Fredman’s technique cannot yield a better result for partial sums.\r\nDynamic connectivity and a record bit-probe bound. With b = 1 and supercon\u0002stant δ, Theorem 4.1 easily implies a nominally superlogarithmic bound on max{tu, tq}. For\r\ninstance, for partial sums in Z/nZ (i.e. δ = lg n), we obtain max{tu, tq} = Ω( lg2 n\r\nlg lg n·lg lg lg n\r\n).\r\nThis is a modest improvement over the Ω( lg2 n\r\n(lg lg n)\r\n2 ) bound of Fredman and Saks [51].\r\nHowever, it is not particularly relevant to judge the magnitude of such bounds, as we are\r\nonly proving a hardness of Ω(lg e n) per bit in the query output and update input, and we can\r\nobtain arbitrarily high nominal bounds. As advocated by Miltersen [72], the proper way to\r\ngauge the power of lower bound techniques is to consider problems with a minimal set of\r\noperations, and, in particular, decision queries. Specifically, for a language L, we look at the\r\ndynamic language membership problem, defined as follows. For any fixed n (the problem\r\nsize), maintain a string w ∈ {0, 1}\r\nn under two operations: flip the i-th bit of w, and report\r\nwhether w ∈ L.\r\nBased on our partial sums lower bound, we prove a lower bound of Ω(( lg n\r\nlg lg n\r\n)\r\n2\r\n) for dynamic\r\nconnectivity, which is a dynamic language membership problem. This has has an important\r\ncomplexity-theoretic significance, as it is the highest known bound for an explicit dynamic\r\nlanguage membership problem. The previous record was Ω(lg n), shown in [74]. Miltersen’s\r\nsurvey of cell-probe complexity [72] lists improving this bound as the first open problem\r\namong three major challenges for future research.\r\nIt should be noted that our Ω(lg e 2 n) bound is far from a mere echo of a Ω(lg e n) bound\r\nin the cell-probe model. Indeed, Ω( lg n\r\nlg lg n\r\n) bounds in the cell-probe model have been known\r\nsince 1989 (including for dynamic connectivity), but the bit-probe record has remained just\r\nthe slightly higher Ω(lg n). Our bound is the first to show a quasi-optimal Ω(lg e n) separation\r\nbetween bit-probe complexity and the cell-probe complexity with cells of Θ(lg n) bits, when\r\nthe cell-probe complexity is superconstant.\r\n61",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b581bd54-b185-42f1-a693-2cec5eb0ac57.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8a382d777cad1571e308efdbc58529e2994778af22155356fb95714adc56357a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 564
      },
      {
        "segments": [
          {
            "segment_id": "b581bd54-b185-42f1-a693-2cec5eb0ac57",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 61,
            "page_width": 612,
            "page_height": 792,
            "content": "probe model, our lower bounds simplify to:\r\ntq lg \u0012\r\ntu\r\nlg n/ lg lg n\r\n\u0013\r\n= Ω(lg n) tu · lg \u0012\r\ntq\r\nlg n\r\n\u0013\r\n· lg tq = Ω(lg n)\r\nThe folklore solution to the problem achieves the following trade-offs:\r\ntq lg tu\r\nlg n\r\n= Ω(lg n) tu · lg tq\r\nlg n\r\n= Ω(lg n)\r\nIt can be seen that our lower bounds come close, but do not exactly match the upper bounds.\r\nIn the most interesting point of balanced running times, the upper bound is max{tu, tq} =\r\nO(lg n), while our lower bound implies max{tu, tq} = Ω( lg n\r\nlg lg lg n\r\n). Thus, our lower bound is\r\noff by just a triply logarithmic factor.\r\nPreviously, the best known lower bound was max{tu, tq} = Ω( lg n\r\nlg lg n\r\n) achieved by Fredman\r\nin 1982 [48]. This was by a reduction to the greater-than problem, which Fredman introduced\r\nspecifically for this purpose. As we showed in [87], there is an O(\r\nlg n\r\nlg lg n\r\n) upper bound for this\r\nproblem, so Fredman’s technique cannot yield a better result for partial sums.\r\nDynamic connectivity and a record bit-probe bound. With b = 1 and supercon\u0002stant δ, Theorem 4.1 easily implies a nominally superlogarithmic bound on max{tu, tq}. For\r\ninstance, for partial sums in Z/nZ (i.e. δ = lg n), we obtain max{tu, tq} = Ω( lg2 n\r\nlg lg n·lg lg lg n\r\n).\r\nThis is a modest improvement over the Ω( lg2 n\r\n(lg lg n)\r\n2 ) bound of Fredman and Saks [51].\r\nHowever, it is not particularly relevant to judge the magnitude of such bounds, as we are\r\nonly proving a hardness of Ω(lg e n) per bit in the query output and update input, and we can\r\nobtain arbitrarily high nominal bounds. As advocated by Miltersen [72], the proper way to\r\ngauge the power of lower bound techniques is to consider problems with a minimal set of\r\noperations, and, in particular, decision queries. Specifically, for a language L, we look at the\r\ndynamic language membership problem, defined as follows. For any fixed n (the problem\r\nsize), maintain a string w ∈ {0, 1}\r\nn under two operations: flip the i-th bit of w, and report\r\nwhether w ∈ L.\r\nBased on our partial sums lower bound, we prove a lower bound of Ω(( lg n\r\nlg lg n\r\n)\r\n2\r\n) for dynamic\r\nconnectivity, which is a dynamic language membership problem. This has has an important\r\ncomplexity-theoretic significance, as it is the highest known bound for an explicit dynamic\r\nlanguage membership problem. The previous record was Ω(lg n), shown in [74]. Miltersen’s\r\nsurvey of cell-probe complexity [72] lists improving this bound as the first open problem\r\namong three major challenges for future research.\r\nIt should be noted that our Ω(lg e 2 n) bound is far from a mere echo of a Ω(lg e n) bound\r\nin the cell-probe model. Indeed, Ω( lg n\r\nlg lg n\r\n) bounds in the cell-probe model have been known\r\nsince 1989 (including for dynamic connectivity), but the bit-probe record has remained just\r\nthe slightly higher Ω(lg n). Our bound is the first to show a quasi-optimal Ω(lg e n) separation\r\nbetween bit-probe complexity and the cell-probe complexity with cells of Θ(lg n) bits, when\r\nthe cell-probe complexity is superconstant.\r\n61",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b581bd54-b185-42f1-a693-2cec5eb0ac57.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8a382d777cad1571e308efdbc58529e2994778af22155356fb95714adc56357a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 564
      },
      {
        "segments": [
          {
            "segment_id": "cb4e1b04-ef79-48df-9318-26f7cd0300a0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 62,
            "page_width": 612,
            "page_height": 792,
            "content": "A[1] A[2] A[3] A[4] A[5] A[6]\r\nFigure 4-1: Our graphs can be viewed as a sequence of √n permutation boxes.\r\nThe main trick for obtaining this lower bound is to use the trade-offs for slow queries\r\nand fast updates, a regime in which we give the first known lower bounds. It is not hard\r\nto convert a decision query into one returning a large output, at the price of an appropriate\r\nslow down. This is the second time, after the analysis of buffer trees, when our extension of\r\nthe chronogram technique for the regime of slow queries turns out to be very relevant.\r\nTheorem 4.2. Consider a bit-probe implementation for dynamic connectivity, in which up\u0002dates take expected amortized time tu, and queries take expected time tq. Then, in the average\r\ncase of an input distribution, tu = Ω \u0010\r\nlg2 n\r\nlg2(tu+tq)\r\n\u0011\r\n. In particular max{tu, tq} = Ω \u0010(\r\nlg n\r\nlg lg n\r\n)\r\n2\r\n\u0011\r\n.\r\nProof. We first describe the shape of the graphs used in the reduction to Theorem 4.1; refer\r\nto Figure 4-1. The vertex set is roughly given by an integer grid of size √n ×\r\n√\r\nn. The\r\nedge set is given by a series of permutation boxes. A permutation box connects the nodes in\r\na column to the nodes in the next column arbitrarily, according to a given permutation in\r\nS√\r\nn. Notice that the permutations decompose the graph into a collection of √\r\nn paths. As\r\nthe paths evolve horizontally, the y coordinates change arbitrarily at each point due to the\r\npermutations. In addition to this, there is a special test vertex to the left, which is connected\r\nto some vertices in the first column.\r\nWe now describe how to implement the partial sums macro-operations in terms of the\r\nconnectivity operations:\r\nupdate(i, π): sets πi = π. This is done by removing all edges in permutation box i and\r\ninserting new edges corresponding to the new permutation π. Thus, the running time\r\nis O(tu ·\r\n√\r\nn).\r\nsum(i): returns σ = π1 ◦ · · · ◦ πi. We use O(lg n) phases, each one guessing a bit of σ(j) for\r\nall j. Phase k begins by removing all edges incident to the test node. Then, we add\r\nedges from the test vertex to all vertices in the first column, whose row number has\r\na one in the k-th bit. Then, we test connectivity of all vertices from the i-th column\r\nand the test node, respectively. This determines the k-th bit of σ(j) for all j. In total,\r\nsum takes time O((tu + tq)\r\n√\r\nn · lg n).\r\n62",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/cb4e1b04-ef79-48df-9318-26f7cd0300a0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c6db49f2df3db042670f39720ef883b55bdde35676a98dd35fa1675bf693932d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 441
      },
      {
        "segments": [
          {
            "segment_id": "b494a5f9-597c-41e4-bb45-2d01f2055822",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 63,
            "page_width": 612,
            "page_height": 792,
            "content": "Finally, we interpret the lower bounds of Theorem 4.1 for these operations. We have\r\nb = 1 and δ = Θ(√n · lg n). The first trade-off is less interesting, as we have slowed down\r\nqueries by a factor of lg n. The second trade-off becomes:\r\ntu\r\n√\r\nn·lg \u0012\r\n(tu + tq)\r\n√\r\nn · lg n\r\n√\r\nn · lg2 n/ lg lg n\r\n\u0013\r\n= Ω \u0012 √\r\nn · lg n\r\nlg(tu + tq)\r\n· lg n\r\n\u0013\r\n⇒ tu lg \u0012\r\ntu + tq\r\nlg n/ lg lg n\r\n\u0013\r\n= Ω \u0012\r\nlg2 n\r\nlg(tu + tq)\r\n\u0013\r\nSince the lower bound implies max{tu, tq} = Ω(( lg n\r\nlg lg n\r\n)\r\n2\r\n), we have lg( tu+tq\r\nlg n/ lg lg n\r\n) = Θ(lg(tu +\r\ntq)), so the bound simplifies to tu = Ω( lg2 n\r\nlg2(tu+tq)\r\n).\r\n4.3 Lower Bounds for Partial Sums\r\nWe begin by reviewing the chronogram method, at an intuitive level. One first generates a\r\nsequence of random updates, ended by one random query. Looking back in time from the\r\nquery, one partitions the updates into exponentially growing epochs: for a certain r, epoch\r\ni contains the r\r\ni updates immediately before epoch i − 1. One then argues that for all i, the\r\nquery needs to read at least one cell from epoch i with constant probability. This is done\r\nas follows. Clearly, information about epoch i cannot be reflected in earlier epochs (those\r\noccurred back in time). On the other hand, the latest i − 1 epochs contain only O(r\r\ni−1\r\n)\r\nupdates. Assume the cell-probe complexity of each update is bounded by tu. Then, during\r\nthe latest i − 1 epochs, only O(t\r\ni−1\r\ntub) bits are written. If r = C · tu\r\nb\r\nδ\r\nfor a sufficiently large\r\nconstant C, this number is at most, say, 1\r\n10 r\r\ni\r\nδ. On the other hand, updates in epoch i contain\r\nr\r\ni\r\nδ bits of entropy, so all information known outside epoch i can only fix a constant fraction\r\nof these updates. If a random query is forced to learn information about a random update\r\nfrom epoch i, it is forced to read a cell from epoch i with constant probability, because the\r\ninformation is not available outside the epoch. This means a query must make Ω(1) probes\r\nin expectation into every epoch, so the lower bound on the query time is given by the number\r\nof epochs that one can construct, i.e. tq = Ω(logr n) = Ω( lg n\r\nlg(tub/δ)\r\n). A trade-off of this form\r\nwas indeed obtained by [8], and is the highest trade-off obtained by the chronogram method.\r\nUnfortunately, even for δ = b, this only implies max{tu, tq} = Ω( lg n\r\nlg lg n\r\n).\r\nWe now describe the new ideas that we use to improve this result. Intuitively, the analysis\r\ndone by the chronogram technique is overly pessimistic, in that it assumes all cells written in\r\nthe latest i − 1 epochs concentrate on epoch i, encoding a maximum amount of information\r\nabout it. In the setup from above, this may actually be tight, up to constant factors, because\r\nthe data structure knows the division into epochs, and can build a strategy based on it.\r\nHowever, we can randomize the construction of epochs to foil such strategies. We generate a\r\nrandom number of updates, followed by one query; since the data structure cannot anticipate\r\nthe number of updates, it cannot base its decisions on a known epoch pattern. Due to this\r\nrandomization, we intuitively expect each update to write O(\r\ntub\r\nlogr n\r\n) bits “about” a random\r\nepoch, as there are Θ(lgr n) epochs in total. In this case, it would suffice to pick r satisfying\r\nr = Θ( tub\r\nδ lgr n\r\n), i.e. lg r = Θ(lg b·tu\r\nδ lg n\r\n). This yields tq = Ω(logr n) = Ω( lg n\r\nlg(tu/ lg n)+lg(b/δ)\r\n), which\r\n63",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b494a5f9-597c-41e4-bb45-2d01f2055822.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d431418e65e8c7f66350ad01ca9d74d44b25a0dd8c4fb19cd69dff5715d68a2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 667
      },
      {
        "segments": [
          {
            "segment_id": "b494a5f9-597c-41e4-bb45-2d01f2055822",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 63,
            "page_width": 612,
            "page_height": 792,
            "content": "Finally, we interpret the lower bounds of Theorem 4.1 for these operations. We have\r\nb = 1 and δ = Θ(√n · lg n). The first trade-off is less interesting, as we have slowed down\r\nqueries by a factor of lg n. The second trade-off becomes:\r\ntu\r\n√\r\nn·lg \u0012\r\n(tu + tq)\r\n√\r\nn · lg n\r\n√\r\nn · lg2 n/ lg lg n\r\n\u0013\r\n= Ω \u0012 √\r\nn · lg n\r\nlg(tu + tq)\r\n· lg n\r\n\u0013\r\n⇒ tu lg \u0012\r\ntu + tq\r\nlg n/ lg lg n\r\n\u0013\r\n= Ω \u0012\r\nlg2 n\r\nlg(tu + tq)\r\n\u0013\r\nSince the lower bound implies max{tu, tq} = Ω(( lg n\r\nlg lg n\r\n)\r\n2\r\n), we have lg( tu+tq\r\nlg n/ lg lg n\r\n) = Θ(lg(tu +\r\ntq)), so the bound simplifies to tu = Ω( lg2 n\r\nlg2(tu+tq)\r\n).\r\n4.3 Lower Bounds for Partial Sums\r\nWe begin by reviewing the chronogram method, at an intuitive level. One first generates a\r\nsequence of random updates, ended by one random query. Looking back in time from the\r\nquery, one partitions the updates into exponentially growing epochs: for a certain r, epoch\r\ni contains the r\r\ni updates immediately before epoch i − 1. One then argues that for all i, the\r\nquery needs to read at least one cell from epoch i with constant probability. This is done\r\nas follows. Clearly, information about epoch i cannot be reflected in earlier epochs (those\r\noccurred back in time). On the other hand, the latest i − 1 epochs contain only O(r\r\ni−1\r\n)\r\nupdates. Assume the cell-probe complexity of each update is bounded by tu. Then, during\r\nthe latest i − 1 epochs, only O(t\r\ni−1\r\ntub) bits are written. If r = C · tu\r\nb\r\nδ\r\nfor a sufficiently large\r\nconstant C, this number is at most, say, 1\r\n10 r\r\ni\r\nδ. On the other hand, updates in epoch i contain\r\nr\r\ni\r\nδ bits of entropy, so all information known outside epoch i can only fix a constant fraction\r\nof these updates. If a random query is forced to learn information about a random update\r\nfrom epoch i, it is forced to read a cell from epoch i with constant probability, because the\r\ninformation is not available outside the epoch. This means a query must make Ω(1) probes\r\nin expectation into every epoch, so the lower bound on the query time is given by the number\r\nof epochs that one can construct, i.e. tq = Ω(logr n) = Ω( lg n\r\nlg(tub/δ)\r\n). A trade-off of this form\r\nwas indeed obtained by [8], and is the highest trade-off obtained by the chronogram method.\r\nUnfortunately, even for δ = b, this only implies max{tu, tq} = Ω( lg n\r\nlg lg n\r\n).\r\nWe now describe the new ideas that we use to improve this result. Intuitively, the analysis\r\ndone by the chronogram technique is overly pessimistic, in that it assumes all cells written in\r\nthe latest i − 1 epochs concentrate on epoch i, encoding a maximum amount of information\r\nabout it. In the setup from above, this may actually be tight, up to constant factors, because\r\nthe data structure knows the division into epochs, and can build a strategy based on it.\r\nHowever, we can randomize the construction of epochs to foil such strategies. We generate a\r\nrandom number of updates, followed by one query; since the data structure cannot anticipate\r\nthe number of updates, it cannot base its decisions on a known epoch pattern. Due to this\r\nrandomization, we intuitively expect each update to write O(\r\ntub\r\nlogr n\r\n) bits “about” a random\r\nepoch, as there are Θ(lgr n) epochs in total. In this case, it would suffice to pick r satisfying\r\nr = Θ( tub\r\nδ lgr n\r\n), i.e. lg r = Θ(lg b·tu\r\nδ lg n\r\n). This yields tq = Ω(logr n) = Ω( lg n\r\nlg(tu/ lg n)+lg(b/δ)\r\n), which\r\n63",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b494a5f9-597c-41e4-bb45-2d01f2055822.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d431418e65e8c7f66350ad01ca9d74d44b25a0dd8c4fb19cd69dff5715d68a2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 667
      },
      {
        "segments": [
          {
            "segment_id": "e3968218-f2fc-4306-bdd1-c1863a63afce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 64,
            "page_width": 612,
            "page_height": 792,
            "content": "means max{tu, tq} = Ω(lg n) when δ = b.\r\nUnfortunately, formalizing the intuition that the information written by updates “splits”\r\nbetween epochs seems to lead to elusive information theoretic arguments. To circumvent\r\nthis, we need a second very important idea: we can look at cell reads, as opposed to cell\r\nwrites. Indeed, regardless of how many cells epochs 1 through i − 1 write, the information\r\nrecorded about epoch i is bounded by the information that was read out of epoch i in the first\r\nplace. On the other hand, the information theoretic value of a read is more easily graspable,\r\nas it is dictated by combinatorial properties, like the time when the read occurs and the\r\ntime when the cell was last written. We can actually show that in expectation, O(\r\ntu\r\nlogr n\r\n) of\r\nthe reads made by each update obtain information about a random epoch. Then, regardless\r\nof how many cells are written, subsequent epochs can only encode little information about\r\nepoch i, because very little information was read by the updates in the first place.\r\nOnce we have this machinery set up, there is a potential for applying a different epoch\r\nconstruction. Assume tu is already “small”. Then, since we don’t need to divide tu by too\r\nmuch to get few probes into each epoch, we can define epochs to grow less than exponentially\r\nfast. In particular, we will define epochs to grow by a factor of r every r times, which means\r\nwe can obtain a higher lower bound on tq (in particular, tq = ω(lg n) is possible). Such\r\na result is inherently impossible to obtain using the classic chronogram technique, which\r\ndecides on the epoch partition in advance. As discussed in the introduction, this is a crucial\r\ncontribution of our paper, since it leads both to an understanding of buffer trees, and a\r\nω(lg n) bit-probe lower bound.\r\n4.3.1 Formal Framework\r\nWe first formalize the overall construction. We consider 2M − 1 random updates, and insert\r\na random query at a uniformly random position after the M-th update. Now we divide\r\nthe last M operations before the query into k epochs. Denote the lengths of the epochs by\r\n`1, . . . , `k, with `1 being the closest to the query. For convenience, we define si =\r\nPi\r\nj=1 `j\r\n.\r\nOur analysis will mainly be concerned with two random variables. Let T\r\nu\r\ni be the number\r\nof probes made during epochs {1, . . . , i − 1} that read a cell written during epoch i. Also let\r\nT\r\nq\r\ni be the number of probes made by the query that read a cell written during epoch i.\r\nAll chronogram lower bounds have relied on an information theoretic argument showing\r\nthat if epochs 1 up to i − 1 write too few cells, T\r\nq\r\ni must be bounded from below (usually by\r\na constant). As explained above, we instead want to argue that if T\r\nu\r\ni\r\nis too small, T\r\nq\r\ni must\r\nbe large. Though crucial, this change is very subtle, and the information theoretic analysis\r\nfollows the same general principles. The following lemma, the proof of which is deferred to\r\nSection 4.3.4, summarizes the results of this analysis:\r\nLemma 4.3. For any i such that si ≤\r\n√3 n, the following holds in expectation over a random\r\ninstance of the problem:\r\nE[T\r\nu\r\ni\r\n]\r\n`i\r\n\u0012\r\nb + lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\n+ E[T\r\nq\r\ni\r\n] · min \u001aδ, b + lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n\u001b\r\n= Ω(δ)\r\n64",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e3968218-f2fc-4306-bdd1-c1863a63afce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8cfb4819fe7bd822acdaaf5b9de9b006d40219e60b816c2cd603e5e54f42eca4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 603
      },
      {
        "segments": [
          {
            "segment_id": "e3968218-f2fc-4306-bdd1-c1863a63afce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 64,
            "page_width": 612,
            "page_height": 792,
            "content": "means max{tu, tq} = Ω(lg n) when δ = b.\r\nUnfortunately, formalizing the intuition that the information written by updates “splits”\r\nbetween epochs seems to lead to elusive information theoretic arguments. To circumvent\r\nthis, we need a second very important idea: we can look at cell reads, as opposed to cell\r\nwrites. Indeed, regardless of how many cells epochs 1 through i − 1 write, the information\r\nrecorded about epoch i is bounded by the information that was read out of epoch i in the first\r\nplace. On the other hand, the information theoretic value of a read is more easily graspable,\r\nas it is dictated by combinatorial properties, like the time when the read occurs and the\r\ntime when the cell was last written. We can actually show that in expectation, O(\r\ntu\r\nlogr n\r\n) of\r\nthe reads made by each update obtain information about a random epoch. Then, regardless\r\nof how many cells are written, subsequent epochs can only encode little information about\r\nepoch i, because very little information was read by the updates in the first place.\r\nOnce we have this machinery set up, there is a potential for applying a different epoch\r\nconstruction. Assume tu is already “small”. Then, since we don’t need to divide tu by too\r\nmuch to get few probes into each epoch, we can define epochs to grow less than exponentially\r\nfast. In particular, we will define epochs to grow by a factor of r every r times, which means\r\nwe can obtain a higher lower bound on tq (in particular, tq = ω(lg n) is possible). Such\r\na result is inherently impossible to obtain using the classic chronogram technique, which\r\ndecides on the epoch partition in advance. As discussed in the introduction, this is a crucial\r\ncontribution of our paper, since it leads both to an understanding of buffer trees, and a\r\nω(lg n) bit-probe lower bound.\r\n4.3.1 Formal Framework\r\nWe first formalize the overall construction. We consider 2M − 1 random updates, and insert\r\na random query at a uniformly random position after the M-th update. Now we divide\r\nthe last M operations before the query into k epochs. Denote the lengths of the epochs by\r\n`1, . . . , `k, with `1 being the closest to the query. For convenience, we define si =\r\nPi\r\nj=1 `j\r\n.\r\nOur analysis will mainly be concerned with two random variables. Let T\r\nu\r\ni be the number\r\nof probes made during epochs {1, . . . , i − 1} that read a cell written during epoch i. Also let\r\nT\r\nq\r\ni be the number of probes made by the query that read a cell written during epoch i.\r\nAll chronogram lower bounds have relied on an information theoretic argument showing\r\nthat if epochs 1 up to i − 1 write too few cells, T\r\nq\r\ni must be bounded from below (usually by\r\na constant). As explained above, we instead want to argue that if T\r\nu\r\ni\r\nis too small, T\r\nq\r\ni must\r\nbe large. Though crucial, this change is very subtle, and the information theoretic analysis\r\nfollows the same general principles. The following lemma, the proof of which is deferred to\r\nSection 4.3.4, summarizes the results of this analysis:\r\nLemma 4.3. For any i such that si ≤\r\n√3 n, the following holds in expectation over a random\r\ninstance of the problem:\r\nE[T\r\nu\r\ni\r\n]\r\n`i\r\n\u0012\r\nb + lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\n+ E[T\r\nq\r\ni\r\n] · min \u001aδ, b + lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n\u001b\r\n= Ω(δ)\r\n64",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e3968218-f2fc-4306-bdd1-c1863a63afce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8cfb4819fe7bd822acdaaf5b9de9b006d40219e60b816c2cd603e5e54f42eca4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 603
      },
      {
        "segments": [
          {
            "segment_id": "2d8bcd06-2691-4fb7-b92a-bfb987eff774",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 65,
            "page_width": 612,
            "page_height": 792,
            "content": "We will set M =\r\n√3 n so that the lemma applies to all epochs i. The lower bound\r\nof the lemma is reasonably easy to grasp intuitively. The first term measures the average\r\ninformation future updates learn about each of the `i updates in epoch i. There are T\r\nu\r\ni\r\nfuture\r\nprobes into epoch i. In principle, each one gathers b bits. However, there is also information\r\nhidden in the choice of which future probes hit epoch i. This amounts to O(lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n) bits\r\nper probe, since the total number of future probes is in expectation tusi−1 (there are si−1\r\nupdates in future epochs). The second term in the expression quantifies the information\r\nlearned by the query about epoch i. If the query makes T\r\nq\r\ni probes into epoch i, each one\r\nextracts b bits of information directly, and another O(lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n) bits indirectly, by the choice\r\nof which probes hit epoch i. However, there is also another way to bound the information\r\n(hence the min). If E[T\r\nq\r\ni\r\n] ≤ 1, we have probability at most T\r\nq\r\ni\r\nthat the query reads any cell\r\nfrom epoch i. If no cell is read, the information is zero. Otherwise, the relevant information\r\nis at most δ, since the answer of the query is δ bits. Finally, the lower bound on the total\r\ninformation gathered (the right hand side of the expression) is Ω(δ) because a random query\r\nneeds a random prefix sum of the updates happening in epoch i, which has Ω(δ) bits of\r\nentropy.\r\nApart from relating to T\r\nu\r\ni\r\ninstead of cell writes, the essential idea of this lemma is not\r\nnovel. However, our version is particularly general, presenting several important features.\r\nFor example, we achieve meaningful results for E[T\r\nq\r\ni\r\n] > 1, which is essential to analyzing\r\nthe case δ > b. We also get a finer bound on the “hidden information” gathered by a cell\r\nprobe, such as the O(lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n) term. In contrast, previous results could only bound this by\r\nO(lg n), which is irrelevant when b = Ω(lg n), but limits the lower bounds for the bit-probe\r\nmodel.\r\nIt is easy and instructive to apply Lemma 4.3 using the ideas of the classic chronogram\r\ntechnique. Define epochs to grow exponentially with rate r ≥ 2, i.e. `i = r\r\ni and si = O(ri\r\n).\r\nAssume for simplicity that tu and tq are worst-case bounds per operation. Then T\r\nu\r\ni ≤ tu·si−1,\r\nsince the number of probes into epoch i is clearly bounded by the total number of probes\r\nmade after epoch i. By Lemma 4.3 we can write O(\r\nsi−1\r\n`i\r\ntub) + E[T\r\nq\r\ni\r\n]δ = Ω(δ), which means\r\nO(\r\ntub\r\nr\r\n) + E[T\r\nq\r\ni\r\n]δ = Ω(δ). Setting r = Ctu\r\nb\r\nδ\r\nfor a sufficiently large constant C, we obtain\r\nE[T\r\nq\r\ni\r\n] = Ω(1). Then tq ≥\r\nP\r\ni E[T\r\nq\r\ni\r\n] = Ω(logr M) = Ω( lg n\r\nlg(tub/δ)\r\n).\r\nAs explained before, the key to improving this bound is to obtain a better bound on\r\nE[T\r\nu\r\ni\r\n]. The next section gives an analysis leading to such a result. Then, Section 4.3.3 uses\r\nthis analysis to derive our lower bounds.\r\n4.3.2 Bounding Probes into an Epoch\r\nSince we will employ two different epoch constructions, our analysis needs to talk about gen\u0002eral `i and si\r\n. However, we will need to relate to a certain exponential behavior of the epoch\r\nsizes. This property is captured by defining a parameter β = maxi\r\n∗\r\n\u0010P\r\ni≥i\r\n∗\r\nmin{`i,si−1,si∗ }\r\n`i\r\n\u0011\r\n.\r\nLemma 4.4. In expectation over a random instance of the problem and a uniformly random\r\ni ∈ {1, . . . , k}, we have E[\r\nT\r\nu\r\ni\r\n`i\r\n] = O(\r\nβ\r\nk\r\n· tu).\r\n65",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/2d8bcd06-2691-4fb7-b92a-bfb987eff774.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=29aece014741c0dd50cb8a67191c7c5128c59fb74bb8ace262139c2966741241",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 660
      },
      {
        "segments": [
          {
            "segment_id": "2d8bcd06-2691-4fb7-b92a-bfb987eff774",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 65,
            "page_width": 612,
            "page_height": 792,
            "content": "We will set M =\r\n√3 n so that the lemma applies to all epochs i. The lower bound\r\nof the lemma is reasonably easy to grasp intuitively. The first term measures the average\r\ninformation future updates learn about each of the `i updates in epoch i. There are T\r\nu\r\ni\r\nfuture\r\nprobes into epoch i. In principle, each one gathers b bits. However, there is also information\r\nhidden in the choice of which future probes hit epoch i. This amounts to O(lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n) bits\r\nper probe, since the total number of future probes is in expectation tusi−1 (there are si−1\r\nupdates in future epochs). The second term in the expression quantifies the information\r\nlearned by the query about epoch i. If the query makes T\r\nq\r\ni probes into epoch i, each one\r\nextracts b bits of information directly, and another O(lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n) bits indirectly, by the choice\r\nof which probes hit epoch i. However, there is also another way to bound the information\r\n(hence the min). If E[T\r\nq\r\ni\r\n] ≤ 1, we have probability at most T\r\nq\r\ni\r\nthat the query reads any cell\r\nfrom epoch i. If no cell is read, the information is zero. Otherwise, the relevant information\r\nis at most δ, since the answer of the query is δ bits. Finally, the lower bound on the total\r\ninformation gathered (the right hand side of the expression) is Ω(δ) because a random query\r\nneeds a random prefix sum of the updates happening in epoch i, which has Ω(δ) bits of\r\nentropy.\r\nApart from relating to T\r\nu\r\ni\r\ninstead of cell writes, the essential idea of this lemma is not\r\nnovel. However, our version is particularly general, presenting several important features.\r\nFor example, we achieve meaningful results for E[T\r\nq\r\ni\r\n] > 1, which is essential to analyzing\r\nthe case δ > b. We also get a finer bound on the “hidden information” gathered by a cell\r\nprobe, such as the O(lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n) term. In contrast, previous results could only bound this by\r\nO(lg n), which is irrelevant when b = Ω(lg n), but limits the lower bounds for the bit-probe\r\nmodel.\r\nIt is easy and instructive to apply Lemma 4.3 using the ideas of the classic chronogram\r\ntechnique. Define epochs to grow exponentially with rate r ≥ 2, i.e. `i = r\r\ni and si = O(ri\r\n).\r\nAssume for simplicity that tu and tq are worst-case bounds per operation. Then T\r\nu\r\ni ≤ tu·si−1,\r\nsince the number of probes into epoch i is clearly bounded by the total number of probes\r\nmade after epoch i. By Lemma 4.3 we can write O(\r\nsi−1\r\n`i\r\ntub) + E[T\r\nq\r\ni\r\n]δ = Ω(δ), which means\r\nO(\r\ntub\r\nr\r\n) + E[T\r\nq\r\ni\r\n]δ = Ω(δ). Setting r = Ctu\r\nb\r\nδ\r\nfor a sufficiently large constant C, we obtain\r\nE[T\r\nq\r\ni\r\n] = Ω(1). Then tq ≥\r\nP\r\ni E[T\r\nq\r\ni\r\n] = Ω(logr M) = Ω( lg n\r\nlg(tub/δ)\r\n).\r\nAs explained before, the key to improving this bound is to obtain a better bound on\r\nE[T\r\nu\r\ni\r\n]. The next section gives an analysis leading to such a result. Then, Section 4.3.3 uses\r\nthis analysis to derive our lower bounds.\r\n4.3.2 Bounding Probes into an Epoch\r\nSince we will employ two different epoch constructions, our analysis needs to talk about gen\u0002eral `i and si\r\n. However, we will need to relate to a certain exponential behavior of the epoch\r\nsizes. This property is captured by defining a parameter β = maxi\r\n∗\r\n\u0010P\r\ni≥i\r\n∗\r\nmin{`i,si−1,si∗ }\r\n`i\r\n\u0011\r\n.\r\nLemma 4.4. In expectation over a random instance of the problem and a uniformly random\r\ni ∈ {1, . . . , k}, we have E[\r\nT\r\nu\r\ni\r\n`i\r\n] = O(\r\nβ\r\nk\r\n· tu).\r\n65",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/2d8bcd06-2691-4fb7-b92a-bfb987eff774.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=29aece014741c0dd50cb8a67191c7c5128c59fb74bb8ace262139c2966741241",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 660
      },
      {
        "segments": [
          {
            "segment_id": "28da0435-7b6c-4e45-b843-3b3898c33d0f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 66,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. Fix the sequence of updates arbitrarily, which fixes all cell probes. Let T be the total\r\nnumber of cell probes made by updates. Now consider an arbitrary cell probe, and analyze\r\nthe probability it will be counted towards T\r\nu\r\ni\r\n. Let r be the time when the probe is executed,\r\nand w the time when the cell was last written, where “time” is given by the index of the\r\nupdate. Let i\r\n∗ be the unique value satisfying si\r\n∗−1 ≤ r − w < si∗ .\r\nNote that if i < i∗, for any choice of the query position after r, epoch i will begin after\r\nw. In this case, the probe cannot contribute to T\r\nu\r\ni\r\n.\r\nNow assume i ≥ i\r\n∗\r\n, and consider the positions for the query such that the cell probe\r\ncontributes to T\r\nu\r\ni\r\n. Since w must fall between the beginning of epoch i and its end, there are\r\nat most `i good query positions. In addition, epoch i − 1 must begin between w + 1 and\r\nr, so there are at most r − w < si\r\n∗ good query positions. Finally, epoch i − 1 must begin\r\nbetween r − si−1 + 1 and r, so there are at most si−1 good query positions. Since there are\r\nM possible choices for the query position, the probability the cell probe contributes to T\r\nu\r\ni\r\nis\r\nat most min{`i,si∗ ,si−1}\r\nM\r\n.\r\nWe now consider the expectation of T\r\nu\r\ni\r\n`i\r\nover the choice of i and the position of the query.\r\nWe apply linearity of expectation over the T cell probes. A probe with a certain value i\r\n∗\r\ncontributes to the terms min{`i,si∗ ,si−1}\r\nMk`i\r\nfor any i ≥ i\r\n∗\r\n. The sum of all terms for one cell\r\nprobe is bounded by β\r\nMk , so the expectation of T\r\nu\r\ni\r\n`i\r\nis bounded by βT\r\nkM . Finally, we also\r\ntake the expectation over random updates. By definition of tu, E[T] ≤ (2M − 1)tu. Then\r\nE[\r\nT\r\nu\r\ni\r\n`i\r\n] = O(\r\nβ\r\nk\r\ntu).\r\nWe now analyze the two epoch constructions that we intend to use. In the first case,\r\nepochs grow exponentially at a rate of r ≥ 2, i.e. `i = r\r\ni\r\n. Then, si ≤ 2r\r\ni\r\n, so:\r\nX\r\ni≥i\r\n∗\r\nmin{`i, si−1, si\r\n∗ }\r\n`i\r\n≤\r\nsi\r\n∗−1\r\n`i\r\n∗\r\n+\r\nX\r\ni>i∗\r\nsi\r\n∗\r\n`i\r\n≤\r\n2\r\nr\r\n+\r\nX∞\r\nj=1\r\n2\r\nr\r\nj\r\n= O\r\n\u0012\r\n1\r\nr\r\n\u0013\r\nThen, β = O(\r\n1\r\nr\r\n), and k = Θ(logr M) = Θ(logr n), so β\r\nk = O(\r\n1\r\nr logr n\r\n).\r\nIn the second case, assume r ≤\r\n√\r\nM and construct r epochs of size r\r\nj\r\n, for all j ≥ 1.\r\nThen k = Θ(r logr\r\nM\r\nr\r\n) = Θ(r logr n). Note that si ≤ (r + 2)`i, since siincludes at most r\r\nterms equal to `i, while the smaller terms represent r copies of an exponentially decreasing\r\nsum with the highest term `i\r\nr\r\n. Now we have:\r\nX\r\ni≥i\r\n∗\r\nmin{`i, si−1, si\r\n∗ }\r\n`i\r\n≤\r\nX\r\ni≥i\r\n∗\r\nmin{1,\r\nsi\r\n∗\r\n`i\r\n} ≤ X\r\ni≥i\r\n∗\r\nmin{1,\r\n(r + 2)`i\r\n∗\r\n`i\r\n} ≤ r·1+r(r+2)X∞\r\nj=1\r\n1\r\nr\r\nj\r\n= O(r)\r\nThis means β = O(r) and β\r\nk = O(\r\nr\r\nr logr n\r\n) = O(\r\n1\r\nlogr n\r\n).\r\nComparing the two constructions, we see that the second one has r times more epochs,\r\nbut also r times more probes per epoch. Intuitively, the first construction is useful for large\r\ntu, since it can still guarantee few probes into each epoch. The second one is useful when tu\r\nis already small, because it can construct more epochs, and thus prove a higher lower bound\r\non tq.\r\n66",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/28da0435-7b6c-4e45-b843-3b3898c33d0f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e38bada957d1737319b22e7b1f5cae16ab1da43b7f514e4376ee8a8f9dece088",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 658
      },
      {
        "segments": [
          {
            "segment_id": "28da0435-7b6c-4e45-b843-3b3898c33d0f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 66,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. Fix the sequence of updates arbitrarily, which fixes all cell probes. Let T be the total\r\nnumber of cell probes made by updates. Now consider an arbitrary cell probe, and analyze\r\nthe probability it will be counted towards T\r\nu\r\ni\r\n. Let r be the time when the probe is executed,\r\nand w the time when the cell was last written, where “time” is given by the index of the\r\nupdate. Let i\r\n∗ be the unique value satisfying si\r\n∗−1 ≤ r − w < si∗ .\r\nNote that if i < i∗, for any choice of the query position after r, epoch i will begin after\r\nw. In this case, the probe cannot contribute to T\r\nu\r\ni\r\n.\r\nNow assume i ≥ i\r\n∗\r\n, and consider the positions for the query such that the cell probe\r\ncontributes to T\r\nu\r\ni\r\n. Since w must fall between the beginning of epoch i and its end, there are\r\nat most `i good query positions. In addition, epoch i − 1 must begin between w + 1 and\r\nr, so there are at most r − w < si\r\n∗ good query positions. Finally, epoch i − 1 must begin\r\nbetween r − si−1 + 1 and r, so there are at most si−1 good query positions. Since there are\r\nM possible choices for the query position, the probability the cell probe contributes to T\r\nu\r\ni\r\nis\r\nat most min{`i,si∗ ,si−1}\r\nM\r\n.\r\nWe now consider the expectation of T\r\nu\r\ni\r\n`i\r\nover the choice of i and the position of the query.\r\nWe apply linearity of expectation over the T cell probes. A probe with a certain value i\r\n∗\r\ncontributes to the terms min{`i,si∗ ,si−1}\r\nMk`i\r\nfor any i ≥ i\r\n∗\r\n. The sum of all terms for one cell\r\nprobe is bounded by β\r\nMk , so the expectation of T\r\nu\r\ni\r\n`i\r\nis bounded by βT\r\nkM . Finally, we also\r\ntake the expectation over random updates. By definition of tu, E[T] ≤ (2M − 1)tu. Then\r\nE[\r\nT\r\nu\r\ni\r\n`i\r\n] = O(\r\nβ\r\nk\r\ntu).\r\nWe now analyze the two epoch constructions that we intend to use. In the first case,\r\nepochs grow exponentially at a rate of r ≥ 2, i.e. `i = r\r\ni\r\n. Then, si ≤ 2r\r\ni\r\n, so:\r\nX\r\ni≥i\r\n∗\r\nmin{`i, si−1, si\r\n∗ }\r\n`i\r\n≤\r\nsi\r\n∗−1\r\n`i\r\n∗\r\n+\r\nX\r\ni>i∗\r\nsi\r\n∗\r\n`i\r\n≤\r\n2\r\nr\r\n+\r\nX∞\r\nj=1\r\n2\r\nr\r\nj\r\n= O\r\n\u0012\r\n1\r\nr\r\n\u0013\r\nThen, β = O(\r\n1\r\nr\r\n), and k = Θ(logr M) = Θ(logr n), so β\r\nk = O(\r\n1\r\nr logr n\r\n).\r\nIn the second case, assume r ≤\r\n√\r\nM and construct r epochs of size r\r\nj\r\n, for all j ≥ 1.\r\nThen k = Θ(r logr\r\nM\r\nr\r\n) = Θ(r logr n). Note that si ≤ (r + 2)`i, since siincludes at most r\r\nterms equal to `i, while the smaller terms represent r copies of an exponentially decreasing\r\nsum with the highest term `i\r\nr\r\n. Now we have:\r\nX\r\ni≥i\r\n∗\r\nmin{`i, si−1, si\r\n∗ }\r\n`i\r\n≤\r\nX\r\ni≥i\r\n∗\r\nmin{1,\r\nsi\r\n∗\r\n`i\r\n} ≤ X\r\ni≥i\r\n∗\r\nmin{1,\r\n(r + 2)`i\r\n∗\r\n`i\r\n} ≤ r·1+r(r+2)X∞\r\nj=1\r\n1\r\nr\r\nj\r\n= O(r)\r\nThis means β = O(r) and β\r\nk = O(\r\nr\r\nr logr n\r\n) = O(\r\n1\r\nlogr n\r\n).\r\nComparing the two constructions, we see that the second one has r times more epochs,\r\nbut also r times more probes per epoch. Intuitively, the first construction is useful for large\r\ntu, since it can still guarantee few probes into each epoch. The second one is useful when tu\r\nis already small, because it can construct more epochs, and thus prove a higher lower bound\r\non tq.\r\n66",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/28da0435-7b6c-4e45-b843-3b3898c33d0f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e38bada957d1737319b22e7b1f5cae16ab1da43b7f514e4376ee8a8f9dece088",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 658
      },
      {
        "segments": [
          {
            "segment_id": "9d186b7b-5696-4082-acf8-9ec1450bb1f2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 67,
            "page_width": 612,
            "page_height": 792,
            "content": "4.3.3 Deriving the Trade-offs of Theorem 4.1\r\nWe now put together Lemma 4.3 with the analysis of the previous section to derive our lower\r\nbound trade-offs. In the previous section, we derived bounds of the form E[\r\nT\r\nu\r\ni\r\n`i\r\n] = O(\r\nβ\r\nk\r\n· tu),\r\nwhere the expectation is also over a random i. By the Markov bound, for at least 2k\r\n3\r\nchoices of\r\ni, the bound holds with the constant in the O-notation tripled. Also note that tq ≥\r\nP\r\ni E[T\r\nq\r\ni\r\n],\r\nso for at least 2k\r\n3\r\nchoices of i, we have E[T\r\nq\r\ni\r\n] ≤\r\n3tq\r\nk\r\n. Then for at least k\r\n3\r\nchoices of i the above\r\nbounds of T\r\nu\r\ni\r\nand T\r\nq\r\ni hold simultaneously. These are the i for which we apply Lemma 4.3.\r\nSince the expression of Lemma 4.3 is increasing in E[\r\nT\r\nu\r\ni\r\n`i\r\n] and E[T\r\nq\r\ni\r\n], we can substitute\r\nupper bounds for these, obtaining:\r\nβ\r\nk\r\ntu\r\n\u0012\r\nb + lg tusi−1/`i\r\n(β/k)tu\r\n\u0013\r\n+\r\ntq\r\nk\r\n· min \u001aδ, b + lg tq\r\n3tq/k\u001b\r\n= Ω(δ)\r\n⇒\r\nβ\r\nk\r\ntu\r\n\u0012\r\nb + lg si−1/`i\r\nβ/k \u0013\r\n+\r\ntq\r\nk\r\n/ max \u001a\r\n1\r\nδ\r\n,\r\n1\r\nb + lg k\r\n\u001b\r\n= Ω(δ)\r\n⇒\r\nβ\r\nk\r\ntu ·\r\nb + lg(si−1k/(`iβ))\r\nδ\r\n+\r\ntq\r\nk\r\n/\r\n\u0018\r\nδ\r\nb + lg k\r\n\u0019\r\n= Ω(1) (4.1)\r\nSince the left hand side is increasing in β\r\nk\r\n, we can again substitute an upper bound. This\r\nbound is Θ(1)\r\nr logr n\r\nfor the first epoch construction, and Θ(1)\r\nlogr n\r\nfor the second one. Also note that\r\nsi−1\r\n`i\r\n= O(\r\n1\r\nr\r\n) in the first construction and O(r) in the second one. Then lg si−1k\r\n`iβ\r\nbecomes\r\nO(lg k).\r\nNow let us analyze the trade-off implied by the first epoch construction. Note that it is\r\nvalid to substitute the upper bound lg k ≤ lg lg n in (4.1). Also, we use the calculated values\r\nfor k and β\r\nk\r\n:\r\ntu\r\nr logr n\r\n·\r\nb + lg lg n\r\nδ\r\n+\r\ntq\r\nlogr n\r\n/\r\n\u0018\r\nδ\r\nb + lg lg n\r\n\u0019\r\n= Ω(1) (4.2)\r\nWe can choose r large enough to make the first term smaller than any constant ε > 0. This\r\nis true for r satisfying ε\r\nr\r\nlg r >\r\ntu\r\nlg n\r\n·\r\nb+lg lg n\r\nδ\r\n, which holds for lg r = Θ(lg( tu\r\nlg n\r\n·\r\nb+lg lg n\r\nδ\r\n)). For a\r\nsmall enough constant ε, the second term in (4.2) must be Ω(1), which implies our tradeoff:\r\ntq lg \u0012\r\ntu\r\nlg n\r\n·\r\nb + lg lg n\r\nδ\r\n\u0013\r\n= Ω \u0012\u0018 δ\r\nb + lg lg n\r\n\u0019\r\n· lg n\r\n\u0013\r\nNow we move to the second epoch construction. Remember that k = Θ(r logr n). We\r\ncan choose r such that the second term of (4.1) is Θ(ε), i.e. bounded both from above and\r\nfrom below by small constants. For small enough ε, the O(ε) upper bound implies that the\r\nfirst term of (4.1) is Ω(1):\r\ntu\r\nlogr n\r\n·\r\nb + lg(r logr n)\r\nδ\r\n= Ω(1) ⇒ tu lg r = Ω \u0012\r\nδ\r\nb + lg(r logr n)\r\n· lg n\r\n\u0013\r\n(4.3)\r\n67",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9d186b7b-5696-4082-acf8-9ec1450bb1f2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e99fce0e920e86133741496af44d4df4dc0e685f91eb5c1a4e2ba37878730fb4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 564
      },
      {
        "segments": [
          {
            "segment_id": "9d186b7b-5696-4082-acf8-9ec1450bb1f2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 67,
            "page_width": 612,
            "page_height": 792,
            "content": "4.3.3 Deriving the Trade-offs of Theorem 4.1\r\nWe now put together Lemma 4.3 with the analysis of the previous section to derive our lower\r\nbound trade-offs. In the previous section, we derived bounds of the form E[\r\nT\r\nu\r\ni\r\n`i\r\n] = O(\r\nβ\r\nk\r\n· tu),\r\nwhere the expectation is also over a random i. By the Markov bound, for at least 2k\r\n3\r\nchoices of\r\ni, the bound holds with the constant in the O-notation tripled. Also note that tq ≥\r\nP\r\ni E[T\r\nq\r\ni\r\n],\r\nso for at least 2k\r\n3\r\nchoices of i, we have E[T\r\nq\r\ni\r\n] ≤\r\n3tq\r\nk\r\n. Then for at least k\r\n3\r\nchoices of i the above\r\nbounds of T\r\nu\r\ni\r\nand T\r\nq\r\ni hold simultaneously. These are the i for which we apply Lemma 4.3.\r\nSince the expression of Lemma 4.3 is increasing in E[\r\nT\r\nu\r\ni\r\n`i\r\n] and E[T\r\nq\r\ni\r\n], we can substitute\r\nupper bounds for these, obtaining:\r\nβ\r\nk\r\ntu\r\n\u0012\r\nb + lg tusi−1/`i\r\n(β/k)tu\r\n\u0013\r\n+\r\ntq\r\nk\r\n· min \u001aδ, b + lg tq\r\n3tq/k\u001b\r\n= Ω(δ)\r\n⇒\r\nβ\r\nk\r\ntu\r\n\u0012\r\nb + lg si−1/`i\r\nβ/k \u0013\r\n+\r\ntq\r\nk\r\n/ max \u001a\r\n1\r\nδ\r\n,\r\n1\r\nb + lg k\r\n\u001b\r\n= Ω(δ)\r\n⇒\r\nβ\r\nk\r\ntu ·\r\nb + lg(si−1k/(`iβ))\r\nδ\r\n+\r\ntq\r\nk\r\n/\r\n\u0018\r\nδ\r\nb + lg k\r\n\u0019\r\n= Ω(1) (4.1)\r\nSince the left hand side is increasing in β\r\nk\r\n, we can again substitute an upper bound. This\r\nbound is Θ(1)\r\nr logr n\r\nfor the first epoch construction, and Θ(1)\r\nlogr n\r\nfor the second one. Also note that\r\nsi−1\r\n`i\r\n= O(\r\n1\r\nr\r\n) in the first construction and O(r) in the second one. Then lg si−1k\r\n`iβ\r\nbecomes\r\nO(lg k).\r\nNow let us analyze the trade-off implied by the first epoch construction. Note that it is\r\nvalid to substitute the upper bound lg k ≤ lg lg n in (4.1). Also, we use the calculated values\r\nfor k and β\r\nk\r\n:\r\ntu\r\nr logr n\r\n·\r\nb + lg lg n\r\nδ\r\n+\r\ntq\r\nlogr n\r\n/\r\n\u0018\r\nδ\r\nb + lg lg n\r\n\u0019\r\n= Ω(1) (4.2)\r\nWe can choose r large enough to make the first term smaller than any constant ε > 0. This\r\nis true for r satisfying ε\r\nr\r\nlg r >\r\ntu\r\nlg n\r\n·\r\nb+lg lg n\r\nδ\r\n, which holds for lg r = Θ(lg( tu\r\nlg n\r\n·\r\nb+lg lg n\r\nδ\r\n)). For a\r\nsmall enough constant ε, the second term in (4.2) must be Ω(1), which implies our tradeoff:\r\ntq lg \u0012\r\ntu\r\nlg n\r\n·\r\nb + lg lg n\r\nδ\r\n\u0013\r\n= Ω \u0012\u0018 δ\r\nb + lg lg n\r\n\u0019\r\n· lg n\r\n\u0013\r\nNow we move to the second epoch construction. Remember that k = Θ(r logr n). We\r\ncan choose r such that the second term of (4.1) is Θ(ε), i.e. bounded both from above and\r\nfrom below by small constants. For small enough ε, the O(ε) upper bound implies that the\r\nfirst term of (4.1) is Ω(1):\r\ntu\r\nlogr n\r\n·\r\nb + lg(r logr n)\r\nδ\r\n= Ω(1) ⇒ tu lg r = Ω \u0012\r\nδ\r\nb + lg(r logr n)\r\n· lg n\r\n\u0013\r\n(4.3)\r\n67",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9d186b7b-5696-4082-acf8-9ec1450bb1f2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e99fce0e920e86133741496af44d4df4dc0e685f91eb5c1a4e2ba37878730fb4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 564
      },
      {
        "segments": [
          {
            "segment_id": "dbe455e8-acf5-4478-8b99-9c6b3dbfdf9f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 68,
            "page_width": 612,
            "page_height": 792,
            "content": "To understand this expression, we need the following upper bounds:\r\ntq\r\nr logr n\r\n/\r\n\u0018\r\nδ\r\nb + lg(r logr n)\r\n\u0019\r\n= Ω(ε)\r\n⇒\r\n\r\n\r\n\r\ntq\r\nr logr n\r\n/\r\n\u0010l δ\r\nb+lg lg n\r\nm\r\n·\r\n1\r\nlg r\r\n\u0011\r\n= Ω(1) ⇒ lg r = O\r\n\u0010\r\nlg \u0010\r\ntq\r\nlg n\r\n/\r\nl\r\nδ\r\nb+lg lg n\r\nm\u0011\u0011\r\ntq\r\nr logr n\r\n/\r\n\u0010\u0006\r\nδ\r\nb\r\n\u0007\r\n·\r\n1\r\nlg(r logr n)\r\n\u0011\r\n= Ω(1) ⇒ lg(r logr n) = O\r\n\u0010\r\nlg \u0010tq /\r\n\u0006\r\nδ\r\nb\r\n\u0007\r\n\u0011\u0011\r\nPlugging into (4.3), we obtain our final tradeoff:\r\ntu lg \u0012\r\ntq\r\nlg n\r\n/\r\n\u0018\r\nδ\r\nb + lg lg n\r\n\u0019\u0013 = Ω \r\nδ\r\nb + lg(tq/d\r\nδ\r\nb\r\ne)\r\n· lg n\r\n!\r\n4.3.4 Proof of Lemma 4.3\r\nRemember that our goal is to prove that for any epoch i with si ≤\r\n√3 n, the following holds\r\nin expectation over a random instance of the problem:\r\nE[T\r\nu\r\ni\r\n]\r\n`i\r\n\u0012\r\nb + lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\n+ E[T\r\nq\r\ni\r\n] · min \u001aδ, b + lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n\u001b\r\n= Ω(δ) (4.4)\r\nPick `i queries independently at random, and imagine that each is run as the query in our\r\nhard instance. That is, each of these queries operates on its own copy of the data structure,\r\nall of which are in the same state. Now we define the following random variables:\r\nQI = the indices of the `i queries.\r\nQA = the correct answers of the `i queries.\r\nU\r\nI\r\ni = the indices of the updates in epoch i.\r\nU\r\n∆\r\ni = the ∆ parameters of the updates in epoch i.\r\nU\r\nI∆\r\n¬i = the indices and ∆ parameters of the updates in all epochs except i.\r\nBy [86, Lemma 5.3], H(QA | QI, UI\r\ni\r\n, UI∆\r\n¬i\r\n) = Ω(`iδ), where H denotes conditional binary\r\nentropy. This result is very intuitive. We expect the set of query indices QIto interleave with\r\nthe set of update indices U\r\nI\r\ni\r\nin Ω(`i) places. Each interleaving gives a query that extracts δ\r\nbits of information about U\r\n∆\r\ni\r\n(it extract a partial sum linearly independent from the rest).\r\nThus, the set of query answers has Ω(`iδ) bits of entropy. The cited lemma assumes our\r\ncondition si ≤\r\n√3 n, because we do not want updates after epoch i to overwrite updates from\r\nepoch i. If there are at most √3 n updates in epoch i and later, they all touch distinct indices\r\nwith probability 1 − o(1).\r\n68",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/dbe455e8-acf5-4478-8b99-9c6b3dbfdf9f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=80fa9e8a4d5b8a426ca7a8b4a552301be36feffeee81ab419ab8ace35fe28c69",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 442
      },
      {
        "segments": [
          {
            "segment_id": "6ab7b6cd-4ebe-4a5e-993f-588d3c98e6d3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 69,
            "page_width": 612,
            "page_height": 792,
            "content": "We now propose an encoding for QA given QI and U\r\nI∆\r\n¬i\r\n. Comparing the size of this\r\nencoding with the previous information lower bound, we will obtain the conclusion of Lemma\r\n4.3. Consider the following random variables:\r\nT\r\nu\r\n<i = the number of cell probes made during epochs {1, . . . , i − 1}.\r\nT\r\nu\r\ni = as defined previously, the number of cell probes made during epochs {1, . . . , i−1} that\r\nread a cell written during epoch i.\r\nT\r\nQ = the total number of cell probes made by all `i queries.\r\nT\r\nQ\r\ni = the total number of cell probes made by all `i queries that read a cell written during\r\nepoch i.\r\nLemma 4.5. There exists an encoding for QA given QI and U\r\nI∆\r\n¬i whose size in bits is:\r\nO\r\n\u0012\r\nT\r\nu\r\ni\r\n· b + lg \u0012\r\nT\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0013\r\n+ min \u001aT\r\nQ\r\ni\r\n· δ + lg \u0012\r\n`i\r\nT\r\nQ\r\ni\r\n\u0013\r\n, T Q\r\ni\r\n· b + lg \u0012\r\nT\r\nQ\r\nT\r\nQ\r\ni\r\n\u0013\u001b\u0013\r\nProof. The encoding begins by describing the cell probes made during epochs {1, . . . , i − 1}\r\ninto epoch i. First, we specify the subset of probes reading a cell from epoch i in the set\r\nof all probes made by future epochs. This takes O\r\n\u0010\r\nlg T\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0001\r\n\u0011\r\nbits, where the O notation\r\naccounts for lower order terms from encoding the integers T\r\nu\r\n<i and T\r\nu\r\ni using O(lg T\r\nu\r\n<i) and\r\nO(lg T\r\nu\r\ni\r\n) bits respectively. Second, for all probes into epoch i, we specify the contents of the\r\ncell, taking T\r\nu\r\ni\r\n· b bits.\r\nWe now claim that based on the previous information, one can deduce the contents of all\r\ncells that were not last written during epoch i. We can of course simulate the data structure\r\nbefore epoch i, because we know the updates from U\r\nI∆\r\n¬i\r\n. Also, we can simulate the data\r\nstructure after epoch i, because we know which probes read a cell from epoch i, and we have\r\nthe cell contents in the encoding.\r\nWe now choose among two strategies for dealing with the `i queries. In the first strategy,\r\nthe encoding specifies all queries which make at least one cell-probe into epoch i. Obviously,\r\nthere are at most T\r\nQ\r\ni\r\nsuch queries, so this takes O\r\n\u0010\r\nlg  `i\r\nT\r\nQ\r\ni\r\n\u0001\r\n\u0011\r\nbits. For each query making at\r\nleast one cell probe into epoch i, we simply encode its answer using at most T\r\nQ\r\ni\r\n· δ bits in\r\ntotal. Otherwise, we can simulate the query and find the answer: we know the queried index\r\nfrom QI, and we know the contents of all cells that were last written in an epoch other than\r\ni.\r\nIn the second strategy, the encoding describes all cell probes made by the queries into\r\nepoch i. This is done by specifying which is the subset of such cell probes, and giving the\r\ncell contents for each one. Thus, in the second strategy we use T\r\nQ\r\ni\r\n· b + O\r\n\u0010\r\nlg \r\nT\r\nQ\r\nT\r\nQ\r\ni\r\n\u0001\r\n\u0011\r\nbits.\r\nGiven this information, we can simulate all queries and obtain the answers.\r\nIt is important to point out that we actually need to know which probes touch a cell\r\nwritten during epoch i. Otherwise, we would have no way to know whether a cell has been\r\nupdated during epoch i, or it still has the old value from the simulation before epoch i.\r\n69",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/6ab7b6cd-4ebe-4a5e-993f-588d3c98e6d3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9bb800e2f6c0f36616a0c717b6a66f9a30e1459bff28b94d973bad20560040ee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 623
      },
      {
        "segments": [
          {
            "segment_id": "6ab7b6cd-4ebe-4a5e-993f-588d3c98e6d3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 69,
            "page_width": 612,
            "page_height": 792,
            "content": "We now propose an encoding for QA given QI and U\r\nI∆\r\n¬i\r\n. Comparing the size of this\r\nencoding with the previous information lower bound, we will obtain the conclusion of Lemma\r\n4.3. Consider the following random variables:\r\nT\r\nu\r\n<i = the number of cell probes made during epochs {1, . . . , i − 1}.\r\nT\r\nu\r\ni = as defined previously, the number of cell probes made during epochs {1, . . . , i−1} that\r\nread a cell written during epoch i.\r\nT\r\nQ = the total number of cell probes made by all `i queries.\r\nT\r\nQ\r\ni = the total number of cell probes made by all `i queries that read a cell written during\r\nepoch i.\r\nLemma 4.5. There exists an encoding for QA given QI and U\r\nI∆\r\n¬i whose size in bits is:\r\nO\r\n\u0012\r\nT\r\nu\r\ni\r\n· b + lg \u0012\r\nT\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0013\r\n+ min \u001aT\r\nQ\r\ni\r\n· δ + lg \u0012\r\n`i\r\nT\r\nQ\r\ni\r\n\u0013\r\n, T Q\r\ni\r\n· b + lg \u0012\r\nT\r\nQ\r\nT\r\nQ\r\ni\r\n\u0013\u001b\u0013\r\nProof. The encoding begins by describing the cell probes made during epochs {1, . . . , i − 1}\r\ninto epoch i. First, we specify the subset of probes reading a cell from epoch i in the set\r\nof all probes made by future epochs. This takes O\r\n\u0010\r\nlg T\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0001\r\n\u0011\r\nbits, where the O notation\r\naccounts for lower order terms from encoding the integers T\r\nu\r\n<i and T\r\nu\r\ni using O(lg T\r\nu\r\n<i) and\r\nO(lg T\r\nu\r\ni\r\n) bits respectively. Second, for all probes into epoch i, we specify the contents of the\r\ncell, taking T\r\nu\r\ni\r\n· b bits.\r\nWe now claim that based on the previous information, one can deduce the contents of all\r\ncells that were not last written during epoch i. We can of course simulate the data structure\r\nbefore epoch i, because we know the updates from U\r\nI∆\r\n¬i\r\n. Also, we can simulate the data\r\nstructure after epoch i, because we know which probes read a cell from epoch i, and we have\r\nthe cell contents in the encoding.\r\nWe now choose among two strategies for dealing with the `i queries. In the first strategy,\r\nthe encoding specifies all queries which make at least one cell-probe into epoch i. Obviously,\r\nthere are at most T\r\nQ\r\ni\r\nsuch queries, so this takes O\r\n\u0010\r\nlg  `i\r\nT\r\nQ\r\ni\r\n\u0001\r\n\u0011\r\nbits. For each query making at\r\nleast one cell probe into epoch i, we simply encode its answer using at most T\r\nQ\r\ni\r\n· δ bits in\r\ntotal. Otherwise, we can simulate the query and find the answer: we know the queried index\r\nfrom QI, and we know the contents of all cells that were last written in an epoch other than\r\ni.\r\nIn the second strategy, the encoding describes all cell probes made by the queries into\r\nepoch i. This is done by specifying which is the subset of such cell probes, and giving the\r\ncell contents for each one. Thus, in the second strategy we use T\r\nQ\r\ni\r\n· b + O\r\n\u0010\r\nlg \r\nT\r\nQ\r\nT\r\nQ\r\ni\r\n\u0001\r\n\u0011\r\nbits.\r\nGiven this information, we can simulate all queries and obtain the answers.\r\nIt is important to point out that we actually need to know which probes touch a cell\r\nwritten during epoch i. Otherwise, we would have no way to know whether a cell has been\r\nupdated during epoch i, or it still has the old value from the simulation before epoch i.\r\n69",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/6ab7b6cd-4ebe-4a5e-993f-588d3c98e6d3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9bb800e2f6c0f36616a0c717b6a66f9a30e1459bff28b94d973bad20560040ee",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 623
      },
      {
        "segments": [
          {
            "segment_id": "329d7c4f-abda-45dc-8dbe-9603f2ea91a8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 70,
            "page_width": 612,
            "page_height": 792,
            "content": "We now aim to analyze the expected size of the encoding. By linearity of expectation over\r\nthe `i random queries, E[T\r\nQ] = tq`i and E[T\r\nQ\r\ni\r\n] = E[T\r\nq\r\ni\r\n]`i. Using convexity of x 7→ x lg y\r\nx\r\n, we\r\nhave:\r\nE\r\n\u0014\r\nlg \u0012\r\nT\r\nQ\r\nT\r\nQ\r\ni\r\n\u0013\u0015 = O\u0012\r\nE\r\n\u0014\r\nT\r\nQ\r\ni\r\n· lg T\r\nQ\r\nT\r\nQ\r\ni\r\n\u0015\u0013 = O\u0012\r\nE[T\r\nQ\r\ni\r\n] · lg E[T\r\nQ]\r\nE[T\r\nQ\r\ni\r\n]\r\n\u0013\r\n= O\r\n\u0012\r\nE[T\r\nq\r\ni\r\n]`i· lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n\u0013\r\nSimilarly, E[lg  `i\r\nT\r\nQ\r\ni\r\n\u0001\r\n] = O(E[T\r\nq\r\ni\r\n]`i· lg 1\r\nE[T\r\nq\r\ni\r\n]\r\n).\r\nTo bound T\r\nu\r\n<i, note that it is the sum of si−1 random variables Xj\r\n, each giving the\r\nnumber of probes made by the j-th update before the query. By definition of tu, the total\r\nnumber of probes made by all 2M − 1 updates is in expectation at most (2M − 1)tu. Our\r\nquery is inserted randomly in one of M possible positions, so the update described by Xjis\r\nchosen randomly among M possibilities. Then, E[Xj] ≤\r\n(2M−1)tu\r\nM < 2tu, and by linearity of\r\nexpectation E[T\r\nu\r\n<i] = O(tusi−1). Then, using convexity as before, we can bound:\r\nE\r\n\u0014\r\nlg \u0012\r\nT\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0013\u0015 = O\u0012\r\nE\r\n\u0014\r\nT\r\nu\r\ni\r\n· lg T\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0015\u0013 = O\u0012\r\nE[T\r\nu\r\ni\r\n] · lg E[T\r\nu\r\n<i]\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\n= O\r\n\u0012\r\nE[T\r\nu\r\ni\r\n] · lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\nWe now use the previous calculations and the fact E[min{a, b}] ≤ min{E[a], E[b]} to\r\nbound the expected size of the encoding. Comparing with the entropy lower bound of\r\nΩ(δ`i), we obtain:\r\nE[T\r\nu\r\ni\r\n]\r\n`i\r\n\u0012\r\nb + lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\n+ E[T\r\nq\r\ni\r\n] · min \u001aδ + lg 1\r\nE[T\r\nq\r\ni\r\n]\r\n, b + lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n\u001b\r\n≥ cδ\r\nHere c is a positive constant. This is the desired (4.4), except that the first term in the\r\nmin is δ + lg 1\r\nT\r\nq\r\ni\r\ninstead of δ. We now show that this makes no difference up to constant\r\nfactors. First of all, when the second term in the min is smaller, the expressions are obviously\r\nidentical. Otherwise, pick a constant c\r\n0 > 0 such that c0\r\nlg 1\r\nc\r\n0 ≤\r\nc\r\n2\r\n. If E[T\r\nq\r\ni\r\n] ≤ c\r\n0\r\n, we have\r\nE[T\r\nq\r\ni\r\nlg 1\r\nT\r\nq\r\ni\r\n] ≤\r\nc\r\n2\r\n. Then, moving the offending term to the right hand side, we obtain a lower\r\nbound of cδ −\r\nc\r\n2 = Ω(δ). Finally, assume E[T\r\nq\r\ni\r\n] > c0. Then (4.4) is trivially true if the\r\nconstant in the Ω notation is at most c\r\n0\r\n, because just the term E[T\r\nq\r\ni\r\nδ] is larger than the\r\nlower bound.\r\n70",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/329d7c4f-abda-45dc-8dbe-9603f2ea91a8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e999bf960d088d70e3bd00b24e61176bfcb88a24549a57cf2df941756561e09b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 519
      },
      {
        "segments": [
          {
            "segment_id": "329d7c4f-abda-45dc-8dbe-9603f2ea91a8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 70,
            "page_width": 612,
            "page_height": 792,
            "content": "We now aim to analyze the expected size of the encoding. By linearity of expectation over\r\nthe `i random queries, E[T\r\nQ] = tq`i and E[T\r\nQ\r\ni\r\n] = E[T\r\nq\r\ni\r\n]`i. Using convexity of x 7→ x lg y\r\nx\r\n, we\r\nhave:\r\nE\r\n\u0014\r\nlg \u0012\r\nT\r\nQ\r\nT\r\nQ\r\ni\r\n\u0013\u0015 = O\u0012\r\nE\r\n\u0014\r\nT\r\nQ\r\ni\r\n· lg T\r\nQ\r\nT\r\nQ\r\ni\r\n\u0015\u0013 = O\u0012\r\nE[T\r\nQ\r\ni\r\n] · lg E[T\r\nQ]\r\nE[T\r\nQ\r\ni\r\n]\r\n\u0013\r\n= O\r\n\u0012\r\nE[T\r\nq\r\ni\r\n]`i· lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n\u0013\r\nSimilarly, E[lg  `i\r\nT\r\nQ\r\ni\r\n\u0001\r\n] = O(E[T\r\nq\r\ni\r\n]`i· lg 1\r\nE[T\r\nq\r\ni\r\n]\r\n).\r\nTo bound T\r\nu\r\n<i, note that it is the sum of si−1 random variables Xj\r\n, each giving the\r\nnumber of probes made by the j-th update before the query. By definition of tu, the total\r\nnumber of probes made by all 2M − 1 updates is in expectation at most (2M − 1)tu. Our\r\nquery is inserted randomly in one of M possible positions, so the update described by Xjis\r\nchosen randomly among M possibilities. Then, E[Xj] ≤\r\n(2M−1)tu\r\nM < 2tu, and by linearity of\r\nexpectation E[T\r\nu\r\n<i] = O(tusi−1). Then, using convexity as before, we can bound:\r\nE\r\n\u0014\r\nlg \u0012\r\nT\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0013\u0015 = O\u0012\r\nE\r\n\u0014\r\nT\r\nu\r\ni\r\n· lg T\r\nu\r\n<i\r\nT\r\nu\r\ni\r\n\u0015\u0013 = O\u0012\r\nE[T\r\nu\r\ni\r\n] · lg E[T\r\nu\r\n<i]\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\n= O\r\n\u0012\r\nE[T\r\nu\r\ni\r\n] · lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\nWe now use the previous calculations and the fact E[min{a, b}] ≤ min{E[a], E[b]} to\r\nbound the expected size of the encoding. Comparing with the entropy lower bound of\r\nΩ(δ`i), we obtain:\r\nE[T\r\nu\r\ni\r\n]\r\n`i\r\n\u0012\r\nb + lg tusi−1\r\nE[T\r\nu\r\ni\r\n]\r\n\u0013\r\n+ E[T\r\nq\r\ni\r\n] · min \u001aδ + lg 1\r\nE[T\r\nq\r\ni\r\n]\r\n, b + lg tq\r\nE[T\r\nq\r\ni\r\n]\r\n\u001b\r\n≥ cδ\r\nHere c is a positive constant. This is the desired (4.4), except that the first term in the\r\nmin is δ + lg 1\r\nT\r\nq\r\ni\r\ninstead of δ. We now show that this makes no difference up to constant\r\nfactors. First of all, when the second term in the min is smaller, the expressions are obviously\r\nidentical. Otherwise, pick a constant c\r\n0 > 0 such that c0\r\nlg 1\r\nc\r\n0 ≤\r\nc\r\n2\r\n. If E[T\r\nq\r\ni\r\n] ≤ c\r\n0\r\n, we have\r\nE[T\r\nq\r\ni\r\nlg 1\r\nT\r\nq\r\ni\r\n] ≤\r\nc\r\n2\r\n. Then, moving the offending term to the right hand side, we obtain a lower\r\nbound of cδ −\r\nc\r\n2 = Ω(δ). Finally, assume E[T\r\nq\r\ni\r\n] > c0. Then (4.4) is trivially true if the\r\nconstant in the Ω notation is at most c\r\n0\r\n, because just the term E[T\r\nq\r\ni\r\nδ] is larger than the\r\nlower bound.\r\n70",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/329d7c4f-abda-45dc-8dbe-9603f2ea91a8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e999bf960d088d70e3bd00b24e61176bfcb88a24549a57cf2df941756561e09b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 519
      },
      {
        "segments": [
          {
            "segment_id": "f4a6d503-c6c3-4f4c-8f11-b854fcd7409c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 71,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 5\r\nCommunication Complexity\r\nA key tool in showing lower bounds for data structures is asymmetric communication com\u0002plexity. In this chapter, we introduce communication games and prove some useful lower\r\nbounds on their complexity. The connections to data structures are sometimes rather subtle,\r\nand will only be apparent in later chapters. While following this chapter, we hope the reader\r\nwill be motivated by the intrinsic appeal of communication problems, and the promise of\r\nfuture applications.\r\n5.1 Definitions\r\nWe consider communication games between two players, traditionally named Alice and Bob.\r\nAlice receives an input x from some set X of allowed inputs, while Bob receives an input\r\ny ∈ Y . The goal of the players is to compute some function f(x, y), by communicating to\r\nshare knowledge about their respective inputs.\r\nThe communication between Alice and Bob proceeds according to a predetermined pro\u0002tocol. Players take alternate turns to send messages; each message is a sequence of bits,\r\nwhose length and contents are dictated by the protocol. Messages must be self-delimiting,\r\ni.e. the receiving party must know when the message is over; in virtually all protocols that\r\nwe discuss, the messages of each player have fixed length, so this property tends to be trivial.\r\nAt the end of the protocol, both Alice and Bob must know the answer (the output of the\r\nfunction that they want to compute). Since most functions we deal with are boolean, a\r\nprotocol that fails this criterion can just be augmented with a final 1-bit message in which\r\nthe answer is communicated to the other player.\r\n5.1.1 Set Disjointness\r\nThe most important communication problem that we will consider is the set disjointness\r\nfunction: Alice receives a set S, Bob receives a set T, and they want to compute a bit\r\nindicating whether S ∩ T = ∅. To specify the problem entirely, assume S and T are subsets\r\nof some universe U. The problem is parameterized by u = |U|, and quantities n and m\r\n71",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f4a6d503-c6c3-4f4c-8f11-b854fcd7409c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c8659994907889664d4084b9d61c97d45dd445db548af78ca5e57dcf8ee995f6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 330
      },
      {
        "segments": [
          {
            "segment_id": "681d8e33-52d8-43f0-8310-f722998dbccd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 72,
            "page_width": 612,
            "page_height": 792,
            "content": "bounding the set sizes: |S| ≤ n, |T| ≤ m. In other words, the set X of Alice’s inputs consists\r\nof all subsets of U with at most n elements; Y consists of all subsets of U with at most m\r\nelements.\r\nIn standard complexity theory, “set disjointness” usually refers to the symmetric case\r\nn = m = u. Unfortunately, this problem is not useful for data structures, and we will\r\nconcentrate on lopsided set disjointness (LSD), where n \u001c m.\r\nAs two trivial examples of communication protocols, consider the following:\r\n• Alice sends a message of O\r\n\r\nlog2\r\n\r\nu\r\nn\r\n\u0001\u0001 bits, specifying her set. Bob now knows S, so he\r\nknows whether S ∩ T = ∅. He replies with a one-bit message, giving this answer.\r\n• Bob sends a message of O\r\n\r\nlog2\r\n\r\nu\r\nm\r\n\u0001\u0001 bits, specifying T. Alice replies with one bit,\r\ngiving the answer.\r\n5.1.2 Complexity Measures\r\nThe protocol solving a communication problem is the equivalent of an algorithm solving a\r\ncomputational problem. Given a communication problem, our goal is to design a protocol\r\nthat is as “efficient” as possible, or prove a lower bound on how efficient the best protocol\r\ncan be. The two common efficiency measures that we will use are:\r\n• the pair (A, B), where A is the total number of bits communicated by Alice during the\r\nprotocol, and B the total number of bits communicated by Bob. These quantities are\r\nmeasured in the worst case, i.e. we look at maximum number of bits communicated\r\nfor any problem instance.\r\n• the number of rounds of communication. Sometimes, we impose the restriction that\r\nmessages from Alice have some fixed length mA and messages from Bob some fixed\r\nlength mB. Then, the number of rounds is the only parameter of the protocol.\r\nBoth of our LSD protocols from above have one round of communication. In the first,\r\nA = dlog2\r\n\r\nu\r\nn\r\n\u0001\r\ne and B = 1. In the second, A = 1 and B = dlog2\r\n\r\nu\r\nm\r\n\u0001\r\ne. These suggest a\r\ntrade-off between A and B; below, we will investigate this trade-off, proving matching upper\r\nand lower bounds.\r\n5.2 Richness Lower Bounds\r\nTo introduce our first communication lower bound, we concentrate on a very simple commu\u0002nication problem: Indexing. In this problem, Alice receives an index x ∈ [m], Bob receives\r\na vector y[1 . . . m] of bits, and they want to compute y[x]. Indexing can be seen as a special\r\ncase of LSD with n = 1 and m = u, where y is the characteristic vector of U \\ T.\r\nThinking of upper bounds, what trade-off between A and B can we come up with? If\r\nAlice is allowed to communicate A bits, she can begin by sending the most significant A − 1\r\nbits of x. Bob can reply with the values of Y at every index x that is possible given Alice’s\r\nmessage. In other words, he sends a subvector from Y of length m/2\r\nA−1\r\n, as the lower order\r\n72",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/681d8e33-52d8-43f0-8310-f722998dbccd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=74d012c8f26ad1b2b2a9a1435b952d4b94caf084a1ad876a02810c1e8a8ea156",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 509
      },
      {
        "segments": [
          {
            "segment_id": "6f651a4a-e124-4df8-9f80-f3f15b31e4ee",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 73,
            "page_width": 612,
            "page_height": 792,
            "content": "log2 m − A + 1 bits of x are unknown to him. Now, Alice can just send a final bit with the\r\ncorrect answer.\r\nWe thus obtained an upper bound trade-off of B = dm/2\r\nA−1\r\ne. Intuitively, this upper\r\nbound seems the best possible. Whatever Alice sends in A bits of communication, there\r\nwill be an uncertainty of m/2\r\nA about her input. Then, it seems Bob is forced to send the\r\nplain values in his vector for all the plausible indices, or otherwise the protocol may make a\r\nmistake. Below, we formalize this intuition into a proof showing that B ≥ m/2\r\nO(A)\r\n.\r\n5.2.1 Rectangles\r\nConsider a communication problem f : X × Y → {0, 1}. A combinatorial rectangle of f is a\r\nmatrix minor of the truth table, i.e. a set X × Y, where X ⊆ X and Y ⊆ Y . While we call\r\nX × Y a rectangle, the reader should note that this is not a rectangle in our usual geometric\r\nsense, because the rows in X and the columns in Y may not be consecutive.\r\nFor a moment, suppose you are an outside observer who doesn’t know the inputs of either\r\nAlice or Bob, and you watch the communication taking place, trying to guess their inputs.\r\nAfter seeing some transcript of the communication, what have you learned about the input?\r\nClaim 5.1. The possible problem instances that lead to a fixed communication transcript are\r\nalways a combinatorial rectangle.\r\nFigure 5-1:\r\nY\r\nX0\r\nX1\r\nAlice\r\n“0”\r\nBob\r\n“1”\r\nX\r\nY0 Y1\r\nProof. This can be seen by induction on the bits sent by the protocol. Before\r\nany communication, all inputs are plausible to the outside observer, giving\r\nthe entire truth table X × Y . Say the next bit is sent by Alice. The value\r\nof the bit breaks the plausible inputs of Alice in two disjoint classes: the\r\ninputs X0 for which she would send a “zero,” and the inputs X1 for which\r\nshe would send a “one.” Observing the bit she sent, the observer’s belief\r\nabout the input changes to Xi×Y . Thus, the belief changes to a subrectangle\r\nthat drops some of the rows of the old rectangle. Similarly, when Bob sends\r\na bit, the belief changes to a subrectangle dropping some columns.\r\nNow assume that the observer has watched the communication until the end. What can\r\nwe say about the resulting rectangle?\r\nClaim 5.2. At the end of a correct deterministic protocol, one always arrives at a monochro\u0002matic rectangle (consisting entirely of zeros, or entirely of ones).\r\nProof. At the end of the protocol, both Alice and Bob must know the answer to the prob\u0002lem. But if the rectangle is not monochromatic, there exists a row or a column that is not\r\nmonochromatic. If, for instance, some row x is bichromatic, Alice sometimes makes a mistake\r\non input x. There are inputs of Bob leading both to zero and one answers, and these inputs\r\nare indistinguishable to Alice because they yield the same communication transcript.\r\nThough this may not be obvious at first, our intuition about the optimality of the protocol\r\nfor Indexing was an intuitive argument based on rectangles. We reasoned that no matter\r\n73",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/6f651a4a-e124-4df8-9f80-f3f15b31e4ee.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=184c0b7683cee72d7109a8ae27793d1aecf2def6322f273bd43e46b05b27fa91",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 540
      },
      {
        "segments": [
          {
            "segment_id": "6f651a4a-e124-4df8-9f80-f3f15b31e4ee",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 73,
            "page_width": 612,
            "page_height": 792,
            "content": "log2 m − A + 1 bits of x are unknown to him. Now, Alice can just send a final bit with the\r\ncorrect answer.\r\nWe thus obtained an upper bound trade-off of B = dm/2\r\nA−1\r\ne. Intuitively, this upper\r\nbound seems the best possible. Whatever Alice sends in A bits of communication, there\r\nwill be an uncertainty of m/2\r\nA about her input. Then, it seems Bob is forced to send the\r\nplain values in his vector for all the plausible indices, or otherwise the protocol may make a\r\nmistake. Below, we formalize this intuition into a proof showing that B ≥ m/2\r\nO(A)\r\n.\r\n5.2.1 Rectangles\r\nConsider a communication problem f : X × Y → {0, 1}. A combinatorial rectangle of f is a\r\nmatrix minor of the truth table, i.e. a set X × Y, where X ⊆ X and Y ⊆ Y . While we call\r\nX × Y a rectangle, the reader should note that this is not a rectangle in our usual geometric\r\nsense, because the rows in X and the columns in Y may not be consecutive.\r\nFor a moment, suppose you are an outside observer who doesn’t know the inputs of either\r\nAlice or Bob, and you watch the communication taking place, trying to guess their inputs.\r\nAfter seeing some transcript of the communication, what have you learned about the input?\r\nClaim 5.1. The possible problem instances that lead to a fixed communication transcript are\r\nalways a combinatorial rectangle.\r\nFigure 5-1:\r\nY\r\nX0\r\nX1\r\nAlice\r\n“0”\r\nBob\r\n“1”\r\nX\r\nY0 Y1\r\nProof. This can be seen by induction on the bits sent by the protocol. Before\r\nany communication, all inputs are plausible to the outside observer, giving\r\nthe entire truth table X × Y . Say the next bit is sent by Alice. The value\r\nof the bit breaks the plausible inputs of Alice in two disjoint classes: the\r\ninputs X0 for which she would send a “zero,” and the inputs X1 for which\r\nshe would send a “one.” Observing the bit she sent, the observer’s belief\r\nabout the input changes to Xi×Y . Thus, the belief changes to a subrectangle\r\nthat drops some of the rows of the old rectangle. Similarly, when Bob sends\r\na bit, the belief changes to a subrectangle dropping some columns.\r\nNow assume that the observer has watched the communication until the end. What can\r\nwe say about the resulting rectangle?\r\nClaim 5.2. At the end of a correct deterministic protocol, one always arrives at a monochro\u0002matic rectangle (consisting entirely of zeros, or entirely of ones).\r\nProof. At the end of the protocol, both Alice and Bob must know the answer to the prob\u0002lem. But if the rectangle is not monochromatic, there exists a row or a column that is not\r\nmonochromatic. If, for instance, some row x is bichromatic, Alice sometimes makes a mistake\r\non input x. There are inputs of Bob leading both to zero and one answers, and these inputs\r\nare indistinguishable to Alice because they yield the same communication transcript.\r\nThough this may not be obvious at first, our intuition about the optimality of the protocol\r\nfor Indexing was an intuitive argument based on rectangles. We reasoned that no matter\r\n73",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/6f651a4a-e124-4df8-9f80-f3f15b31e4ee.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=184c0b7683cee72d7109a8ae27793d1aecf2def6322f273bd43e46b05b27fa91",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 540
      },
      {
        "segments": [
          {
            "segment_id": "b475d9b1-7f9f-48ef-af69-d0f7162e3490",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 74,
            "page_width": 612,
            "page_height": 792,
            "content": "what Alice communicates in a total of A bits, she cannot (in the worst case) reduce her side\r\nof the rectangle to |X | < |X|/2\r\nA = m/2A. Suppose by symmetry that our final rectangle is\r\nmonochromatically one. Then, all of Bob’s inputs from Y must have a one at positions in\r\nX , so we have |Y| ≤ |Y |/2\r\n|X|. Since a bit of communication from Bob can at most halve Y\r\non average, Bob must communicate Ω(|X |) = Ω(m/2\r\nA) bits.\r\n5.2.2 Richness\r\nThe next step to formalizing our intuition is to break it into two claims:\r\n• If Alice sends A bits and Bob B bits, in the worst case they will obtain a 1-rectangle\r\nof size roughly |X|/2\r\nA × |Y |/2B.\r\n• Any large enough rectangle of the problem at hand contains some zeroes. The quan\u0002titative meaning of “large enough” dictates the lower bounds that can be proved.\r\nWe now formalize the first claim. Clearly, some minimal assumption about the function\r\nis needed (if, say, f were identically zero, one could never arrive at a 1-rectangle).\r\nDefinition 5.3. A function f : X ×Y → {0, 1} is called [u, v]-rich if its truth table contains\r\nat least v columns that have at least u one-entries.\r\nLemma 5.4. Let f be a [u, v]-rich problem. If f has a deterministic protocol in which\r\nAlice sends A bits and Bob sends B bits, then f contains a 1-rectangle of size at least\r\nu/2\r\nA × v/2A+B.\r\nProof. By induction on the length of the protocol. Let’s say that we are currently in a\r\nrectangle X × Y that is [u, v]-rich. We have two cases:\r\n• Bob communicates the next bit. Let’s say Y0 ⊂ Y is the set of columns for which he\r\nsends zero, and Y1 ⊂ Y is the set for which he sends one. Since X × Y contains v\r\ncolumns with at least u ones, either X × Y0 or X × Y1 contain v/2 columns with at\r\nleast u ones. We continue the induction in the [u, v\r\n2\r\n]-rich rectangle.\r\n• Alice communicates the next bit, breaking X into X0 ∪ X1. For an arbitrary column\r\namong the v columns that made X × Y rich, we can say that it either has u/2 ones in\r\nX0, or u/2 ones in X1. Thus, in either X0 ×Y or X1 ×Y, there are at least v/2 columns\r\nthat have at least u/2 ones. We continue in a rectangle that is [ u\r\n2\r\n,\r\nv\r\n2\r\n]-rich.\r\nAt the end of the protocol, we reach a monochromatic rectangle that is [u/2\r\na\r\n, v/2\r\na+b\r\n]-rich.\r\nSince the rectangle has some ones (it has nonzero richness), it must be monochromatically\r\none. Furthermore, it must have size at least u/2\r\na by v/2a+b\r\nto accommodate the richness.\r\n5.2.3 Application to Indexing\r\nTo complete the analysis of Indexing, note that the problem is [ m\r\n2\r\n, 2\r\nm−1\r\n]-rich. Indeed, half\r\nof the vector settings (i.e. 2m−1columns) have at least m\r\n2\r\nones, because either a vector or\r\nits negation have this property. By Lemma 5.4, we obtain a 1-rectangle of size m/2\r\nA+1 by\r\n2\r\nm/2A+B+1\r\n.\r\n74",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b475d9b1-7f9f-48ef-af69-d0f7162e3490.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=479c047457badc511d5671a2e8fcd8972685901c3f42d7a68cd68add74e6f888",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 541
      },
      {
        "segments": [
          {
            "segment_id": "b475d9b1-7f9f-48ef-af69-d0f7162e3490",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 74,
            "page_width": 612,
            "page_height": 792,
            "content": "what Alice communicates in a total of A bits, she cannot (in the worst case) reduce her side\r\nof the rectangle to |X | < |X|/2\r\nA = m/2A. Suppose by symmetry that our final rectangle is\r\nmonochromatically one. Then, all of Bob’s inputs from Y must have a one at positions in\r\nX , so we have |Y| ≤ |Y |/2\r\n|X|. Since a bit of communication from Bob can at most halve Y\r\non average, Bob must communicate Ω(|X |) = Ω(m/2\r\nA) bits.\r\n5.2.2 Richness\r\nThe next step to formalizing our intuition is to break it into two claims:\r\n• If Alice sends A bits and Bob B bits, in the worst case they will obtain a 1-rectangle\r\nof size roughly |X|/2\r\nA × |Y |/2B.\r\n• Any large enough rectangle of the problem at hand contains some zeroes. The quan\u0002titative meaning of “large enough” dictates the lower bounds that can be proved.\r\nWe now formalize the first claim. Clearly, some minimal assumption about the function\r\nis needed (if, say, f were identically zero, one could never arrive at a 1-rectangle).\r\nDefinition 5.3. A function f : X ×Y → {0, 1} is called [u, v]-rich if its truth table contains\r\nat least v columns that have at least u one-entries.\r\nLemma 5.4. Let f be a [u, v]-rich problem. If f has a deterministic protocol in which\r\nAlice sends A bits and Bob sends B bits, then f contains a 1-rectangle of size at least\r\nu/2\r\nA × v/2A+B.\r\nProof. By induction on the length of the protocol. Let’s say that we are currently in a\r\nrectangle X × Y that is [u, v]-rich. We have two cases:\r\n• Bob communicates the next bit. Let’s say Y0 ⊂ Y is the set of columns for which he\r\nsends zero, and Y1 ⊂ Y is the set for which he sends one. Since X × Y contains v\r\ncolumns with at least u ones, either X × Y0 or X × Y1 contain v/2 columns with at\r\nleast u ones. We continue the induction in the [u, v\r\n2\r\n]-rich rectangle.\r\n• Alice communicates the next bit, breaking X into X0 ∪ X1. For an arbitrary column\r\namong the v columns that made X × Y rich, we can say that it either has u/2 ones in\r\nX0, or u/2 ones in X1. Thus, in either X0 ×Y or X1 ×Y, there are at least v/2 columns\r\nthat have at least u/2 ones. We continue in a rectangle that is [ u\r\n2\r\n,\r\nv\r\n2\r\n]-rich.\r\nAt the end of the protocol, we reach a monochromatic rectangle that is [u/2\r\na\r\n, v/2\r\na+b\r\n]-rich.\r\nSince the rectangle has some ones (it has nonzero richness), it must be monochromatically\r\none. Furthermore, it must have size at least u/2\r\na by v/2a+b\r\nto accommodate the richness.\r\n5.2.3 Application to Indexing\r\nTo complete the analysis of Indexing, note that the problem is [ m\r\n2\r\n, 2\r\nm−1\r\n]-rich. Indeed, half\r\nof the vector settings (i.e. 2m−1columns) have at least m\r\n2\r\nones, because either a vector or\r\nits negation have this property. By Lemma 5.4, we obtain a 1-rectangle of size m/2\r\nA+1 by\r\n2\r\nm/2A+B+1\r\n.\r\n74",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b475d9b1-7f9f-48ef-af69-d0f7162e3490.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=479c047457badc511d5671a2e8fcd8972685901c3f42d7a68cd68add74e6f888",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 541
      },
      {
        "segments": [
          {
            "segment_id": "b1db1647-55e0-4428-abec-697700966140",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 75,
            "page_width": 612,
            "page_height": 792,
            "content": "If the rectangle is X × Y = {x1, x2, . . . } × {y1, y2, . . . }, every yi\r\n[xj] must be equal\r\nto one. There are only 2m−|X0\r\n| distinct yi\r\n’s which have all xj coordinates equal to one.\r\nThus, |Y| ≤ 2\r\nm−|X|, so A + B + 1 ≥ |X | = m/2A+1. We obtain the trade-off lower bound\r\nB ≥ m/2\r\nA+1 − A − 1, which implies B ≥ m/2O(A)\r\n.\r\n5.3 Direct Sum for Richness\r\nWe now return to our original goal of understanding the complexity of LSD. As with\r\nindexing, we begin by considering the upper bounds. Armed with a good understanding of\r\nthe upper bound, the lower bound will become very intuitive.\r\nFigure 5-2:\r\nA\r\nB\r\n1 Θ(n) Θ(n lg u\r\nn\r\n)\r\n1\r\nΘ(n)\r\nΘ(u)\r\nOur protocol is a simple generalization of the protocol for\r\nIndexing, in which Alice sent as many high-order bits of her\r\nvalue as she could afford. Formally, let k ≥ u be a parameter.\r\nWe break the universe [u] into k blocks of size Θ(u/k). The\r\nprotocol proceeds as follows:\r\n1. Alice sends the set of blocks in which her elements lie.\r\nThis takes A = O\r\n\r\nlg \r\nk\r\nn\r\n\u0001\u0001 = O(n lg k\r\nn\r\n) bits.\r\n2. For every block containing an element of Alice, Bob\r\nsends a vector of Θ(u/k) bits, indicating which elements\r\nare in T. This takes B = n ·\r\nu\r\nk\r\nbits.\r\n3. Alice replies with one more bit giving the answer.\r\nTo compute the trade-off, eliminate the parameter k between A and B: we have k = n·2\r\nO(A/n)\r\nand thus B = u/2\r\nO(A/n)\r\n. Values of A = o(n) are ineffective: Bob has to send Θ(u) bits,\r\njust as in the trivial protocol in which he describes his entire set. Similarly, to achieve any\r\nB = o(n), Alice has to send Θ(n lg u\r\nn\r\n) bits, which allows her to describe her set entirely. The\r\ntrade-off curve is plotted symbolically in Figure 5-2.\r\n5.3.1 A Direct Sum of Indexing Problems\r\nFigure 5-3:\r\nU\r\nS T AliceBob\r\nWe now aim to prove a lower bound matching the trade-off\r\nfrom above. This can be done by the elementary richness argu\u0002ment we used for Indexing, but the proof requires some care\u0002ful bounding of binomial coefficients that describe the rectan\u0002gle size. Instead, we choose to analyze LSD in an indirect but\r\nclean way, which also allows us to introduce an interesting\r\ntopic in communication complexity: direct sum problems.\r\nThinking back of our LSD protocol, the difficult case is\r\nwhen the values in Alice’s set fall into different blocks (if two\r\nvalues are in the same block, Bob saves some communication\r\nbecause he only needs to describe one block instead of two).\r\nThis suggests that we should construct a hard instance by breaking the universe into n\r\n75",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b1db1647-55e0-4428-abec-697700966140.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=41c623801e80baeb40afcae92e7ac4d392a7b43da3a8e17b6de748d762acdb78",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 483
      },
      {
        "segments": [
          {
            "segment_id": "da828eae-fe5a-4cd7-a12a-01b75b4505b1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 76,
            "page_width": 612,
            "page_height": 792,
            "content": "blocks, and placing one of Alice’s value in each block (Figure 5-3). Intuitively, this LSD\r\nproblem consists of n independent copies of indexing: each of Alice’s values indexes into a\r\nvector of u/n bits, describing a block of Bob’s set. The sets S and T are disjoint if and only\r\nif all of the n indices hit elements outside Bob’s set (which we can indicate by a “one” in\r\nthe vector being indexed). Thus, in our family of instances, the LSD query has become the\r\nlogical and of n Indexing queries.\r\nDefinition 5.5. Given a communication problem f : X ×Y → {0, 1}, let the communication\r\nproblem Vnf : Xn × Y\r\nn → {0, 1} be defined by  Vn\r\nf\r\n\u0001\r\n(~x, ~y) = Q\r\ni\r\nfi(xi, yi).\r\nThis is an example of a direct sum problem, in which Alice and Bob each receive n\r\nindependent inputs; the players want to output an aggregate (in this case, the logical and)\r\nof the function f applied to each pair of corresponding inputs. As we have explained above\r\nVn\r\nIndexing is a special case of LSD.\r\nIntuitively, a n-wise direct sum problem should have a lower bound that is n times larger\r\nthan the original. While it can be shown that this property is not always true, we can prove\r\nthat any richness lower bound gets amplified k-fold, in the following sense:\r\nTheorem 5.6. Let f : X × Y → {0, 1} be [ρ|X|, v]-rich, and assume Vnf has a communi\u0002cation protocol in which Alice sends A = n · a bits and Bob sends B = n · b bits. Then f has\r\na 1-rectangle of size ρ\r\nO(1)|X|/2O(a) × v/2O(a+b)\r\n.\r\nBefore we prove the theorem, let us see that it implies an optimal lower bound for LSD.\r\nWe showed that Indexing is [ |X|\r\n2\r\n,\r\n|Y |\r\n2\r\n]-rich. Then, if LSD has a protocol in which Alice sends\r\nA bits and Bob sends B bits, the theorem finds a 1-rectangle of Indexing of size |X|/2\r\nO(A/n)\r\nby |Y |/2\r\nO(A+B)/n. But we showed that any 1-rectangle X × Y must have |Y| ≤ |Y |/2|X|, so\r\n2\r\nO(A+B)/n ≥ |X | = |X|/2O(A/n)\r\n. The instance of Indexing are on blocks of size u/n, thus\r\n|X| =\r\nu\r\nn\r\n, and we obtain the trade-off:\r\nTheorem 5.7. Fix δ > 0. In a deterministic protocol for LSD, either Alice sends Ω(n lg m\r\nn\r\n)\r\nbits, or Bob sends n ·\r\nm\r\nn\r\n\u00011−δ\r\nbits.\r\n5.3.2 Proof of Theorem 5.6\r\nWe begin by restricting Y to the v columns with u one entries. This maintains richness, and\r\ndoesn’t affect anything about the protocol.\r\nClaim 5.8. Vnf is [(ρ|X|)\r\nn\r\n, vn]-rich.\r\nProof. Since Vnf only has v\r\nn\r\ncolumns, we want to show that all columns contain enough\r\nones. Let ~y ∈ Y\r\nn be arbitrary. The set of ~x ∈ Xn with  Vn\r\nf\r\n\u0001\r\n(~x, ~y) = 1 is just the n-wise\r\nCartesian product of the sets {x ∈ X | f(x, yi) = 1}. But each set in the product has at\r\nleast ρ|X| elements by richness of f.\r\nNow we apply Lemma 5.4 to find a 1-rectangle of Vnf of size (ρ|X|)\r\nn/2A × vn/2A+B,\r\nwhich can be rewritten as ( ρ\r\n2\r\na |X|)\r\nn × (\r\n1\r\n2\r\na+b\r\n|Y |)\r\nn\r\n. Then, we complete the proof of the\r\ntheorem by applying the following claim:\r\n76",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/da828eae-fe5a-4cd7-a12a-01b75b4505b1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d934602ba63ae498c196e3a04aecc29bb0a799172f97b7207c91d7c024c3542b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 579
      },
      {
        "segments": [
          {
            "segment_id": "da828eae-fe5a-4cd7-a12a-01b75b4505b1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 76,
            "page_width": 612,
            "page_height": 792,
            "content": "blocks, and placing one of Alice’s value in each block (Figure 5-3). Intuitively, this LSD\r\nproblem consists of n independent copies of indexing: each of Alice’s values indexes into a\r\nvector of u/n bits, describing a block of Bob’s set. The sets S and T are disjoint if and only\r\nif all of the n indices hit elements outside Bob’s set (which we can indicate by a “one” in\r\nthe vector being indexed). Thus, in our family of instances, the LSD query has become the\r\nlogical and of n Indexing queries.\r\nDefinition 5.5. Given a communication problem f : X ×Y → {0, 1}, let the communication\r\nproblem Vnf : Xn × Y\r\nn → {0, 1} be defined by  Vn\r\nf\r\n\u0001\r\n(~x, ~y) = Q\r\ni\r\nfi(xi, yi).\r\nThis is an example of a direct sum problem, in which Alice and Bob each receive n\r\nindependent inputs; the players want to output an aggregate (in this case, the logical and)\r\nof the function f applied to each pair of corresponding inputs. As we have explained above\r\nVn\r\nIndexing is a special case of LSD.\r\nIntuitively, a n-wise direct sum problem should have a lower bound that is n times larger\r\nthan the original. While it can be shown that this property is not always true, we can prove\r\nthat any richness lower bound gets amplified k-fold, in the following sense:\r\nTheorem 5.6. Let f : X × Y → {0, 1} be [ρ|X|, v]-rich, and assume Vnf has a communi\u0002cation protocol in which Alice sends A = n · a bits and Bob sends B = n · b bits. Then f has\r\na 1-rectangle of size ρ\r\nO(1)|X|/2O(a) × v/2O(a+b)\r\n.\r\nBefore we prove the theorem, let us see that it implies an optimal lower bound for LSD.\r\nWe showed that Indexing is [ |X|\r\n2\r\n,\r\n|Y |\r\n2\r\n]-rich. Then, if LSD has a protocol in which Alice sends\r\nA bits and Bob sends B bits, the theorem finds a 1-rectangle of Indexing of size |X|/2\r\nO(A/n)\r\nby |Y |/2\r\nO(A+B)/n. But we showed that any 1-rectangle X × Y must have |Y| ≤ |Y |/2|X|, so\r\n2\r\nO(A+B)/n ≥ |X | = |X|/2O(A/n)\r\n. The instance of Indexing are on blocks of size u/n, thus\r\n|X| =\r\nu\r\nn\r\n, and we obtain the trade-off:\r\nTheorem 5.7. Fix δ > 0. In a deterministic protocol for LSD, either Alice sends Ω(n lg m\r\nn\r\n)\r\nbits, or Bob sends n ·\r\nm\r\nn\r\n\u00011−δ\r\nbits.\r\n5.3.2 Proof of Theorem 5.6\r\nWe begin by restricting Y to the v columns with u one entries. This maintains richness, and\r\ndoesn’t affect anything about the protocol.\r\nClaim 5.8. Vnf is [(ρ|X|)\r\nn\r\n, vn]-rich.\r\nProof. Since Vnf only has v\r\nn\r\ncolumns, we want to show that all columns contain enough\r\nones. Let ~y ∈ Y\r\nn be arbitrary. The set of ~x ∈ Xn with  Vn\r\nf\r\n\u0001\r\n(~x, ~y) = 1 is just the n-wise\r\nCartesian product of the sets {x ∈ X | f(x, yi) = 1}. But each set in the product has at\r\nleast ρ|X| elements by richness of f.\r\nNow we apply Lemma 5.4 to find a 1-rectangle of Vnf of size (ρ|X|)\r\nn/2A × vn/2A+B,\r\nwhich can be rewritten as ( ρ\r\n2\r\na |X|)\r\nn × (\r\n1\r\n2\r\na+b\r\n|Y |)\r\nn\r\n. Then, we complete the proof of the\r\ntheorem by applying the following claim:\r\n76",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/da828eae-fe5a-4cd7-a12a-01b75b4505b1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d934602ba63ae498c196e3a04aecc29bb0a799172f97b7207c91d7c024c3542b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 579
      },
      {
        "segments": [
          {
            "segment_id": "ebd04e91-4f27-4033-a6e6-6df8dd07fe76",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 77,
            "page_width": 612,
            "page_height": 792,
            "content": "Claim 5.9. If Vn\r\nf contains a 1-rectangle of dimensions (α|X|)\r\nn × (β|Y |)n\r\n, then f contains\r\na 1-rectangle of dimensions α\r\n3\r\n|X| × β\r\n3\r\n|Y |.\r\nProof. Let X × Y be the 1-rectangle of Vn\r\nf. Also let Xi and Yi be the projections of X and\r\nY on the i-th coordinate, i.e. Xi = {xi| ~x ∈ X }. Note that for all i, Xi × Yiis a 1-rectangle\r\nfor fi. Indeed, for any (x, y) ∈ Xi × Yi, there must exists some (~x, ~y) ∈ X × Y with ~x[i] = x\r\nand ~y[i] = y. But  Vnf\r\n\u0001\r\n(~x, ~y) = Q\r\nj\r\nf(xj, yj ) = 1 by assumption, so f(x, y) = 1.\r\nNow note that there must be at least 2\r\n3\r\nn dimensions with |Xi| ≥ α\r\n3\r\n|X|. Otherwise, we\r\nwould have |X | ≤ Q\r\ni\r\n|Xi| < (α\r\n3\r\n|X|)\r\nk/3\r\n· |X|\r\n2k/3 = (α|X|)k = |X |, contradiction. Similarly,\r\nthere must be at least 2\r\n3\r\nk dimensions with |Yi| ≥ β\r\n3\r\n|Y |. Consequently, there must be an\r\noverlap of these good dimensions, satisfying the statement of the lemma.\r\nThis completes the proof of Theorem 5.6.\r\n5.4 Randomized Lower Bounds\r\n5.4.1 Warm-Up\r\nWe first prove a slightly weaker randomized lower bound for LSD:\r\nTheorem 5.10. Assume Alice receives a set S, |S| = m and Bob receives a set T, |T| = n,\r\nboth sets coming from a universe of size 2mn, for m < nγ, where γ < 1/3 is a constant. In\r\nany randomized, two-sided error communication protocol deciding disjointness of S and T,\r\neither Alice sends Ω( m\r\nlog m\r\nlg n) bits or Bob sends Ω(n\r\n1−δ/m2\r\n) bits, for any δ > 0.\r\nFirst we define the hard instance. The elements of our sets come from the universe\r\n[2m] × [n]. Alice receives S = {(i, si) | i ∈ [m]}, for s1, . . . , sm chosen independently at\r\nrandom from [n]. Bob receives T = {(tj, j) | j ∈ [n], for t1, . . . , tn chosen independently from\r\n[2m]. The output should be 1 iff the sets are disjoint. Note that the number of choices is\r\nn\r\nm for S and (2m)n\r\nfor T, and that S and T are chosen independently.\r\nThe lower bound follows from the following variant of the richness lemma, based on [73,\r\nLemma 6]. The only change is that we make the dependence on ε explicit, because we will\r\nuse ε = o(1).\r\nLemma 5.11. Consider a problem f : X × Y → {0, 1}, such that the density of {(x, y) |\r\nf(x, y) = 1} in X × Y is Ω(1). If f has a randomized two-sided error [a, b]-protocol, then\r\nthere is a rectangle of f of dimensions at least |X|/2\r\nO(a lg(1/ε)) × |Y |/2O((a+b) lg(1/ε)) in which\r\nthe density of zeros is at most ε.\r\nTo apply the lemma, we first show the disjointness function is 1 with constant probability.\r\nLemma 5.12. As S and T are chosen randomly as described above, Pr[S ∩ T = ∅] = Ω(1).\r\nProof. Note that S ∩T ⊂ [n]×[m]. We have Pr[(i, j) ∈ S ∩T] = 1\r\nn(2m) when i ∈ [n], j ∈ [m].\r\nThen by linearity of expectation E[|S ∩ T|] = 1\r\n2\r\n. Since |S ∩ T| ∈ {0, 1, 2, . . . }, we must have\r\nPr[|S ∩ T| = 0] ≥\r\n1\r\n2\r\n.\r\n77",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/ebd04e91-4f27-4033-a6e6-6df8dd07fe76.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=02c0f030fe134e014e4d5d01cf632b82f4a3d7f4f21cfa1bf97418dde2eae00e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 593
      },
      {
        "segments": [
          {
            "segment_id": "ebd04e91-4f27-4033-a6e6-6df8dd07fe76",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 77,
            "page_width": 612,
            "page_height": 792,
            "content": "Claim 5.9. If Vn\r\nf contains a 1-rectangle of dimensions (α|X|)\r\nn × (β|Y |)n\r\n, then f contains\r\na 1-rectangle of dimensions α\r\n3\r\n|X| × β\r\n3\r\n|Y |.\r\nProof. Let X × Y be the 1-rectangle of Vn\r\nf. Also let Xi and Yi be the projections of X and\r\nY on the i-th coordinate, i.e. Xi = {xi| ~x ∈ X }. Note that for all i, Xi × Yiis a 1-rectangle\r\nfor fi. Indeed, for any (x, y) ∈ Xi × Yi, there must exists some (~x, ~y) ∈ X × Y with ~x[i] = x\r\nand ~y[i] = y. But  Vnf\r\n\u0001\r\n(~x, ~y) = Q\r\nj\r\nf(xj, yj ) = 1 by assumption, so f(x, y) = 1.\r\nNow note that there must be at least 2\r\n3\r\nn dimensions with |Xi| ≥ α\r\n3\r\n|X|. Otherwise, we\r\nwould have |X | ≤ Q\r\ni\r\n|Xi| < (α\r\n3\r\n|X|)\r\nk/3\r\n· |X|\r\n2k/3 = (α|X|)k = |X |, contradiction. Similarly,\r\nthere must be at least 2\r\n3\r\nk dimensions with |Yi| ≥ β\r\n3\r\n|Y |. Consequently, there must be an\r\noverlap of these good dimensions, satisfying the statement of the lemma.\r\nThis completes the proof of Theorem 5.6.\r\n5.4 Randomized Lower Bounds\r\n5.4.1 Warm-Up\r\nWe first prove a slightly weaker randomized lower bound for LSD:\r\nTheorem 5.10. Assume Alice receives a set S, |S| = m and Bob receives a set T, |T| = n,\r\nboth sets coming from a universe of size 2mn, for m < nγ, where γ < 1/3 is a constant. In\r\nany randomized, two-sided error communication protocol deciding disjointness of S and T,\r\neither Alice sends Ω( m\r\nlog m\r\nlg n) bits or Bob sends Ω(n\r\n1−δ/m2\r\n) bits, for any δ > 0.\r\nFirst we define the hard instance. The elements of our sets come from the universe\r\n[2m] × [n]. Alice receives S = {(i, si) | i ∈ [m]}, for s1, . . . , sm chosen independently at\r\nrandom from [n]. Bob receives T = {(tj, j) | j ∈ [n], for t1, . . . , tn chosen independently from\r\n[2m]. The output should be 1 iff the sets are disjoint. Note that the number of choices is\r\nn\r\nm for S and (2m)n\r\nfor T, and that S and T are chosen independently.\r\nThe lower bound follows from the following variant of the richness lemma, based on [73,\r\nLemma 6]. The only change is that we make the dependence on ε explicit, because we will\r\nuse ε = o(1).\r\nLemma 5.11. Consider a problem f : X × Y → {0, 1}, such that the density of {(x, y) |\r\nf(x, y) = 1} in X × Y is Ω(1). If f has a randomized two-sided error [a, b]-protocol, then\r\nthere is a rectangle of f of dimensions at least |X|/2\r\nO(a lg(1/ε)) × |Y |/2O((a+b) lg(1/ε)) in which\r\nthe density of zeros is at most ε.\r\nTo apply the lemma, we first show the disjointness function is 1 with constant probability.\r\nLemma 5.12. As S and T are chosen randomly as described above, Pr[S ∩ T = ∅] = Ω(1).\r\nProof. Note that S ∩T ⊂ [n]×[m]. We have Pr[(i, j) ∈ S ∩T] = 1\r\nn(2m) when i ∈ [n], j ∈ [m].\r\nThen by linearity of expectation E[|S ∩ T|] = 1\r\n2\r\n. Since |S ∩ T| ∈ {0, 1, 2, . . . }, we must have\r\nPr[|S ∩ T| = 0] ≥\r\n1\r\n2\r\n.\r\n77",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/ebd04e91-4f27-4033-a6e6-6df8dd07fe76.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=02c0f030fe134e014e4d5d01cf632b82f4a3d7f4f21cfa1bf97418dde2eae00e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 593
      },
      {
        "segments": [
          {
            "segment_id": "da2eb2e9-cead-4a05-9c1d-98c642a2446f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 78,
            "page_width": 612,
            "page_height": 792,
            "content": "Thus, it remains to show that no big enough rectangle has a small density of zeros.\r\nSpecifically, we show the following:\r\nLemma 5.13. Let δ > 0 be arbitrary. If we choose S ∈ S, T ∈ T uniformly and indepen\u0002dently at random, where |S| > 2n\r\n(1−δ)m and T ≥ (2m)n\r\n· 2/en\r\n1−δ/(8m2\r\n)\r\n, then the probability\r\nS ∩ T 6= ∅ is at least 1\r\n16m2\r\n.\r\nWe use the richness lemma with ε =\r\n1\r\n32m2\r\n. If there exists an [a, b] protocol for our\r\nproblem, we can find a rectangle of size n\r\nm/2O(a lg m)\r\n\u0001\r\n×\r\n\r\n(2m)\r\nn/2O((a+b) lg m)\r\n\u0001\r\n, in which\r\nthe fraction of zeros is at most ε. To avoid contradicting Lemma 5.13, we must either have\r\n2\r\nO(a lg m) > nδm/2, or 2O((a+b) lg m) > en\r\n1−δ/(8m2\r\n)/2. This means either a = Ω( m\r\nlg m\r\nlg n) or\r\na + b = Ω(n\r\n1−δ/(m2\r\nlg m)). If m < nγ, for constant γ < 1\r\n3\r\n, this implies that a = Ω( m\r\nlg m\r\nlg n)\r\nor b = Ω(n\r\n1−δ/m2\r\n), for any δ > 0.\r\nProof. (of Lemma 5.13) Choosing S at random from S induces a marginal distribution on\r\n[n]. Now consider the heaviest n\r\n1−δ\r\nelements in this distribution. If the total probability\r\nmass of these elements is at most 1 −\r\n1\r\n2m\r\n, we call i a well-spread coordinate.\r\nLemma 5.14. If |S| > 2n\r\n(1−δ)m, there exists a well-spread coordinate.\r\nProof. Assume for contradiction that no coordinate is well-spread. Consider the set S\r\n0\r\nformed by S ∈ S such that no siis outside the heaviest n\r\n1−δ\r\nelements in Si. By a union\r\nbound, the probability over S ∈ S that some siis not among the heavy elements is at most\r\nm 1\r\n2m =\r\n1\r\n2\r\n. Then, |S0| ≥ |S|/2. On the other hand |S0| ≤ (n\r\n1−δ\r\n)\r\nm, since for each coordinate\r\nwe have at most n\r\n1−δ\r\nchoices. This contradicts the lower bound on |S|.\r\nLet i be a well-spread coordinate. We now lower bound the probability of S ∩ T 6= ∅ by\r\nthe probability of S ∩ T containing an element on coordinate i. Furthermore, we ignore the\r\nn\r\n1−δ heaviest elements of Si\r\n. Let the remaining elements be W, and p(j) = Pr[si = j] when\r\nj ∈ W. Note that p(j) ≤ 1/n1−δ, and P\r\nj∈W p(j) ≥\r\n1\r\n2m\r\n.\r\nDefine σ(T) = P\r\nj∈W:tj=i\r\np(j). For some choice of T, σ(T) gives exactly the probability\r\nof an interesting intersection, over the choice of S ∈ S. Thus, we want to lower bound\r\nET [σ(T) | T ∈ T ].\r\nAssume for now that T is uniformly distributed in the original space (not in the subspace\r\nT ). Note that σ(T) = P\r\nj∈W Xj\r\n, where Xjis a variable equal to p(j) when tj = i and 0\r\notherwise. By linearity of expectation, ET [σ(T)] = P\r\nj∈W\r\np(j)\r\n2m ≥ 1/(2m)\r\n2\r\n. Since Xj’s are\r\nindependent (tj’s are independent when T is not restricted), we can use a Chernoff bound\r\nto deduce σ(T) is close to this expectation with very high probability over the choice of T.\r\nIndeed, Pr[σ(T) <\r\n1\r\n2\r\n·\r\n1\r\n(2m)\r\n2\r\n] < e−n\r\n1−δ/(8m2\r\n)\r\n.\r\nNow we can restrict ourselves to T ∈ T . The probability σ(T) <\r\n1\r\n8m2\r\nis so small,\r\nthat it remains small even in this restricted subspace. Specifically, this probability is at\r\nmost Pr[σ(T) <\r\n1\r\n8m2\r\n]/ Pr[T ∈ T ] ≤ exp(−n\r\n1−δ/(8m2\r\n))/(2 exp(−n\r\n1−δ/(8m2\r\n))) = 1\r\n2\r\n. Since\r\nσ(T) ≥ 0,(∀)T, we conclude that ET [σ(T) | T ∈ T ] ≥\r\n1\r\n2\r\n·\r\n1\r\n8m2 =\r\n1\r\n16m2\r\n.\r\n78",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/da2eb2e9-cead-4a05-9c1d-98c642a2446f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d289780ee866ee375b54cb63b58de0a508c15abbc742a872d4358101222ad512",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 643
      },
      {
        "segments": [
          {
            "segment_id": "da2eb2e9-cead-4a05-9c1d-98c642a2446f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 78,
            "page_width": 612,
            "page_height": 792,
            "content": "Thus, it remains to show that no big enough rectangle has a small density of zeros.\r\nSpecifically, we show the following:\r\nLemma 5.13. Let δ > 0 be arbitrary. If we choose S ∈ S, T ∈ T uniformly and indepen\u0002dently at random, where |S| > 2n\r\n(1−δ)m and T ≥ (2m)n\r\n· 2/en\r\n1−δ/(8m2\r\n)\r\n, then the probability\r\nS ∩ T 6= ∅ is at least 1\r\n16m2\r\n.\r\nWe use the richness lemma with ε =\r\n1\r\n32m2\r\n. If there exists an [a, b] protocol for our\r\nproblem, we can find a rectangle of size n\r\nm/2O(a lg m)\r\n\u0001\r\n×\r\n\r\n(2m)\r\nn/2O((a+b) lg m)\r\n\u0001\r\n, in which\r\nthe fraction of zeros is at most ε. To avoid contradicting Lemma 5.13, we must either have\r\n2\r\nO(a lg m) > nδm/2, or 2O((a+b) lg m) > en\r\n1−δ/(8m2\r\n)/2. This means either a = Ω( m\r\nlg m\r\nlg n) or\r\na + b = Ω(n\r\n1−δ/(m2\r\nlg m)). If m < nγ, for constant γ < 1\r\n3\r\n, this implies that a = Ω( m\r\nlg m\r\nlg n)\r\nor b = Ω(n\r\n1−δ/m2\r\n), for any δ > 0.\r\nProof. (of Lemma 5.13) Choosing S at random from S induces a marginal distribution on\r\n[n]. Now consider the heaviest n\r\n1−δ\r\nelements in this distribution. If the total probability\r\nmass of these elements is at most 1 −\r\n1\r\n2m\r\n, we call i a well-spread coordinate.\r\nLemma 5.14. If |S| > 2n\r\n(1−δ)m, there exists a well-spread coordinate.\r\nProof. Assume for contradiction that no coordinate is well-spread. Consider the set S\r\n0\r\nformed by S ∈ S such that no siis outside the heaviest n\r\n1−δ\r\nelements in Si. By a union\r\nbound, the probability over S ∈ S that some siis not among the heavy elements is at most\r\nm 1\r\n2m =\r\n1\r\n2\r\n. Then, |S0| ≥ |S|/2. On the other hand |S0| ≤ (n\r\n1−δ\r\n)\r\nm, since for each coordinate\r\nwe have at most n\r\n1−δ\r\nchoices. This contradicts the lower bound on |S|.\r\nLet i be a well-spread coordinate. We now lower bound the probability of S ∩ T 6= ∅ by\r\nthe probability of S ∩ T containing an element on coordinate i. Furthermore, we ignore the\r\nn\r\n1−δ heaviest elements of Si\r\n. Let the remaining elements be W, and p(j) = Pr[si = j] when\r\nj ∈ W. Note that p(j) ≤ 1/n1−δ, and P\r\nj∈W p(j) ≥\r\n1\r\n2m\r\n.\r\nDefine σ(T) = P\r\nj∈W:tj=i\r\np(j). For some choice of T, σ(T) gives exactly the probability\r\nof an interesting intersection, over the choice of S ∈ S. Thus, we want to lower bound\r\nET [σ(T) | T ∈ T ].\r\nAssume for now that T is uniformly distributed in the original space (not in the subspace\r\nT ). Note that σ(T) = P\r\nj∈W Xj\r\n, where Xjis a variable equal to p(j) when tj = i and 0\r\notherwise. By linearity of expectation, ET [σ(T)] = P\r\nj∈W\r\np(j)\r\n2m ≥ 1/(2m)\r\n2\r\n. Since Xj’s are\r\nindependent (tj’s are independent when T is not restricted), we can use a Chernoff bound\r\nto deduce σ(T) is close to this expectation with very high probability over the choice of T.\r\nIndeed, Pr[σ(T) <\r\n1\r\n2\r\n·\r\n1\r\n(2m)\r\n2\r\n] < e−n\r\n1−δ/(8m2\r\n)\r\n.\r\nNow we can restrict ourselves to T ∈ T . The probability σ(T) <\r\n1\r\n8m2\r\nis so small,\r\nthat it remains small even in this restricted subspace. Specifically, this probability is at\r\nmost Pr[σ(T) <\r\n1\r\n8m2\r\n]/ Pr[T ∈ T ] ≤ exp(−n\r\n1−δ/(8m2\r\n))/(2 exp(−n\r\n1−δ/(8m2\r\n))) = 1\r\n2\r\n. Since\r\nσ(T) ≥ 0,(∀)T, we conclude that ET [σ(T) | T ∈ T ] ≥\r\n1\r\n2\r\n·\r\n1\r\n8m2 =\r\n1\r\n16m2\r\n.\r\n78",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/da2eb2e9-cead-4a05-9c1d-98c642a2446f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d289780ee866ee375b54cb63b58de0a508c15abbc742a872d4358101222ad512",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 643
      },
      {
        "segments": [
          {
            "segment_id": "1b33bce5-7e86-4772-96a9-67f7fc39c22a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 79,
            "page_width": 612,
            "page_height": 792,
            "content": "5.4.2 A Strong Lower Bound\r\nWe now strengthen Theorem 5.10 to the following:\r\nTheorem 5.15. Assume Alice receives a set S, |S| = m and Bob receives a set T, |T| = n,\r\nboth sets coming from a universe of size 2mn, for m < nγ, where γ < 1 is a constant. In\r\nany randomized, two-sided error communication protocol deciding disjointness of S and T,\r\neither Alice sends Ω(m lg n) bits or Bob sends Ω(n\r\n1−δ\r\n) bits, for any δ > 0.\r\nProof. We have a quadratic universe of size m · n, which we view as n blocks of size m.\r\nAlice’s set contains a random point in each block, and Bob’s set contains m/n points in the\r\neach block. Note that this is a product distribution and the function is balanced (Pr[S ∩T =\r\n∅] = Ω(1) and Pr[S ∩T 6= ∅] = Ω(1)). This suggests that we should use randomized richness.\r\nBy Lemma 5.11, the problem boils down to proving that in a big enough rectangle S ×T ,\r\nPr[S ∩ T 6= ∅ | S ∈ S, T ∈ T ] = Ω(1).\r\nFor values in our universe i ∈ [mn], we let p(i) be the probability that a set S ∈ S\r\ncontains i. Note that S contains one element in each block, so p(·) is a probability density\r\nfunction on each block. We have H(S ∈ S) ≥ n lg m − o(n lg m\r\nn\r\n) by assuming that the\r\nrectangle is large (Alice communicate o(n lg m\r\nn\r\n) bits). Each S is a vector of n choices, each\r\nfrom a block of size m. The entropy of almost all coordinates must be lg m − o(lg m\r\nn\r\n). This\r\nmeans that the distribution p(·) has weight 1 − o(1) on values with p(i) <\r\n(m/n)\r\no(1)\r\nm\r\n. (If too\r\nmany points have high probability, the entropy must be far from the maximum lg m.)\r\nHaving fixed p(·) (depending on S), let us calculate f(T) = ES∈S[|S ∩ T|]. This is\r\nobviously P\r\ni∈T\r\np(i). Now we are going to pick T from the original marginal distribution\r\n(ignore T for now), and we’re going to look at the distribution induced on f(T). The\r\nmean is m ·\r\n1\r\nm = 1, because the p(i)’s on each block summed up to one. Now note that\r\neach element of T is chosen independently and picks some p(i) to be added to the sum.\r\nThus we have a Chernoff bound kicking in, and f(T) is concentrated around its mean. The\r\nprobability of deviating a constant from the mean is given by the upper bounds on p(i). If\r\nmax p(i) = α · mean p(i) = α\r\nm\r\n, the Chernoff bound is exponential in m/ poly(α). But we\r\nshowed that 1 − o(1) of the weight is on values with p(i) <\r\n(m/n)\r\no(1)\r\nm\r\n. So we can discard the\r\nbig values, still keeping the expectation of f(T) = 1 − o(1), and apply a Chernoff bound\r\nexponential in m/(m/n)\r\no(1) = n · (\r\nm\r\nn\r\n)\r\n1−o(1)\r\n.\r\nNow if Bob communicated n · (\r\nm\r\nn\r\n)\r\n1−ε\r\nfor ε > 0, then T is so large, that even if all the\r\ndeviation from the Chernoff bound is inside T , it cannot change the average of f(T) by more\r\nthan a constant. (Formally Pr[S] is ω of the Chernoff probability.) Then the average of f(T)\r\neven on T is Ω(1).\r\nWe have just shown that ES×T [|S ∩ T|] = Ω(1). But how do we get E[|S ∩ T|] = Ω(1)\r\nto mean Pr[S ∩ T 6= ∅] = Ω(1)? We are going to consider a bicriterion error function:\r\nεe(S, T) = ε(S, T) + α · |S ∩ T|\r\n2\r\n. Here ε was the error measure of the protocol, and α is a\r\nsmall enough constant. By concentration Var[|S ∩ T|] = O(1) so εe is bounded by a small\r\nconstant: amplify the original protocol to reduce ε, and choose α small.\r\nRandomized richness gives us a large rectangle for which the protocol answers “disjoint”,\r\nand in which the error is a constant bigger than the original. This means that ES×T [|S ∩\r\n79",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/1b33bce5-7e86-4772-96a9-67f7fc39c22a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=196691154c08f0799db0acddc66a384b416d982c7eb23defbc99b2361905ad6e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 711
      },
      {
        "segments": [
          {
            "segment_id": "1b33bce5-7e86-4772-96a9-67f7fc39c22a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 79,
            "page_width": 612,
            "page_height": 792,
            "content": "5.4.2 A Strong Lower Bound\r\nWe now strengthen Theorem 5.10 to the following:\r\nTheorem 5.15. Assume Alice receives a set S, |S| = m and Bob receives a set T, |T| = n,\r\nboth sets coming from a universe of size 2mn, for m < nγ, where γ < 1 is a constant. In\r\nany randomized, two-sided error communication protocol deciding disjointness of S and T,\r\neither Alice sends Ω(m lg n) bits or Bob sends Ω(n\r\n1−δ\r\n) bits, for any δ > 0.\r\nProof. We have a quadratic universe of size m · n, which we view as n blocks of size m.\r\nAlice’s set contains a random point in each block, and Bob’s set contains m/n points in the\r\neach block. Note that this is a product distribution and the function is balanced (Pr[S ∩T =\r\n∅] = Ω(1) and Pr[S ∩T 6= ∅] = Ω(1)). This suggests that we should use randomized richness.\r\nBy Lemma 5.11, the problem boils down to proving that in a big enough rectangle S ×T ,\r\nPr[S ∩ T 6= ∅ | S ∈ S, T ∈ T ] = Ω(1).\r\nFor values in our universe i ∈ [mn], we let p(i) be the probability that a set S ∈ S\r\ncontains i. Note that S contains one element in each block, so p(·) is a probability density\r\nfunction on each block. We have H(S ∈ S) ≥ n lg m − o(n lg m\r\nn\r\n) by assuming that the\r\nrectangle is large (Alice communicate o(n lg m\r\nn\r\n) bits). Each S is a vector of n choices, each\r\nfrom a block of size m. The entropy of almost all coordinates must be lg m − o(lg m\r\nn\r\n). This\r\nmeans that the distribution p(·) has weight 1 − o(1) on values with p(i) <\r\n(m/n)\r\no(1)\r\nm\r\n. (If too\r\nmany points have high probability, the entropy must be far from the maximum lg m.)\r\nHaving fixed p(·) (depending on S), let us calculate f(T) = ES∈S[|S ∩ T|]. This is\r\nobviously P\r\ni∈T\r\np(i). Now we are going to pick T from the original marginal distribution\r\n(ignore T for now), and we’re going to look at the distribution induced on f(T). The\r\nmean is m ·\r\n1\r\nm = 1, because the p(i)’s on each block summed up to one. Now note that\r\neach element of T is chosen independently and picks some p(i) to be added to the sum.\r\nThus we have a Chernoff bound kicking in, and f(T) is concentrated around its mean. The\r\nprobability of deviating a constant from the mean is given by the upper bounds on p(i). If\r\nmax p(i) = α · mean p(i) = α\r\nm\r\n, the Chernoff bound is exponential in m/ poly(α). But we\r\nshowed that 1 − o(1) of the weight is on values with p(i) <\r\n(m/n)\r\no(1)\r\nm\r\n. So we can discard the\r\nbig values, still keeping the expectation of f(T) = 1 − o(1), and apply a Chernoff bound\r\nexponential in m/(m/n)\r\no(1) = n · (\r\nm\r\nn\r\n)\r\n1−o(1)\r\n.\r\nNow if Bob communicated n · (\r\nm\r\nn\r\n)\r\n1−ε\r\nfor ε > 0, then T is so large, that even if all the\r\ndeviation from the Chernoff bound is inside T , it cannot change the average of f(T) by more\r\nthan a constant. (Formally Pr[S] is ω of the Chernoff probability.) Then the average of f(T)\r\neven on T is Ω(1).\r\nWe have just shown that ES×T [|S ∩ T|] = Ω(1). But how do we get E[|S ∩ T|] = Ω(1)\r\nto mean Pr[S ∩ T 6= ∅] = Ω(1)? We are going to consider a bicriterion error function:\r\nεe(S, T) = ε(S, T) + α · |S ∩ T|\r\n2\r\n. Here ε was the error measure of the protocol, and α is a\r\nsmall enough constant. By concentration Var[|S ∩ T|] = O(1) so εe is bounded by a small\r\nconstant: amplify the original protocol to reduce ε, and choose α small.\r\nRandomized richness gives us a large rectangle for which the protocol answers “disjoint”,\r\nand in which the error is a constant bigger than the original. This means that ES×T [|S ∩\r\n79",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/1b33bce5-7e86-4772-96a9-67f7fc39c22a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=196691154c08f0799db0acddc66a384b416d982c7eb23defbc99b2361905ad6e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 711
      },
      {
        "segments": [
          {
            "segment_id": "9a13afb2-ae21-448d-8ef0-02b92a2f72bf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 80,
            "page_width": 612,
            "page_height": 792,
            "content": "T|\r\n2\r\n] = O(1). But since |S ∩T| is positive, have expectation Ω(1), and has a bounded second\r\nmoment, it must be that Pr[|S ∩ T| > 0] = Ω(1). Thus Pr[S ∩ T 6= ∅] = Ω(1), contradicting\r\nthe fact that the error was a very small constant.\r\n5.4.3 Direct Sum for Randomized Richness\r\nIt will be useful to have a version of our direct-sum result for randomized richness. We\r\nnow describe such a result, which comes from our paper [88]. Consider a vector of problems\r\n~f = (f1, . . . , fk), where fi\r\n: X × Y → {0, 1}. We define another data structure problem\r\nLk ~f : ([k] × X) × Y\r\nk → {0, 1} as follows. The data structure receives a vector of inputs\r\n(y1, . . . , yk) ∈ Y\r\nk\r\n. The representation depends arbitrarily on all of these inputs. The query\r\nis the index of a subproblem i ∈ [k], and an element x ∈ X. The output of Lk ~f is fi(x, yi).\r\nLet us first recapitulate how randomized richness is normally applied to communication\r\ngames. We say problem f is α-dense if Ex∈X,y∈Y [f(x, y)] ≥ α, i.e. at least an α fraction of\r\nthe truth table of f contains ones. Then, one applies Lemma 5.11, showing that, in order\r\nto prove a communication lower bound, one has to prove that every large rectangle contains\r\nΩ(1) zeros. Unfortunately, we cannot use this lemma directly because we do not know how\r\nto convert k outputs, some of which may contain errors, into a single meaningful boolean\r\noutput. Instead, we need a new lemma, which reuses ideas of the old lemma in a more subtle\r\nway. A technical difference is that our new lemma will talk directly about data structures,\r\ninstead of going through communication complexity.\r\nDefine ρi: X × Y × {0, 1} → {0, 1} by ρi(x, y, z) = 1 if fi(x, y) 6= z, and 0 otherwise.\r\nAlso let ρ : Xk × Y\r\nk × {0, 1}k → [0, 1] be ρ(x, y, z) = 1\r\nk\r\nP\r\ni\r\nρi(xi, yi, zi). In other words, ρ\r\nmeasures the fraction of the outputs from z which are wrong.\r\nLemma 5.16. Let ε > 99\r\nk\r\nbe arbitrary, and f1, . . . , fk be ε-dense. Assume Lk ~f can be\r\nsolved in the cell-probe model with w-bit cells, using space S, cell-probe complexity T, and\r\nerror rate ≤ ε. Then there exists a canonical rectangle X × Y ⊂ Xk × Y\r\nk\r\nfor some output\r\nz ∈ {0, 1}\r\nk\r\nsatisfying:\r\n|X | ≥ |X|\r\nk\r\n/2\r\nO(T k lg S\r\nk\r\n)\r\n, |Y| ≥ |Y |\r\nk\r\n/2\r\nO(T kw)\r\nX\r\ni\r\nzi ≥\r\nε\r\n3\r\nk, Ex∈X,y∈Y [ρ(x, y, z)] ≤ ε\r\n2\r\n.\r\nProof. First we decrease the error probability of the data structure to ε\r\n2\r\n9\r\n. This requires O(1)\r\nrepetitions, so it only changes constant factors in S and T. Now we use the easy direction of\r\nYao’s minimax principle to fix the coins of the data structure (nonuniformly) and maintain\r\nthe same error over the uniform distribution on the inputs.\r\nWe now convert the data structure to a communication protocol. We simulate one query\r\nto each of the k subproblems in parallel. In each round, Alice sends the subset of k cells\r\nprobed, and Bob replies with the contents of the cells. Alice sends a total of O(T k lg S\r\nk\r\n) bits,\r\nand Bob a total of O(T kw) bits. At the end, the protocol outputs the vector of k answers.\r\n80",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9a13afb2-ae21-448d-8ef0-02b92a2f72bf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bae5c0030d90376e5595c44acfc6405abae03365f244c31145665bab9e0e7449",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 611
      },
      {
        "segments": [
          {
            "segment_id": "9a13afb2-ae21-448d-8ef0-02b92a2f72bf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 80,
            "page_width": 612,
            "page_height": 792,
            "content": "T|\r\n2\r\n] = O(1). But since |S ∩T| is positive, have expectation Ω(1), and has a bounded second\r\nmoment, it must be that Pr[|S ∩ T| > 0] = Ω(1). Thus Pr[S ∩ T 6= ∅] = Ω(1), contradicting\r\nthe fact that the error was a very small constant.\r\n5.4.3 Direct Sum for Randomized Richness\r\nIt will be useful to have a version of our direct-sum result for randomized richness. We\r\nnow describe such a result, which comes from our paper [88]. Consider a vector of problems\r\n~f = (f1, . . . , fk), where fi\r\n: X × Y → {0, 1}. We define another data structure problem\r\nLk ~f : ([k] × X) × Y\r\nk → {0, 1} as follows. The data structure receives a vector of inputs\r\n(y1, . . . , yk) ∈ Y\r\nk\r\n. The representation depends arbitrarily on all of these inputs. The query\r\nis the index of a subproblem i ∈ [k], and an element x ∈ X. The output of Lk ~f is fi(x, yi).\r\nLet us first recapitulate how randomized richness is normally applied to communication\r\ngames. We say problem f is α-dense if Ex∈X,y∈Y [f(x, y)] ≥ α, i.e. at least an α fraction of\r\nthe truth table of f contains ones. Then, one applies Lemma 5.11, showing that, in order\r\nto prove a communication lower bound, one has to prove that every large rectangle contains\r\nΩ(1) zeros. Unfortunately, we cannot use this lemma directly because we do not know how\r\nto convert k outputs, some of which may contain errors, into a single meaningful boolean\r\noutput. Instead, we need a new lemma, which reuses ideas of the old lemma in a more subtle\r\nway. A technical difference is that our new lemma will talk directly about data structures,\r\ninstead of going through communication complexity.\r\nDefine ρi: X × Y × {0, 1} → {0, 1} by ρi(x, y, z) = 1 if fi(x, y) 6= z, and 0 otherwise.\r\nAlso let ρ : Xk × Y\r\nk × {0, 1}k → [0, 1] be ρ(x, y, z) = 1\r\nk\r\nP\r\ni\r\nρi(xi, yi, zi). In other words, ρ\r\nmeasures the fraction of the outputs from z which are wrong.\r\nLemma 5.16. Let ε > 99\r\nk\r\nbe arbitrary, and f1, . . . , fk be ε-dense. Assume Lk ~f can be\r\nsolved in the cell-probe model with w-bit cells, using space S, cell-probe complexity T, and\r\nerror rate ≤ ε. Then there exists a canonical rectangle X × Y ⊂ Xk × Y\r\nk\r\nfor some output\r\nz ∈ {0, 1}\r\nk\r\nsatisfying:\r\n|X | ≥ |X|\r\nk\r\n/2\r\nO(T k lg S\r\nk\r\n)\r\n, |Y| ≥ |Y |\r\nk\r\n/2\r\nO(T kw)\r\nX\r\ni\r\nzi ≥\r\nε\r\n3\r\nk, Ex∈X,y∈Y [ρ(x, y, z)] ≤ ε\r\n2\r\n.\r\nProof. First we decrease the error probability of the data structure to ε\r\n2\r\n9\r\n. This requires O(1)\r\nrepetitions, so it only changes constant factors in S and T. Now we use the easy direction of\r\nYao’s minimax principle to fix the coins of the data structure (nonuniformly) and maintain\r\nthe same error over the uniform distribution on the inputs.\r\nWe now convert the data structure to a communication protocol. We simulate one query\r\nto each of the k subproblems in parallel. In each round, Alice sends the subset of k cells\r\nprobed, and Bob replies with the contents of the cells. Alice sends a total of O(T k lg S\r\nk\r\n) bits,\r\nand Bob a total of O(T kw) bits. At the end, the protocol outputs the vector of k answers.\r\n80",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9a13afb2-ae21-448d-8ef0-02b92a2f72bf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bae5c0030d90376e5595c44acfc6405abae03365f244c31145665bab9e0e7449",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 611
      },
      {
        "segments": [
          {
            "segment_id": "4a6e225c-088d-404c-8bf6-c49202d97d01",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 81,
            "page_width": 612,
            "page_height": 792,
            "content": "Let Pi(xi\r\n, y) be the output of the data structure when running query (i, xi) on input\r\ny. Note that this may depend arbitrarily on the entire input y, but depends only on one\r\nquery (since the query algorithm cannot consider parallel queries). When the communication\r\nprotocol receives x and y as inputs, it will output P(x, y) = (P1(x1, y), . . . , Pk(xk, y)). Note\r\nthat some values Pi(xi, y) may be wrong (different from fi(xi, yi)), hence some coordinates\r\nof P(x, y) will contain erroneous answers. To quantify that, note Ex,y[ρ(x, y, P(x, y))] =\r\nEi,xi,y[ρi(xi, yi, Pi(xi, y))] ≤\r\nε\r\n2\r\n9\r\n, i.e. the average fraction of wrong answers is precisely the\r\nerror probability of the data structure.\r\nWe now wish to show that the set W = {(x, y) |\r\nP\r\ni Pi(xi\r\n, y) ≥\r\nε\r\n3\r\nk} has density Ω(1)\r\nin Xk × Y\r\nk\r\n. First consider the set W1 = {(x, y) |\r\nP\r\ni\r\nfi(xi, yi) ≥\r\n2ε\r\n3\r\nk}. As (x, y) is chosen\r\nuniformly from Xk × Y\r\nk\r\n, fi(xi, yi) are independent random variables with expectation ≥ ε.\r\nThen, by the Chernoff bound, Prx,y[(x, y) ∈ W1] ≥ 1 − e\r\nkε/18 ≥ 1 − e−99/18 ≥\r\n2\r\n3\r\n. Now\r\nconsider W2 = {(x, y) | ρ(x, y, P(x, y)) ≥\r\nε\r\n3\r\n}. Since Ex,y[ρ(x, y, P(x, y))] = ε\r\n2\r\n9\r\n, the Markov\r\nbound shows that the density of W2 is at most ε\r\n3\r\n. Finally, observe that W1 \\ W2 ⊆ W, so W\r\nhas density ≥\r\n1\r\n3\r\n.\r\nThe communication protocol breaks Xk×Y\r\nk\r\ninto disjoint canonical rectangles, over which\r\nP(x, y) is constant. Consider all rectangles for which P(x, y) has at least ε\r\n3\r\nk one entries. The\r\nunion of these rectangles is W. Now eliminate all rectangles R with E(x,y)∈R[ρ(x, y, P(x, y))] ≥\r\nε\r\n2\r\n, and let W0 be the union of the remaining ones. Since the average of ρ(x, y, P(x, y)) over\r\nXk × Y\r\nk\r\nis ε\r\n2\r\n9\r\n, a Markov bound shows the total density of the eliminated rectangles is at\r\nmost 1\r\n9\r\n. Then, |W0| ≥ 2\r\n3\r\n|W|.\r\nNow observe that membership in W0is [Ω(|X|\r\nk\r\n), Ω(|Y |\r\nk\r\n)]-rich. Indeed, since |W0| =\r\nΩ(|X|\r\nk\r\n|Y |\r\nk\r\n), a constant fraction of the rows must contain Ω(|Y |\r\nk\r\n) elements from W0. Now\r\nnote that the communication protocol can be used to decide membership in W0, so we apply\r\nLemma 5.4. This shows that one of the rectangles reached at the end of the protocol must\r\ncontain only elements of W0, and have size Ω(|X|\r\nk\r\n)/2\r\nO(T k lg(S/k)) × Ω(|Y |k\r\n)/2\r\nO(T kw)\r\n. In fact,\r\nbecause Lemma 5.4 finds a large canonical rectangle, this must be one of the rectangles\r\ncomposing W0, so we know the answer corresponding to this rectangle has at least ε\r\n3\r\nk ones,\r\nand the average ρ(x, y, P(x, y)) over the rectangle is at most ε\r\n2\r\n.\r\nThe direct-sum result that we want will rely on the following key combinatorial lemma,\r\nwhose proof is deferred to §5.4.4:\r\nLemma 5.17. For i ∈ [d], consider a family of functions φi: X × Y → {0, 1}, and\r\ndefine φ : Xd × Y\r\nd → [0, 1] by φ(x, y) = 1\r\nd\r\nP\r\ni\r\nφi(xi, yi). Let X ⊂ Xd,Y ⊂ Y\r\nd with\r\n|X | ≥ (|X|/α)\r\nd\r\n, |Y| ≥ (|Y |/β)\r\nd\r\n, where α, β ≥ 2. Then there exists i ∈ [d] and a rectan\u0002gle A × B ⊂ X × Y with |A| ≥ |X|\r\n\u000e\r\nα\r\nO(1)\r\n, |B| ≥ |Y |\r\n\u000e\r\nβ\r\nO(1), such that Ea∈A,b∈B[φi(a, b)] =\r\nO(Ex∈X,y∈Y [φ(x, y)]).\r\nUsing this technical result, we can show our main direct-sum property:\r\nTheorem 5.18. Let ε > 99\r\nk\r\nbe arbitrary, and f1, . . . , fk be ε-dense. Assume Lk ~f can be\r\nsolved in the cell-probe model with w-bit cells, using space S, cell-probe complexity T, and\r\nerror ε. Then some fi has a rectangle of dimensions |X|/2\r\nO(T lg(S/k)) × |Y |/2O(T w)\r\nin which\r\nthe density of zeros is at most ε.\r\n81",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/4a6e225c-088d-404c-8bf6-c49202d97d01.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=75b0f76a8365cc965ff8699ece870f69bc65106a37b5585f9cff12bdceb756bd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 720
      },
      {
        "segments": [
          {
            "segment_id": "4a6e225c-088d-404c-8bf6-c49202d97d01",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 81,
            "page_width": 612,
            "page_height": 792,
            "content": "Let Pi(xi\r\n, y) be the output of the data structure when running query (i, xi) on input\r\ny. Note that this may depend arbitrarily on the entire input y, but depends only on one\r\nquery (since the query algorithm cannot consider parallel queries). When the communication\r\nprotocol receives x and y as inputs, it will output P(x, y) = (P1(x1, y), . . . , Pk(xk, y)). Note\r\nthat some values Pi(xi, y) may be wrong (different from fi(xi, yi)), hence some coordinates\r\nof P(x, y) will contain erroneous answers. To quantify that, note Ex,y[ρ(x, y, P(x, y))] =\r\nEi,xi,y[ρi(xi, yi, Pi(xi, y))] ≤\r\nε\r\n2\r\n9\r\n, i.e. the average fraction of wrong answers is precisely the\r\nerror probability of the data structure.\r\nWe now wish to show that the set W = {(x, y) |\r\nP\r\ni Pi(xi\r\n, y) ≥\r\nε\r\n3\r\nk} has density Ω(1)\r\nin Xk × Y\r\nk\r\n. First consider the set W1 = {(x, y) |\r\nP\r\ni\r\nfi(xi, yi) ≥\r\n2ε\r\n3\r\nk}. As (x, y) is chosen\r\nuniformly from Xk × Y\r\nk\r\n, fi(xi, yi) are independent random variables with expectation ≥ ε.\r\nThen, by the Chernoff bound, Prx,y[(x, y) ∈ W1] ≥ 1 − e\r\nkε/18 ≥ 1 − e−99/18 ≥\r\n2\r\n3\r\n. Now\r\nconsider W2 = {(x, y) | ρ(x, y, P(x, y)) ≥\r\nε\r\n3\r\n}. Since Ex,y[ρ(x, y, P(x, y))] = ε\r\n2\r\n9\r\n, the Markov\r\nbound shows that the density of W2 is at most ε\r\n3\r\n. Finally, observe that W1 \\ W2 ⊆ W, so W\r\nhas density ≥\r\n1\r\n3\r\n.\r\nThe communication protocol breaks Xk×Y\r\nk\r\ninto disjoint canonical rectangles, over which\r\nP(x, y) is constant. Consider all rectangles for which P(x, y) has at least ε\r\n3\r\nk one entries. The\r\nunion of these rectangles is W. Now eliminate all rectangles R with E(x,y)∈R[ρ(x, y, P(x, y))] ≥\r\nε\r\n2\r\n, and let W0 be the union of the remaining ones. Since the average of ρ(x, y, P(x, y)) over\r\nXk × Y\r\nk\r\nis ε\r\n2\r\n9\r\n, a Markov bound shows the total density of the eliminated rectangles is at\r\nmost 1\r\n9\r\n. Then, |W0| ≥ 2\r\n3\r\n|W|.\r\nNow observe that membership in W0is [Ω(|X|\r\nk\r\n), Ω(|Y |\r\nk\r\n)]-rich. Indeed, since |W0| =\r\nΩ(|X|\r\nk\r\n|Y |\r\nk\r\n), a constant fraction of the rows must contain Ω(|Y |\r\nk\r\n) elements from W0. Now\r\nnote that the communication protocol can be used to decide membership in W0, so we apply\r\nLemma 5.4. This shows that one of the rectangles reached at the end of the protocol must\r\ncontain only elements of W0, and have size Ω(|X|\r\nk\r\n)/2\r\nO(T k lg(S/k)) × Ω(|Y |k\r\n)/2\r\nO(T kw)\r\n. In fact,\r\nbecause Lemma 5.4 finds a large canonical rectangle, this must be one of the rectangles\r\ncomposing W0, so we know the answer corresponding to this rectangle has at least ε\r\n3\r\nk ones,\r\nand the average ρ(x, y, P(x, y)) over the rectangle is at most ε\r\n2\r\n.\r\nThe direct-sum result that we want will rely on the following key combinatorial lemma,\r\nwhose proof is deferred to §5.4.4:\r\nLemma 5.17. For i ∈ [d], consider a family of functions φi: X × Y → {0, 1}, and\r\ndefine φ : Xd × Y\r\nd → [0, 1] by φ(x, y) = 1\r\nd\r\nP\r\ni\r\nφi(xi, yi). Let X ⊂ Xd,Y ⊂ Y\r\nd with\r\n|X | ≥ (|X|/α)\r\nd\r\n, |Y| ≥ (|Y |/β)\r\nd\r\n, where α, β ≥ 2. Then there exists i ∈ [d] and a rectan\u0002gle A × B ⊂ X × Y with |A| ≥ |X|\r\n\u000e\r\nα\r\nO(1)\r\n, |B| ≥ |Y |\r\n\u000e\r\nβ\r\nO(1), such that Ea∈A,b∈B[φi(a, b)] =\r\nO(Ex∈X,y∈Y [φ(x, y)]).\r\nUsing this technical result, we can show our main direct-sum property:\r\nTheorem 5.18. Let ε > 99\r\nk\r\nbe arbitrary, and f1, . . . , fk be ε-dense. Assume Lk ~f can be\r\nsolved in the cell-probe model with w-bit cells, using space S, cell-probe complexity T, and\r\nerror ε. Then some fi has a rectangle of dimensions |X|/2\r\nO(T lg(S/k)) × |Y |/2O(T w)\r\nin which\r\nthe density of zeros is at most ε.\r\n81",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/4a6e225c-088d-404c-8bf6-c49202d97d01.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=75b0f76a8365cc965ff8699ece870f69bc65106a37b5585f9cff12bdceb756bd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 720
      },
      {
        "segments": [
          {
            "segment_id": "bec05892-d16d-498b-b61a-c242abd5094d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 82,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. First we apply Lemma 5.16, yielding a rectangle X × Y. By reordering coordinates,\r\nassume the first d =\r\nε\r\n3\r\nk elements of z are ones. We now wish to fix xd+1, . . . , xk and\r\nyd+1, . . . , yk such that the remaining d-dimensional rectangle is still large, and the average\r\nof ρ(x, y, z) over it is small. There are at most |X|\r\nk−d\r\nchoices for fixing the x elements. We\r\ncan eliminate all choices which would reduce the rectangle by a factor of at least 3|X|\r\nk−d\r\n.\r\nIn doing so, we have lost a 1\r\n3\r\nfraction of the density. Similarly, we eliminate all choices for\r\nthe y elements which would reduce the rectangle by a factor of 3|Y |\r\nk−d\r\n.\r\nWe still have a third of the mass remaining, so the average of ρ(x, y, z) can only have in\u0002creased by a factor of 3. That means Ei∈[k]\r\n[ρi(xi, yi, zi)] ≤ 3ε\r\n2\r\n, which implies Ei∈[d][ρi(xi, yi, zi)] ≤\r\n3ε\r\n2\r\n·\r\nk\r\nd = 9ε. We now fix xd+1, . . . , xk and yd+1, . . . , yk among the remaining choices,\r\nsuch that this expected error is preserved. Thus, we have found a rectangle X\r\n0 × Y0 ⊂\r\nXd × Y\r\nd with |X 0\r\n| ≥ |X|\r\nd/2O(T k lg(S/k)) and |Y0\r\n| ≥ |Y |\r\nd/2O(T kw)\r\n. Since d = Θ(k), we\r\ncan freely substitute d for k in these exponents. Besides largeness, the rectangle satisfies\r\nEi∈[d],x∈X0\r\n,y∈Y0[ρi(xi\r\n, yi, 1)] ≤ 9ε.\r\nWe now apply Lemma 5.17 on the rectangle X\r\n0 × Y0\r\n, with α = 2O(T lg(S/k)), β =\r\n2\r\nO(T w) and φi(x, y) = ρi(x, y, 1). We obtain a rectangle A × B ⊂ X × Y of dimensions\r\n|A| ≥ |X|/2\r\nO(T lg(S/k))\r\n, |B| ≥ |Y |/2\r\nO(T w)\r\n, which has the property Ea∈A,b∈B[ρi(a, b, 1)] = O(ε),\r\ni.e. Pra∈A,b∈B[fi(a, b) = 0] = O(ε).\r\n5.4.4 Proof of Lemma 5.17\r\nDefine Xi to be the weighted projection of X on dimension i (i.e. a distribution giving the\r\nfrequency of every value on coordinate i). Thus, Xiis a distribution on X with density\r\nfunction ℘Xi(z) = |{x∈X|xi=z}|\r\n|X| .\r\nWe identify sets like X and Y with the uniform distributions on the sets. Treating φ\r\nand φi as random variables (measuring some error to be minimized), let ε = EX ×Y [φ] =\r\n1\r\nd\r\nP\r\ni EXi×Yi\r\n[φi].\r\nWe now interpret the lower bound on the size of X as bounding the entropy, and use\r\nsubmodularity of the Shannon entropy H to write:\r\nX\r\ni\r\nH(Xi) ≥ H(X ) ≥ d · (lg |X| − lg α) ⇒\r\n1\r\nd\r\n·\r\nX\r\ni\r\n\r\nlg |X| − H(Xi)\r\n\u0001\r\n≤ lg α\r\nObserve that each term in the sum is positive, since H(Xi) ≤ lg |X|. We can conclude that:\r\n(∃)i : lg |X| − H(Xi) ≤ 3 lg α; lg |Y | − H(Yi) ≤ 3 lg β; EXi×Yi[φi] ≤ 3ε,\r\nbecause there are strictly less than d\r\n3\r\ncoordinates that violate each of these three constraints.\r\nFor the remainder of the proof, fix some i satisfying these constraints.\r\nLet A0 be the set of elements z ∈ X with ℘Xi\r\n(z) ≤ α\r\n8/|X|, where ℘Xi\r\nis the density\r\nfunction of the distribution Xi. In the probability space on which distribution Xiis observed,\r\n82",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/bec05892-d16d-498b-b61a-c242abd5094d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71e8584ac34c18493add801602bbad03aaf0f76baef20d777975f9bd0e4ceb19",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 578
      },
      {
        "segments": [
          {
            "segment_id": "bec05892-d16d-498b-b61a-c242abd5094d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 82,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. First we apply Lemma 5.16, yielding a rectangle X × Y. By reordering coordinates,\r\nassume the first d =\r\nε\r\n3\r\nk elements of z are ones. We now wish to fix xd+1, . . . , xk and\r\nyd+1, . . . , yk such that the remaining d-dimensional rectangle is still large, and the average\r\nof ρ(x, y, z) over it is small. There are at most |X|\r\nk−d\r\nchoices for fixing the x elements. We\r\ncan eliminate all choices which would reduce the rectangle by a factor of at least 3|X|\r\nk−d\r\n.\r\nIn doing so, we have lost a 1\r\n3\r\nfraction of the density. Similarly, we eliminate all choices for\r\nthe y elements which would reduce the rectangle by a factor of 3|Y |\r\nk−d\r\n.\r\nWe still have a third of the mass remaining, so the average of ρ(x, y, z) can only have in\u0002creased by a factor of 3. That means Ei∈[k]\r\n[ρi(xi, yi, zi)] ≤ 3ε\r\n2\r\n, which implies Ei∈[d][ρi(xi, yi, zi)] ≤\r\n3ε\r\n2\r\n·\r\nk\r\nd = 9ε. We now fix xd+1, . . . , xk and yd+1, . . . , yk among the remaining choices,\r\nsuch that this expected error is preserved. Thus, we have found a rectangle X\r\n0 × Y0 ⊂\r\nXd × Y\r\nd with |X 0\r\n| ≥ |X|\r\nd/2O(T k lg(S/k)) and |Y0\r\n| ≥ |Y |\r\nd/2O(T kw)\r\n. Since d = Θ(k), we\r\ncan freely substitute d for k in these exponents. Besides largeness, the rectangle satisfies\r\nEi∈[d],x∈X0\r\n,y∈Y0[ρi(xi\r\n, yi, 1)] ≤ 9ε.\r\nWe now apply Lemma 5.17 on the rectangle X\r\n0 × Y0\r\n, with α = 2O(T lg(S/k)), β =\r\n2\r\nO(T w) and φi(x, y) = ρi(x, y, 1). We obtain a rectangle A × B ⊂ X × Y of dimensions\r\n|A| ≥ |X|/2\r\nO(T lg(S/k))\r\n, |B| ≥ |Y |/2\r\nO(T w)\r\n, which has the property Ea∈A,b∈B[ρi(a, b, 1)] = O(ε),\r\ni.e. Pra∈A,b∈B[fi(a, b) = 0] = O(ε).\r\n5.4.4 Proof of Lemma 5.17\r\nDefine Xi to be the weighted projection of X on dimension i (i.e. a distribution giving the\r\nfrequency of every value on coordinate i). Thus, Xiis a distribution on X with density\r\nfunction ℘Xi(z) = |{x∈X|xi=z}|\r\n|X| .\r\nWe identify sets like X and Y with the uniform distributions on the sets. Treating φ\r\nand φi as random variables (measuring some error to be minimized), let ε = EX ×Y [φ] =\r\n1\r\nd\r\nP\r\ni EXi×Yi\r\n[φi].\r\nWe now interpret the lower bound on the size of X as bounding the entropy, and use\r\nsubmodularity of the Shannon entropy H to write:\r\nX\r\ni\r\nH(Xi) ≥ H(X ) ≥ d · (lg |X| − lg α) ⇒\r\n1\r\nd\r\n·\r\nX\r\ni\r\n\r\nlg |X| − H(Xi)\r\n\u0001\r\n≤ lg α\r\nObserve that each term in the sum is positive, since H(Xi) ≤ lg |X|. We can conclude that:\r\n(∃)i : lg |X| − H(Xi) ≤ 3 lg α; lg |Y | − H(Yi) ≤ 3 lg β; EXi×Yi[φi] ≤ 3ε,\r\nbecause there are strictly less than d\r\n3\r\ncoordinates that violate each of these three constraints.\r\nFor the remainder of the proof, fix some i satisfying these constraints.\r\nLet A0 be the set of elements z ∈ X with ℘Xi\r\n(z) ≤ α\r\n8/|X|, where ℘Xi\r\nis the density\r\nfunction of the distribution Xi. In the probability space on which distribution Xiis observed,\r\n82",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/bec05892-d16d-498b-b61a-c242abd5094d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71e8584ac34c18493add801602bbad03aaf0f76baef20d777975f9bd0e4ceb19",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 578
      },
      {
        "segments": [
          {
            "segment_id": "bb093686-52ba-4651-a6de-c2dfbe63994e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 83,
            "page_width": 612,
            "page_height": 792,
            "content": "A0\r\nis an event. We have:\r\nH(Xi) = H(A\r\n0\r\n) + Pr[A\r\n0\r\n] · H(Xi| A\r\n0\r\n) + (1 − Pr[A\r\n0\r\n]) · H(Xi| ¬A\r\n0\r\n)\r\n≤ 1 + Pr[A\r\n0\r\n] · lg |X| +\r\n\r\n1 − Pr[A\r\n0\r\n]\r\n\u0001\r\n· lg |X|\r\nα8\r\n= lg |X| + 1 −\r\n\r\n1 − Pr[A\r\n0\r\n]\r\n\u0001\r\n· 8 lg α\r\nWe claim that Pr[A0] ≥\r\n1\r\n2\r\n. Otherwise, we would have H(Xi) ≤ lg |X|+1−4 lg α, contradicting\r\nthe lower bound H(Xi) ≥ lg |X| − 3 lg α, given α ≥ 2.\r\nNow let X\r\n0 be the distribution Xi conditioned on A0\r\n(equivalently, the distribution re\u0002stricted to the support A0\r\n). Performing an analogous analysis on Yi, we define a support B0\r\nand restricted distribution Y\r\n0\r\n. Observe that:\r\nEX0×Y0[φi\r\n] = EXi×Yi\r\n[φi| A\r\n0 ∧ B0\r\n] ≤\r\nEXi×Yi\r\n[φi]\r\nPr[A0 ∧ B0]\r\n≤ 4 · EXi×Yi\r\n[φi] ≤ 12ε\r\nWe now want to conclude that EA0×B0[φi] is small. This is not immediately true, because\r\nchanging from some distribution X\r\n0 on support A0\r\nto the uniform distribution on A0 may\r\nincrease the average error. To fix this, we consider a subset A ⊆ A0, discarding from A0every\r\nvalue x with E{x}×Y0[φi] > 24ε. Since the expectation over x is 12ε, a Markov bound implies\r\nthat Pr[A] ≥\r\n1\r\n2\r\nPr[A0] ≥\r\n1\r\n4\r\n. We now have a bound for every x ∈ A, and thus EA×Y0[φi\r\n] ≤ 24ε.\r\nNow perform a similar pruning of B, concluding that EA×B[φi] ≤ 48ε.\r\nFinally, we must show that |A| ≥ |X|/αO(1). This follows because PrXi[A] ≥\r\n1\r\n4\r\n, and for\r\nany x ∈ A we had ℘Xi(x) ≤ α\r\n8/|X|. The same analysis holds for |B|.\r\n5.5 Bibliographical Notes\r\nCommunication complexity is a major topic of research in complexity theory, with numerous\r\ninteresting applications. However, “traditional” communication complexity has focused on\r\nsymmetric problems, and usually studied the total communication of both players, i.e. A+B.\r\nThis measure of complexity does not seem useful for data structures.\r\nAsymmetric communication complexity was introduced by Miltersen, Nisan, Safra, and\r\nWigderson [73] in STOC’95, though it was implicit in previous work by Ajtai [3] and Mil\u0002tersen [71]. The richness technique, and the first lower bounds for Indexing and LSD date\r\nback to this seminal paper [73]. The direct-sum property for richness lower bounds was\r\nshown in our paper [88] from FOCS’06.\r\nThe randomized lower bound for symmetric set disjointness, originally due to [65, 92],\r\nis probably the most used result from traditional communication complexity. Bar-Yossef,\r\nJayram, Kumar, and Sivakumar [19] provide the most accessible (though by no means simple)\r\nproof of this result, based on a direct sum idea similar to that of §5.3. The randomized LSD\r\nlower bounds date back to our paper [16] from FOCS’06.\r\n83",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/bb093686-52ba-4651-a6de-c2dfbe63994e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2c166bc96b516274f615b33276f4ceb8fa9f6af0e74926c835805b1e47e47d7a",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "69bd6114-4c1b-4fa9-a135-46db6ef1b0e8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 84,
            "page_width": 612,
            "page_height": 792,
            "content": "84",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/69bd6114-4c1b-4fa9-a135-46db6ef1b0e8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4da34770c11a3d9be0f50eb1961436b1596afb94bcbb7d41cff2a9795fb3e433",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 478
      },
      {
        "segments": [
          {
            "segment_id": "f21236f3-1aef-450c-84ae-873a01b9bbec",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 85,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 6\r\nStatic Lower Bounds\r\nIn this chapter, we prove our first lower bounds for static data structures. The standard\r\napproach for such bounds is to convert a cell-probe algorithm into an asymmetric commu\u0002nication protocol. In the communication model, Alice holds a query, and Bob holds the\r\ndatabase. The two players communicate to answer the query on the database. Each round\r\nof communication simulates a cell probe: the querier sends lg S bits, where S is the space\r\n(the number of cells used by the data structure), and the database responds with w bits,\r\nwhere w is the cell size.\r\nAs in the previous chapter, we can analyze this communication problem by the richness\r\nmethod, and show lower bounds of the following form: either Alice must send a bits, or\r\nBob must send b bits. If the data structure makes t cell probes to answer the query, in the\r\ncommunication protocol, Alice sends tlg S bits, and Bob sends tw bits. Comparing with the\r\nlower bounds, one concludes that the cell-probe complexity must be at least t ≥ min{\r\na\r\nlg S\r\n,\r\nb\r\nw\r\n}.\r\nIn general, b is prohibitively large, so the first bound dominates for reasonable word size.\r\nThus, the time-space trade-off can be rewritten as S ≥ 2\r\nΩ(a/t)\r\n.\r\nWe analyze two problems in this framework: partial match (in §6.1), and (1 + ε)-\r\napproximate near neighbor search (in §6.2). These results appeared in our papers [16, 82],\r\nand both follow by reduction from the communication complexity of set disjointness. See\r\nChapter 2 for background on these problems.\r\nDecision trees. Intuitively, we do not expect the relation between cell-probe and com\u0002munication complexity to be tight. In the communication model, Bob can remember past\r\ncommunication, and answer new queries based on this. Needless to say, if Bob is just a table\r\nof cells, he cannot remember anything, and his responses must be a function of Alice’s last\r\nmessage (i.e. the address of the cell probe).\r\nThis reduction to a much stronger model has its limitations. The bounds that we obtain,\r\nwhich look like S ≥ 2\r\nΩ(a/t)\r\n, are tight for constant query time (demonstrating interesting phe\u0002nomena about the problem), but they degrade too quickly with t, and become uninteresting\r\neven for small t.\r\nOn the other hand, asymmetric communication complexity can be used to obtain very\r\ngood lower bounds in a more restricted model of computation: decision trees. In this model,\r\n85",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f21236f3-1aef-450c-84ae-873a01b9bbec.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=663ac05947051aa0201ef31189d88a6e0fd777c5d42951ee27f5d4c4a1948596",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 407
      },
      {
        "segments": [
          {
            "segment_id": "e4335af8-2be4-407e-bd22-b8bf94722d42",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 86,
            "page_width": 612,
            "page_height": 792,
            "content": "we can typically show that the decision tree must have size 2Ω(a)\r\n, unless its depth (query\r\ntime) is prohibitively large.\r\nThe reduction between decision trees and asymmetric communication is explored in §6.3.\r\nBeating communication complexity. As mentioned above, the implications of commu\u0002nication lower bounds for cell-probe data structures are far from satisfactory. For example,\r\nthe entire strategy is, by design, insensitive to polynomial changes in the space (up to con\u0002stants in the lower bound). But this is unrealistic for data structures, where the difference\r\nbetween space n lgO(1) n and (say) space O(n\r\n3\r\n) is plainly the difference between an interesting\r\nsolution and an uninteresting one.\r\nTo put this in a different light, note that a communication complexity of O(d) bits from\r\nthe querier equates data structures of size 2O(d) which solve the problem in constant time, and\r\ndata structures of size O(n) which solve the problem in a mere O(d/ lg n) time. Needless to\r\nsay, this equivalence appears unlikely. Thus, we need new approaches which can understand\r\nthe time/space trade-offs in the cell-probe model at a finer granularity than direct reduction\r\nto communication. We make progress in this direction, in the case when the space is n\r\n1+o(1)\r\n.\r\nInterestingly, we do not need to throw out the old work in the communication model.\r\nWe can take any lower bound shown by the richness method, for problems with a certain\r\ncompositional structure, and obtain a better lower bound for small-space data structures by\r\nblack-box use of the old result. Thus, we can boost old bounds for polynomial space, in the\r\ncase of near-linear space.\r\nLet S be the space in cells used by the data structure. If one uses richness to show\r\na lower bound of Ω(d) bits for the communication of the querier, the standard approach\r\nwould imply a cell-probe lower bound of Ω(d/ lg S). In contrast, we can show a lower bound\r\nof Ω(d/ lg Sd\r\nn\r\n), which is an improvement for S = n\r\n1+o(1). In the most interesting case of\r\nnear-linear space S = n(d lg n)\r\nO(1), the bound becomes Ω(d/ lg d). Compared to Ω(d/ lg n),\r\nthis is a significant improvement for natural values of d, such as d = lgO(1) n. In particular,\r\nfor d = O(lg n), previous lower bounds could not exceed a constant, whereas we obtain\r\nΩ(lg n/ lg lg n). Note that for d = O(lg n) we have constant upper bounds via tabulation,\r\ngiven large enough polynomial space.\r\nOur technique is introduced in §6.4, where it is illustrated with the familiar partial match\r\nproblem. In the next chapter, we give more exciting application to range queries, where our\r\nidea yields many tight bounds.\r\n6.1 Partial Match\r\nFirst, we must clarify the notion of reduction from the communication complexity of LSD\r\nto a data-structure problem. In such a reduction, Bob constructs a database based on his\r\nset T, and Alice constructs a query based on S. It is then shown that LSD can be solved\r\nbased on the answer to the k queries on Bob’s database. If the data structure has size S\r\nand query time t, this reduction in fact gives a communication protocol for LSD, in which\r\nAlice communicates tlg S bits, and Bob communicates tw bits. This is done by simulating\r\n86",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e4335af8-2be4-407e-bd22-b8bf94722d42.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8e58326b07ada8a2240b71aa14d4b6eb00b4cba8977bc6ff89bf2c87674eddfd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 553
      },
      {
        "segments": [
          {
            "segment_id": "e4335af8-2be4-407e-bd22-b8bf94722d42",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 86,
            "page_width": 612,
            "page_height": 792,
            "content": "we can typically show that the decision tree must have size 2Ω(a)\r\n, unless its depth (query\r\ntime) is prohibitively large.\r\nThe reduction between decision trees and asymmetric communication is explored in §6.3.\r\nBeating communication complexity. As mentioned above, the implications of commu\u0002nication lower bounds for cell-probe data structures are far from satisfactory. For example,\r\nthe entire strategy is, by design, insensitive to polynomial changes in the space (up to con\u0002stants in the lower bound). But this is unrealistic for data structures, where the difference\r\nbetween space n lgO(1) n and (say) space O(n\r\n3\r\n) is plainly the difference between an interesting\r\nsolution and an uninteresting one.\r\nTo put this in a different light, note that a communication complexity of O(d) bits from\r\nthe querier equates data structures of size 2O(d) which solve the problem in constant time, and\r\ndata structures of size O(n) which solve the problem in a mere O(d/ lg n) time. Needless to\r\nsay, this equivalence appears unlikely. Thus, we need new approaches which can understand\r\nthe time/space trade-offs in the cell-probe model at a finer granularity than direct reduction\r\nto communication. We make progress in this direction, in the case when the space is n\r\n1+o(1)\r\n.\r\nInterestingly, we do not need to throw out the old work in the communication model.\r\nWe can take any lower bound shown by the richness method, for problems with a certain\r\ncompositional structure, and obtain a better lower bound for small-space data structures by\r\nblack-box use of the old result. Thus, we can boost old bounds for polynomial space, in the\r\ncase of near-linear space.\r\nLet S be the space in cells used by the data structure. If one uses richness to show\r\na lower bound of Ω(d) bits for the communication of the querier, the standard approach\r\nwould imply a cell-probe lower bound of Ω(d/ lg S). In contrast, we can show a lower bound\r\nof Ω(d/ lg Sd\r\nn\r\n), which is an improvement for S = n\r\n1+o(1). In the most interesting case of\r\nnear-linear space S = n(d lg n)\r\nO(1), the bound becomes Ω(d/ lg d). Compared to Ω(d/ lg n),\r\nthis is a significant improvement for natural values of d, such as d = lgO(1) n. In particular,\r\nfor d = O(lg n), previous lower bounds could not exceed a constant, whereas we obtain\r\nΩ(lg n/ lg lg n). Note that for d = O(lg n) we have constant upper bounds via tabulation,\r\ngiven large enough polynomial space.\r\nOur technique is introduced in §6.4, where it is illustrated with the familiar partial match\r\nproblem. In the next chapter, we give more exciting application to range queries, where our\r\nidea yields many tight bounds.\r\n6.1 Partial Match\r\nFirst, we must clarify the notion of reduction from the communication complexity of LSD\r\nto a data-structure problem. In such a reduction, Bob constructs a database based on his\r\nset T, and Alice constructs a query based on S. It is then shown that LSD can be solved\r\nbased on the answer to the k queries on Bob’s database. If the data structure has size S\r\nand query time t, this reduction in fact gives a communication protocol for LSD, in which\r\nAlice communicates tlg S bits, and Bob communicates tw bits. This is done by simulating\r\n86",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e4335af8-2be4-407e-bd22-b8bf94722d42.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8e58326b07ada8a2240b71aa14d4b6eb00b4cba8977bc6ff89bf2c87674eddfd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 553
      },
      {
        "segments": [
          {
            "segment_id": "21b6fffc-c002-4d7b-ac6f-c51fd14b833c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 87,
            "page_width": 612,
            "page_height": 792,
            "content": "the query algorithm: for each cell probe, Alice sends the address, and Bob sends the content\r\nfrom his constructed database. At the end, the answer to LSD is determined from the\r\nanswer of the query.\r\nIn the previous chapter, we showed a lower bound for LSD via a direct sum argument\r\nfor indexing problems. This gives a special case of LSD, that we call Blocked-LSD. In\r\nthis problem, the universe is interpreted as [N] × [B], and elements as pairs (u, v). It is\r\nguaranteed that (∀)x ∈ [N], S contains a single element of the form (x, ?).\r\nReduction 6.1. Blocked-LSD reduces to one partial match query over n = N · B strings\r\nin dimension d = O(N lg B).\r\nProof. Consider a constant weight code φ mapping the universe [B] to {0, 1}\r\nb\r\n. If we use\r\nweight b/2, we have \r\nb\r\nb/2\r\n\u0001\r\n= 2Ω(b)codewords. Thus, we may set b = O(lg B).\r\nIf S = {(1, s1), . . . ,(N, sN )}, Alice constructs the query string φ(s1)φ(s2)· · · , i.e. the\r\nconcatenation of the codewords of each si. We have dimension d = N · b = O(N lg B).\r\nFor each point (x, y) ∈ T, Bob places the string 0(x−1)b φ(y) 0(N−x)bin the database.\r\nNow, if (i, si) ∈ T, the database contains a string with φ(si) at position (i − 1)b, and the\r\nrest zeros. This string is dominated by the query, which also has φ(si) at that position. On\r\nthe other hand, if a query dominates some string in the database, then for some (i, si) ∈ S\r\nand (i, y) ∈ T, φ(si) dominates φ(y). But this means si = y because in a constant weight\r\ncode, no codeword can dominate another.\r\nFrom the lower bound on Blocked-LSD, we know that in a communication protocol\r\nsolving the problem, either Alice sends Ω(N lg B) bits, or Bob sends N · B1−δ ≥ n\r\n1−δ bits.\r\nRewriting this bound in terms of n and d, either Alice sends Ω(d) bits, or Bob sends n\r\n1−δ\r\nbits, for constant δ > 0.\r\nThis implies that a data structure with query time t requires space 2Ω(d/t), as long as the\r\nword size is w ≤ n\r\n1−δ/t.\r\n6.2 Approximate Near Neighbor\r\nWe consider the decision version of approximate near neighbor over the Hamming space.\r\nGiven a set P ⊂ {0, 1}\r\nd of n points and a distance r, build a data structure which given\r\nq ∈ {0, 1}\r\nd does the following, with probability at least, say, 2/3:\r\n• If there is p ∈ P such that kq − pk ≤ r, answer Yes;\r\n• If there is no p ∈ P such that kq − pk ≤ (1 + ε)r, answer No.\r\nHere we use k · k for the Hamming norm. It is standard to assume cells have Θ(d) bits, i.e. a\r\npoint can be stored in one cell. The lower bound holds for the Euclidean space as well.\r\nTo prove the lower bound, we consider the asymmetric communication complexity of the\r\nproblem for dimension d = ( 1\r\nε\r\nlg n)\r\nO(1). We assume that Alice holds q, while Bob holds P.\r\nWe show that to solve the problem, either Alice sends Ω( 1\r\nε\r\n2\r\nlg n) bits, or Bob sends Ω(n\r\n1−δ\r\n)\r\nbits, for any constant δ > 0.\r\n87",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/21b6fffc-c002-4d7b-ac6f-c51fd14b833c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71ae0adf7f2444c0c1831bbc6071c3a362a270bceb07efa08c4d43e63df58043",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 568
      },
      {
        "segments": [
          {
            "segment_id": "21b6fffc-c002-4d7b-ac6f-c51fd14b833c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 87,
            "page_width": 612,
            "page_height": 792,
            "content": "the query algorithm: for each cell probe, Alice sends the address, and Bob sends the content\r\nfrom his constructed database. At the end, the answer to LSD is determined from the\r\nanswer of the query.\r\nIn the previous chapter, we showed a lower bound for LSD via a direct sum argument\r\nfor indexing problems. This gives a special case of LSD, that we call Blocked-LSD. In\r\nthis problem, the universe is interpreted as [N] × [B], and elements as pairs (u, v). It is\r\nguaranteed that (∀)x ∈ [N], S contains a single element of the form (x, ?).\r\nReduction 6.1. Blocked-LSD reduces to one partial match query over n = N · B strings\r\nin dimension d = O(N lg B).\r\nProof. Consider a constant weight code φ mapping the universe [B] to {0, 1}\r\nb\r\n. If we use\r\nweight b/2, we have \r\nb\r\nb/2\r\n\u0001\r\n= 2Ω(b)codewords. Thus, we may set b = O(lg B).\r\nIf S = {(1, s1), . . . ,(N, sN )}, Alice constructs the query string φ(s1)φ(s2)· · · , i.e. the\r\nconcatenation of the codewords of each si. We have dimension d = N · b = O(N lg B).\r\nFor each point (x, y) ∈ T, Bob places the string 0(x−1)b φ(y) 0(N−x)bin the database.\r\nNow, if (i, si) ∈ T, the database contains a string with φ(si) at position (i − 1)b, and the\r\nrest zeros. This string is dominated by the query, which also has φ(si) at that position. On\r\nthe other hand, if a query dominates some string in the database, then for some (i, si) ∈ S\r\nand (i, y) ∈ T, φ(si) dominates φ(y). But this means si = y because in a constant weight\r\ncode, no codeword can dominate another.\r\nFrom the lower bound on Blocked-LSD, we know that in a communication protocol\r\nsolving the problem, either Alice sends Ω(N lg B) bits, or Bob sends N · B1−δ ≥ n\r\n1−δ bits.\r\nRewriting this bound in terms of n and d, either Alice sends Ω(d) bits, or Bob sends n\r\n1−δ\r\nbits, for constant δ > 0.\r\nThis implies that a data structure with query time t requires space 2Ω(d/t), as long as the\r\nword size is w ≤ n\r\n1−δ/t.\r\n6.2 Approximate Near Neighbor\r\nWe consider the decision version of approximate near neighbor over the Hamming space.\r\nGiven a set P ⊂ {0, 1}\r\nd of n points and a distance r, build a data structure which given\r\nq ∈ {0, 1}\r\nd does the following, with probability at least, say, 2/3:\r\n• If there is p ∈ P such that kq − pk ≤ r, answer Yes;\r\n• If there is no p ∈ P such that kq − pk ≤ (1 + ε)r, answer No.\r\nHere we use k · k for the Hamming norm. It is standard to assume cells have Θ(d) bits, i.e. a\r\npoint can be stored in one cell. The lower bound holds for the Euclidean space as well.\r\nTo prove the lower bound, we consider the asymmetric communication complexity of the\r\nproblem for dimension d = ( 1\r\nε\r\nlg n)\r\nO(1). We assume that Alice holds q, while Bob holds P.\r\nWe show that to solve the problem, either Alice sends Ω( 1\r\nε\r\n2\r\nlg n) bits, or Bob sends Ω(n\r\n1−δ\r\n)\r\nbits, for any constant δ > 0.\r\n87",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/21b6fffc-c002-4d7b-ac6f-c51fd14b833c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71ae0adf7f2444c0c1831bbc6071c3a362a270bceb07efa08c4d43e63df58043",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 568
      },
      {
        "segments": [
          {
            "segment_id": "3441eb8f-062b-41f0-bd1d-114454707752",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 88,
            "page_width": 612,
            "page_height": 792,
            "content": "By the standard relation to cell-probe complexity, this implies that any data structure\r\nwith constant cell-probe complexity must use space n\r\nΩ(1/ε2)\r\n, with is optimal (see Chapter 2).\r\nNote that the cell size w is usually much smaller than n\r\n1−δ\r\n, typically b = d logO(1) n, so that\r\nbound on Bob’s communication is very permissive.\r\nThus, our result establishes a tight quantitative dependence between the approximation\r\nfactor and the exponent in the space bound (for the constant query time case). Given\r\nthat the exponent must be quadratic in 1/ε, our results indicate a fundamental difficulty in\r\ndesigning practical data structures which are very accurate and very fast.\r\nTheorem 6.2. Consider the communication complexity of (1+ε)-approximate near neighbor\r\nin {0, 1}\r\nd\r\n, where d = O(\r\nlog2 n\r\nε\r\n5 ). For any ε = Ω(n\r\n−γ\r\n), γ < 1/2, in any randomized protocol\r\ndeciding the (1 + ε)-NN problem, either Alice sends Ω(log n\r\nε\r\n2 ) bits or Bob sends Ω(n\r\n1−δ\r\n) bits,\r\nfor any δ > 0.\r\nProof. We show how to map an instance of lopsided set disjointness, given by T and S,\r\ninto an instance of (1 + ε)-approximate near neighbor, given by respectively the dataset\r\nP ⊂ {0, 1}\r\nd and the query q ∈ {0, 1}d\r\n. For this purpose, first, Alice and Bob map their sets\r\nS and T into query ˜q ∈ <U and dataset P˜ ⊂ <U, i.e., an (1 + ε)-NN instance in Euclidean\r\nU-dimensional space, l\r\nU\r\n2\r\n. Then, Alice and Bob map their points from the l\r\nU\r\n2 metric to\r\nHamming cube {0, 1}\r\nO(log2 n/ε5)\r\n, essentially preserving the distances among all the points ˜q\r\nand P˜.\r\nFor the set T ⊂ [U], we define P˜ , {eu | u ∈ T}, where eu is a standard <\r\nd basis\r\nvector, with 1 in the coordinate u, and 0 everywhere else. For the set S, we set the query\r\nq˜ , 3ε ·\r\nP\r\nu∈S\r\neu; note that kq˜k\r\n2\r\n2 = m · (3ε)\r\n2 = 1.\r\nWe show that if S ∩ T = ∅, then kq˜− p˜k2 =\r\n√\r\n2 for all ˜p ∈ P˜, and, if S ∩ T 6= ∅, then\r\nthere exists a point ˜p ∈ P˜ such that kq˜− p˜k2 ≤ (1 −\r\n4ε\r\n3\r\n)\r\n√\r\n2. Indeed, we have that\r\n• if S ∩ T = ∅, then for any ˜p ∈ P˜, we have that kq˜− p˜k\r\n2\r\n2 = kq˜k\r\n2\r\n2 + kp˜k\r\n2\r\n2 − 2˜q · p˜ = 2;\r\n• if S ∩ P 6= ∅, then for u\r\n∗ ∈ S ∩ P and for ˜p = eu∗ ∈ P, we have kq˜ − p˜k\r\n2\r\n2 =\r\nkq˜k\r\n2\r\n2 + kp˜k\r\n2\r\n2 − 2˜q · p˜ = 2 − 2(3εeu∗ ) · eu∗ = 2(1 − 3ε).\r\nTo construct P ⊂ {0, 1}\r\nd and q ∈ {0, 1}d\r\n, Alice and Bob perform a randomized mapping\r\nof l\r\nU\r\n2\r\ninto {0, 1}\r\nd\r\nfor d = O(log2 n/ε5), such that the distances are only insignificantly\r\ndistorted, with high probability. Alice and Bob use a source of public random coins to\r\nconstruct the same randomized mapping. First, they construct a randomized embedding\r\nf1 mapping l\r\nU\r\n2\r\ninto l\r\nO(log n/ε2)\r\n1 with distortion less than (1 + ε/16). Then, they construct\r\nthe standard embedding f2 mapping l\r\nO(log n/ε2)\r\n1\r\ninto {0, 1}\r\nO(log2 n/ε5)\r\n. The embedding f2 first\r\nscales up all coordinates by D = O(\r\nlog n\r\nε\r\n3 ), then rounds the coordinates, and finally transforms\r\neach coordinate into its unary representation. We set the constants such that the resulting\r\napproximation of f2 is an additive term O(\r\nlog n\r\nε\r\n2 ) <\r\nDε√\r\n2\r\n16 .\r\nNext, Alice and Bob construct q = f2(f1(˜q)) ∈ {0, 1}\r\nd and P = {f2(f1(˜p)) | p˜ ∈ P˜} ⊂\r\n{0, 1}\r\nd\r\n. Notice that for any p = f2(f1(˜p)) ∈ P, if kq˜ − p˜k2 ≥\r\n√\r\n2, then kq − pkH ≥\r\nD\r\n√\r\n2(1 − ε/16) −\r\nDε√\r\n2\r\n16 = D\r\n√\r\n2(1 −\r\nε\r\n8\r\n), and if kq˜ − p˜k2 ≤\r\n√\r\n2(1 −\r\n4ε\r\n3\r\n), then kq − pkH ≤\r\nD\r\n√\r\n2(1 −\r\n4ε\r\n3\r\n)(1 + ε/16) + Dε√\r\n2\r\n16 ≤ D\r\n√\r\n2(1 − ε −\r\n5ε\r\n24 ).\r\n88",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/3441eb8f-062b-41f0-bd1d-114454707752.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f4f6f833664a2e2992cb8cadb420a20bc6771c90e281fd8f9619cb70793fdeef",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 747
      },
      {
        "segments": [
          {
            "segment_id": "3441eb8f-062b-41f0-bd1d-114454707752",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 88,
            "page_width": 612,
            "page_height": 792,
            "content": "By the standard relation to cell-probe complexity, this implies that any data structure\r\nwith constant cell-probe complexity must use space n\r\nΩ(1/ε2)\r\n, with is optimal (see Chapter 2).\r\nNote that the cell size w is usually much smaller than n\r\n1−δ\r\n, typically b = d logO(1) n, so that\r\nbound on Bob’s communication is very permissive.\r\nThus, our result establishes a tight quantitative dependence between the approximation\r\nfactor and the exponent in the space bound (for the constant query time case). Given\r\nthat the exponent must be quadratic in 1/ε, our results indicate a fundamental difficulty in\r\ndesigning practical data structures which are very accurate and very fast.\r\nTheorem 6.2. Consider the communication complexity of (1+ε)-approximate near neighbor\r\nin {0, 1}\r\nd\r\n, where d = O(\r\nlog2 n\r\nε\r\n5 ). For any ε = Ω(n\r\n−γ\r\n), γ < 1/2, in any randomized protocol\r\ndeciding the (1 + ε)-NN problem, either Alice sends Ω(log n\r\nε\r\n2 ) bits or Bob sends Ω(n\r\n1−δ\r\n) bits,\r\nfor any δ > 0.\r\nProof. We show how to map an instance of lopsided set disjointness, given by T and S,\r\ninto an instance of (1 + ε)-approximate near neighbor, given by respectively the dataset\r\nP ⊂ {0, 1}\r\nd and the query q ∈ {0, 1}d\r\n. For this purpose, first, Alice and Bob map their sets\r\nS and T into query ˜q ∈ <U and dataset P˜ ⊂ <U, i.e., an (1 + ε)-NN instance in Euclidean\r\nU-dimensional space, l\r\nU\r\n2\r\n. Then, Alice and Bob map their points from the l\r\nU\r\n2 metric to\r\nHamming cube {0, 1}\r\nO(log2 n/ε5)\r\n, essentially preserving the distances among all the points ˜q\r\nand P˜.\r\nFor the set T ⊂ [U], we define P˜ , {eu | u ∈ T}, where eu is a standard <\r\nd basis\r\nvector, with 1 in the coordinate u, and 0 everywhere else. For the set S, we set the query\r\nq˜ , 3ε ·\r\nP\r\nu∈S\r\neu; note that kq˜k\r\n2\r\n2 = m · (3ε)\r\n2 = 1.\r\nWe show that if S ∩ T = ∅, then kq˜− p˜k2 =\r\n√\r\n2 for all ˜p ∈ P˜, and, if S ∩ T 6= ∅, then\r\nthere exists a point ˜p ∈ P˜ such that kq˜− p˜k2 ≤ (1 −\r\n4ε\r\n3\r\n)\r\n√\r\n2. Indeed, we have that\r\n• if S ∩ T = ∅, then for any ˜p ∈ P˜, we have that kq˜− p˜k\r\n2\r\n2 = kq˜k\r\n2\r\n2 + kp˜k\r\n2\r\n2 − 2˜q · p˜ = 2;\r\n• if S ∩ P 6= ∅, then for u\r\n∗ ∈ S ∩ P and for ˜p = eu∗ ∈ P, we have kq˜ − p˜k\r\n2\r\n2 =\r\nkq˜k\r\n2\r\n2 + kp˜k\r\n2\r\n2 − 2˜q · p˜ = 2 − 2(3εeu∗ ) · eu∗ = 2(1 − 3ε).\r\nTo construct P ⊂ {0, 1}\r\nd and q ∈ {0, 1}d\r\n, Alice and Bob perform a randomized mapping\r\nof l\r\nU\r\n2\r\ninto {0, 1}\r\nd\r\nfor d = O(log2 n/ε5), such that the distances are only insignificantly\r\ndistorted, with high probability. Alice and Bob use a source of public random coins to\r\nconstruct the same randomized mapping. First, they construct a randomized embedding\r\nf1 mapping l\r\nU\r\n2\r\ninto l\r\nO(log n/ε2)\r\n1 with distortion less than (1 + ε/16). Then, they construct\r\nthe standard embedding f2 mapping l\r\nO(log n/ε2)\r\n1\r\ninto {0, 1}\r\nO(log2 n/ε5)\r\n. The embedding f2 first\r\nscales up all coordinates by D = O(\r\nlog n\r\nε\r\n3 ), then rounds the coordinates, and finally transforms\r\neach coordinate into its unary representation. We set the constants such that the resulting\r\napproximation of f2 is an additive term O(\r\nlog n\r\nε\r\n2 ) <\r\nDε√\r\n2\r\n16 .\r\nNext, Alice and Bob construct q = f2(f1(˜q)) ∈ {0, 1}\r\nd and P = {f2(f1(˜p)) | p˜ ∈ P˜} ⊂\r\n{0, 1}\r\nd\r\n. Notice that for any p = f2(f1(˜p)) ∈ P, if kq˜ − p˜k2 ≥\r\n√\r\n2, then kq − pkH ≥\r\nD\r\n√\r\n2(1 − ε/16) −\r\nDε√\r\n2\r\n16 = D\r\n√\r\n2(1 −\r\nε\r\n8\r\n), and if kq˜ − p˜k2 ≤\r\n√\r\n2(1 −\r\n4ε\r\n3\r\n), then kq − pkH ≤\r\nD\r\n√\r\n2(1 −\r\n4ε\r\n3\r\n)(1 + ε/16) + Dε√\r\n2\r\n16 ≤ D\r\n√\r\n2(1 − ε −\r\n5ε\r\n24 ).\r\n88",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/3441eb8f-062b-41f0-bd1d-114454707752.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f4f6f833664a2e2992cb8cadb420a20bc6771c90e281fd8f9619cb70793fdeef",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 747
      },
      {
        "segments": [
          {
            "segment_id": "94f50531-2699-4cbc-bce2-af2d8a8886ce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 89,
            "page_width": 612,
            "page_height": 792,
            "content": "Finally, Alice and Bob can run the (1+ε)-NN communication protocol with r = D\r\n√\r\n2(1−\r\nε −\r\n5ε\r\n24 ) to decide whether S ∩ T = ∅. Note that the error probability of the resulting set\r\ndisjointness protocol is bounded away from 1/2 since (1+ε)-NN communication protocol has\r\nerror probability bounded away from 1/2, and the embedding f2 ◦ f1 fails with probability\r\nat most n\r\n−Ω(1)\r\n.\r\n6.3 Decision Trees\r\nWe formally define what we mean by a decision tree for a data structure problem. Consider\r\na partial problem F : I → {0, 1} with I ⊂ X × Y , where X is the set of “queries” and Y is\r\nthe set of “datasets”.\r\nFor y ∈ Y , a decision tree Ty is a complete binary tree in which:\r\n• each internal node v is labeled with a predicate function fv : X → {0, 1}. We assume\r\nfv comes from some set F of allowed predicates.\r\n• each edge is labeled with 0 or 1, indicating the answer to the parent’s predicate.\r\n• each leaf is labeled with 0 or 1, indicating the outcome of the computation.\r\nEvaluating Ty on x is done by computing the root’s predicate on x, following the corre\u0002sponding edge, computing the next node’s predicate, and so on until a leaf is reached. The\r\nlabel of the leaf is the output, denoted Ty(x).\r\nWe let the size s of the tree to be the total number of the nodes. The depth d of the tree\r\nis the longest path from the root to a leaf. The predicate size is w = dlog2 Fe.\r\nWe say that problem F can be solved by a decision tree of size s, depth d, and predicate\r\nsize w iff, for any y, there exists some tree Ty of size at most s, depth at most d, and node\r\nsize at most w, such that Ty(x) = F(x, y) whenever (x, y) ∈ I.\r\nOur result on the decision tree lower bound follows from the following folklore lemma,\r\nwhich converts an efficient decision tree solving a problem F into an efficient communication\r\nprotocol.\r\nLemma 6.3. Consider any (promise) problem F : I → {0, 1}, where I ⊂ X × Y . Suppose\r\nthere exists a decision tree of size s, depth d, and node size w.\r\nIf Alice receives x ∈ X and Bob receives y ∈ Y , there exists a communication protocol\r\nsolving the problem F, in which Alice sends a total of a = O(log s) bits and Bob sends\r\nb = O(dw log s) bits.\r\nProof. Before the protocol, Bob constructs his decision tree Ty. Suppose, for a moment, that\r\nthe decision tree is balanced, that is d = O(log s). Then, Alice and Bob can run the following\r\n“ideal” protocol. In round one, Bob sends the predicate fr of the root r of the decision tree.\r\nAlice computes fr(x) (a bit) and sends it back. Then Bob follows the corresponding edge in\r\nthe tree, and sends the predicate of the corresponding child, etc. We obtain communication\r\na ≤ d and b ≤ w · d.\r\nIn general, however, the decision tree TD is not balanced. In this case, Alice and Bob\r\ncan simulate a standard binary search on a tree. Specifically, Bob finds a separator edge\r\n89",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/94f50531-2699-4cbc-bce2-af2d8a8886ce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63a8485d5e5a600f72b83f233553713e1c1931da2622d053ac67b13e12216c98",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "94f50531-2699-4cbc-bce2-af2d8a8886ce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 89,
            "page_width": 612,
            "page_height": 792,
            "content": "Finally, Alice and Bob can run the (1+ε)-NN communication protocol with r = D\r\n√\r\n2(1−\r\nε −\r\n5ε\r\n24 ) to decide whether S ∩ T = ∅. Note that the error probability of the resulting set\r\ndisjointness protocol is bounded away from 1/2 since (1+ε)-NN communication protocol has\r\nerror probability bounded away from 1/2, and the embedding f2 ◦ f1 fails with probability\r\nat most n\r\n−Ω(1)\r\n.\r\n6.3 Decision Trees\r\nWe formally define what we mean by a decision tree for a data structure problem. Consider\r\na partial problem F : I → {0, 1} with I ⊂ X × Y , where X is the set of “queries” and Y is\r\nthe set of “datasets”.\r\nFor y ∈ Y , a decision tree Ty is a complete binary tree in which:\r\n• each internal node v is labeled with a predicate function fv : X → {0, 1}. We assume\r\nfv comes from some set F of allowed predicates.\r\n• each edge is labeled with 0 or 1, indicating the answer to the parent’s predicate.\r\n• each leaf is labeled with 0 or 1, indicating the outcome of the computation.\r\nEvaluating Ty on x is done by computing the root’s predicate on x, following the corre\u0002sponding edge, computing the next node’s predicate, and so on until a leaf is reached. The\r\nlabel of the leaf is the output, denoted Ty(x).\r\nWe let the size s of the tree to be the total number of the nodes. The depth d of the tree\r\nis the longest path from the root to a leaf. The predicate size is w = dlog2 Fe.\r\nWe say that problem F can be solved by a decision tree of size s, depth d, and predicate\r\nsize w iff, for any y, there exists some tree Ty of size at most s, depth at most d, and node\r\nsize at most w, such that Ty(x) = F(x, y) whenever (x, y) ∈ I.\r\nOur result on the decision tree lower bound follows from the following folklore lemma,\r\nwhich converts an efficient decision tree solving a problem F into an efficient communication\r\nprotocol.\r\nLemma 6.3. Consider any (promise) problem F : I → {0, 1}, where I ⊂ X × Y . Suppose\r\nthere exists a decision tree of size s, depth d, and node size w.\r\nIf Alice receives x ∈ X and Bob receives y ∈ Y , there exists a communication protocol\r\nsolving the problem F, in which Alice sends a total of a = O(log s) bits and Bob sends\r\nb = O(dw log s) bits.\r\nProof. Before the protocol, Bob constructs his decision tree Ty. Suppose, for a moment, that\r\nthe decision tree is balanced, that is d = O(log s). Then, Alice and Bob can run the following\r\n“ideal” protocol. In round one, Bob sends the predicate fr of the root r of the decision tree.\r\nAlice computes fr(x) (a bit) and sends it back. Then Bob follows the corresponding edge in\r\nthe tree, and sends the predicate of the corresponding child, etc. We obtain communication\r\na ≤ d and b ≤ w · d.\r\nIn general, however, the decision tree TD is not balanced. In this case, Alice and Bob\r\ncan simulate a standard binary search on a tree. Specifically, Bob finds a separator edge\r\n89",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/94f50531-2699-4cbc-bce2-af2d8a8886ce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63a8485d5e5a600f72b83f233553713e1c1931da2622d053ac67b13e12216c98",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "0e6db466-cb2a-463a-a9fb-58b2f6eda0b0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 90,
            "page_width": 612,
            "page_height": 792,
            "content": "that splits the tree in two components, each of size at least s/3. Let this separating edge\r\nbe (u, v). In round one, Alice and Bob want to detect whether, in the ideal protocol, Alice\r\nwould eventually follow the edge (u, v). To determine this, Bob sends the predicates for all\r\nnodes on the path from the root r to u. Alice evaluates these predicates on x and sends\r\nback a 1 if she would follow the edge (u, v), and 0 otherwise. Then, the players recurse on\r\nthe remaining part of the tree; they are done after O(log s) such rounds.\r\nIn the end, Alice sends only a = O(log s) bits, i.e. one bit per round. Bob sends O(d · w)\r\nbits per round, and thus b = O(dw log s).\r\nFrom this, we obtain the following very strong implications for the partial match and\r\nnear neighbor problems:\r\nTheorem 6.4. A decision tree for the partial match problem with predicate size O(n\r\nδ\r\n) must\r\neither have size 2\r\nΩ(d)\r\n, or depth Ω(n\r\n1−2δ/d).\r\nTheorem 6.5. A decision tree with predicate size O(n\r\nδ\r\n), for (1+ε)-approximate near neigh\u0002bor search in the Hamming cube {0, 1}\r\nd\r\n, must either have size n\r\nΩ(1/ε2)\r\n, or depth Ω(n\r\n1−2δ/d).\r\nNote that with depth O(n), one can just perform a linear scan of the database to find\r\nthe answer. Then, the space (decision tree size) is just linear. These bounds show that for\r\nstrongly sublinear query time, the space must be prohibitively large.\r\n6.4 Near-Linear Space\r\nWe first describe the intuition for why a lower bound of Ω(d/ lg n) for space S = n\r\nO(1)\r\n,\r\nshould also imply a lower bound of Ω(d/ lg d), when the space is S = n·(d lg n)\r\nO(1). For very\r\nsmall databases, namely n = d\r\nO(1), the lower bound for polynomial space can be rewritten as\r\nΩ(d/ lg d). If n is larger, one can hope to partition the problem into k = n/dO(1) independent\r\nsubproblems, each with database of size N = d\r\nO(1). Intuitively, each subproblem “gets” space\r\nS/k = (d · lg n)\r\nO(1) = N O(1), and hence it requires Ω(d/ lg d) cell probes.\r\nTransforming this intuition into an actual lower bound is surprisingly simple. Instead\r\nof simulating one query as part of a communication protocol, we will simulate k queries in\r\nparallel. In each step, the queriers need to send the subset of k cells which are probed, among\r\nthe S cells in memory. Sending this information requires O(lg \r\nS\r\nk\r\n\u0001\r\n) = O(k lg S\r\nk\r\n) bits. This\r\nis O(lg S\r\nk\r\n) bits “on average” per query, whereas the normal reduction sends O(lg S) bits for\r\none query. We will typically use k = n/ lgO(1) n.\r\nOur direct sum results from the previous chapter are crucial in this context. They can\r\nshow that considering k independent copies increases the communication lower bound by a\r\nfactor of Ω(k), which is exactly the intuition described above.\r\nTechnical details. Let PMd\r\nn be the partial match problem with a query in {0, 1, ?}\r\nd and\r\na database of n strings in {0, 1}\r\nd\r\n.\r\n90",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/0e6db466-cb2a-463a-a9fb-58b2f6eda0b0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0c70c7a281424bf20a26262a936aca2063ffb652e388a797876d3ac455cd490b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 537
      },
      {
        "segments": [
          {
            "segment_id": "0e6db466-cb2a-463a-a9fb-58b2f6eda0b0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 90,
            "page_width": 612,
            "page_height": 792,
            "content": "that splits the tree in two components, each of size at least s/3. Let this separating edge\r\nbe (u, v). In round one, Alice and Bob want to detect whether, in the ideal protocol, Alice\r\nwould eventually follow the edge (u, v). To determine this, Bob sends the predicates for all\r\nnodes on the path from the root r to u. Alice evaluates these predicates on x and sends\r\nback a 1 if she would follow the edge (u, v), and 0 otherwise. Then, the players recurse on\r\nthe remaining part of the tree; they are done after O(log s) such rounds.\r\nIn the end, Alice sends only a = O(log s) bits, i.e. one bit per round. Bob sends O(d · w)\r\nbits per round, and thus b = O(dw log s).\r\nFrom this, we obtain the following very strong implications for the partial match and\r\nnear neighbor problems:\r\nTheorem 6.4. A decision tree for the partial match problem with predicate size O(n\r\nδ\r\n) must\r\neither have size 2\r\nΩ(d)\r\n, or depth Ω(n\r\n1−2δ/d).\r\nTheorem 6.5. A decision tree with predicate size O(n\r\nδ\r\n), for (1+ε)-approximate near neigh\u0002bor search in the Hamming cube {0, 1}\r\nd\r\n, must either have size n\r\nΩ(1/ε2)\r\n, or depth Ω(n\r\n1−2δ/d).\r\nNote that with depth O(n), one can just perform a linear scan of the database to find\r\nthe answer. Then, the space (decision tree size) is just linear. These bounds show that for\r\nstrongly sublinear query time, the space must be prohibitively large.\r\n6.4 Near-Linear Space\r\nWe first describe the intuition for why a lower bound of Ω(d/ lg n) for space S = n\r\nO(1)\r\n,\r\nshould also imply a lower bound of Ω(d/ lg d), when the space is S = n·(d lg n)\r\nO(1). For very\r\nsmall databases, namely n = d\r\nO(1), the lower bound for polynomial space can be rewritten as\r\nΩ(d/ lg d). If n is larger, one can hope to partition the problem into k = n/dO(1) independent\r\nsubproblems, each with database of size N = d\r\nO(1). Intuitively, each subproblem “gets” space\r\nS/k = (d · lg n)\r\nO(1) = N O(1), and hence it requires Ω(d/ lg d) cell probes.\r\nTransforming this intuition into an actual lower bound is surprisingly simple. Instead\r\nof simulating one query as part of a communication protocol, we will simulate k queries in\r\nparallel. In each step, the queriers need to send the subset of k cells which are probed, among\r\nthe S cells in memory. Sending this information requires O(lg \r\nS\r\nk\r\n\u0001\r\n) = O(k lg S\r\nk\r\n) bits. This\r\nis O(lg S\r\nk\r\n) bits “on average” per query, whereas the normal reduction sends O(lg S) bits for\r\none query. We will typically use k = n/ lgO(1) n.\r\nOur direct sum results from the previous chapter are crucial in this context. They can\r\nshow that considering k independent copies increases the communication lower bound by a\r\nfactor of Ω(k), which is exactly the intuition described above.\r\nTechnical details. Let PMd\r\nn be the partial match problem with a query in {0, 1, ?}\r\nd and\r\na database of n strings in {0, 1}\r\nd\r\n.\r\n90",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/0e6db466-cb2a-463a-a9fb-58b2f6eda0b0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0c70c7a281424bf20a26262a936aca2063ffb652e388a797876d3ac455cd490b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 537
      },
      {
        "segments": [
          {
            "segment_id": "072db2e4-b859-4b30-97b8-0e176a073c14",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 91,
            "page_width": 612,
            "page_height": 792,
            "content": "Theorem 6.6. Consider a bounded error (Monte Carlo) data structure solving PMd\r\nn\r\nin the\r\ncell-probe model with cells of d\r\nO(1) bits, using space S. Assuming d ≥ 2 lg n, the cell-probe\r\ncomplexity of a query must be Ω(d/ lg Sd\r\nn\r\n).\r\nProof. It is easy to convert a solution to PMd\r\nn\r\ninto a solution to Lk PMD\r\nN , where N = n/k\r\nand D = d−lg k ≥ d/2. One simply prefixes query and database strings with the subproblem\r\nnumber, taking lg k bits.\r\nAs we showed above, a lower bound for the communication complexity of partial match\r\ncan be obtained by a very simple reduction from LSD. Our LSD lower bound from Chapter 5\r\nwas by richness. Interpreting this richness lower bound in the context of partial match, we\r\nsee that on a certain domain X × Y for PMD\r\nN , we have:\r\n• PMD\r\nN is 1\r\n2\r\n-dense.\r\n• for any δ > 0, in any rectangle of size |X|/2\r\nO(δD)×|Y |/2O(N1−δ/D2)\r\n, the density of zeros\r\nis Ω(1).\r\nFor concreteness, set δ =\r\n1\r\n2\r\nin the above result. Applying our direct sum result for\r\nrandomized richness, Theorem 5.18, to Lk PMD\r\nN , we obtain that either T lg S\r\nk = Ω(D), or\r\nT\r\n0w = Ω(√\r\nN/D2). Setting N = w\r\n2\r\n·D4·d = d\r\nO(1), the second inequality becomes T = Ω(d),\r\nwhile the first becomes T = Ω(lg Sd\r\nn\r\n). We thus conclude that T ≥ min{Ω(d), Ω(d/ lg Sd\r\nn\r\n)} =\r\nΩ(d/ lg Sd\r\nn\r\n).\r\n91",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/072db2e4-b859-4b30-97b8-0e176a073c14.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=79b99821d83b414d08a385bde1c670e55281e4d3add7201f83401a3c36578ed0",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "72fb3d6d-99cb-4bfc-a7a7-9be50d69786f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 92,
            "page_width": 612,
            "page_height": 792,
            "content": "92",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/72fb3d6d-99cb-4bfc-a7a7-9be50d69786f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ff081b9e804bd7834dd645b246ebf7c277fb7552929262ad70b5cb7178e97dd0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 267
      },
      {
        "segments": [
          {
            "segment_id": "28f11147-4f30-4caf-9bd3-be977a535866",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 93,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 7\r\nRange Query Problems\r\nIn the previous chapter, we introduced a neat trick in using asymmetric communication\r\ncomplexity to obtain cell-probe lower bounds: consider multiple queries at the same time\r\n(on Alice’s side) communicating to the same database. This allowed us to obtain lower\r\nbounds of Ω(lg n/ lg lg n) for a data structure using space O(n · polylog n) in the cell-probe\r\nmodel with words of O(lg n) bits.\r\nUnfortunately, our application was not so compelling. We showed Ω(lg n/ lg lg n) lower\r\nbounds for partial match, a very hard problem, where the optimal bound should probably be\r\naround n\r\n1−o(1). In this chapter, we demonstrate significantly more interesting applications.\r\nThe simple idea of analyzing multiple queries at the same time turns out to capture the\r\nfundamental complexity of very important range query problems.\r\nSee Chapter 2 for ample background on the various range query problems that we consider\r\nhere.\r\nOur results stem from a lower bound on an unusual problem: reachability oracles in\r\nbutterfly graphs.\r\nTheorem 7.1. Consider a data structure that represents a directed graph G that is a subgraph\r\nof a (directed) butterfly graph, and answers the following query:\r\nreachable(u, v): is node v reachable from u through a directed path in G?\r\nIf the data structure uses space nσ in the cell probe model with w-bit cells, the query time\r\nmust be Ω( lg n\r\nlg(σ+w)\r\n).\r\nThe proof of this result is given in §7.1, and it follows, somewhat surprisingly, by reduction\r\nfrom the complexity of lopsided set disjointness.\r\nIn §7.3, we show two corollaries of this result:\r\nTheorem 7.2. A data structure for orthogonal range stabbing in 2 dimensions using space\r\nn · σ in the cell probe model with w-bit cells, requires query time Ω( lg n\r\nlg(σw)\r\n).\r\nTheorem 7.3. A data structure for the dynamic marked ancestor problem, with amortized\r\nupdate time tu = O(polylog n) requires query time tq = Ω\r\nlg n\r\n(lg lg n)\r\n2\r\n\u0001\r\n.\r\n93",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/28f11147-4f30-4caf-9bd3-be977a535866.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a5245307209ecb33aa4d5de23f4d4166a41488b4a009e5413bd971b57a231c5f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 334
      },
      {
        "segments": [
          {
            "segment_id": "05a5c864-9583-4e25-92b3-108a879de005",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 94,
            "page_width": 612,
            "page_height": 792,
            "content": "Figure 7-1: A butterfly with degree 2 and depth 4.\r\nThe lower bound for 2-dimensional stabbing immediately implies our lower bound for 4-\r\ndimensional range reporting, and 2-dimensional range counting by the reductions discussed\r\nin Chapter 2.\r\nOur lower bound for the marked ancestor problem reproves the important result of Al\u0002strup, Husfeldt, and Rauhe [8], with a lg lg n loss in the query bound. Despite this slightly\r\nweaker bound, we feel our proof is interesting, as it shows a dynamic lower bound by arguing\r\nonly about static problems, and significantly clears up the structure of the problems under\r\nconsideration.\r\n7.1 The Butterfly Effect\r\nThe butterfly is a well-known graph structure with high “shuffle abilities.” The graph (Fig\u0002ure 7-1) is specified by two parameters: the degree b, and the depth d. The graph has d + 1\r\nlayers, each having b\r\nd vertices. The vertices on level 0 are sources, while the ones on level d\r\nare sinks. Each vertex except the sinks has out-degree d, and each vertex except the sources\r\nhas in-degree d. If we view vertices on each level as vectors in [b]\r\nd\r\n, the edges going out of\r\na vertex on level i go to vectors that may differ only on the ith coordinate. This ensures\r\nthat there is a unique path between any source and any sink: the path “morphs” the source\r\nvector into the sink node by changing one coordinate at each level.\r\nFor convenience, we will slightly abuse terminology and talk about “reachability oracles\r\nfor G,” where G is a butterfly graph. This problem is defined as follows: preprocess a\r\nsubgraph of G, to answer queries of the form: is sink v reachable from source u? The query\r\ncan be restated as: is any edge on the unique source–sink path missing from the subgraph?\r\n7.1.1 Reachability Oracles to Stabbing\r\nThe reduction from reachability oracles to stabbing is very easy to explain formally, and we\r\nproceed to do that now. However, there is a deeper meaning to this reduction, which will\r\nbe explored in §7.1.2.\r\n94",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/05a5c864-9583-4e25-92b3-108a879de005.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=dd2959d0bbb7358c0e08c6e615fa36248a6cbf57910eb93b0a87ace6e230aecc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 343
      },
      {
        "segments": [
          {
            "segment_id": "e3305c81-9ccf-4f74-b509-fe1cafb8e182",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 95,
            "page_width": 612,
            "page_height": 792,
            "content": "Reduction 7.4. Let G be a butterfly with M edges. The reachability oracle problem on G\r\nreduces to 2-dimensional stabbing over M rectangles.\r\nProof. If some edge of G does not appear in the subgraph, what source-sink paths does\r\nthis cut off? Say the edge is on level i, and is between vertices (· · · , vi−1, vi, vi+1, · · ·) and\r\n(· · · , vi−1, v0\r\ni\r\n, vi+1, · · ·). The sources that can reach this edge are precisely (?, · · · , ?, vi, vi+1, · · ·),\r\nwhere ? indicates an arbitrary value. The sinks that can be reached from the edge are\r\n(· · · , vi−1, v0\r\ni\r\n, ?, · · ·). The source–sink pairs that route through the missing edge are the\r\nCartesian product of these two sets.\r\nThis Cartesian product has precisely the format of a 2D rectangle. If we read a source\r\nvector (v1, . . . , vd) as a number in base b with the most significant digit being vd, the set of\r\nsources that can reach the edge is an interval of length b\r\ni−1\r\n. Similarly, a sink is treated as a\r\nnumber with the most significant digit v1, giving an interval of length b\r\nd−i\r\n.\r\nFor every missing edge, we define a rectangle with the source and sink pairs that route\r\nthrough it. Then, a sink is reachable from a source iff no rectangle is stabbed by the (sink,\r\nsource) point.\r\nObserve that the rectangles that we constructed overlap in complicated ways. This is in\r\nfact needed, because 2-dimensional range stabbing with non-overlapping rectangles can be\r\nsolved with query time O(lg2lg n) [35].\r\nAs explained in Chapter 2, 2D range stabbing reduces to 2D range counting and 4D\r\nrange reporting.\r\n7.1.2 The Structure of Dynamic Problems\r\nThe more interesting reduction is to the marked ancestor problem. The goal is to convert\r\na solution to the dynamic problem into a solution to some static problem for which we can\r\nprove a lower bound.\r\nA natural candidate would be to define the static problem to be the persistent version of\r\nthe dynamic problem. Abstractly, this is defined as follows:\r\ninput: an (offline) sequence of updates to a dynamic problem, denoted by u1, . . . , um.\r\nquery: a query q to dynamic problem and a time stamp τ ≤ m. The answer should be the\r\nanswer to q if it were executed by the dynamic data structure after updates u1, . . . , uτ .\r\nAn algorithm result for making data structures persistent can be used to imply a lower\r\nbound for the dynamic problem, based on a lower bound for the static problem. The following\r\nis a standard persistence result:\r\nLemma 7.5. If a dynamic problem can be solved with update time tu and query time tq, its\r\n(static) persistent version will have a solution with space O(m · tu) and query time O(tq ·\r\nlg lg(m · tu)).\r\nProof. We simulate the updates in order, and record their cell writes. Each cell in the\r\nsimulation has a collection of values and timestamps (which indicate when the value was\r\n95",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e3305c81-9ccf-4f74-b509-fe1cafb8e182.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ba404010a892f81a696f8f29a14c69b93bd7927a2fb780cb3beac5479295b765",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 532
      },
      {
        "segments": [
          {
            "segment_id": "e3305c81-9ccf-4f74-b509-fe1cafb8e182",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 95,
            "page_width": 612,
            "page_height": 792,
            "content": "Reduction 7.4. Let G be a butterfly with M edges. The reachability oracle problem on G\r\nreduces to 2-dimensional stabbing over M rectangles.\r\nProof. If some edge of G does not appear in the subgraph, what source-sink paths does\r\nthis cut off? Say the edge is on level i, and is between vertices (· · · , vi−1, vi, vi+1, · · ·) and\r\n(· · · , vi−1, v0\r\ni\r\n, vi+1, · · ·). The sources that can reach this edge are precisely (?, · · · , ?, vi, vi+1, · · ·),\r\nwhere ? indicates an arbitrary value. The sinks that can be reached from the edge are\r\n(· · · , vi−1, v0\r\ni\r\n, ?, · · ·). The source–sink pairs that route through the missing edge are the\r\nCartesian product of these two sets.\r\nThis Cartesian product has precisely the format of a 2D rectangle. If we read a source\r\nvector (v1, . . . , vd) as a number in base b with the most significant digit being vd, the set of\r\nsources that can reach the edge is an interval of length b\r\ni−1\r\n. Similarly, a sink is treated as a\r\nnumber with the most significant digit v1, giving an interval of length b\r\nd−i\r\n.\r\nFor every missing edge, we define a rectangle with the source and sink pairs that route\r\nthrough it. Then, a sink is reachable from a source iff no rectangle is stabbed by the (sink,\r\nsource) point.\r\nObserve that the rectangles that we constructed overlap in complicated ways. This is in\r\nfact needed, because 2-dimensional range stabbing with non-overlapping rectangles can be\r\nsolved with query time O(lg2lg n) [35].\r\nAs explained in Chapter 2, 2D range stabbing reduces to 2D range counting and 4D\r\nrange reporting.\r\n7.1.2 The Structure of Dynamic Problems\r\nThe more interesting reduction is to the marked ancestor problem. The goal is to convert\r\na solution to the dynamic problem into a solution to some static problem for which we can\r\nprove a lower bound.\r\nA natural candidate would be to define the static problem to be the persistent version of\r\nthe dynamic problem. Abstractly, this is defined as follows:\r\ninput: an (offline) sequence of updates to a dynamic problem, denoted by u1, . . . , um.\r\nquery: a query q to dynamic problem and a time stamp τ ≤ m. The answer should be the\r\nanswer to q if it were executed by the dynamic data structure after updates u1, . . . , uτ .\r\nAn algorithm result for making data structures persistent can be used to imply a lower\r\nbound for the dynamic problem, based on a lower bound for the static problem. The following\r\nis a standard persistence result:\r\nLemma 7.5. If a dynamic problem can be solved with update time tu and query time tq, its\r\n(static) persistent version will have a solution with space O(m · tu) and query time O(tq ·\r\nlg lg(m · tu)).\r\nProof. We simulate the updates in order, and record their cell writes. Each cell in the\r\nsimulation has a collection of values and timestamps (which indicate when the value was\r\n95",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e3305c81-9ccf-4f74-b509-fe1cafb8e182.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ba404010a892f81a696f8f29a14c69b93bd7927a2fb780cb3beac5479295b765",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 532
      },
      {
        "segments": [
          {
            "segment_id": "7042e17c-4a57-48e1-b2b8-be4f5f34375c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 96,
            "page_width": 612,
            "page_height": 792,
            "content": "updated). For each cell, we build a predecessor structure a la van Emde Boas [101] over\r\nthe time-stamps. The structures occupy O(m · tu) space in total, supporting queries in\r\nO(lg lg(mtu)) time. To simulate the query, we run a predecessor query for every cell read,\r\nfinding the last update that changed the cell before time τ .\r\nThus, if the static problem is hard, so is the dynamic problem (to within a doubly\r\nlogarithmic factor). However, the reverse is not necessarily true, and the persistent version\r\nof marked ancestor turns out to be easy, at least for the incremental case. To see that,\r\ncompute for each node the minimum time when it becomes marked. Then, we can propagate\r\ndown to every leaf the minimum time seen on the root-to-leaf path. To query the persistent\r\nversion, it suffices to compare the time stamp with this value stored at the leaf.\r\nAs it turns out, persistence is still the correct intuition for generating a hard static\r\nproblem. However, we need the stronger notion of full persistence. In partial persistence, as\r\nseen above, the updates create a linear chain of versions (an update always affects the more\r\nrecent version). In full persistence, the updates create a tree of versions, since updates are\r\nallowed to modify any historic version.\r\nFor an abstract dynamic problem, its fully-persistent version is defined as follows:\r\ninput: a rooted tree (called the version tree) in which every node is labeled with a sequence\r\nof update operations. The total number of updates is m.\r\nquery: a query q to the dynamic problem, and a node τ of the version tree. The answer\r\nshould be the answer to q if it were executed after the sequence of updates found on\r\nthe path through the version tree from the root to τ .\r\nLike the partially persistent problem, the fully persistent one can be solved by efficient\r\nsimulation of the dynamic problem:\r\nLemma 7.6. If a dynamic problem can be solved with update time tu and query time tq, the\r\nfully-persistent static problem has a solution with space O(m·tu) and query time O(tq lg lg(m·\r\ntu)).\r\nProof. For each cell of the simulated machine, consider the various nodes of the version tree\r\nin which the cell is written. Given a “time stamp” (node) τ , we must determine the most\r\nrecent change that happened on the path from τ to the root. This is the longest matching\r\nprefix problem, which is equivalent to predecessor search. Thus, the simulation complexity\r\nis the same as in Lemma 7.5.\r\nWe now have to prove a lower bound for the fully-persistent version of marked ancestor,\r\nwhich we accomplish by a reduction from reachability oracles in the butterfly:\r\nReduction 7.7. Let G be a subgraph of a butterfly with M edges. The reachability oracle\r\nproblem on G reduces to the fully-persistent version of the marked ancestor problem, with an\r\ninput of O(M) offline updates. The tree in the marked ancestor problem has the same degree\r\nand depth as the butterfly.\r\n96",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7042e17c-4a57-48e1-b2b8-be4f5f34375c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=31d0d55d7b67713c7ca2818d091af828d8a3883c80ba40ccabfdc67ccea0f508",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 505
      },
      {
        "segments": [
          {
            "segment_id": "04806755-480e-4d93-a242-f14c8a6715b1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 97,
            "page_width": 612,
            "page_height": 792,
            "content": "(a)\r\na0 a1\r\nb0 b1 b2 b3\r\nc0 c1 c2 c3 c4 c5 c6 c7\r\nd0 d1 d2 d3 d4 d5 d6 d7 d8 d9d10d11d12d13d14d15 (b) d0 d1 d2 d3 d4 d5 d6 d7 d8 d9d10d11d12d13d14d15\r\nc0 c1 c2 c3 c4 c5 c6 c7 c0 c1 c2 c3 c4 c5 c6 c7\r\nb0 b1 b2 b3 b0 b1 b2 b3 b0 b1 b2 b3 b0 b1 b2 b3\r\na0 a1 a0 a1 a0 a1 a0 a1 a0 a1 a0 a1 a0 a1 a0 a1\r\nFigure 7-2: (a) The marked ancestor problem. (b) An instance of fully-persistent marked\r\nancestor.\r\nProof. Our inputs to the fully-persistent problem have the pattern illustrated in Figure 7-2.\r\nAt the root of the version tree, we have update operations for the leaves of the marked\r\nancestor tree. If we desire a lower bound for the incremental marked ancestor problems, all\r\nnodes start unmarked, and we have an update for every leaf that needs to be marked. If we\r\nwant a decremental lower bound, all nodes start marked, and all operations are unmark.\r\nThe root has b subversions; in each subversion, the level above the leaves in the marked\r\nancestor tree is updates. The construction continues similarly, branching our more versions\r\nat the rate at which level size decreases. Thus, on each level of the version tree we have b\r\nd\r\nupdates, giving b\r\nd\r\n· d updates in total.\r\nWith this construction of the updates, the structure of the fully persistent marked ances\u0002tor problem is isomorphic to a butterfly. Imagine what happens when we query a leaf v of\r\nthe marked ancestor tree, at a leaf t of the version tree. We think of both v and t as vectors\r\nin [b]\r\nd\r\n, spelling out the root to leaf paths. The path from the root to v goes through every\r\nlevel of the version tree:\r\n• on the top level, there is a single version (t is irrelevant), in which v is updated.\r\n• on the next level, the subversion we descend to is decided by the first coordinate of t.\r\nIn this subversion, v’s parent is updated. Note that v’s parent is determined by the\r\nfirst d − 1 coordinates of v.\r\n• on the next level, the relevant subversion is dictated by the first two coordinates of\r\nt. In this subversion, v’s grandparent is updated, which depends on the first d − 2\r\ncoordinates of v.\r\n• etc.\r\nThis is precisely the definition of a source-to-sink path in the butterfly graph, morphing\r\nthe source into the sink one coordinate at a time. Each update will mark a node if the\r\ncorresponding edge in the butterfly is missing in the subgraph. Thus, we encounter a marked\r\nancestor iff some edge is missing.\r\n97",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/04806755-480e-4d93-a242-f14c8a6715b1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ea39c005751965c2457f64068f18a7018f6a66be8952d5b2d4556fe81acaf8fb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 459
      },
      {
        "segments": [
          {
            "segment_id": "33c2da89-c804-48f2-b48b-00ad104cfd19",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 98,
            "page_width": 612,
            "page_height": 792,
            "content": "Let us see how Reduction 7.7 combines with Lemma 7.6 to give an actual lower bound.\r\nGiven a butterfly graph with m edges, we generate at most m updates. From Lemma 7.6, the\r\nspace of the fully persistent structure is S = O(m · tu), and the query time O(tq lg lg(mtq)),\r\nwhere tu and tq are the assumed running times for dynamic marked ancestor. If tu =\r\nO(polylog m), the space is S = O(m polylog m).\r\nThe lower bound for reachability oracles from Theorem 7.1 implies that for space O(m polylog m),\r\nthe query time must be Ω\r\nlg m\r\nlg lg m\r\n\u0001\r\n. But we have an upper bound of O(tq lg lg(mtq)) for the\r\nquery time, so tq = Ω\r\nlg m\r\nlg2lg m\r\n\u0001\r\n. This is weaker by a lg lg m factor compared to the original\r\nbound of [8].\r\n7.2 Adding Structure to Set Disjointness\r\nRemember that in the lopsided set disjointness (LSD) problem, Alice and Bob receive sets\r\nS and T and must determine whether S ∩ T = ∅. Denote the size of Alice’s set by |S| = N,\r\nand let B be the fraction between the universe and N. In other words, S, T ⊆ [N · B]. Note\r\nthat |T| may be as large as N · B.\r\nIn Chapter 5, we showed the following deterministic lower bound for LSD:\r\nTheorem 7.8. Fix δ > 0. In a deterministic protocol for LSD, either Alice sends Ω(N lg B)\r\nbits, or Bob sends NB1−δbits.\r\nJust as it is more convenient to work with 3SAT that circuit-SAT for showing NP\u0002completeness, our reductions uses a more restricted version of LSD, which we call 2-\r\nBlocked-LSD. In this problem, the universe is interpreted as [ N\r\nB\r\n] × [B] × [B]. It is guar\u0002anteed that for all x ∈ [\r\nN\r\nB\r\n] and y ∈ [B], S contains a single element of the form (x, y, ?) and\r\na single element of the form (x, ?, y).\r\nIt is possible (and in fact easy) to reanalyze the lower bounds of Chapter 5, and show that\r\nthey also apply to 2-Blocked-LSD. However, in the spirit of this chapter, we choose to\r\ndesign a reduction from general LSD to this special case. We show that LSD is reducible to\r\n2-Blocked-LSD with communication complexity essentially O(N). Since the lower bound\r\nis ω(N), it must also hold for 2-Blocked-LSD.\r\nLemma 7.9. LSD reduces to 2-Blocked-LSD by a deterministic protocol with communi\u0002cation complexity O(N + lg lg B).\r\nProof. Consider a random permutation π of the universe. If Alice and Bob can agree on a\r\npermutation, they can apply it on their own inputs and solve LSD on π(S) and π(T).\r\nIn 2-Blocked-LSD, there are (B!)N/B valid instances of S. Since π(S) is a uni\u0002formly random N-subset of the universe, the probability that it generates an instance of\r\n2-Blocked-LSD is:\r\n(B!)N/B\u000e\r\n\u0012\r\nNB\r\nN\r\n\u0013\r\n≥\r\n\u0012\r\nB\r\ne\r\n\u0013B·(N/B)\r\n\u000e\r\n\u0012\r\neNB\r\nN\r\n\u0013N\r\n=\r\n\u0012\r\nB\r\ne\r\n\u0013N\r\n\u000eB · e\u0001N\r\n= e\r\n−2N\r\n98",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/33c2da89-c804-48f2-b48b-00ad104cfd19.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4c4e545e511ff9eedeec0af8b9bfd79a03165d5769619766dece262b07b369e3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "b071e1ff-4c6b-4dfb-9f5d-4ebf592cc75f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 99,
            "page_width": 612,
            "page_height": 792,
            "content": "We are looking for a small set F of permutations, such that for any input S, there exists\r\nπ ∈ F for which π(S) is an instance of 2-Blocked-LSD. Then, Alice and Bob can agree\r\non F in advance. When seeing an instance S of LSD, Alice chooses a π that maps it to an\r\ninstance of 2-Blocked-LSD, and sends the index of π in F to Bob, using lg |F| bits.\r\nBy the probabilistic method, we show there exist F of size e\r\n2N ·2N lg B, i.e. lg |F| = O(N).\r\nChoose every element of F randomly. For a fixed S, the probability that no element is\r\ngood is at most (1 − e\r\n−2N )|F| ≤ exp(−|F|/e−2N ) = e−2N lg B. But there are only NB\r\nN\r\n\u0001\r\n≤\r\n(eB)\r\nN < e2N lg B choices of S, so by a union bound F works for all possible S with nonzero\r\nprobability.\r\nFinally, we must clarify the notion of reduction from a communication problem to a\r\ndata-structure problem. In such a reduction, Bob constructs a database based on his set T,\r\nand Alice constructs a set of k queries. It is then shown that LSD can be solved based on\r\nthe answer to the k queries on Bob’s database. If we are interested in lower bounds for space\r\nn\r\n1+o(1), we must reduce to a large number k of queries. In each cell probe, the queries want\r\nto read some k cells from the memory of size S. Then, Alice can send lg \r\nS\r\nk\r\n\u0001\r\nbits, and Bob\r\ncan reply with k · w. Observe that lg \r\nS\r\nk\r\n\u0001\r\n= Θ(k lg S\r\nk\r\n) \u001c k lg S, if k is large enough.\r\n7.2.1 Randomized Bounds\r\nIn Chapter 5, we also showed the following randomized lower bound for LSD:\r\nTheorem 7.10. Assume Alice receives a set S, |S| = n and Bob receives a set T, |T| = m,\r\nboth sets coming from a universe of size 2nm, for m < nγ, where γ < 1 is a constant. In\r\nany randomized, two-sided error communication protocol deciding disjointness of S and T,\r\neither Alice sends Ω(m lg n) bits or Bob sends Ω(n\r\n1−δ\r\n) bits, for any δ > 0.\r\nThis bound will not be enough for our reduction to reachability oracles in butterfly\r\ngraphs, because the universe is quadratic in the set sizes. We can replace this lower bound\r\nby an optimal randomized lower bound for LSD which only needs a linear universe.\r\nHowever, we can also give a simple self-contained proof of a randomized lower bound\r\nwithout requiring this more complicated result. This alternative proof works by reducing\r\nfrom Lk LSD, a direct sum of randomized LSD problems. All this is needed is the direct\r\nsum result for randomized richness, which we proved in §5.4.3.\r\n7.3 Set Disjointness to Reachability Oracles\r\nSince we want a lower bound for near-linear space, we must reduce LSD to k parallel queries\r\non the reachability oracle. The entire action is in what value of k we can achieve. Note,\r\nfor instance, that k = N is trivial, because Alice can pose a query for each item in her\r\nset. However, a reduction with k = N is also useless. Remember that the communication\r\ncomplexity of Alice is t·lg \r\nS\r\nk\r\n\u0001\r\n≥ tlg NB\r\nN\r\n\u0001\r\n. But LSD is trivially solvable with communication\r\n99",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b071e1ff-4c6b-4dfb-9f5d-4ebf592cc75f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8374d514fb0f5bb1d0cc8520f5cbd986aa384ebef243849124f55641f944d5d4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 571
      },
      {
        "segments": [
          {
            "segment_id": "b071e1ff-4c6b-4dfb-9f5d-4ebf592cc75f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 99,
            "page_width": 612,
            "page_height": 792,
            "content": "We are looking for a small set F of permutations, such that for any input S, there exists\r\nπ ∈ F for which π(S) is an instance of 2-Blocked-LSD. Then, Alice and Bob can agree\r\non F in advance. When seeing an instance S of LSD, Alice chooses a π that maps it to an\r\ninstance of 2-Blocked-LSD, and sends the index of π in F to Bob, using lg |F| bits.\r\nBy the probabilistic method, we show there exist F of size e\r\n2N ·2N lg B, i.e. lg |F| = O(N).\r\nChoose every element of F randomly. For a fixed S, the probability that no element is\r\ngood is at most (1 − e\r\n−2N )|F| ≤ exp(−|F|/e−2N ) = e−2N lg B. But there are only NB\r\nN\r\n\u0001\r\n≤\r\n(eB)\r\nN < e2N lg B choices of S, so by a union bound F works for all possible S with nonzero\r\nprobability.\r\nFinally, we must clarify the notion of reduction from a communication problem to a\r\ndata-structure problem. In such a reduction, Bob constructs a database based on his set T,\r\nand Alice constructs a set of k queries. It is then shown that LSD can be solved based on\r\nthe answer to the k queries on Bob’s database. If we are interested in lower bounds for space\r\nn\r\n1+o(1), we must reduce to a large number k of queries. In each cell probe, the queries want\r\nto read some k cells from the memory of size S. Then, Alice can send lg \r\nS\r\nk\r\n\u0001\r\nbits, and Bob\r\ncan reply with k · w. Observe that lg \r\nS\r\nk\r\n\u0001\r\n= Θ(k lg S\r\nk\r\n) \u001c k lg S, if k is large enough.\r\n7.2.1 Randomized Bounds\r\nIn Chapter 5, we also showed the following randomized lower bound for LSD:\r\nTheorem 7.10. Assume Alice receives a set S, |S| = n and Bob receives a set T, |T| = m,\r\nboth sets coming from a universe of size 2nm, for m < nγ, where γ < 1 is a constant. In\r\nany randomized, two-sided error communication protocol deciding disjointness of S and T,\r\neither Alice sends Ω(m lg n) bits or Bob sends Ω(n\r\n1−δ\r\n) bits, for any δ > 0.\r\nThis bound will not be enough for our reduction to reachability oracles in butterfly\r\ngraphs, because the universe is quadratic in the set sizes. We can replace this lower bound\r\nby an optimal randomized lower bound for LSD which only needs a linear universe.\r\nHowever, we can also give a simple self-contained proof of a randomized lower bound\r\nwithout requiring this more complicated result. This alternative proof works by reducing\r\nfrom Lk LSD, a direct sum of randomized LSD problems. All this is needed is the direct\r\nsum result for randomized richness, which we proved in §5.4.3.\r\n7.3 Set Disjointness to Reachability Oracles\r\nSince we want a lower bound for near-linear space, we must reduce LSD to k parallel queries\r\non the reachability oracle. The entire action is in what value of k we can achieve. Note,\r\nfor instance, that k = N is trivial, because Alice can pose a query for each item in her\r\nset. However, a reduction with k = N is also useless. Remember that the communication\r\ncomplexity of Alice is t·lg \r\nS\r\nk\r\n\u0001\r\n≥ tlg NB\r\nN\r\n\u0001\r\n. But LSD is trivially solvable with communication\r\n99",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b071e1ff-4c6b-4dfb-9f5d-4ebf592cc75f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8374d514fb0f5bb1d0cc8520f5cbd986aa384ebef243849124f55641f944d5d4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 571
      },
      {
        "segments": [
          {
            "segment_id": "b9d819e3-c8d3-4f37-8d5e-304ebf62ad84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 100,
            "page_width": 612,
            "page_height": 792,
            "content": "lg NB\r\nN\r\n\u0001\r\n, since Alice can communicate her entire set. Thus, there is no contradiction with\r\nthe lower bound.\r\nTo get a lower bound on t, k must be made as small as possible compared to N. Intu\u0002itively, a source–sink path in a butterfly of depth d traverses d edges, so it should be possible\r\nto test d elements by a single query. To do that, the edges must assemble in contiguous\r\nsource–sink paths, which turns out to be possible if we carefully match the structure of the\r\nbutterfly and the 2-Blocked-LSD problem:\r\nReduction 7.11. Let G be a degree-B butterfly graph with N non-sink vertices and N · B\r\nedges, and let d be its depth. 2-Blocked-LSD reduces to N\r\nd\r\nparallel queries to a reachability\r\noracle for a subgraph of G.\r\nProof. Remember that in 2-Blocked-LSD, elements are triples (x, y, z) from the universe\r\n[\r\nN\r\nB\r\n] × [B] × [B]. We define below a bijection between [ N\r\nB\r\n] × [B] and the non-sink vertices of\r\nG. Since (x, y) is mapped to a non-sink vertex, it is natural to associate (x, y, z) to an edge,\r\nspecifically edge number z going out of vertex (x, y).\r\nBob constructs a reachability oracle for the graph G excluding the edges in his set T.\r\nThen, Alice must find out whether any edge from her set S has been deleted. By mapping\r\nthe universe [ N\r\nB\r\n] × [B] to the nodes carefully, we will ensure that Alice’s edges on each level\r\nform a perfect matching. Then, her set of N edges form N\r\nd\r\ndisjoint paths from sources to\r\nsinks. Using this property, Alice can just issue N\r\nd\r\nqueries for these paths. If any of the\r\nsource–sink pairs is unreachable, some edge in S has been deleted.\r\nTo ensure Alice’s edges form perfect matchings at each level, we first decompose the\r\nnon-sink vertices of G into N\r\nB microsets of B elements each. Each microset is associated to\r\nsome level i, and contains nodes of the form (· · · , vi−1, ?, vi+1, ·) on level i. A value (x, y) is\r\nmapped to node number y in a microset identified by x (through some arbitrary bijection\r\nbetween [ N\r\nB\r\n] and microsets).\r\nLet (x, 1, z1), . . . ,(x, B, zB) be the values in S that give edges going out of microset x. If\r\nthe nodes of the microset are the vectors (· · · , vi−1, ?, vi+1, ·), the nodes to which the edges of\r\nS go are the vectors (· · · , vi−1, zj, vi+1, ·) on the next level, where j ∈ [B]. Observe that edges\r\nfrom different microsets cannot go to the same vertex. Also, edges from the same microset\r\ngo to distinct vertices by the 2-Blocked property: for any fixed x, the zj’s are distinct. Since\r\nall edges on a level point to distinct vertices, they form a perfect matching.\r\nLet us now compute the lower bounds implied by the reduction. We obtain a protocol for\r\n2-Blocked-LSD in which Alice communicates tlg \r\nS\r\nk\r\n\u0001\r\n= O(tk lg S\r\nk\r\n) = O(N ·\r\nt\r\nd\r\nlg Sd\r\nN\r\n) bits,\r\nand Bob communicates k · t · w = O(N ·\r\nt\r\nd\r\n· w) bits. On the other hand, the lower bound\r\nfor 2-Blocked-LSD says that Alice needs to communicate Ω(N lg B) bits, or Bob needs to\r\ncommunicate NB1−δ, for any constant δ > 0. It suffices to use, for instance, δ =\r\n1\r\n2\r\n.\r\nComparing the lower bounds with the reduction upper bound, we conclude that either\r\nt\r\nd\r\nlg Sd\r\nN = Ω(lg B), or tdw = Ω(√\r\nB). Set the degree of the butterfly to satisfy B ≥ w\r\n2 and\r\nlg B ≥ lg Sd\r\nN\r\n. Then, t\r\nd = Ω(1), i.e. t = Ω(d). This is intuitive: it shows that the query needs\r\nto be as slow as the depth, essentially traversing a source to sink path.\r\n100",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b9d819e3-c8d3-4f37-8d5e-304ebf62ad84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=80887c90a5b1572810ffce8d851ff0bc945a5d363d7d0672fa22621e85ae6563",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 679
      },
      {
        "segments": [
          {
            "segment_id": "b9d819e3-c8d3-4f37-8d5e-304ebf62ad84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 100,
            "page_width": 612,
            "page_height": 792,
            "content": "lg NB\r\nN\r\n\u0001\r\n, since Alice can communicate her entire set. Thus, there is no contradiction with\r\nthe lower bound.\r\nTo get a lower bound on t, k must be made as small as possible compared to N. Intu\u0002itively, a source–sink path in a butterfly of depth d traverses d edges, so it should be possible\r\nto test d elements by a single query. To do that, the edges must assemble in contiguous\r\nsource–sink paths, which turns out to be possible if we carefully match the structure of the\r\nbutterfly and the 2-Blocked-LSD problem:\r\nReduction 7.11. Let G be a degree-B butterfly graph with N non-sink vertices and N · B\r\nedges, and let d be its depth. 2-Blocked-LSD reduces to N\r\nd\r\nparallel queries to a reachability\r\noracle for a subgraph of G.\r\nProof. Remember that in 2-Blocked-LSD, elements are triples (x, y, z) from the universe\r\n[\r\nN\r\nB\r\n] × [B] × [B]. We define below a bijection between [ N\r\nB\r\n] × [B] and the non-sink vertices of\r\nG. Since (x, y) is mapped to a non-sink vertex, it is natural to associate (x, y, z) to an edge,\r\nspecifically edge number z going out of vertex (x, y).\r\nBob constructs a reachability oracle for the graph G excluding the edges in his set T.\r\nThen, Alice must find out whether any edge from her set S has been deleted. By mapping\r\nthe universe [ N\r\nB\r\n] × [B] to the nodes carefully, we will ensure that Alice’s edges on each level\r\nform a perfect matching. Then, her set of N edges form N\r\nd\r\ndisjoint paths from sources to\r\nsinks. Using this property, Alice can just issue N\r\nd\r\nqueries for these paths. If any of the\r\nsource–sink pairs is unreachable, some edge in S has been deleted.\r\nTo ensure Alice’s edges form perfect matchings at each level, we first decompose the\r\nnon-sink vertices of G into N\r\nB microsets of B elements each. Each microset is associated to\r\nsome level i, and contains nodes of the form (· · · , vi−1, ?, vi+1, ·) on level i. A value (x, y) is\r\nmapped to node number y in a microset identified by x (through some arbitrary bijection\r\nbetween [ N\r\nB\r\n] and microsets).\r\nLet (x, 1, z1), . . . ,(x, B, zB) be the values in S that give edges going out of microset x. If\r\nthe nodes of the microset are the vectors (· · · , vi−1, ?, vi+1, ·), the nodes to which the edges of\r\nS go are the vectors (· · · , vi−1, zj, vi+1, ·) on the next level, where j ∈ [B]. Observe that edges\r\nfrom different microsets cannot go to the same vertex. Also, edges from the same microset\r\ngo to distinct vertices by the 2-Blocked property: for any fixed x, the zj’s are distinct. Since\r\nall edges on a level point to distinct vertices, they form a perfect matching.\r\nLet us now compute the lower bounds implied by the reduction. We obtain a protocol for\r\n2-Blocked-LSD in which Alice communicates tlg \r\nS\r\nk\r\n\u0001\r\n= O(tk lg S\r\nk\r\n) = O(N ·\r\nt\r\nd\r\nlg Sd\r\nN\r\n) bits,\r\nand Bob communicates k · t · w = O(N ·\r\nt\r\nd\r\n· w) bits. On the other hand, the lower bound\r\nfor 2-Blocked-LSD says that Alice needs to communicate Ω(N lg B) bits, or Bob needs to\r\ncommunicate NB1−δ, for any constant δ > 0. It suffices to use, for instance, δ =\r\n1\r\n2\r\n.\r\nComparing the lower bounds with the reduction upper bound, we conclude that either\r\nt\r\nd\r\nlg Sd\r\nN = Ω(lg B), or tdw = Ω(√\r\nB). Set the degree of the butterfly to satisfy B ≥ w\r\n2 and\r\nlg B ≥ lg Sd\r\nN\r\n. Then, t\r\nd = Ω(1), i.e. t = Ω(d). This is intuitive: it shows that the query needs\r\nto be as slow as the depth, essentially traversing a source to sink path.\r\n100",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/b9d819e3-c8d3-4f37-8d5e-304ebf62ad84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=80887c90a5b1572810ffce8d851ff0bc945a5d363d7d0672fa22621e85ae6563",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 679
      },
      {
        "segments": [
          {
            "segment_id": "9591e86d-392d-4761-b93d-5e1722e3e9f2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 101,
            "page_width": 612,
            "page_height": 792,
            "content": "Finally, note that the depth is d = Θ(logB N). Since lg B ≥ max \b\r\n2 lg w, lg Sd\r\nN\r\n\t\r\n= Ωlg w+\r\nlg Sd\r\nN\r\n\u0001\r\n= Ωlg Sdw\r\nN\r\n\u0001\r\n. Note that certainly d < w, so lg B = Ω(lg Sw\r\nN\r\n). We obtain t = Ω(d) =\r\nΩ(lg N/ lg Sw\r\nN\r\n).\r\n101",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9591e86d-392d-4761-b93d-5e1722e3e9f2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6f07788ea608e20ce7e2ce7a15bee67d895cf46368e3360a338223c135e5518c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "fe4dde16-c74c-44c9-aee4-7ec0068add3c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 102,
            "page_width": 612,
            "page_height": 792,
            "content": "102",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/fe4dde16-c74c-44c9-aee4-7ec0068add3c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bf396bc6f3241177942118bdaf770828fcd7822b8f50094feb6f349c5d916e73",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "14f55f83-ddb1-4501-b37d-c834825ad2e8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 103,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 8\r\nNear Neighbor Search in `∞\r\nIn this chapter, we deal with near neighbor search (NNS) under the distance d(p, q) =\r\nkp−qk∞ = maxi∈[d]|pi −qi|, called the `∞ norm. See §2.4.3 for background on near-neighbor\r\nsearch, and, in particular, for a discussion of this important metric.\r\nThe structure of the `∞ space is intriguingly different, and more mysterious than other\r\nnatural spaces, such as the `1 and `2 norms. In fact, there is precisely one data structure\r\nfor NNS under `∞ with provable guarantees. In FOCS’98, Indyk [61] described an NNS\r\nalgorithm for d-dimensional `∞ with approximation 4dlogρlog2 4de+ 1, which required space\r\ndnρlgO(1) n and query time d · lgO(1) n, for any ρ > 1. For 3-approximation, Indyk also\r\ngives a solution with storage O(n\r\nlog2 d+1). Note that in the regime of polynomial space, the\r\nalgorithm achieves an uncommon approximation factor of O(lg lg d).\r\nIn this chapter, we begin by describing Indyk’s data structure in a manner that is con\u0002ceptually different from the original description. Our view relies on an information-theoretic\r\nunderstanding of the algorithm, which we feel explains its behavior much more clearly.\r\nInspired by this understanding, we are able to prove a lower bound for the asymmetric\r\ncommunication complexity of c-approximate NNS in `∞:\r\nTheorem 8.1. Assume Alice holds a point q ∈ {0, . . . , m}\r\nd\r\n, and Bob holds a database\r\nD ⊂ {−m, . . . m}\r\nd of n points. They communicate via a deterministic protocol to output:\r\n“1” if there exists some p ∈ D such that kq − pk∞ ≤ 1;\r\n“0” if, for all p ∈ D, we have kq − pk∞ ≥ c.\r\nFix δ, ε > 0; assume the dimension d satisfies Ω(lg1+ε n) ≤ d ≤ o(n), and the approximation\r\nratio satisfies 3 < c ≤ O(lg lg d). Further define ρ =\r\n1\r\n2\r\n(\r\nε\r\n2\r\nlg d)\r\n1/c > 10.\r\nThen, either Alice sends Ω(δρ lg n) bits, or Bob sends Ω(n\r\n1−δ\r\n) bits.\r\nNote that this result is tight in the communication model, suggesting the Indyk’s unusual\r\napproximation is in fact inherent to NNS in `∞. As explained in Chapter 6, this lower bound\r\non asymmetric communication complexity immediately implies the following corollaries for\r\ndata structures:\r\nCorollary 8.2. Let δ > 0 be constant, and assume Ω(lg1+δ n) ≤ d ≤ o(n). Consider\r\nany cell-probe data structure solving d-dimensional NNS under `∞ with approximation c =\r\n103",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/14f55f83-ddb1-4501-b37d-c834825ad2e8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1d79e5bcab4c77a302676986743eff5f299e00c912243ada099ec9dc1cbaf107",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 476
      },
      {
        "segments": [
          {
            "segment_id": "4026c603-4ae9-478b-b5ce-8f37ee45d880",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 104,
            "page_width": 612,
            "page_height": 792,
            "content": "O(logρ\r\nlog2 d). If the word size is w = n\r\n1−δ and the query complexity is t, the data structure\r\nrequires space n\r\nΩ(ρ/t)\r\n.\r\nCorollary 8.3. Let δ > 0 be constant, and assume Ω(lg1+δ n) ≤ d ≤ o(n). A decision tree\r\nof depth n\r\n1−2δ with predicate size nδ\r\nthat solves d-dimensional near-neighbor search under\r\n`∞ with approximation c = O(logρlog2 d), must have size n\r\nΩ(ρ)\r\n.\r\nAs with all known lower bounds for large space, Corollary 8.2 is primarily interesting for\r\nconstant query time, and degrades exponentially with t. On the other hand, the lower bound\r\nfor decision trees holds even for extremely high running time (depth) of n\r\n1−δ\r\n. A decision\r\ntree with depth n and predicate size O(d lg M) is trivial: simply test all database points.\r\nIndyk’s result is a deterministic decision tree with depth O(d · poly log n) and predicate\r\nsize O(lg d + lg M). Thus, we show an optimal trade-off between space and approximation,\r\nat least in the decision tree model. In particular, for polynomial space, the approximation\r\nfactor of Θ(lg lg d) is intrinsic to NNS under `∞.\r\n8.1 Review of Indyk’s Upper Bound\r\nDecision trees. Due to the decomposability of `∞ as a maximum over coordinates, a\r\nnatural idea is to solve NNS by a decision tree in which every node is a coordinate comparison.\r\nA node v is reached for some set Qv ⊆ Z\r\nd of queries. If the node compares coordinate i ∈ [d]\r\nwith a “separator” x, its two children will be reached for queries in Q` = Qv ∩ {q | qi < x},\r\nrespectively in Qr = Qv ∩ {q | qi > x} (assume x is non-integral to avoid ties).\r\npi\r\nx − 1 x x + 1\r\nN` Nr\r\nQ` Qr\r\nv\r\n` r\r\nFigure 8-1: A separator x on coordinate i.\r\nDefine [x, y]i =\r\n\b\r\np | pi ∈ [x, y]\r\n\t\r\n. Then, Q` =\r\nQv ∩ [−∞, x]i and Qr = Qv ∩ [x, ∞]i.\r\nIf the query is known to lie in some Qv, the\r\nset of database points that could still be a near\r\nneighbor is Nv = D ∩\r\n\r\nQv + [−1, 1]d\r\n\u0001\r\n, i.e. the\r\npoints inside the Minkowski sum of the query\r\nset with the `∞ “ball” of radius one. For our\r\nexample node comparing coordinate i ∈ [d] with\r\nx, the children nodes have N` = Nv ∩ [−∞, x +\r\n1]i, respectively Nr = Nv ∩ [x − 1, +∞]i.\r\nObserve that N` ∩ Nr = Nv ∩ [x − 1, x + 1]i.\r\nIn some sense, the database points in this slab\r\nare being “replicated,” since both the left and\r\nright subtrees must consider them as potential\r\nnear neighbors. This recursive replication of database points is the cause of superlinear\r\nspace. The contribution of Indyk [61] is an intriguing scheme for choosing a separator that\r\nguarantees a good bound on this recursive growth.\r\nInformation progress. Our first goal is to get a handle on the growth of the decision\r\ntree, as database points are replicated recursively. Imagine, for now, that queries come from\r\n104",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/4026c603-4ae9-478b-b5ce-8f37ee45d880.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=32c8de937fe53ab421a52dea5c5bda8a8eb7d83ba85ce4ae768563510a5cd4cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 530
      },
      {
        "segments": [
          {
            "segment_id": "4026c603-4ae9-478b-b5ce-8f37ee45d880",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 104,
            "page_width": 612,
            "page_height": 792,
            "content": "O(logρ\r\nlog2 d). If the word size is w = n\r\n1−δ and the query complexity is t, the data structure\r\nrequires space n\r\nΩ(ρ/t)\r\n.\r\nCorollary 8.3. Let δ > 0 be constant, and assume Ω(lg1+δ n) ≤ d ≤ o(n). A decision tree\r\nof depth n\r\n1−2δ with predicate size nδ\r\nthat solves d-dimensional near-neighbor search under\r\n`∞ with approximation c = O(logρlog2 d), must have size n\r\nΩ(ρ)\r\n.\r\nAs with all known lower bounds for large space, Corollary 8.2 is primarily interesting for\r\nconstant query time, and degrades exponentially with t. On the other hand, the lower bound\r\nfor decision trees holds even for extremely high running time (depth) of n\r\n1−δ\r\n. A decision\r\ntree with depth n and predicate size O(d lg M) is trivial: simply test all database points.\r\nIndyk’s result is a deterministic decision tree with depth O(d · poly log n) and predicate\r\nsize O(lg d + lg M). Thus, we show an optimal trade-off between space and approximation,\r\nat least in the decision tree model. In particular, for polynomial space, the approximation\r\nfactor of Θ(lg lg d) is intrinsic to NNS under `∞.\r\n8.1 Review of Indyk’s Upper Bound\r\nDecision trees. Due to the decomposability of `∞ as a maximum over coordinates, a\r\nnatural idea is to solve NNS by a decision tree in which every node is a coordinate comparison.\r\nA node v is reached for some set Qv ⊆ Z\r\nd of queries. If the node compares coordinate i ∈ [d]\r\nwith a “separator” x, its two children will be reached for queries in Q` = Qv ∩ {q | qi < x},\r\nrespectively in Qr = Qv ∩ {q | qi > x} (assume x is non-integral to avoid ties).\r\npi\r\nx − 1 x x + 1\r\nN` Nr\r\nQ` Qr\r\nv\r\n` r\r\nFigure 8-1: A separator x on coordinate i.\r\nDefine [x, y]i =\r\n\b\r\np | pi ∈ [x, y]\r\n\t\r\n. Then, Q` =\r\nQv ∩ [−∞, x]i and Qr = Qv ∩ [x, ∞]i.\r\nIf the query is known to lie in some Qv, the\r\nset of database points that could still be a near\r\nneighbor is Nv = D ∩\r\n\r\nQv + [−1, 1]d\r\n\u0001\r\n, i.e. the\r\npoints inside the Minkowski sum of the query\r\nset with the `∞ “ball” of radius one. For our\r\nexample node comparing coordinate i ∈ [d] with\r\nx, the children nodes have N` = Nv ∩ [−∞, x +\r\n1]i, respectively Nr = Nv ∩ [x − 1, +∞]i.\r\nObserve that N` ∩ Nr = Nv ∩ [x − 1, x + 1]i.\r\nIn some sense, the database points in this slab\r\nare being “replicated,” since both the left and\r\nright subtrees must consider them as potential\r\nnear neighbors. This recursive replication of database points is the cause of superlinear\r\nspace. The contribution of Indyk [61] is an intriguing scheme for choosing a separator that\r\nguarantees a good bound on this recursive growth.\r\nInformation progress. Our first goal is to get a handle on the growth of the decision\r\ntree, as database points are replicated recursively. Imagine, for now, that queries come from\r\n104",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/4026c603-4ae9-478b-b5ce-8f37ee45d880.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=32c8de937fe53ab421a52dea5c5bda8a8eb7d83ba85ce4ae768563510a5cd4cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 530
      },
      {
        "segments": [
          {
            "segment_id": "703a61a7-3589-4ffa-81e9-34bb36563f38",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 105,
            "page_width": 612,
            "page_height": 792,
            "content": "some distribution µ. The reader who enjoys worst-case algorithms need not worry: µ is just\r\nan analysis gimmick, and the algorithm will be deterministic.\r\nWe can easily bound the tree size in terms of the measure of the smallest Qv ever reached:\r\nthere can be at most 1/ minv Prµ[Qv] distinct leaves in the decision tree, since different\r\nleaves are reached for disjoint Qv’s. Let IQ(v) = log2\r\n1\r\nPrµ[Qv]\r\n; this can be understood as the\r\ninformation learned about the query, when computation reaches node v. We can now rewrite\r\nthe space bound as O\r\n\r\n2\r\nmaxv IQ(v)\r\n\u0001\r\n.\r\nAnother quantity that can track the behavior of the decision tree is HN (v) = log2|Nv|.\r\nEssentially, this is the “entropy” of the identity of the near neighbor, assuming that all\r\ndatabase points are equally likely neighbors.\r\nAt the root λ, we have IQ(λ) = 0 and HN (λ) = lg n. Decision nodes must reduce the\r\nentropy of the near neighbor until HN reaches zero (|Nv| = 1). Then, the algorithm can\r\nsimply read the single remaining candidate, and test whether it is a near neighbor of the\r\nquery. Unfortunately, decision nodes also increase IQ along the way, increasing the space\r\nbound. The key to the algorithm is to balance this tension between reducing the entropy of\r\nthe answer, HD, and not increasing the information about the query, IQ, too much.\r\nIn this information-theoretic view, Indyk’s algorithm shows that we can (essentially)\r\nalways find a separator that decreases HN by some δ but does not increase IQ by more than\r\nρ · δ. Thus, HD can be pushed from lg n down to 0, without ever increasing IQ by more than\r\nρ lg n. That is, space O(n\r\nρ\r\n) is achieved.\r\nSearching for separators. At the root λ, we let i ∈ [d] be an arbitrary coordinate,\r\nand search for a good separator x on that coordinate. Let π be the frequency distribution\r\n(the empirical probability distribution) of the projection on coordinate i of all points in the\r\ndatabase. To simplify expressions, let π(x : y) = Py\r\nj=x\r\nπ(j).\r\nIf x is chosen as a separator at the root, the entropy of the near neighbor in the two child\r\nnodes is reduced by:\r\nHN (λ) − HN (`) = log2\r\n|Nλ|\r\n|N`|\r\n= log2\r\n|D|\r\n|D ∩ [−∞, x + 1]i|\r\n= log2\r\n1\r\nπ(−∞ : x + 1)\r\nHN (λ) − HN (r) = log2\r\n1\r\nπ(x − 1 : ∞)\r\nRemember that we have not yet defined µ, the assumed probability distribution on the\r\nquery. From the point of view of the root, it only matters what probability µ assigns to Q` and\r\nQr. Let us reason, heuristically, about what assignments are needed for these probabilities\r\nin order to generate difficult problem instances. If we understand the most difficult instance,\r\nwe can use that setting of probabilities to obtain an upper bound for all instances.\r\nFirst, it seems that in a hard instance, the query needs to be close to some database point\r\n(at least with decent probability). Let us simply assume that the query is always planted in\r\nthe neighborhood of a database point; the problem remains to find this near neighbor.\r\nAssume by symmetry that HN (`) ≥ HN (r), i.e. the right side is smaller. Under our\r\nheuristic assumption that the query is planted next to a random database point, we can\r\n105",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/703a61a7-3589-4ffa-81e9-34bb36563f38.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=218b5c2fd3959fd2dbee2e726979668c37002635e2f733cd2de9bd1398bc0d35",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 572
      },
      {
        "segments": [
          {
            "segment_id": "703a61a7-3589-4ffa-81e9-34bb36563f38",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 105,
            "page_width": 612,
            "page_height": 792,
            "content": "some distribution µ. The reader who enjoys worst-case algorithms need not worry: µ is just\r\nan analysis gimmick, and the algorithm will be deterministic.\r\nWe can easily bound the tree size in terms of the measure of the smallest Qv ever reached:\r\nthere can be at most 1/ minv Prµ[Qv] distinct leaves in the decision tree, since different\r\nleaves are reached for disjoint Qv’s. Let IQ(v) = log2\r\n1\r\nPrµ[Qv]\r\n; this can be understood as the\r\ninformation learned about the query, when computation reaches node v. We can now rewrite\r\nthe space bound as O\r\n\r\n2\r\nmaxv IQ(v)\r\n\u0001\r\n.\r\nAnother quantity that can track the behavior of the decision tree is HN (v) = log2|Nv|.\r\nEssentially, this is the “entropy” of the identity of the near neighbor, assuming that all\r\ndatabase points are equally likely neighbors.\r\nAt the root λ, we have IQ(λ) = 0 and HN (λ) = lg n. Decision nodes must reduce the\r\nentropy of the near neighbor until HN reaches zero (|Nv| = 1). Then, the algorithm can\r\nsimply read the single remaining candidate, and test whether it is a near neighbor of the\r\nquery. Unfortunately, decision nodes also increase IQ along the way, increasing the space\r\nbound. The key to the algorithm is to balance this tension between reducing the entropy of\r\nthe answer, HD, and not increasing the information about the query, IQ, too much.\r\nIn this information-theoretic view, Indyk’s algorithm shows that we can (essentially)\r\nalways find a separator that decreases HN by some δ but does not increase IQ by more than\r\nρ · δ. Thus, HD can be pushed from lg n down to 0, without ever increasing IQ by more than\r\nρ lg n. That is, space O(n\r\nρ\r\n) is achieved.\r\nSearching for separators. At the root λ, we let i ∈ [d] be an arbitrary coordinate,\r\nand search for a good separator x on that coordinate. Let π be the frequency distribution\r\n(the empirical probability distribution) of the projection on coordinate i of all points in the\r\ndatabase. To simplify expressions, let π(x : y) = Py\r\nj=x\r\nπ(j).\r\nIf x is chosen as a separator at the root, the entropy of the near neighbor in the two child\r\nnodes is reduced by:\r\nHN (λ) − HN (`) = log2\r\n|Nλ|\r\n|N`|\r\n= log2\r\n|D|\r\n|D ∩ [−∞, x + 1]i|\r\n= log2\r\n1\r\nπ(−∞ : x + 1)\r\nHN (λ) − HN (r) = log2\r\n1\r\nπ(x − 1 : ∞)\r\nRemember that we have not yet defined µ, the assumed probability distribution on the\r\nquery. From the point of view of the root, it only matters what probability µ assigns to Q` and\r\nQr. Let us reason, heuristically, about what assignments are needed for these probabilities\r\nin order to generate difficult problem instances. If we understand the most difficult instance,\r\nwe can use that setting of probabilities to obtain an upper bound for all instances.\r\nFirst, it seems that in a hard instance, the query needs to be close to some database point\r\n(at least with decent probability). Let us simply assume that the query is always planted in\r\nthe neighborhood of a database point; the problem remains to find this near neighbor.\r\nAssume by symmetry that HN (`) ≥ HN (r), i.e. the right side is smaller. Under our\r\nheuristic assumption that the query is planted next to a random database point, we can\r\n105",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/703a61a7-3589-4ffa-81e9-34bb36563f38.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=218b5c2fd3959fd2dbee2e726979668c37002635e2f733cd2de9bd1398bc0d35",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 572
      },
      {
        "segments": [
          {
            "segment_id": "45430f5e-648d-47d3-b292-89161943307c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 106,
            "page_width": 612,
            "page_height": 792,
            "content": "lower bound Prµ[Qr] ≥ π(x + 1,∞). Indeed, whenever the query is planted next to a point\r\nin [x+1,∞]i, it cannot escape from Qr = [x, ∞]i. Remember that our space guarantee blows\r\nup when the information about Qv increases quickly (i.e. the probability of Qv decreases).\r\nThus, the worst case seems to be when Prµ[Qr] is as low as possible, namely equal to the\r\nlower bound.\r\nTo summarize, we have convinced ourselves that it’s reasonable to define µ such that:\r\nPr\r\nµ\r\n[Q`] = π(−∞ : x + 1); Pr\r\nµ\r\n[Qr] = π(x + 1,∞) (8.1)\r\nWe apply similar conditions at all nodes of the decision tree. Note that there exists a µ\r\nsatisfying all these conditions: the space of queries is partitioned recursively between the left\r\nand right subtrees, so defining the probability of the left and right subspace at all nodes is\r\nan (incomplete) definition of µ.\r\nFrom (8.1), we can compute the information revealed about the query:\r\nIQ(`) − IQ(λ) = log2\r\nPr[Qλ]\r\nPr[Q`]\r\n= log2\r\n1\r\nπ(−∞ : x + 1)\r\nIQ(r) − IQ(λ) = log2\r\n1\r\nπ(x + 1 : ∞)\r\nRemember that our rule for a good separator was “∆IQ ≤ ρ · ∆HN .” On the left side,\r\nIQ(`) − IQ(λ) = HN (λ) − HN (`), so the rule is trivially satisfied. On the right, the rule\r\nasks that: log2\r\n1\r\nπ(x+1:∞) ≤ ρ · log2\r\n1\r\nπ(x−1:∞)\r\n. Thus, x is a good separator iff π(x + 1 : ∞) ≥\r\n\u0002\r\nπ(x − 1 : ∞)\r\n\u0003ρ\r\n.\r\nFinale. As defined above, any good separator satisfies the bound on the information\r\nprogress, and guarantees the desired space bound of O(n\r\nρ\r\n). We now ask what happens\r\nwhen no good separator exists.\r\nWe may assume by translation that the median of π is 0, so π([1 : ∞]) ≤\r\n1\r\n2\r\n. If x = 1 1\r\n2\r\nis\r\nnot a good separator, it means that π(3 : ∞) <\r\n\u0002\r\nπ(1 : ∞)\r\n\u0003ρ\r\n≤ 2\r\n−ρ\r\n. If x = 3 1\r\n2\r\nis not a good\r\nseparator, then π(5 : ∞) <\r\n\u0002\r\nπ(3 : ∞)\r\n\u0003ρ\r\n≤ 2\r\n−ρ\r\n2\r\n. By induction, the lack of a good separator\r\nimplies that π(2j + 1 : ∞) < 2\r\n−ρ\r\nj\r\n. The reasoning works symmetrically to negative values,\r\nso π(−∞ : −2j − 1) < 2\r\n−ρ\r\nj\r\n.\r\nThus, if no good separator exists on coordinate i, the distribution of the values on that\r\ncoordinate is very concentrated around the median. In particular, only a fraction of 1\r\n2d\r\nof\r\nthe database points can have |xi| > R = 2 logρlog2 4d. Since there is no good separator on\r\nany coordinate, it follows that less than d ·\r\nn\r\n2d =\r\nn\r\n2\r\npoints have some coordinate exceeding\r\nR. Let D? be the set of such database points.\r\nTo handle the case when no good separator exists, we can introduce a different type of\r\nnode in the decision tree. This node tests whether the query lies in an `∞ ball of radius\r\nR + 1 (which is equivalent to d coordinate comparisons). If it does, the decision tree simply\r\noutputs any point in D \\ D?. Such a point must be within distance 2R + 1 of the query, so\r\n106",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/45430f5e-648d-47d3-b292-89161943307c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3712545f855e94630771cd5c132dead2c5a040b9dbfc724d866623a146793014",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 558
      },
      {
        "segments": [
          {
            "segment_id": "45430f5e-648d-47d3-b292-89161943307c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 106,
            "page_width": 612,
            "page_height": 792,
            "content": "lower bound Prµ[Qr] ≥ π(x + 1,∞). Indeed, whenever the query is planted next to a point\r\nin [x+1,∞]i, it cannot escape from Qr = [x, ∞]i. Remember that our space guarantee blows\r\nup when the information about Qv increases quickly (i.e. the probability of Qv decreases).\r\nThus, the worst case seems to be when Prµ[Qr] is as low as possible, namely equal to the\r\nlower bound.\r\nTo summarize, we have convinced ourselves that it’s reasonable to define µ such that:\r\nPr\r\nµ\r\n[Q`] = π(−∞ : x + 1); Pr\r\nµ\r\n[Qr] = π(x + 1,∞) (8.1)\r\nWe apply similar conditions at all nodes of the decision tree. Note that there exists a µ\r\nsatisfying all these conditions: the space of queries is partitioned recursively between the left\r\nand right subtrees, so defining the probability of the left and right subspace at all nodes is\r\nan (incomplete) definition of µ.\r\nFrom (8.1), we can compute the information revealed about the query:\r\nIQ(`) − IQ(λ) = log2\r\nPr[Qλ]\r\nPr[Q`]\r\n= log2\r\n1\r\nπ(−∞ : x + 1)\r\nIQ(r) − IQ(λ) = log2\r\n1\r\nπ(x + 1 : ∞)\r\nRemember that our rule for a good separator was “∆IQ ≤ ρ · ∆HN .” On the left side,\r\nIQ(`) − IQ(λ) = HN (λ) − HN (`), so the rule is trivially satisfied. On the right, the rule\r\nasks that: log2\r\n1\r\nπ(x+1:∞) ≤ ρ · log2\r\n1\r\nπ(x−1:∞)\r\n. Thus, x is a good separator iff π(x + 1 : ∞) ≥\r\n\u0002\r\nπ(x − 1 : ∞)\r\n\u0003ρ\r\n.\r\nFinale. As defined above, any good separator satisfies the bound on the information\r\nprogress, and guarantees the desired space bound of O(n\r\nρ\r\n). We now ask what happens\r\nwhen no good separator exists.\r\nWe may assume by translation that the median of π is 0, so π([1 : ∞]) ≤\r\n1\r\n2\r\n. If x = 1 1\r\n2\r\nis\r\nnot a good separator, it means that π(3 : ∞) <\r\n\u0002\r\nπ(1 : ∞)\r\n\u0003ρ\r\n≤ 2\r\n−ρ\r\n. If x = 3 1\r\n2\r\nis not a good\r\nseparator, then π(5 : ∞) <\r\n\u0002\r\nπ(3 : ∞)\r\n\u0003ρ\r\n≤ 2\r\n−ρ\r\n2\r\n. By induction, the lack of a good separator\r\nimplies that π(2j + 1 : ∞) < 2\r\n−ρ\r\nj\r\n. The reasoning works symmetrically to negative values,\r\nso π(−∞ : −2j − 1) < 2\r\n−ρ\r\nj\r\n.\r\nThus, if no good separator exists on coordinate i, the distribution of the values on that\r\ncoordinate is very concentrated around the median. In particular, only a fraction of 1\r\n2d\r\nof\r\nthe database points can have |xi| > R = 2 logρlog2 4d. Since there is no good separator on\r\nany coordinate, it follows that less than d ·\r\nn\r\n2d =\r\nn\r\n2\r\npoints have some coordinate exceeding\r\nR. Let D? be the set of such database points.\r\nTo handle the case when no good separator exists, we can introduce a different type of\r\nnode in the decision tree. This node tests whether the query lies in an `∞ ball of radius\r\nR + 1 (which is equivalent to d coordinate comparisons). If it does, the decision tree simply\r\noutputs any point in D \\ D?. Such a point must be within distance 2R + 1 of the query, so\r\n106",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/45430f5e-648d-47d3-b292-89161943307c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3712545f855e94630771cd5c132dead2c5a040b9dbfc724d866623a146793014",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 558
      },
      {
        "segments": [
          {
            "segment_id": "c93770a8-c82a-4216-91d3-4fde4070a431",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 107,
            "page_width": 612,
            "page_height": 792,
            "content": "it is an O(logρ\r\nlog d) approximation.\r\nIf the query is outside the ball of radius R+1, a near neighbor must be outside the ball of\r\nradius R, i.e. must be in D?. We continue with the recursive construction of a decision tree\r\nfor point set D?. Since |D?| ≤ |D|/2, we get a one-bit reduction in the entropy of the answer\r\nfor free. (Formally, our µ just assigns probability one to the query being outside the ball of\r\nradius R + 1, because in the “inside” case the query algorithm terminates immediately.)\r\n8.2 Lower Bound\r\nArmed with this information-theoretic understanding of Indyk’s algorithm, the path to a\r\nlower bound is more intuitive. We will define a distribution on coordinates decaying roughly\r\nlike 2−ρ\r\nx\r\n, since we know that more probability in the tail gives the algorithm an advantage.\r\nDatabase points will be independent and identically distributed, with each coordinate drawn\r\nindependently from this distribution.\r\nIn the communication view, Alice’s message sends a certain amount of information re\u0002stricting the query space to some Q. The entropy of the answer is given by the measure of\r\nN(Q) = Q + [−1, 1]d, since the expected number of points in this space is just n · Pr[N(Q)].\r\nThe question that must be answered is: fixing Pr[Q], how small can Pr[N(Q)\r\n\u0003\r\nbe?\r\nWe will show an isoperimetric inequality proving that the least expanding sets are exactly\r\nthe ones generated by Indyk’s algorithm: intersections of coordinate cuts [x, ∞]i. Note that\r\nPr \u0002[x, ∞]i\r\n\u0003\r\n≈ 2\r\n−ρ\r\nx\r\n, and N\r\n\r\n[x, ∞]i\r\n\u0001\r\n= [x − 1,∞]i. Thus, the set expands to measure\r\nPr \u0002x − 1, ∞]i\r\n\u0003\r\n≈ 2\r\n−ρ\r\nx−1\r\n≈ Pr \u0002[x, ∞]i\r\n\u00031/ρ. Our isoperimetric inequality will show that for\r\nany Q, its neighborhood has measure Pr[N(Q)] ≥ Pr[Q]\r\n1/ρ\r\n.\r\nThen, if Alice’s message has o(ρ lg n) bits of information, the entropy of the near neighbor\r\ndecreases by only o(lg n) bits. In other words, n\r\n1−o(1) of the points are still candidate near\r\nneighbors, and we can use this to lower bound the message that Bob must send.\r\nThe crux of the lower bound is not the analysis of the communication protocol (which is\r\nstandard), but proving the isoperimetric inequality. Of course, the key to the isopermitetric\r\ninequality is the initial conceptual step of defining an appropriate biased distribution, in\r\nwhich the right inequality is possible. The proof is rather non-standard for an isoperimetric\r\ninequality, because we are dealing with a very particular measure on a very particular space.\r\nFortunately, a few mathematical tricks save it from being too technical.\r\nFormal details. We denote the communication problem, c-approximate NNS, by the\r\npartial function F. Let the domain of F¯ be X × Y , where X = {0, 1, . . . m}\r\nd and Y = \r\n{0, 1, . . . m}\r\nd\r\n\u0001n\r\n. Complete the function F by setting F¯(q, D) = F(q, D) whenever F(q, D)\r\nis defined (in the “0” or “1” instances), and F¯(q, D) = ? otherwise.\r\nAs explained already, our lower bound only applies to deterministic protocols, but it\r\nrequires conceptual use of distributions on the input domains X and Y . First define a\r\nmeasure π over the set {0, 1, . . . m}: for i ≥ 1, let π({i}) = πi = 2−(2ρ)\r\ni\r\n; and let π0 =\r\n1 −\r\nP\r\ni≥1\r\nπi ≥\r\n1\r\n2\r\n. Here ρ is a parameter to be determined.\r\n107",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/c93770a8-c82a-4216-91d3-4fde4070a431.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2920efd7993991c132c0948a09053cafd7295d1f456b1134b2faeb2da9c78017",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 586
      },
      {
        "segments": [
          {
            "segment_id": "c93770a8-c82a-4216-91d3-4fde4070a431",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 107,
            "page_width": 612,
            "page_height": 792,
            "content": "it is an O(logρ\r\nlog d) approximation.\r\nIf the query is outside the ball of radius R+1, a near neighbor must be outside the ball of\r\nradius R, i.e. must be in D?. We continue with the recursive construction of a decision tree\r\nfor point set D?. Since |D?| ≤ |D|/2, we get a one-bit reduction in the entropy of the answer\r\nfor free. (Formally, our µ just assigns probability one to the query being outside the ball of\r\nradius R + 1, because in the “inside” case the query algorithm terminates immediately.)\r\n8.2 Lower Bound\r\nArmed with this information-theoretic understanding of Indyk’s algorithm, the path to a\r\nlower bound is more intuitive. We will define a distribution on coordinates decaying roughly\r\nlike 2−ρ\r\nx\r\n, since we know that more probability in the tail gives the algorithm an advantage.\r\nDatabase points will be independent and identically distributed, with each coordinate drawn\r\nindependently from this distribution.\r\nIn the communication view, Alice’s message sends a certain amount of information re\u0002stricting the query space to some Q. The entropy of the answer is given by the measure of\r\nN(Q) = Q + [−1, 1]d, since the expected number of points in this space is just n · Pr[N(Q)].\r\nThe question that must be answered is: fixing Pr[Q], how small can Pr[N(Q)\r\n\u0003\r\nbe?\r\nWe will show an isoperimetric inequality proving that the least expanding sets are exactly\r\nthe ones generated by Indyk’s algorithm: intersections of coordinate cuts [x, ∞]i. Note that\r\nPr \u0002[x, ∞]i\r\n\u0003\r\n≈ 2\r\n−ρ\r\nx\r\n, and N\r\n\r\n[x, ∞]i\r\n\u0001\r\n= [x − 1,∞]i. Thus, the set expands to measure\r\nPr \u0002x − 1, ∞]i\r\n\u0003\r\n≈ 2\r\n−ρ\r\nx−1\r\n≈ Pr \u0002[x, ∞]i\r\n\u00031/ρ. Our isoperimetric inequality will show that for\r\nany Q, its neighborhood has measure Pr[N(Q)] ≥ Pr[Q]\r\n1/ρ\r\n.\r\nThen, if Alice’s message has o(ρ lg n) bits of information, the entropy of the near neighbor\r\ndecreases by only o(lg n) bits. In other words, n\r\n1−o(1) of the points are still candidate near\r\nneighbors, and we can use this to lower bound the message that Bob must send.\r\nThe crux of the lower bound is not the analysis of the communication protocol (which is\r\nstandard), but proving the isoperimetric inequality. Of course, the key to the isopermitetric\r\ninequality is the initial conceptual step of defining an appropriate biased distribution, in\r\nwhich the right inequality is possible. The proof is rather non-standard for an isoperimetric\r\ninequality, because we are dealing with a very particular measure on a very particular space.\r\nFortunately, a few mathematical tricks save it from being too technical.\r\nFormal details. We denote the communication problem, c-approximate NNS, by the\r\npartial function F. Let the domain of F¯ be X × Y , where X = {0, 1, . . . m}\r\nd and Y = \r\n{0, 1, . . . m}\r\nd\r\n\u0001n\r\n. Complete the function F by setting F¯(q, D) = F(q, D) whenever F(q, D)\r\nis defined (in the “0” or “1” instances), and F¯(q, D) = ? otherwise.\r\nAs explained already, our lower bound only applies to deterministic protocols, but it\r\nrequires conceptual use of distributions on the input domains X and Y . First define a\r\nmeasure π over the set {0, 1, . . . m}: for i ≥ 1, let π({i}) = πi = 2−(2ρ)\r\ni\r\n; and let π0 =\r\n1 −\r\nP\r\ni≥1\r\nπi ≥\r\n1\r\n2\r\n. Here ρ is a parameter to be determined.\r\n107",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/c93770a8-c82a-4216-91d3-4fde4070a431.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2920efd7993991c132c0948a09053cafd7295d1f456b1134b2faeb2da9c78017",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 586
      },
      {
        "segments": [
          {
            "segment_id": "097adb52-9d95-49a7-96cf-8497285645f9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 108,
            "page_width": 612,
            "page_height": 792,
            "content": "Now, define a measure µ over points by generating each coordinate independently ac\u0002cording to π: µ(x1, x2, . . . , xd)}) = πx1\r\n· · · πxd\r\n. Finally, define a measure η over the database\r\nby generating each point independently according to µ.\r\nFirst, we show that F¯ is zero with probability Ω(1):\r\nClaim 8.4. If d ≥ lg1+ε n and ρ ≤\r\n1\r\n2\r\n(\r\nε\r\n2\r\nlg d)\r\n1/c, then Prq←µ,D←η[F¯(q, D) 6= 0] ≤\r\n1\r\n2\r\n.\r\nProof. Consider q and some p ∈ D: their jth coordinates differ by c or more with probability\r\nat least 2π0πc ≥ πc. Thus,\r\nPr[kq−pk∞ < c] ≤ (1−πc)\r\nd ≤ e−πcd ≤ e−2−(ε/2) lg d·d ≤ e−d\r\n1−ε/2\r\n≤ e\r\n−(lg n)\r\n(1+ε)(1−ε/2)\r\n≤ e\r\n−(lg n)\r\n1+ε/4\r\nBy a union bound over all p ∈ D, we get that q has no neighbor at distance less than c with\r\nprobability at least 1 − n · exp(−(lg n)\r\n1+ε/4\r\n) = 1 − o(1).\r\nClaim 8.5. If Alice sends a bits and Bob sends b bits, there exists a combinatorial rectangle\r\nQ × D ⊆ X × Y of measure µ(Q) ≥ 2\r\n−O(a) and η(D) ≥ 2−O(a+b)\r\n, on which F¯ only takes\r\nvalues in {0, ?}.\r\nProof. This is just the deterministic richness lemma (Lemma 5.4) in disguise. Let F\r\n0\r\n:\r\nX × Y → {0, 1} be the output of the protocol. We have F\r\n0\r\n(q, D) = F¯(q, D) whenever\r\nF¯(q, D) 6= ?. Since Pr[F\r\n0\r\n(q, D) = 0] ≥\r\n1\r\n2\r\n, F\r\n0\r\nis rich: half of the columns are at least half\r\nzero (in the weighted sense). Thus, we can find a rectangle of size µ(Q) ≥ 2\r\n−O(a) and\r\nη(D) ≥ 2\r\n−O(a+b)\r\n, on which F\r\n0\r\nis identically zero. Since the protocol is always correct, this\r\nmeans that F¯ is 0 or ? on the rectangle.\r\nTo obtain a lower bound, we show that any big rectangle must contain at least a value\r\nof “1.” This will follow from the following isoperimetric inequality in our measure space,\r\nshown in §8.3:\r\nTheorem 8.6. Consider any set S ⊆ {0, 1, . . . m}\r\nd\r\n, and let N(S) be the set of points at\r\ndistance at most 1 from S under `∞: N(S) = {p | ∃s ∈ S : kp − sk∞ ≤ 1}. Then,\r\nµ(N(S)) ≥\r\n\r\nµ(S)\r\n\u00011/ρ\r\n.\r\nClaim 8.7. Consider any rectangle Q × D ⊆ X × Y of size µ(Q) ≥ 2\r\n−δρ lg n and η(D) ≥\r\n2\r\n−O(n\r\n1−δ\r\n)\r\n. Then, there exists some (q, D) ∈ Q × D such that F¯(q, D) = 1.\r\nProof. By isoperimetry, µ(N(Q)) ≥\r\n\r\nµd(Q)\r\n\u00011/ρ ≥ 1/nδ\r\n. All we need to show is that there\r\nexists a set D ∈ D that intersects with N(Q).\r\nFor D ∈ Y , let σ(D) = |D ∩ N(Q)|. The proof uses a standard concentration trick on\r\nσ. Suppose D is chosen randomly according to η, i.e. not restricted to D. Then E[σ(D)] =\r\nn · Prµ[N(Q)] ≥ n\r\n1−δ\r\n. Furthermore, σ(D) is tightly concentrated around this mean, by the\r\nChernoff bound. In particular, Pr[σ(D) = 0] ≤ e\r\n−Ω(n\r\n1−δ\r\n)\r\n.\r\nThis probability is so small, that it remains small even is we restrict to D. We have\r\nPr[σ(D) = 0 | D ∈ D] ≤\r\nPr[σ(D)=0]\r\nPr[D∈D] ≤ e\r\n−Ω(n\r\n1−δ\r\n)/η(D). Thus, if η(D) ≥ 2−γ·n\r\n1−δ\r\nfor some\r\n108",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/097adb52-9d95-49a7-96cf-8497285645f9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c41ac1c6a64a60a140dc9feef7be8d483b792c64ae55ea79bac40da7d79ff47e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 588
      },
      {
        "segments": [
          {
            "segment_id": "097adb52-9d95-49a7-96cf-8497285645f9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 108,
            "page_width": 612,
            "page_height": 792,
            "content": "Now, define a measure µ over points by generating each coordinate independently ac\u0002cording to π: µ(x1, x2, . . . , xd)}) = πx1\r\n· · · πxd\r\n. Finally, define a measure η over the database\r\nby generating each point independently according to µ.\r\nFirst, we show that F¯ is zero with probability Ω(1):\r\nClaim 8.4. If d ≥ lg1+ε n and ρ ≤\r\n1\r\n2\r\n(\r\nε\r\n2\r\nlg d)\r\n1/c, then Prq←µ,D←η[F¯(q, D) 6= 0] ≤\r\n1\r\n2\r\n.\r\nProof. Consider q and some p ∈ D: their jth coordinates differ by c or more with probability\r\nat least 2π0πc ≥ πc. Thus,\r\nPr[kq−pk∞ < c] ≤ (1−πc)\r\nd ≤ e−πcd ≤ e−2−(ε/2) lg d·d ≤ e−d\r\n1−ε/2\r\n≤ e\r\n−(lg n)\r\n(1+ε)(1−ε/2)\r\n≤ e\r\n−(lg n)\r\n1+ε/4\r\nBy a union bound over all p ∈ D, we get that q has no neighbor at distance less than c with\r\nprobability at least 1 − n · exp(−(lg n)\r\n1+ε/4\r\n) = 1 − o(1).\r\nClaim 8.5. If Alice sends a bits and Bob sends b bits, there exists a combinatorial rectangle\r\nQ × D ⊆ X × Y of measure µ(Q) ≥ 2\r\n−O(a) and η(D) ≥ 2−O(a+b)\r\n, on which F¯ only takes\r\nvalues in {0, ?}.\r\nProof. This is just the deterministic richness lemma (Lemma 5.4) in disguise. Let F\r\n0\r\n:\r\nX × Y → {0, 1} be the output of the protocol. We have F\r\n0\r\n(q, D) = F¯(q, D) whenever\r\nF¯(q, D) 6= ?. Since Pr[F\r\n0\r\n(q, D) = 0] ≥\r\n1\r\n2\r\n, F\r\n0\r\nis rich: half of the columns are at least half\r\nzero (in the weighted sense). Thus, we can find a rectangle of size µ(Q) ≥ 2\r\n−O(a) and\r\nη(D) ≥ 2\r\n−O(a+b)\r\n, on which F\r\n0\r\nis identically zero. Since the protocol is always correct, this\r\nmeans that F¯ is 0 or ? on the rectangle.\r\nTo obtain a lower bound, we show that any big rectangle must contain at least a value\r\nof “1.” This will follow from the following isoperimetric inequality in our measure space,\r\nshown in §8.3:\r\nTheorem 8.6. Consider any set S ⊆ {0, 1, . . . m}\r\nd\r\n, and let N(S) be the set of points at\r\ndistance at most 1 from S under `∞: N(S) = {p | ∃s ∈ S : kp − sk∞ ≤ 1}. Then,\r\nµ(N(S)) ≥\r\n\r\nµ(S)\r\n\u00011/ρ\r\n.\r\nClaim 8.7. Consider any rectangle Q × D ⊆ X × Y of size µ(Q) ≥ 2\r\n−δρ lg n and η(D) ≥\r\n2\r\n−O(n\r\n1−δ\r\n)\r\n. Then, there exists some (q, D) ∈ Q × D such that F¯(q, D) = 1.\r\nProof. By isoperimetry, µ(N(Q)) ≥\r\n\r\nµd(Q)\r\n\u00011/ρ ≥ 1/nδ\r\n. All we need to show is that there\r\nexists a set D ∈ D that intersects with N(Q).\r\nFor D ∈ Y , let σ(D) = |D ∩ N(Q)|. The proof uses a standard concentration trick on\r\nσ. Suppose D is chosen randomly according to η, i.e. not restricted to D. Then E[σ(D)] =\r\nn · Prµ[N(Q)] ≥ n\r\n1−δ\r\n. Furthermore, σ(D) is tightly concentrated around this mean, by the\r\nChernoff bound. In particular, Pr[σ(D) = 0] ≤ e\r\n−Ω(n\r\n1−δ\r\n)\r\n.\r\nThis probability is so small, that it remains small even is we restrict to D. We have\r\nPr[σ(D) = 0 | D ∈ D] ≤\r\nPr[σ(D)=0]\r\nPr[D∈D] ≤ e\r\n−Ω(n\r\n1−δ\r\n)/η(D). Thus, if η(D) ≥ 2−γ·n\r\n1−δ\r\nfor some\r\n108",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/097adb52-9d95-49a7-96cf-8497285645f9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c41ac1c6a64a60a140dc9feef7be8d483b792c64ae55ea79bac40da7d79ff47e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 588
      },
      {
        "segments": [
          {
            "segment_id": "727fae68-3230-4779-aef4-df56825dea95",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 109,
            "page_width": 612,
            "page_height": 792,
            "content": "small enough constant γ, we have Pr[σ(D) = 0 | D ∈ D] = o(1). In other words, there exists\r\nsome D ∈ D such that N(Q) ∩ D 6= ∅, and thus, there exists an instance in the rectangle on\r\nwhich F¯ = 1.\r\nCombining Claims 8.5 and 8.7, we immediately conclude that either Alice sends a =\r\nΩ(δρ lg n) bits or Bob sends b = Ω(n\r\n1−δ\r\n) bits. This concludes the proof of Theorem 8.1.\r\n8.3 An Isoperimetric Inequality\r\nThis section proves the inequality of Theorem 8.6: for any S, µ(N(S)) ≥\r\n\r\nµ(S)\r\n\u00011/ρ. As with\r\nmost isoperimetric inequalities, the proof is by induction on the dimensions. In our case, the\r\ninductive step is provided by the following inequality, whose proof is deferred to §8.4:\r\nLemma 8.8. Let ρ ≥ 10 be an integer, and define πi = 2−(2ρ)\r\ni\r\nfor all i ∈ {1, . . . , m}, and\r\nπ0 = 1−\r\nPm\r\ni=1 πi. For any β0, . . . , βm ∈ R+ satisfying Pmi=0 πiβ\r\nρ\r\ni = 1, the following inequality\r\nholds (where we interpret β−1 and βm+1 as zero):\r\nXm\r\ni=0\r\nπi· max {βi−1, βi, βi+1} ≥ 1 (8.2)\r\nTo proceed to our inductive proof, let µd be the d-dimensional variant of our distribution.\r\nThe base case is d = 0. This space has exactly one point, and µ0(S) is either 0 or 1. We\r\nhave N(S) = S, so µ0(N(S)) = µ0(S) = µ0(S)\r\n\u00011/ρ\r\n.\r\nNow consider the induction step for d−1 to d dimensions. Given a set S ⊂ {0, 1, . . . m}\r\nd\r\n,\r\nlet S[i] be the set of points in S whose first coordinate is i, i.e. S[i] = {(s2, . . . , sd) |\r\n(i, s2, . . . , sd) ∈ S}. Define:\r\nβi =\r\n\u0012\r\nµd−1(S[i])\r\nµd(S)\r\n\u00131/ρ\r\n⇒\r\nXm\r\ni=0\r\nπiβ\r\nρ\r\ni =\r\nXm\r\ni=0\r\nπi·\r\nµd−1(S[i])\r\nµd(S)\r\n=\r\n1\r\nµd(S)\r\nXm\r\ni=0\r\nπiµd−1(S[i]) = 1\r\nWe have N(S)[i] = N(S[i−1]) ∪ N(S[i]) ∪ N(S[i+1]). Thus, we can lower bound:\r\nµd(N(S)) = Xm\r\ni=0\r\nπi·µd−1\r\n\r\nN(S)[i]\r\n\u0001\r\n≥\r\nXm\r\ni=0\r\nπi·max \bµd−1(N(S[i−1])), µd−1(N(S[i])), µd−1(N(S[i+1]))\t\r\nBut the inductive hypothesis assures us that µd−1(N(S[i])) ≥\r\n\r\nµd−1(S[i])\r\n\u00011/ρ\r\n= βi·\r\n\r\nµd(S)\r\n\u00011/ρ. Thus:\r\nµd(N(S)) ≥\r\n\r\nµd(S)\r\n\u00011/ρ\r\n·\r\nXm\r\ni=0\r\nπi· max \bβi−1, βi, βi+1\t≥\r\n\r\nµd(S)\r\n\u00011/ρ\r\n,\r\nwhere we have used inequality (8.2) in the last step. This finishes the proof of Theorem 8.6.\r\n109",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/727fae68-3230-4779-aef4-df56825dea95.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cc92b367a445c8dfa23274db734f84f961c78552cf6c02e3b38dda49f42a76ea",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 409
      },
      {
        "segments": [
          {
            "segment_id": "af0972c0-3d94-49e9-af82-e6da9c97f286",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 110,
            "page_width": 612,
            "page_height": 792,
            "content": "8.4 Expansion in One Dimension\r\nLet Γ = \b(β0, . . . , βm) ∈ R\r\nm+1 |\r\nPm\r\ni=0 πiβ\r\nρ\r\ni = 1\t\r\n, and denote by f (β0, . . . , βm) the left hand\r\nside of (8.2). Since f is a continuous function on the compact set Γ ⊂ R\r\nm+1, it achieves its\r\nminimum. Call an (m + 1)-tuple (β0, . . . , βm) ∈ Γ optimal if f (β0, . . . , βm) is minimal. Our\r\nproof strategy will be to show that if (β0, . . . , βm) is optimal, then βi = 1.\r\nWe consider several possible configurations for sizes of βi’s in an optimal β, and rule\r\nthem out in three separate lemmas. We then prove the inequality by showing that these\r\nconfigurations are all the configurations that we need to consider.\r\nLemma 8.9. If there exists an index i ∈ {1, . . . , m − 1} such that βi−1 > βi < βi+1, then\r\nβ¯ = (β0, . . . , βm) is not optimal.\r\nProof. Define a new vector β¯0 = (β0, . . . , βi−2, βi−1 − \u000f, βi + δ, βi+1 − \u000f, βi+2, . . . , βm), where\r\n\u000f, δ > 0 are chosen suitably so that β¯0 ∈ Γ, and βi−1 − \u000f > βi + δ < βi+1 − \u000f. It’s easy to see\r\nthat f\r\n\r\nβ¯\r\n\u0001\r\n> f \r\nβ¯0\r\n\u0001\r\n, which contradicts the optimality of β¯.\r\nLemma 8.10. If there exists an index i ∈ {1, . . . , m} such that βi−1 > βi ≥ βi+1, then\r\nβ¯ =(β0, . . . , βm) is not optimal.\r\nProof. Let β =\r\n\u0010\r\nπi−1β\r\nρ\r\ni−1+πiβ\r\nρ\r\ni\r\nπi−1+πi\r\n\u00111/ρ\r\nand define β¯0 = (β0, . . . , βi−2, β, β, βi+1, . . . βm). Then\r\nβ¯0 ∈ Γ, and βi−1 > β > βi.\r\nWe claim that f(β¯) > f(β¯0). Comparing the expressions for f\r\n\r\nβ¯\r\n\u0001\r\nand f\r\n\r\nβ¯0\r\n\u0001\r\nterm by\r\nterm, we see that it’s enough to check that:\r\nπi max nβi−1, βi, βi+1o+ πi+1 max nβi, βi+1, βi+2o> πi max nβ, βi+1o+ πi+1 max nβ, βi+1, βi+2o\r\nwhere the terms involving πi+1 are ignored when i = m. For i = m, the inequality becomes\r\nβi−1 > β which holds by assumption. For i = 1, . . . , m − 1, this inequality is equivalent to:\r\nπi(βi−1 − β) > πi+1 · (max {β, βi+2} − max {βi, βi+2})\r\nwhich, in its strongest form (when βi ≥ βi+2), is equivalent to πi(βi−1 − β) > πi+1(β − βi).\r\nBut this is equivalent to:\r\n\u0012\r\nπiβi−1 + πi+1βi\r\nπi + πi+1 \u0013ρ\r\n>\r\nπi−1β\r\nρ\r\ni−1 + πiβ\r\nρ\r\ni\r\nπi−1 + πi\r\nwhich we can rewrite as:\r\n\u0012\r\nci + t\r\nci + 1\u0013ρ\r\n−\r\nci−1 + t\r\nρ\r\nci−1 + 1\r\n> 0, (8.3)\r\nletting t =\r\nβi\r\nβi−1\r\n∈ [0, 1), and ci =\r\nπi\r\nπi+1\r\n≥ 2\r\n(2ρ)\r\ni+1−(2ρ)i\r\n(for i > 0 this is an equality; only for\r\ni = 0 is this a strict inequality, because p is large).\r\n110",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/af0972c0-3d94-49e9-af82-e6da9c97f286.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0c47926276890f399a729956382dba3748e17bb3e6a81c2268434aac5db46bb8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 532
      },
      {
        "segments": [
          {
            "segment_id": "af0972c0-3d94-49e9-af82-e6da9c97f286",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 110,
            "page_width": 612,
            "page_height": 792,
            "content": "8.4 Expansion in One Dimension\r\nLet Γ = \b(β0, . . . , βm) ∈ R\r\nm+1 |\r\nPm\r\ni=0 πiβ\r\nρ\r\ni = 1\t\r\n, and denote by f (β0, . . . , βm) the left hand\r\nside of (8.2). Since f is a continuous function on the compact set Γ ⊂ R\r\nm+1, it achieves its\r\nminimum. Call an (m + 1)-tuple (β0, . . . , βm) ∈ Γ optimal if f (β0, . . . , βm) is minimal. Our\r\nproof strategy will be to show that if (β0, . . . , βm) is optimal, then βi = 1.\r\nWe consider several possible configurations for sizes of βi’s in an optimal β, and rule\r\nthem out in three separate lemmas. We then prove the inequality by showing that these\r\nconfigurations are all the configurations that we need to consider.\r\nLemma 8.9. If there exists an index i ∈ {1, . . . , m − 1} such that βi−1 > βi < βi+1, then\r\nβ¯ = (β0, . . . , βm) is not optimal.\r\nProof. Define a new vector β¯0 = (β0, . . . , βi−2, βi−1 − \u000f, βi + δ, βi+1 − \u000f, βi+2, . . . , βm), where\r\n\u000f, δ > 0 are chosen suitably so that β¯0 ∈ Γ, and βi−1 − \u000f > βi + δ < βi+1 − \u000f. It’s easy to see\r\nthat f\r\n\r\nβ¯\r\n\u0001\r\n> f \r\nβ¯0\r\n\u0001\r\n, which contradicts the optimality of β¯.\r\nLemma 8.10. If there exists an index i ∈ {1, . . . , m} such that βi−1 > βi ≥ βi+1, then\r\nβ¯ =(β0, . . . , βm) is not optimal.\r\nProof. Let β =\r\n\u0010\r\nπi−1β\r\nρ\r\ni−1+πiβ\r\nρ\r\ni\r\nπi−1+πi\r\n\u00111/ρ\r\nand define β¯0 = (β0, . . . , βi−2, β, β, βi+1, . . . βm). Then\r\nβ¯0 ∈ Γ, and βi−1 > β > βi.\r\nWe claim that f(β¯) > f(β¯0). Comparing the expressions for f\r\n\r\nβ¯\r\n\u0001\r\nand f\r\n\r\nβ¯0\r\n\u0001\r\nterm by\r\nterm, we see that it’s enough to check that:\r\nπi max nβi−1, βi, βi+1o+ πi+1 max nβi, βi+1, βi+2o> πi max nβ, βi+1o+ πi+1 max nβ, βi+1, βi+2o\r\nwhere the terms involving πi+1 are ignored when i = m. For i = m, the inequality becomes\r\nβi−1 > β which holds by assumption. For i = 1, . . . , m − 1, this inequality is equivalent to:\r\nπi(βi−1 − β) > πi+1 · (max {β, βi+2} − max {βi, βi+2})\r\nwhich, in its strongest form (when βi ≥ βi+2), is equivalent to πi(βi−1 − β) > πi+1(β − βi).\r\nBut this is equivalent to:\r\n\u0012\r\nπiβi−1 + πi+1βi\r\nπi + πi+1 \u0013ρ\r\n>\r\nπi−1β\r\nρ\r\ni−1 + πiβ\r\nρ\r\ni\r\nπi−1 + πi\r\nwhich we can rewrite as:\r\n\u0012\r\nci + t\r\nci + 1\u0013ρ\r\n−\r\nci−1 + t\r\nρ\r\nci−1 + 1\r\n> 0, (8.3)\r\nletting t =\r\nβi\r\nβi−1\r\n∈ [0, 1), and ci =\r\nπi\r\nπi+1\r\n≥ 2\r\n(2ρ)\r\ni+1−(2ρ)i\r\n(for i > 0 this is an equality; only for\r\ni = 0 is this a strict inequality, because p is large).\r\n110",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/af0972c0-3d94-49e9-af82-e6da9c97f286.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0c47926276890f399a729956382dba3748e17bb3e6a81c2268434aac5db46bb8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 532
      },
      {
        "segments": [
          {
            "segment_id": "ffdaa9ed-5ca4-4ea6-bc0e-b212ed18d2d5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 111,
            "page_width": 612,
            "page_height": 792,
            "content": "We are now left to prove (8.3). Let F(t) denote the left hand side of this inequality, and\r\nnote that F(0) > 0, because:\r\n\u0012\r\nci\r\nci + 1\u0013ρ\r\n=\r\n\u0012\r\n1 −\r\n1\r\nci + 1\u0013ρ\r\n≥ 1 −\r\nρ\r\nci + 1\r\n> 1 −\r\n1\r\nci−1 + 1\r\n=\r\nci−1\r\nci−1 + 1\r\nHere we used Bernoulli’s inequality: (1 − x)\r\nn ≥ 1 − nx for 0 < x < 1/n. Then, we observed\r\nthat ci + 1 > 2\r\n(2ρ)\r\ni+1−(2ρ)i\r\n> ρ · (2(2ρ)\r\ni\r\n+ 1) = ρ(\r\n1\r\nπi−1\r\nci−1 + 1) > ρ(ci−1 + 1).\r\nNow we let t ∈ (0, 1) and write F(t) = F(0) + t\r\nρG(t), where:\r\nG(t) = 1\r\n(ci+1)ρ\r\n\u0010\r\nρ\r\n1\r\n\u0001\r\nc\r\nρ−1\r\ni\r\n1\r\nt +\r\n\r\nρ\r\n2\r\n\u0001\r\nc\r\nρ−2\r\ni\r\n1\r\nt\r\n2 + · · · +\r\n\r\nρ\r\nρ−1\r\n\u0001\r\nci\r\n1\r\nt\r\nρ−1\r\n\u0011\r\n+\r\n\u0010\r\n1\r\n(ci+1)ρ −\r\n1\r\nci−1+1\u0011\r\n.\r\nIf G(t) ≥ 0, then clearly F(t) ≥ F(0) > 0, so we are done. Otherwise, G(t) < 0, and in\r\nthis case it easily follows that G(1) < G(t) < 0, hence F(t) = F(0)+t\r\nρG(t) > F(0)+G(1) =\r\nF(1) = 0, as desired. This concludes the proof of the lemma.\r\nLemma 8.11. If there is an index i ∈ {0, 1 . . . , m − 1} such that βi−1 ≤ βi < βi+1, then\r\nβ = (β0, β1, . . . , βm) is not optimal.\r\nProof. We proceed as in the previous lemma. Let β =\r\n\u0010\r\nπiβ\r\nρ\r\ni +πi+1β\r\nρ\r\ni+1\r\nπi+πi+1 \u00111/ρ\r\n, and define β¯0 =\r\n(β0, . . . , βi−1, β, β, βi+2, . . . , βm). As before, β¯0 ∈ Γ and βi < β < βi+1. We claim that\r\nf(β¯) > f(β¯0). Comparing the expressions for f\r\n\r\nβ¯\r\n\u0001\r\nand f\r\n\r\nβ¯0\r\n\u0001\r\nterm by term, we see that it’s\r\nenough to check that\r\nπi−1·max {βi−2, βi−1, βi}+πi·max {βi−1, βi, βi+1} > πi−1·max {βi−2, βi−1, β}+πi·max {βi−1, β, β}\r\nwhere the terms involving πi−1 appear unless i = 0. If i = 0, the above inequality becomes\r\nβi+1 > β and we are done. For i = 1, . . . m − 1, the inequality is equivalent to\r\nπi(βi+1 − β) > πi−1 · (max {β, βi−2} − max {βi, βi−2})\r\nwhich, in its strongest form (when βi ≥ βi−2) is equivalent to πi(βi+1 − β) > πi−1(β − βi).\r\nThe latter inequality is equivalent to\r\n\u0012\r\nπiβi+1 + πi−1βi\r\nπi + πi−1\r\n\u0013ρ\r\n>\r\nπi+1β\r\nρ\r\ni+1 + πiβ\r\nρ\r\ni\r\nπi+1 + πi\r\nwhich we can rewrite as\r\n\u0012\r\nci−1t + 1\r\nci−1 + 1 \u0013ρ\r\n−\r\ncit\r\nρ + 1\r\nci + 1\r\n> 0, (8.4)\r\nwhere ci = πi/πi+1 as before, and t = βi/βi+1 ∈ [0, 1).\r\nWe are left to prove (8.4). Let F(t) denote the left hand side of this inequality, and note\r\n111",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/ffdaa9ed-5ca4-4ea6-bc0e-b212ed18d2d5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=079ccebb43b3c153e0ccde83eec04d6166ef37ead255265f08cdec584974df54",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 500
      },
      {
        "segments": [
          {
            "segment_id": "dc267219-4a70-4a6d-aa2a-83fb8384fa4f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 112,
            "page_width": 612,
            "page_height": 792,
            "content": "that F(0) > 0, because:\r\n\u0012\r\n1\r\nci−1 + 1\u0013ρ\r\n>\r\n1\r\n(2ci−1)\r\nρ\r\n=\r\n1\r\nπ\r\nρ\r\ni−1\r\n· 2\r\n−ρ·(2ρ)\r\ni−ρ > 2\r\n−ρ·(2ρ)\r\ni−ρ ≥ 2\r\n(2ρ)\r\ni−(2ρ)i+1\r\n=\r\n1\r\nci\r\n>\r\n1\r\nci + 1\r\nNow we let t ∈ (0, 1) and write F(t) = F(0) + t\r\nρG(t), where\r\nG(t) = 1\r\n(ci−1+1)ρ\r\n\u0010\r\nρ\r\n1\r\n\u0001\r\nci−1\r\n1\r\nt +\r\n\r\nρ\r\n2\r\n\u0001\r\nc\r\n2\r\ni−1\r\n1\r\nt\r\n2 + · · · +\r\n\r\nρ\r\nρ−1\r\n\u0001\r\nc\r\nρ−1\r\ni−1\r\n1\r\nt\r\nρ−1\r\n\u0011\r\n+\r\n\u0010 ci−1\r\nci−1+1 \u0001ρ\r\n−\r\nci\r\nci−1+1\u0011\r\n.\r\nIf G(t) ≥ 0, then clearly F(t) ≥ F(0) > 0, so we are done. Otherwise, G(t) < 0, in which case\r\nit easily follows that G(1) < G(t) < 0, hence F(t) = F(0)+t\r\nρG(t) > F(0)+G(1) = F(1) = 0,\r\nas desired. This concludes the proof of the lemma.\r\nTo prove Lemma 8.8, assume β¯ = (β0, . . . , βm) ∈ Γ is optimal. By Lemmas 8.9 and 8.10,\r\nit follows that β0 ≤ β1 ≤ · · · ≤ βm. Now Lemma 8.11 implies that β0 = β1 = · · · = βm.\r\nSince β¯ ∈ Γ, we have βi = 1, and hence the minimal value of f over Γ is f (1, 1, . . . , 1) = 1.\r\nThis concludes the proof of Lemma 8.8.\r\n112",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/dc267219-4a70-4a6d-aa2a-83fb8384fa4f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bcc2e354479f091e125515fbb5a872739a8ea113162ea7ec4eacbfb194befb37",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 236
      },
      {
        "segments": [
          {
            "segment_id": "5d83ab25-6616-4df8-b6df-7abc115199a5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 113,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 9\r\nPredecessor Search\r\nIn this chapter, we tell the fascinating story of the predecessor problem, reviewing upper\r\nbounds before delving into lower bounds, in an attempt to illustrate the powerful information\u0002theoretic structures that abound in this problem.\r\nSee §2.1 for an introduction to the problem and a survey of known results. In this\r\nchapter, we concentrate on the static problem. When talking about upper bounds, we tend\r\nto focus on the information-theoretic aspects and gloss over implementation details on the\r\nword RAM. Formally, our descriptions can be seen as giving algorithms in the cell-probe\r\nmodel, in which it is free to compute anything on the data that has been read. We also\r\nignore details regarding construction. In all cases, the preprocessing time can be made\r\nlinear in the size of the data structure, starting from sorted input. For the data structures\r\nthat use hash tables, the preprocessing time holds with high probability; the query is always\r\ndeterministic.\r\nUpper bounds. We begin in §9.1 with two fundamental techniques giving static, linear\u0002space data structures: the data structure of van Emde Boas [101], and the fusion trees of\r\nFredman and Willard [52].\r\nThe van Emde Boas data structure, dating back to FOCS’75, is undoubtedly a corner\u0002stone of modern data structure theory. As the first interesting algorithm that exploited\r\nbounded precision for faster running times, it has prompted the modern study of predeces\u0002sor search, sorting, sublogarithmic point location etc. The basic idea has continued to be an\r\ninspiration in other contexts, being applied for instance in cache oblivious layouts.\r\nFusion trees showed that o(lg n) search is always achievable, and demonstrated the the\u0002oretical power of word-level parallelism. This is perhaps the most widely used technique for\r\nexploiting bounded precision. The idea is to sketch multiple items down to a smaller size,\r\nand pack them in a single word. Then, regular word operations act “in parallel” over this\r\nvector of sketched data items.\r\n113",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/5d83ab25-6616-4df8-b6df-7abc115199a5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d492c262baa1a8abb0a93e322d20500ec304fa29518442387cfc5101473d374",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 320
      },
      {
        "segments": [
          {
            "segment_id": "f06a627e-a9bc-43cc-a7bd-65aadaeaaeda",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 114,
            "page_width": 612,
            "page_height": 792,
            "content": "Lower bounds. In §9.2, we prove our lower bounds, implying that the query time is, up\r\nto constant factors:\r\nmin\r\n\r\n\r\n\r\nlogw n\r\nlg w−lg n\r\na\r\nlg w\r\na\r\nlg(\r\na\r\nlg n\r\n· lg w\r\na )\r\nlg w\r\na\r\nlg(lg w\r\na\r\n/ lg lg n\r\na )\r\nHere, we defined a = lg S·w\r\nn\r\n, where S was the space. Our main contribution is the tight\r\nlower bounds for a = o(lg n) (in particular, branches two and four of the trade-off). As\r\nmentioned already, previous techniques were helpless, since none could separate linear and\r\npolynomial space.\r\nTo avoid technical details and concentrate on the main developments, this thesis only\r\ngives the proof for the simplest case w = 3 lg n, and a = o(lg n). In addition, we only talk\r\nabout deterministic data structures. For the randomized lower bounds, see our publica\u0002tion [90].\r\nA very strong consequence of our proofs is the idea that sharing between subproblems\r\ndoes not help for predecessor search. Formally, the best cell-probe complexity achievable\r\nby a data structure representing k independent subproblems (with the same parameters) in\r\nspace k · σ is asymptotically equal to the best complexity achievable by a data structure for\r\none subproblem, which uses space σ. The simplicity and strength of this statement make it\r\ninteresting from both the data-structural and complexity-theoretic perspectives.\r\nAt a high level, it is precisely this sort of direct-sum property that enables us to beat\r\ncommunication complexity. Say we have k independent subproblems, and total space S.\r\nWhile in the communication game Alice sends lg S bits per round, our results intuitively\r\nstate that lg S\r\nk\r\nbits are sufficient. Then, by carefully controlling the increase in k and the\r\ndecrease in key length (the query size), we can prevent Alice from communicating her entire\r\ninput over a superconstant number of rounds.\r\nA nice illustration of the strength of our result are the tight bounds for near linear\r\nuniverses, i.e. w = lg n + δ, with δ = o(lg n). On the upper bound side, the algorithm can\r\njust start by a table lookup based on the first lg n bits of the key, which requires linear space.\r\nThen, it continues to apply van Emde Boas for δ-bit keys inside each subproblem, which\r\ngives a complexity of O(lg δ\r\na\r\n). Obtaining a lower bound is just as easy, given our techniques.\r\nWe first consider n/2\r\nδ\r\nindependent subproblems, where each has 2δintegers of 2δ bits each.\r\nThen, we prefix the integers in each subproblem by the number of the subproblem (taking\r\nlg n−δ bits), and prefix the query with a random subproblem number. Because the universe\r\nof each subproblem (22δ) is quadratically bigger than the number of keys, we can apply\r\nthe usual proof showing the optimality of van Emde Boas’ bound for polynomial universes.\r\nThus, the complexity is Ω(lg δ\r\na\r\n).\r\nIn the course of proving the deterministic lower bounds, we introduce a new concept\r\nwhich is crucial to the induction hypothesis: we allow the algorithm to reject queries, under\r\ncertain conditions. In fact, the deterministic proof rejects almost all queries; nonetheless the\r\n114",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f06a627e-a9bc-43cc-a7bd-65aadaeaaeda.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ce9d545eec32b202c5c957f2aaef75e5ac100c12e27109a3cfafb2b762d6d9f1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 531
      },
      {
        "segments": [
          {
            "segment_id": "f06a627e-a9bc-43cc-a7bd-65aadaeaaeda",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 114,
            "page_width": 612,
            "page_height": 792,
            "content": "Lower bounds. In §9.2, we prove our lower bounds, implying that the query time is, up\r\nto constant factors:\r\nmin\r\n\r\n\r\n\r\nlogw n\r\nlg w−lg n\r\na\r\nlg w\r\na\r\nlg(\r\na\r\nlg n\r\n· lg w\r\na )\r\nlg w\r\na\r\nlg(lg w\r\na\r\n/ lg lg n\r\na )\r\nHere, we defined a = lg S·w\r\nn\r\n, where S was the space. Our main contribution is the tight\r\nlower bounds for a = o(lg n) (in particular, branches two and four of the trade-off). As\r\nmentioned already, previous techniques were helpless, since none could separate linear and\r\npolynomial space.\r\nTo avoid technical details and concentrate on the main developments, this thesis only\r\ngives the proof for the simplest case w = 3 lg n, and a = o(lg n). In addition, we only talk\r\nabout deterministic data structures. For the randomized lower bounds, see our publica\u0002tion [90].\r\nA very strong consequence of our proofs is the idea that sharing between subproblems\r\ndoes not help for predecessor search. Formally, the best cell-probe complexity achievable\r\nby a data structure representing k independent subproblems (with the same parameters) in\r\nspace k · σ is asymptotically equal to the best complexity achievable by a data structure for\r\none subproblem, which uses space σ. The simplicity and strength of this statement make it\r\ninteresting from both the data-structural and complexity-theoretic perspectives.\r\nAt a high level, it is precisely this sort of direct-sum property that enables us to beat\r\ncommunication complexity. Say we have k independent subproblems, and total space S.\r\nWhile in the communication game Alice sends lg S bits per round, our results intuitively\r\nstate that lg S\r\nk\r\nbits are sufficient. Then, by carefully controlling the increase in k and the\r\ndecrease in key length (the query size), we can prevent Alice from communicating her entire\r\ninput over a superconstant number of rounds.\r\nA nice illustration of the strength of our result are the tight bounds for near linear\r\nuniverses, i.e. w = lg n + δ, with δ = o(lg n). On the upper bound side, the algorithm can\r\njust start by a table lookup based on the first lg n bits of the key, which requires linear space.\r\nThen, it continues to apply van Emde Boas for δ-bit keys inside each subproblem, which\r\ngives a complexity of O(lg δ\r\na\r\n). Obtaining a lower bound is just as easy, given our techniques.\r\nWe first consider n/2\r\nδ\r\nindependent subproblems, where each has 2δintegers of 2δ bits each.\r\nThen, we prefix the integers in each subproblem by the number of the subproblem (taking\r\nlg n−δ bits), and prefix the query with a random subproblem number. Because the universe\r\nof each subproblem (22δ) is quadratically bigger than the number of keys, we can apply\r\nthe usual proof showing the optimality of van Emde Boas’ bound for polynomial universes.\r\nThus, the complexity is Ω(lg δ\r\na\r\n).\r\nIn the course of proving the deterministic lower bounds, we introduce a new concept\r\nwhich is crucial to the induction hypothesis: we allow the algorithm to reject queries, under\r\ncertain conditions. In fact, the deterministic proof rejects almost all queries; nonetheless the\r\n114",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f06a627e-a9bc-43cc-a7bd-65aadaeaaeda.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ce9d545eec32b202c5c957f2aaef75e5ac100c12e27109a3cfafb2b762d6d9f1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 531
      },
      {
        "segments": [
          {
            "segment_id": "a11ae48d-dff1-405e-b8f3-0a9bb241d78c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 115,
            "page_width": 612,
            "page_height": 792,
            "content": "few accepted queries remaining carry enough information to contradict a fast query time.\r\nWe note that which queries are accepted depends non-trivially on the data structure and\r\nquery algorithm.\r\nImplications to range searching. Another problem closely related to predecessor search\r\nis static range searching in two dimensions. Given a set of n points at integer coordinates\r\nin the plane, the query asks whether an axis-parallel rectangle contains any point. Consider\r\nthe colored predecessor problem, where elements of the set Y are red or blue, and the query\r\nshould only return the color of the predecessor. Lower bounds for this problem (such as\r\nours) also apply to range queries. The trick is to consider the interval stabbing problem,\r\nwhere intervals are define by a red point and the next blue point. This problem is itself\r\neasily reducible to 2D range searching (even to the special case of dominance queries, where\r\none corner of the rectangle is always the origin).\r\nAn interesting special case is when coordinates are distinct integers in [n], i.e. the problem\r\nis in rank space. This restriction occurs naturally in many important cases, such as recursion\r\nfrom higher-dimensional range structures, or geometric approaches to pattern matching. In\r\nFOCS’00, Alstrup et al. [7] gave a query time of O(lg lg n), using space O(n lgε n). Clearly,\r\npredecessor lower bounds are irrelevant, since predecessors in [n] are trivial to find with O(n)\r\nspace. In fact, no previous technique could prove a superconstant lower bound.\r\nOur results imply a tight Ω(lg lg n) time bound for space Oe(n). Note that this requires\r\na novel approach, since for dominance queries, as obtained by the old reduction, there is a\r\nconstant-time upper bound (the RMQ data structures). In a nutshell, we consider a uniform\r\n√3 n ×\r\n√3 n grid on top of our original space. In each cell, we construct a hard subproblem\r\nusing the colored predecessor problem. This is possible since we get to place √3 n points in\r\nthe space \u0002n\r\n2/3\r\n\u00032\r\n. Finally, we can use the direct-sum properties of our lower bound, to argue\r\nthat for this set of problems, the query time cannot be better than for one problem with √3 n\r\npoints and Oe(\r\n√3 n) space.\r\n9.1 Data Structures Using Linear Space\r\n9.1.1 Equivalence to Longest Common Prefix\r\nWe write lcp(x, y) for the longest common prefix of x and y, i.e. the largest i, such that the\r\nmost significant i bits of x and y are identical. The longest common prefix query on a set\r\nS = {y1, . . . , yn} is defined as:\r\nlcp(x): returns some i ∈ [n] that maximizes lcp(x, yi).\r\nIt is immediate that predecessor search can solve lcp queries, because either the prede\u0002cessor or the successor maximizes the common prefix.\r\nIt turns out that a reverse reduction also holds, and we will only reason about lcp queries\r\nfrom now on. (Note, though, that this reduction is not needed in practice. Most lcp data\r\nstructures can solve predecessor search with some ad hoc tweaks, which enjoy better constant\r\nfactors.)\r\n115",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/a11ae48d-dff1-405e-b8f3-0a9bb241d78c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d56fcc3f920ac46296f798a77e910aec70e82fbeb6e95543884430c0ca2cf6a2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "a11ae48d-dff1-405e-b8f3-0a9bb241d78c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 115,
            "page_width": 612,
            "page_height": 792,
            "content": "few accepted queries remaining carry enough information to contradict a fast query time.\r\nWe note that which queries are accepted depends non-trivially on the data structure and\r\nquery algorithm.\r\nImplications to range searching. Another problem closely related to predecessor search\r\nis static range searching in two dimensions. Given a set of n points at integer coordinates\r\nin the plane, the query asks whether an axis-parallel rectangle contains any point. Consider\r\nthe colored predecessor problem, where elements of the set Y are red or blue, and the query\r\nshould only return the color of the predecessor. Lower bounds for this problem (such as\r\nours) also apply to range queries. The trick is to consider the interval stabbing problem,\r\nwhere intervals are define by a red point and the next blue point. This problem is itself\r\neasily reducible to 2D range searching (even to the special case of dominance queries, where\r\none corner of the rectangle is always the origin).\r\nAn interesting special case is when coordinates are distinct integers in [n], i.e. the problem\r\nis in rank space. This restriction occurs naturally in many important cases, such as recursion\r\nfrom higher-dimensional range structures, or geometric approaches to pattern matching. In\r\nFOCS’00, Alstrup et al. [7] gave a query time of O(lg lg n), using space O(n lgε n). Clearly,\r\npredecessor lower bounds are irrelevant, since predecessors in [n] are trivial to find with O(n)\r\nspace. In fact, no previous technique could prove a superconstant lower bound.\r\nOur results imply a tight Ω(lg lg n) time bound for space Oe(n). Note that this requires\r\na novel approach, since for dominance queries, as obtained by the old reduction, there is a\r\nconstant-time upper bound (the RMQ data structures). In a nutshell, we consider a uniform\r\n√3 n ×\r\n√3 n grid on top of our original space. In each cell, we construct a hard subproblem\r\nusing the colored predecessor problem. This is possible since we get to place √3 n points in\r\nthe space \u0002n\r\n2/3\r\n\u00032\r\n. Finally, we can use the direct-sum properties of our lower bound, to argue\r\nthat for this set of problems, the query time cannot be better than for one problem with √3 n\r\npoints and Oe(\r\n√3 n) space.\r\n9.1 Data Structures Using Linear Space\r\n9.1.1 Equivalence to Longest Common Prefix\r\nWe write lcp(x, y) for the longest common prefix of x and y, i.e. the largest i, such that the\r\nmost significant i bits of x and y are identical. The longest common prefix query on a set\r\nS = {y1, . . . , yn} is defined as:\r\nlcp(x): returns some i ∈ [n] that maximizes lcp(x, yi).\r\nIt is immediate that predecessor search can solve lcp queries, because either the prede\u0002cessor or the successor maximizes the common prefix.\r\nIt turns out that a reverse reduction also holds, and we will only reason about lcp queries\r\nfrom now on. (Note, though, that this reduction is not needed in practice. Most lcp data\r\nstructures can solve predecessor search with some ad hoc tweaks, which enjoy better constant\r\nfactors.)\r\n115",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/a11ae48d-dff1-405e-b8f3-0a9bb241d78c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d56fcc3f920ac46296f798a77e910aec70e82fbeb6e95543884430c0ca2cf6a2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "7963ddd0-747b-4d91-9404-c28af8e3d608",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 116,
            "page_width": 612,
            "page_height": 792,
            "content": "Lemma 9.1. If the lcp problem can be solved with space S and query time tq, predecessor\r\nsearch can be solved with space S + O(n) and query time tq + O(1).\r\nProof. Consider a binary trie of depth w, and the root-to-leaf paths representing the values\r\nin S = {y1, . . . , yn}. There are precisely n − 1 branching nodes among these n paths. Our\r\ndata structure has two components:\r\n• A hash table stores all the branching nodes, i.e. pairs (`, v), where ` is the depth of\r\nthe node and v is the prefix leading to that node. We also store as associated data the\r\nminimum and maximum values in S beginning with prefix v.\r\n• For each yi ∈ S, we store a bit vector of size w (exactly a word) indicating which\r\nancestors of x are branching nodes.\r\nThe query for the predecessor of x proceeds as follows:\r\n• Run the lcp query; let i = lcp(x).\r\n• Compute ` = lcp(x, yi).\r\n• Find the highest branching node, v, on the path yi below height `. This is done by\r\nexamining the bit vector indicating the branching nodes above yi, which takes constant\r\ntime because the bit vector is a word.\r\n• If v is to the left of x, the maximum value under v (stored as associated data in the\r\nhash table) is the predecessor.\r\n• If v is to the right of x, the minimum value under v is the successor, and the predecessor\r\nis immediately before it in S.\r\n9.1.2 The Data Structure of van Emde Boas\r\nTo introduce this idea, let us consider the following communication problem:\r\nDefinition 9.2. In the Greater-Than problem, Alice has an n-bit number x, and Bob has\r\nan n-bit number y. They must communicate to determine whether x > y.\r\nPerhaps the most natural idea for this problem is to binary search for the length of the\r\nlongest common prefix between x and y, giving a randomized protocol with lg n rounds.\r\nFormally, the algorithm works as follows:\r\n• If n = 1, Alice sends x, and Bob outputs the answer.\r\n• If n ≥ 2, Alice sends a hash code of the most significant d\r\nn\r\n2\r\ne bits of x. Call these bits\r\nhi(x), and the remaining bits lo(x). Let h be the hash function.\r\n• Bob sends a bit indicating whether h(hi(x)) is equal to h(hi(y)).\r\n– In the “different” case, the players know hi(x) 6= hi(y), so x < y iff hi(x) < hi(y).\r\nThey recurse on hi(x) and hi(y).\r\n116",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7963ddd0-747b-4d91-9404-c28af8e3d608.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a902688ce72f9b86ef5ec81a1f90e73d209bb607e77932011e36c4c3a72aa656",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 433
      },
      {
        "segments": [
          {
            "segment_id": "9bda5f09-3d17-4591-8a75-20dfa8a28435",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 117,
            "page_width": 612,
            "page_height": 792,
            "content": "– In the “equal” case, the players assume hi(x) = hi(y), which holds with good 1\r\nprobability, and recurse on lo(x) and lo(y).\r\nThe van Emde Boas data structure implements essentially the same idea for predecessor\r\nsearch: it binary searches for the length of the longest common prefix between the query\r\nand a value in the set. This idea was presented in somewhat obscure terms in the original\r\npaper of van Emde Boas [101]. The version we describe here was a later simplification of\r\nWillard [103], introduced under the name of “y-fast trees.”\r\nThe algorithm. Let hi(y; `) be the most significant ` bits of a w-bit integer y. Given the\r\ninput set S = {y1, . . . , yn}, the data structure works as follows:\r\nconstruction: For ` = 1 . . w, we hold a hash table H` with {hi(y1; `), . . . , hi(yn; `)}. This\r\nrequires space O(n · w), but we show below how to improve this to O(n).\r\nquery: Binary search for the largest ` such that hi(x; `) ∈ H`.\r\nThe query time is a very efficient O(lg w). The query time may also be expressed as\r\nO(lg lg u), where u = 2w is the universe of the values.\r\nWhile the double logarithm has historically been used for some artistic effect, we feel it\r\nmakes readers overly optimistic about performance, and should be avoided. Consider the\r\nfollowing back of the envelope calculation. If keys have 128 bits, as addresses in IPv6 do,\r\nthen lg w is 7; note that the double logarithm comes from the huge universe 2128. The leading\r\nconstant is nontrivial, since each step needs a query to a hash table. If we approximate this\r\nconstant as “three times slower than binary search” (computing a hash function, plus two\r\nmemory accesses), then van Emde Boas becomes competitive with binary search only for\r\nsets of size n = 221 = 2M.\r\nBucketing. The space can be reduced to O(n) by a standard bucketing trick. We group\r\nthe n elements of S in O(n/w) buckets of O(w) elements each. Each bucket is stored as a\r\nsorted array, and the minimum in each bucket is inserted in the predecessor structure from\r\nabove. The space becomes O(n).\r\nTo answer a query, first run the query algorithm from above to find the bucket i in\r\nwhich the predecessor should be searched. Then, do a binary search inside bucket i, taking\r\nadditional O(lg w) time.\r\n9.1.3 Fusion Trees\r\nIn external memory, if a page of B words can be read in constant time, predecessor search is\r\nsolved in time O(logB n) via B-trees. That basis of this solution is that fact that in external\r\n1 We omit a rigorous analysis of the failure probability, since this is just a toy example. To make the\r\nanalysis precise, h is chosen from a universal family using public coins. The range of h is O(lg lg n) bits,\r\nmaking a false positive occur with probability 1/ polylog(n). The protocol fails with probability o(1) by a\r\nunion bound over the lg n rounds.\r\n117",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9bda5f09-3d17-4591-8a75-20dfa8a28435.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d049db3fd2454c403ab42f1e97fcc2b78230c03a0622136977de1d9f4aa16c7e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 516
      },
      {
        "segments": [
          {
            "segment_id": "9bda5f09-3d17-4591-8a75-20dfa8a28435",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 117,
            "page_width": 612,
            "page_height": 792,
            "content": "– In the “equal” case, the players assume hi(x) = hi(y), which holds with good 1\r\nprobability, and recurse on lo(x) and lo(y).\r\nThe van Emde Boas data structure implements essentially the same idea for predecessor\r\nsearch: it binary searches for the length of the longest common prefix between the query\r\nand a value in the set. This idea was presented in somewhat obscure terms in the original\r\npaper of van Emde Boas [101]. The version we describe here was a later simplification of\r\nWillard [103], introduced under the name of “y-fast trees.”\r\nThe algorithm. Let hi(y; `) be the most significant ` bits of a w-bit integer y. Given the\r\ninput set S = {y1, . . . , yn}, the data structure works as follows:\r\nconstruction: For ` = 1 . . w, we hold a hash table H` with {hi(y1; `), . . . , hi(yn; `)}. This\r\nrequires space O(n · w), but we show below how to improve this to O(n).\r\nquery: Binary search for the largest ` such that hi(x; `) ∈ H`.\r\nThe query time is a very efficient O(lg w). The query time may also be expressed as\r\nO(lg lg u), where u = 2w is the universe of the values.\r\nWhile the double logarithm has historically been used for some artistic effect, we feel it\r\nmakes readers overly optimistic about performance, and should be avoided. Consider the\r\nfollowing back of the envelope calculation. If keys have 128 bits, as addresses in IPv6 do,\r\nthen lg w is 7; note that the double logarithm comes from the huge universe 2128. The leading\r\nconstant is nontrivial, since each step needs a query to a hash table. If we approximate this\r\nconstant as “three times slower than binary search” (computing a hash function, plus two\r\nmemory accesses), then van Emde Boas becomes competitive with binary search only for\r\nsets of size n = 221 = 2M.\r\nBucketing. The space can be reduced to O(n) by a standard bucketing trick. We group\r\nthe n elements of S in O(n/w) buckets of O(w) elements each. Each bucket is stored as a\r\nsorted array, and the minimum in each bucket is inserted in the predecessor structure from\r\nabove. The space becomes O(n).\r\nTo answer a query, first run the query algorithm from above to find the bucket i in\r\nwhich the predecessor should be searched. Then, do a binary search inside bucket i, taking\r\nadditional O(lg w) time.\r\n9.1.3 Fusion Trees\r\nIn external memory, if a page of B words can be read in constant time, predecessor search is\r\nsolved in time O(logB n) via B-trees. That basis of this solution is that fact that in external\r\n1 We omit a rigorous analysis of the failure probability, since this is just a toy example. To make the\r\nanalysis precise, h is chosen from a universal family using public coins. The range of h is O(lg lg n) bits,\r\nmaking a false positive occur with probability 1/ polylog(n). The protocol fails with probability o(1) by a\r\nunion bound over the lg n rounds.\r\n117",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/9bda5f09-3d17-4591-8a75-20dfa8a28435.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d049db3fd2454c403ab42f1e97fcc2b78230c03a0622136977de1d9f4aa16c7e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 516
      },
      {
        "segments": [
          {
            "segment_id": "17f7eee0-04a3-45a2-af11-cf85ff9fdecf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 118,
            "page_width": 612,
            "page_height": 792,
            "content": "memory, predecessor search among O(B) elements can solved in constant time. B-trees are\r\nsimply recursive B-ary search, based on this primitive.\r\nFredman and Willard asked the following intriguing question: on the word RAM, can we\r\nhave constant-time search among B elements, for some B = ω(1)? The memory unit is a\r\nword, which can only store one complete value. But can we somehow compress ω(1) words\r\ninto just one word, and still be able to search for predecessors in compressed form?\r\nThe answer of Fredman and Willard [52] was affirmative. They designed a sketch that\r\ncould compress B = O(\r\n√\r\nw) words down to a single word, such that lcp queries can be\r\nanswered based on these sketches. By Lemma 9.1, this gives a predecessor structure among\r\nB elements with O(1) query time and O(B) space.\r\nThis primitive is used to construct a B-tree, giving query time O(logw n). Since w =\r\nΩ(lg n), this is always O(lg n/ lg lg n), i.e. it is theoretically better than binary search for any\r\nword size. In fact, taking the best of fusion trees and van Emde Boas yields min{lg w, logw n} =\r\nO(\r\n√\r\nlg n), a quadratic improvement over binary search. However, it should be noted that,\r\nunlike van Emde Boas, fusion trees do not lead to convincing practical improvements, due\r\nto the large constant factors involved.\r\nSketching. Let S = {y1, . . . , yB} be the values we want to sketch. As before, we view these\r\nvalues as root-to-leaf paths in a binary trie of depth w. There are B − 1 branching nodes on\r\nthese paths; let T be the set of depths at which these nodes occur. We write projT (x) for\r\nthe projection of a value x on the bit positions of T, i.e. an integer of |T| bits, with the bits\r\nof x appearing at the positions in T.\r\nThe sketch of S is simply projT (S) = {projT (y1), . . . , projT (yB)}. This takes B · |T| =\r\nO(B2) bits, which fits in a word for B = O(\r\n√\r\nw). We now claim that answering lcp(x)\r\non the original set S is equivalent to answering lcp(projT (x)) on the sketch projT (S), a\r\nconstant-time operation because the sketch is a word.\r\nTo prove our claim formally, consider a mental experiment in which we trace the path\r\nof x in the trie of S, and, in parallel, trace the path of projT (x) in the trie of depth |T|\r\nrepresenting projT (S).\r\nWe show inductively that lcp(projT (x)) in the sketch trie is also a valid answer for\r\nlcp(x) in the original trie. We have the following cases for a node met by the path of x in\r\nthe original trie:\r\n• a branching node. This level is included in T, hence in the sketch. By induction, the\r\nlcp is valid in the two subtrees.\r\n• a non-branching node, on some path(s) from S. The level is not necessarily included\r\nin the sketch, but may be, due to an unrelated branching node. If this level of the trie,\r\nthe path in the sketch tree follows the corresponding node. Otherwise\r\n• a node off any path in S. The first time this happens, we stop tracing the paths, since\r\nthe lcp has been determined.\r\n118",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/17f7eee0-04a3-45a2-af11-cf85ff9fdecf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1271dd7255f367f0548fe1afde912d31c994010edbe310883fbb600e63d5560b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 557
      },
      {
        "segments": [
          {
            "segment_id": "17f7eee0-04a3-45a2-af11-cf85ff9fdecf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 118,
            "page_width": 612,
            "page_height": 792,
            "content": "memory, predecessor search among O(B) elements can solved in constant time. B-trees are\r\nsimply recursive B-ary search, based on this primitive.\r\nFredman and Willard asked the following intriguing question: on the word RAM, can we\r\nhave constant-time search among B elements, for some B = ω(1)? The memory unit is a\r\nword, which can only store one complete value. But can we somehow compress ω(1) words\r\ninto just one word, and still be able to search for predecessors in compressed form?\r\nThe answer of Fredman and Willard [52] was affirmative. They designed a sketch that\r\ncould compress B = O(\r\n√\r\nw) words down to a single word, such that lcp queries can be\r\nanswered based on these sketches. By Lemma 9.1, this gives a predecessor structure among\r\nB elements with O(1) query time and O(B) space.\r\nThis primitive is used to construct a B-tree, giving query time O(logw n). Since w =\r\nΩ(lg n), this is always O(lg n/ lg lg n), i.e. it is theoretically better than binary search for any\r\nword size. In fact, taking the best of fusion trees and van Emde Boas yields min{lg w, logw n} =\r\nO(\r\n√\r\nlg n), a quadratic improvement over binary search. However, it should be noted that,\r\nunlike van Emde Boas, fusion trees do not lead to convincing practical improvements, due\r\nto the large constant factors involved.\r\nSketching. Let S = {y1, . . . , yB} be the values we want to sketch. As before, we view these\r\nvalues as root-to-leaf paths in a binary trie of depth w. There are B − 1 branching nodes on\r\nthese paths; let T be the set of depths at which these nodes occur. We write projT (x) for\r\nthe projection of a value x on the bit positions of T, i.e. an integer of |T| bits, with the bits\r\nof x appearing at the positions in T.\r\nThe sketch of S is simply projT (S) = {projT (y1), . . . , projT (yB)}. This takes B · |T| =\r\nO(B2) bits, which fits in a word for B = O(\r\n√\r\nw). We now claim that answering lcp(x)\r\non the original set S is equivalent to answering lcp(projT (x)) on the sketch projT (S), a\r\nconstant-time operation because the sketch is a word.\r\nTo prove our claim formally, consider a mental experiment in which we trace the path\r\nof x in the trie of S, and, in parallel, trace the path of projT (x) in the trie of depth |T|\r\nrepresenting projT (S).\r\nWe show inductively that lcp(projT (x)) in the sketch trie is also a valid answer for\r\nlcp(x) in the original trie. We have the following cases for a node met by the path of x in\r\nthe original trie:\r\n• a branching node. This level is included in T, hence in the sketch. By induction, the\r\nlcp is valid in the two subtrees.\r\n• a non-branching node, on some path(s) from S. The level is not necessarily included\r\nin the sketch, but may be, due to an unrelated branching node. If this level of the trie,\r\nthe path in the sketch tree follows the corresponding node. Otherwise\r\n• a node off any path in S. The first time this happens, we stop tracing the paths, since\r\nthe lcp has been determined.\r\n118",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/17f7eee0-04a3-45a2-af11-cf85ff9fdecf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1271dd7255f367f0548fe1afde912d31c994010edbe310883fbb600e63d5560b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 557
      },
      {
        "segments": [
          {
            "segment_id": "645b99f5-ad95-420c-9671-36856150df1e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 119,
            "page_width": 612,
            "page_height": 792,
            "content": "Implementation on the RAM. Implementing this algorithm on the word RAM is tech\u0002nically involved. The main ingredient of [52] is an implementation of projT (x), which can\r\ncompress |T| scattered bits of x, via some carefully chosen multiplications and mask, into a\r\nspace of O(|T|\r\n4\r\n) contiguous bits. Thus, one can only sketch B = O(w\r\n1/5\r\n) values into a word,\r\nwhich is still enough for an asymptotic query time of O(logw n).\r\nAn objection to [52] is that it uses multiplication, which is not an AC0 operation (i.e. can\u0002not be implemented in constant depth by a polynomial size circuit). Andersson, Miltersen\r\nand Thorup [11] show how to implement fusion trees using nonstandard AC0 operations.\r\nThe atomic heaps of Fredman and Willard [53] obtain O(lg n/ lg lg n) query time without\r\nmultiplication, via look-up tables of size O(n\r\nε\r\n).\r\n9.2 Lower Bounds\r\n9.2.1 The Cell-Probe Elimination Lemma\r\nAn abstract decision data structure problem is defined by a function f : D ×Q → {0, 1}. An\r\ninput from D is given at preprocessing time, and the data structure must store a representa\u0002tion of it in some bounded space. An input from Q is given at query time, and the function\r\nof the two inputs must be computed through cell probes. We restrict the preprocessing and\r\nquery algorithms to be deterministic. In general, we consider a problem in conjunction with\r\na distribution D over D ×Q. Note that the distribution need not (and, in our case, will not)\r\nbe a product distribution. We care about the probability the query algorithm is successful\r\nunder the distribution D, for a notion of success to be defined shortly.\r\nAs mentioned before, we work in the cell-probe model, and let w be the number of bits in\r\na cell. We assume the query’s input consists of at most w bits, and that the space bound is at\r\nmost 2w. For the sake of an inductive argument, we extend the cell-probe model by allowing\r\nthe data structure to publish some bits at preprocessing time. These are bits depending\r\non the data structure’s input, which the query algorithm can inspect at no charge. Closely\r\nrelated to this concept is our model for a query being accepted. We allow the query algorithm\r\nnot to return the correct answer, but only in the following very limited way. After inspecting\r\nthe query and the published bits, the algorithm can declare that it cannot answer the query\r\n(we say it rejects the query). Otherwise, the query is accepted: the algorithm can make cell\r\nprobes, and at the end it must answer the query correctly. Thus, it is not possible to reject\r\nlater. In contrast to more common models of error, it actually makes sense to talk about\r\ntiny (close to zero) probabilities of accept, even for problems with boolean output.\r\nFor an arbitrary problem f and an integer k ≤ 2\r\nw L\r\n, we define a direct-sum problem\r\nk\r\nf : Dk × ([k] × Q) → {0, 1} as follows. The data structure receives a vector of inputs\r\n(d\r\n1\r\n, . . . , dk). The representation depends arbitrarily on all of these inputs. The query is\r\nthe index of a subproblem i ∈ [k], and an element q ∈ Q. The output of Lkf is f(q, di).\r\nWe also define a distribution Lk D for Lkf, given a distribution D for f. Each d\r\ni\r\nis\r\nchosen independently at random from the marginal distribution on D induced by D. The\r\nsubproblem i is chosen uniformly from [k], and q is chosen from the distribution on Q\r\n119",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/645b99f5-ad95-420c-9671-36856150df1e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63b49378b180c2e973f731cd8bab4ab8f517b2eb16d3b9bedb91c551c3d866b4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 604
      },
      {
        "segments": [
          {
            "segment_id": "645b99f5-ad95-420c-9671-36856150df1e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 119,
            "page_width": 612,
            "page_height": 792,
            "content": "Implementation on the RAM. Implementing this algorithm on the word RAM is tech\u0002nically involved. The main ingredient of [52] is an implementation of projT (x), which can\r\ncompress |T| scattered bits of x, via some carefully chosen multiplications and mask, into a\r\nspace of O(|T|\r\n4\r\n) contiguous bits. Thus, one can only sketch B = O(w\r\n1/5\r\n) values into a word,\r\nwhich is still enough for an asymptotic query time of O(logw n).\r\nAn objection to [52] is that it uses multiplication, which is not an AC0 operation (i.e. can\u0002not be implemented in constant depth by a polynomial size circuit). Andersson, Miltersen\r\nand Thorup [11] show how to implement fusion trees using nonstandard AC0 operations.\r\nThe atomic heaps of Fredman and Willard [53] obtain O(lg n/ lg lg n) query time without\r\nmultiplication, via look-up tables of size O(n\r\nε\r\n).\r\n9.2 Lower Bounds\r\n9.2.1 The Cell-Probe Elimination Lemma\r\nAn abstract decision data structure problem is defined by a function f : D ×Q → {0, 1}. An\r\ninput from D is given at preprocessing time, and the data structure must store a representa\u0002tion of it in some bounded space. An input from Q is given at query time, and the function\r\nof the two inputs must be computed through cell probes. We restrict the preprocessing and\r\nquery algorithms to be deterministic. In general, we consider a problem in conjunction with\r\na distribution D over D ×Q. Note that the distribution need not (and, in our case, will not)\r\nbe a product distribution. We care about the probability the query algorithm is successful\r\nunder the distribution D, for a notion of success to be defined shortly.\r\nAs mentioned before, we work in the cell-probe model, and let w be the number of bits in\r\na cell. We assume the query’s input consists of at most w bits, and that the space bound is at\r\nmost 2w. For the sake of an inductive argument, we extend the cell-probe model by allowing\r\nthe data structure to publish some bits at preprocessing time. These are bits depending\r\non the data structure’s input, which the query algorithm can inspect at no charge. Closely\r\nrelated to this concept is our model for a query being accepted. We allow the query algorithm\r\nnot to return the correct answer, but only in the following very limited way. After inspecting\r\nthe query and the published bits, the algorithm can declare that it cannot answer the query\r\n(we say it rejects the query). Otherwise, the query is accepted: the algorithm can make cell\r\nprobes, and at the end it must answer the query correctly. Thus, it is not possible to reject\r\nlater. In contrast to more common models of error, it actually makes sense to talk about\r\ntiny (close to zero) probabilities of accept, even for problems with boolean output.\r\nFor an arbitrary problem f and an integer k ≤ 2\r\nw L\r\n, we define a direct-sum problem\r\nk\r\nf : Dk × ([k] × Q) → {0, 1} as follows. The data structure receives a vector of inputs\r\n(d\r\n1\r\n, . . . , dk). The representation depends arbitrarily on all of these inputs. The query is\r\nthe index of a subproblem i ∈ [k], and an element q ∈ Q. The output of Lkf is f(q, di).\r\nWe also define a distribution Lk D for Lkf, given a distribution D for f. Each d\r\ni\r\nis\r\nchosen independently at random from the marginal distribution on D induced by D. The\r\nsubproblem i is chosen uniformly from [k], and q is chosen from the distribution on Q\r\n119",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/645b99f5-ad95-420c-9671-36856150df1e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63b49378b180c2e973f731cd8bab4ab8f517b2eb16d3b9bedb91c551c3d866b4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 604
      },
      {
        "segments": [
          {
            "segment_id": "72a7a6a4-ca0c-4c54-a673-7f0cb78a873f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 120,
            "page_width": 612,
            "page_height": 792,
            "content": "conditioned on d\r\ni\r\n.\r\nGiven an arbitrary problem f and an integer h ≤ w, we can define another problem f\r\n(h)\r\nas follows. The query is a vector (q1, . . . , qh). The data structure receives a regular input\r\nd ∈ D, and integer r ∈ [h] and the prefix of the query q1, . . . , qr−1. The output of f\r\n(h)\r\nis\r\nf(d, qr). Note that we have shared information between the data structure and the querier\r\n(i.e. the prefix of the query), so f\r\n(h)\r\nis a partial function on the domain D ×\r\nSt−1\r\ni=0 Qi × Q.\r\nNow we define an input distribution D(h)for f\r\n(h)\r\n, given an input distribution D for f. The\r\nvalue r is chosen uniformly at random. Each query coordinate qiis chosen independently\r\nat random from the marginal distribution on Q induced by D. Now d is chosen from the\r\ndistribution on D, conditioned on qr.\r\nWe give the f\r\n(h) operator precedence over the direct sum operator, i.e. Lk\r\nf\r\nL\r\n(h) means\r\nk\r\n\u0002\r\nf\r\n(h)\r\n\u0003\r\n. Using this notation, we are ready to state our central cell-probe elimination\r\nlemma:\r\nLemma 9.3. There exists a universal constant C, such that for any problem f, distribution\r\nD\r\nL\r\n, and positive integers h and k, the following holds. Assume there exists a solution to\r\nk\r\nf\r\n(h) with accept probability α over Lk D(h)\r\n, which uses at most kσ words of space,\r\n1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k published bits and T cell probes. Then, there exists a solution to Lk\r\nf with accept\r\nprobability α\r\n4h\r\nover Lk D, which uses the same space, k\r\n√h σ · Cw2 published bits and T − 1\r\ncell probes.\r\n9.2.2 Setup for the Predecessor Problem\r\nLet P(n, `) be the colored predecessor problem on n integers of ` bits each. Remember that\r\nthis is the decision version of predecessor search, where elements are colored red or blue, and\r\na query just returns the color of the predecessor. We first show how to identify the structure\r\nof P(n, `)\r\n(h)\r\ninside P(n, h`), making it possible to apply our cell-probe elimination lemma.\r\nLemma 9.4. For any integers n, `, h ≥ 1 and distribution D for P(n, `), there exists a\r\ndistribution D∗(h)for P(n, h`) such that the following holds. Given a solution to LkP(n, h`)\r\nwith accept probability α over Lk D∗(h), one can obtain a solution to LkP(n, `)\r\n(h) with accept\r\nprobability α over Lk D(h), which has the same complexity in terms of space, published bits,\r\nand cell probes.\r\nProof. We give a reduction from P(n, `)\r\n(h)\r\nto P(n, h`), which naturally defines the dis\u0002tribution D∗(h)\r\nin terms of D(h). A query for P(n, `)\r\n(h)\r\nconsists of x1, . . . , xh ∈ {0, 1}\r\n`\r\n.\r\nConcatenating these, we obtain a query for P(n, h`). In the case of P(n, `)\r\n(h)\r\n, the data\r\nstructure receives i ∈ [h], the query prefix x1, . . . , xi−1 and a set Y of `-bit integers. We\r\nprepend the query prefix to all integers in Y , and append zeros up to h` bits. Then, finding\r\nthe predecessor of xiin Y is equivalent to finding the predecessor of the concatenation of\r\nx1, . . . , xh in this new set.\r\nObserve that to apply the cell-probe elimination lemma, the number of published bits\r\nmust be just a fraction of k, but applying the lemma increases the published bits signifi\u0002cantly. We want to repeatedly eliminate cell probes, so we need to amplify the number of\r\n120",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/72a7a6a4-ca0c-4c54-a673-7f0cb78a873f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=52da3803110ae65ee8edcfe0eabed8576373173061ac895cd920846641723154",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 611
      },
      {
        "segments": [
          {
            "segment_id": "72a7a6a4-ca0c-4c54-a673-7f0cb78a873f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 120,
            "page_width": 612,
            "page_height": 792,
            "content": "conditioned on d\r\ni\r\n.\r\nGiven an arbitrary problem f and an integer h ≤ w, we can define another problem f\r\n(h)\r\nas follows. The query is a vector (q1, . . . , qh). The data structure receives a regular input\r\nd ∈ D, and integer r ∈ [h] and the prefix of the query q1, . . . , qr−1. The output of f\r\n(h)\r\nis\r\nf(d, qr). Note that we have shared information between the data structure and the querier\r\n(i.e. the prefix of the query), so f\r\n(h)\r\nis a partial function on the domain D ×\r\nSt−1\r\ni=0 Qi × Q.\r\nNow we define an input distribution D(h)for f\r\n(h)\r\n, given an input distribution D for f. The\r\nvalue r is chosen uniformly at random. Each query coordinate qiis chosen independently\r\nat random from the marginal distribution on Q induced by D. Now d is chosen from the\r\ndistribution on D, conditioned on qr.\r\nWe give the f\r\n(h) operator precedence over the direct sum operator, i.e. Lk\r\nf\r\nL\r\n(h) means\r\nk\r\n\u0002\r\nf\r\n(h)\r\n\u0003\r\n. Using this notation, we are ready to state our central cell-probe elimination\r\nlemma:\r\nLemma 9.3. There exists a universal constant C, such that for any problem f, distribution\r\nD\r\nL\r\n, and positive integers h and k, the following holds. Assume there exists a solution to\r\nk\r\nf\r\n(h) with accept probability α over Lk D(h)\r\n, which uses at most kσ words of space,\r\n1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k published bits and T cell probes. Then, there exists a solution to Lk\r\nf with accept\r\nprobability α\r\n4h\r\nover Lk D, which uses the same space, k\r\n√h σ · Cw2 published bits and T − 1\r\ncell probes.\r\n9.2.2 Setup for the Predecessor Problem\r\nLet P(n, `) be the colored predecessor problem on n integers of ` bits each. Remember that\r\nthis is the decision version of predecessor search, where elements are colored red or blue, and\r\na query just returns the color of the predecessor. We first show how to identify the structure\r\nof P(n, `)\r\n(h)\r\ninside P(n, h`), making it possible to apply our cell-probe elimination lemma.\r\nLemma 9.4. For any integers n, `, h ≥ 1 and distribution D for P(n, `), there exists a\r\ndistribution D∗(h)for P(n, h`) such that the following holds. Given a solution to LkP(n, h`)\r\nwith accept probability α over Lk D∗(h), one can obtain a solution to LkP(n, `)\r\n(h) with accept\r\nprobability α over Lk D(h), which has the same complexity in terms of space, published bits,\r\nand cell probes.\r\nProof. We give a reduction from P(n, `)\r\n(h)\r\nto P(n, h`), which naturally defines the dis\u0002tribution D∗(h)\r\nin terms of D(h). A query for P(n, `)\r\n(h)\r\nconsists of x1, . . . , xh ∈ {0, 1}\r\n`\r\n.\r\nConcatenating these, we obtain a query for P(n, h`). In the case of P(n, `)\r\n(h)\r\n, the data\r\nstructure receives i ∈ [h], the query prefix x1, . . . , xi−1 and a set Y of `-bit integers. We\r\nprepend the query prefix to all integers in Y , and append zeros up to h` bits. Then, finding\r\nthe predecessor of xiin Y is equivalent to finding the predecessor of the concatenation of\r\nx1, . . . , xh in this new set.\r\nObserve that to apply the cell-probe elimination lemma, the number of published bits\r\nmust be just a fraction of k, but applying the lemma increases the published bits signifi\u0002cantly. We want to repeatedly eliminate cell probes, so we need to amplify the number of\r\n120",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/72a7a6a4-ca0c-4c54-a673-7f0cb78a873f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=52da3803110ae65ee8edcfe0eabed8576373173061ac895cd920846641723154",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 611
      },
      {
        "segments": [
          {
            "segment_id": "72e753c2-fd95-4d04-952a-c515e5ccdbfd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 121,
            "page_width": 612,
            "page_height": 792,
            "content": "subproblems each time, making the new number of published bits insignificant compared to\r\nthe new k.\r\nLemma 9.5. For any integers t, `, n ≥ 1 and distribution D for P(n, `), there exists a\r\ndistribution D∗t\r\nL\r\nfor P(n ·t, `+ lg t) such that the following holds. Starting from a solution to\r\nk\r\nP(n · t, ` + lg t) with accept probability α over Lk D∗t\r\nL\r\n, one can construct a solution to\r\nkt P(n, `) with accept probability α over Lkt D, which has the same complexity in terms\r\nof space, published bits, and cell probes.\r\nProof. We first describe the distribution D∗t. We draw Y1, . . . , Ytindependently from D,\r\nwhere Yiis a set of integers, representing the data structures input. Prefix all numbers in Yj\r\nby j using lg t bits, and take the union of all these sets to form the data structure’s input\r\nfor P(nt, ` + lg t). To obtain the query, pick j ∈ {0, . . . , t − 1} uniformly at random, pick\r\nthe query from D conditioned on Yj, and prefix this query by j. Now note that Lkt D and\r\nLk D∗t are really the same distribution, except that the lower lg t bits of the problems index\r\nfor Lkt D are interpreted as a prefix in Lk D∗t. Thus, obtaining the new solution is simply\r\na syntactic transformation.\r\nOur goal is to eliminate all cell probes, and then reach a contradiction. For this, we need\r\nthe following:\r\nLemma 9.6. For any n ≥ 1 and ` ≥ log2(n + 1), there exists a distribution D for P(n, `)\r\nsuch that the following holds. For all (∀)0 < α ≤ 1 and k ≥ 1, there does not exist a solution\r\nto LkP(n, `) with accept probability α over Lk D, which uses no cell probes and less than\r\nαk published bits.\r\nProof. The distribution D is quite simple: the integers in the set are always 0 up to n − 1,\r\nand the query is n. All that matters is the color of n − 1, which is chosen uniformly at\r\nrandom among red and blue. Note that for LkP(n, `) there are only k possible queries,\r\ni.e. only the index of the subproblem matters.\r\nLet p be the random variable denoting the published bits. Since there are no cell probes,\r\nthe answers to the queries are a function of p alone. Let α(p) be the fraction of subproblems\r\nthat the query algorithm doesn’t reject when seeing the published bits p. In our model,\r\nthe answer must be correct for all these subproblems. Then, Pr[p = p] ≤ 2\r\n−α(p)k\r\n, as only\r\ninputs which agree with the α(p)k answers of the algorithm can lead to these published bits.\r\nNow observe that α = Ep[α(p)] ≤ Ep\r\nh\r\n1\r\nk\r\nlog2\r\n1\r\nPr[p=p]\r\ni\r\n=\r\n1\r\nkH(p), where H(·) denotes binary\r\nentropy. Since the entropy of the published bits is bounded by their number (less than αk),\r\nwe have a contradiction.\r\n9.2.3 Deriving the Trade-Offs\r\nBecause we will only be dealing with ` = w = O(lg n), the bounds do not change if the space\r\nis S words instead of S bits. To simplify calculations, the exposition in this section assume\r\nthe space is S words.\r\n121",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/72e753c2-fd95-4d04-952a-c515e5ccdbfd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c7dafba36f1a178d5c0f101e2bab93ff66affacde1a75520866a0195fdc980b7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 557
      },
      {
        "segments": [
          {
            "segment_id": "72e753c2-fd95-4d04-952a-c515e5ccdbfd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 121,
            "page_width": 612,
            "page_height": 792,
            "content": "subproblems each time, making the new number of published bits insignificant compared to\r\nthe new k.\r\nLemma 9.5. For any integers t, `, n ≥ 1 and distribution D for P(n, `), there exists a\r\ndistribution D∗t\r\nL\r\nfor P(n ·t, `+ lg t) such that the following holds. Starting from a solution to\r\nk\r\nP(n · t, ` + lg t) with accept probability α over Lk D∗t\r\nL\r\n, one can construct a solution to\r\nkt P(n, `) with accept probability α over Lkt D, which has the same complexity in terms\r\nof space, published bits, and cell probes.\r\nProof. We first describe the distribution D∗t. We draw Y1, . . . , Ytindependently from D,\r\nwhere Yiis a set of integers, representing the data structures input. Prefix all numbers in Yj\r\nby j using lg t bits, and take the union of all these sets to form the data structure’s input\r\nfor P(nt, ` + lg t). To obtain the query, pick j ∈ {0, . . . , t − 1} uniformly at random, pick\r\nthe query from D conditioned on Yj, and prefix this query by j. Now note that Lkt D and\r\nLk D∗t are really the same distribution, except that the lower lg t bits of the problems index\r\nfor Lkt D are interpreted as a prefix in Lk D∗t. Thus, obtaining the new solution is simply\r\na syntactic transformation.\r\nOur goal is to eliminate all cell probes, and then reach a contradiction. For this, we need\r\nthe following:\r\nLemma 9.6. For any n ≥ 1 and ` ≥ log2(n + 1), there exists a distribution D for P(n, `)\r\nsuch that the following holds. For all (∀)0 < α ≤ 1 and k ≥ 1, there does not exist a solution\r\nto LkP(n, `) with accept probability α over Lk D, which uses no cell probes and less than\r\nαk published bits.\r\nProof. The distribution D is quite simple: the integers in the set are always 0 up to n − 1,\r\nand the query is n. All that matters is the color of n − 1, which is chosen uniformly at\r\nrandom among red and blue. Note that for LkP(n, `) there are only k possible queries,\r\ni.e. only the index of the subproblem matters.\r\nLet p be the random variable denoting the published bits. Since there are no cell probes,\r\nthe answers to the queries are a function of p alone. Let α(p) be the fraction of subproblems\r\nthat the query algorithm doesn’t reject when seeing the published bits p. In our model,\r\nthe answer must be correct for all these subproblems. Then, Pr[p = p] ≤ 2\r\n−α(p)k\r\n, as only\r\ninputs which agree with the α(p)k answers of the algorithm can lead to these published bits.\r\nNow observe that α = Ep[α(p)] ≤ Ep\r\nh\r\n1\r\nk\r\nlog2\r\n1\r\nPr[p=p]\r\ni\r\n=\r\n1\r\nkH(p), where H(·) denotes binary\r\nentropy. Since the entropy of the published bits is bounded by their number (less than αk),\r\nwe have a contradiction.\r\n9.2.3 Deriving the Trade-Offs\r\nBecause we will only be dealing with ` = w = O(lg n), the bounds do not change if the space\r\nis S words instead of S bits. To simplify calculations, the exposition in this section assume\r\nthe space is S words.\r\n121",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/72e753c2-fd95-4d04-952a-c515e5ccdbfd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c7dafba36f1a178d5c0f101e2bab93ff66affacde1a75520866a0195fdc980b7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 557
      },
      {
        "segments": [
          {
            "segment_id": "bcbb9ab7-7598-4ed2-9cf2-dfcf434d8a67",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 122,
            "page_width": 612,
            "page_height": 792,
            "content": "Our proof starts assuming that we for any possible distribution have a solution to P(n, `)\r\nwhich uses n · 2\r\na\r\nspace, no published bits, and successfully answers all queries in T probes,\r\nwhere T is small. We will then try to apply T rounds of the cell-probe elimination from\r\nLemma 9.3 and 9.4 followed by the problem amplification from Lemma 9.5. After T rounds,\r\nwe will be left with a non-trivial problem but no cell probes, and then we will reach a\r\ncontradiction with Lemma 9.6. Below, we first run this strategy ignoring details about the\r\ndistribution, but analyzing the parameters for each round. Later in Lemma 9.7, we will\r\npresent a formal inductive proof using these parameters in reverse order, deriving difficult\r\ndistributions for more and more cell probes.\r\nWe denote the problem parameters after i rounds by a subscript i. We have the key length\r\n`i and the number of subproblems ki. The total number of keys remains n, so the have n/ki\r\nkeys in each subproblem. Thus, the problem we deal with in round i + 1 is Lki P(\r\nn\r\nki\r\n, `i),\r\nand we will have some target accept probability αi. The number of cells per subproblem is\r\nσi =\r\nn\r\nki\r\n2\r\na\r\n. We start the first round with `0 = `, α0 = 1, k0 = 1 and σ0 = n · 2\r\na\r\n.\r\nFor the cell probe elimination in Lemma 9.3 and 9.4, our proof will use the same value\r\nof h ≥ 2 in all rounds. Then αi+1 ≥\r\nαi\r\n4h\r\n, so αi ≥ (4h)\r\n−i\r\n. To analyze the evolution of `i\r\nand ki, we let ti be the factor by which we increase the number of subproblems in round i\r\nwhen applying the problem amplification from Lemma 9.5. We now have ki+1 = ti· ki and\r\n`i+1 =\r\n`i\r\nh − lg ti\r\n.\r\nWhen we start the first round, we have no published bits, but when we apply Lemma 9.3\r\nin round i + 1, it leaves us with up to ki\r\n√h σi\r\n· Cw2 published bits for round i + 2. We have\r\nto choose tilarge enough to guarantee that this number of published bits is small enough\r\ncompared to the number of subproblems in round i + 2. To apply Lemma 9.3 in round\r\ni + 2, the number of published bits must be at most 1\r\nC\r\n(\r\nαi+1\r\nh\r\n)\r\n3ki+1 =\r\nα\r\n3\r\ni\r\nti\r\n64Ch6 ki\r\n. Hence we\r\nmust set ti ≥\r\n√h σi\r\n· 64C\r\n2w2h6\r\n(\r\n1\r\nαi\r\n)\r\n3\r\n. Assume for now that T = O(lg `). Using h ≤ `, and\r\nαi ≥ (4h)\r\n−T ≥ 2O(lg2`)\r\n, we conclude it is enough to set:\r\n(∀)i : ti ≥ h\r\nr n\r\nki\r\n· 2\r\na/h\r\n· w\r\n2\r\n· 2\r\nΘ(lg2`)\r\n(9.1)\r\nNow we discuss the conclusion reached at the end of the T rounds. We intend to apply\r\nLemma 9.6 to deduce that the algorithm after T stages cannot make zero cell probes, implying\r\nthat the original algorithm had to make more than T probes. Above we made sure that we\r\nafter T rounds had 1\r\nC\r\n(\r\nαT\r\nh\r\n)\r\n3kT < αT kT published bits, which are few enough compared to the\r\nnumber kT of subproblems. The remaining conditions of Lemma 9.6 are:\r\n`T ≥ 1 and n\r\nkT\r\n≥ 1 (9.2)\r\nSince `i+1 ≤\r\n`i\r\n2\r\n, this condition entails T = O(lg `), as assumed earlier.\r\nLemma 9.7. With the above parameters satisfying (9.1) and (9.2), for i = 0, . . . , T, there is\r\na distribution Di for P(\r\nn\r\nki\r\n, `i) so that no solution for Lki P(\r\nn\r\nki\r\n, `i) can have accept probability\r\nαi over Lki Di using n · 2\r\na\r\nspace, 1\r\nC\r\n(\r\nαi\r\nh\r\n)\r\n3ki published bits, and T − i cell probes.\r\n122",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/bcbb9ab7-7598-4ed2-9cf2-dfcf434d8a67.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d647c4529136a184055fc97ba44c56abe5990529a4203e174fee446346da2897",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 663
      },
      {
        "segments": [
          {
            "segment_id": "bcbb9ab7-7598-4ed2-9cf2-dfcf434d8a67",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 122,
            "page_width": 612,
            "page_height": 792,
            "content": "Our proof starts assuming that we for any possible distribution have a solution to P(n, `)\r\nwhich uses n · 2\r\na\r\nspace, no published bits, and successfully answers all queries in T probes,\r\nwhere T is small. We will then try to apply T rounds of the cell-probe elimination from\r\nLemma 9.3 and 9.4 followed by the problem amplification from Lemma 9.5. After T rounds,\r\nwe will be left with a non-trivial problem but no cell probes, and then we will reach a\r\ncontradiction with Lemma 9.6. Below, we first run this strategy ignoring details about the\r\ndistribution, but analyzing the parameters for each round. Later in Lemma 9.7, we will\r\npresent a formal inductive proof using these parameters in reverse order, deriving difficult\r\ndistributions for more and more cell probes.\r\nWe denote the problem parameters after i rounds by a subscript i. We have the key length\r\n`i and the number of subproblems ki. The total number of keys remains n, so the have n/ki\r\nkeys in each subproblem. Thus, the problem we deal with in round i + 1 is Lki P(\r\nn\r\nki\r\n, `i),\r\nand we will have some target accept probability αi. The number of cells per subproblem is\r\nσi =\r\nn\r\nki\r\n2\r\na\r\n. We start the first round with `0 = `, α0 = 1, k0 = 1 and σ0 = n · 2\r\na\r\n.\r\nFor the cell probe elimination in Lemma 9.3 and 9.4, our proof will use the same value\r\nof h ≥ 2 in all rounds. Then αi+1 ≥\r\nαi\r\n4h\r\n, so αi ≥ (4h)\r\n−i\r\n. To analyze the evolution of `i\r\nand ki, we let ti be the factor by which we increase the number of subproblems in round i\r\nwhen applying the problem amplification from Lemma 9.5. We now have ki+1 = ti· ki and\r\n`i+1 =\r\n`i\r\nh − lg ti\r\n.\r\nWhen we start the first round, we have no published bits, but when we apply Lemma 9.3\r\nin round i + 1, it leaves us with up to ki\r\n√h σi\r\n· Cw2 published bits for round i + 2. We have\r\nto choose tilarge enough to guarantee that this number of published bits is small enough\r\ncompared to the number of subproblems in round i + 2. To apply Lemma 9.3 in round\r\ni + 2, the number of published bits must be at most 1\r\nC\r\n(\r\nαi+1\r\nh\r\n)\r\n3ki+1 =\r\nα\r\n3\r\ni\r\nti\r\n64Ch6 ki\r\n. Hence we\r\nmust set ti ≥\r\n√h σi\r\n· 64C\r\n2w2h6\r\n(\r\n1\r\nαi\r\n)\r\n3\r\n. Assume for now that T = O(lg `). Using h ≤ `, and\r\nαi ≥ (4h)\r\n−T ≥ 2O(lg2`)\r\n, we conclude it is enough to set:\r\n(∀)i : ti ≥ h\r\nr n\r\nki\r\n· 2\r\na/h\r\n· w\r\n2\r\n· 2\r\nΘ(lg2`)\r\n(9.1)\r\nNow we discuss the conclusion reached at the end of the T rounds. We intend to apply\r\nLemma 9.6 to deduce that the algorithm after T stages cannot make zero cell probes, implying\r\nthat the original algorithm had to make more than T probes. Above we made sure that we\r\nafter T rounds had 1\r\nC\r\n(\r\nαT\r\nh\r\n)\r\n3kT < αT kT published bits, which are few enough compared to the\r\nnumber kT of subproblems. The remaining conditions of Lemma 9.6 are:\r\n`T ≥ 1 and n\r\nkT\r\n≥ 1 (9.2)\r\nSince `i+1 ≤\r\n`i\r\n2\r\n, this condition entails T = O(lg `), as assumed earlier.\r\nLemma 9.7. With the above parameters satisfying (9.1) and (9.2), for i = 0, . . . , T, there is\r\na distribution Di for P(\r\nn\r\nki\r\n, `i) so that no solution for Lki P(\r\nn\r\nki\r\n, `i) can have accept probability\r\nαi over Lki Di using n · 2\r\na\r\nspace, 1\r\nC\r\n(\r\nαi\r\nh\r\n)\r\n3ki published bits, and T − i cell probes.\r\n122",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/bcbb9ab7-7598-4ed2-9cf2-dfcf434d8a67.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d647c4529136a184055fc97ba44c56abe5990529a4203e174fee446346da2897",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 663
      },
      {
        "segments": [
          {
            "segment_id": "f81a348c-f99d-4dd4-bb2e-5c8863fbcc0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 123,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. The proof is by induction over T − i. A distribution that defies a good solution as in\r\nthe lemma is called difficult. In the base case i = T, the space doesn’t matter, and we get\r\nthe difficult distribution directly from (9.2) and Lemma 9.6. Inductively, we use a difficult\r\ndistribution Di to construct a difficult distribution Di−1.\r\nRecall that ki = ki−1ti−1. Given our difficult distribution Di, we use the problem am\u0002plification in Lemma 9.5, to construct a distribution D\r\n∗ti−1\r\ni\r\nfor P(\r\nn\r\nki\r\n· ti−1, `i + lg ti−1) =\r\nP(\r\nn\r\nki−1\r\n, `i + lg ti−1), which guarantees that no solution for Lki−1 P(\r\nn\r\nki−1\r\n, `i + lg ti−1) can have\r\naccept probability αi over Lki−1 D\r\n∗ti−1\r\ni using n · 2\r\na\r\nspace, 1\r\nC\r\n(\r\nαi\r\nh\r\n)\r\n3ki published bits, and T − i\r\ncell probes.\r\nRecall that (9.1) implies ki−1\r\n√h σi−1 · Cw2 ≤\r\n1\r\nC\r\n(\r\nαi\r\nh\r\n)\r\n3ki\r\n, hence that ki−1\r\n√h σi−1 is less than\r\nthe number of bits allowed published for our difficult distribution D\r\n∗ti−1\r\ni\r\n. Also, recall that\r\nσjkj = n · 2\r\na\r\nfor all j. We can therefore use the cell probe elimination in Lemma 9.3,\r\nto construct a distribution \u0010D\r\n∗ti−1\r\ni\r\n\u0011(h)\r\nfor P(\r\nn\r\nki−1\r\n, `i + lg ti−1)\r\n(h)\r\nso that no solution for\r\nLki−1 P(\r\nn\r\nki−1\r\n, `i + lg ti−1)\r\n(h)\r\ncan have accept probability αi−1 ≥ hαi over Lki−1\r\n\u0010\r\nD\r\n∗ti−1\r\ni\r\n\u0011(h)\r\nusing n · 2\r\na\r\nspace, 1\r\nC\r\n(\r\nαi−1\r\nh\r\n)\r\n3ki−1 published bits, and T − i + 1 cell probes. Finally, us\u0002ing Lemma 9.4, we use \u0010\r\nD\r\n∗ti−1\r\ni\r\n\u0011(h)\r\nto construct the desired difficult distribution Di−1 for\r\nP(\r\nn\r\nki−1\r\n, h(`i + lg ti−1)) = P(\r\nn\r\nki−1\r\n, `i−1).\r\nWe now show how to choose h and tiin order to maximize the lower bound T, under the\r\nconditions of (9.1) and (9.2). We only consider the case ` = w = γ lg n, for constant γ ≥ 3.\r\nIn this case, it is enough to set h = 2 and ti = ( n\r\nki\r\n)\r\n3/4\r\n. Then, n\r\nki+1\r\n= ( n\r\nki\r\n)\r\n1/4\r\n, so lg n\r\nki\r\n= 4−ilg n\r\nand lg ti =\r\n3\r\n4\r\n4\r\n−i\r\nlg n. By our recursion for `i, we have `i+1 =\r\n`i\r\n2 −\r\n3\r\n4\r\n4\r\n−i\r\nlg n. Given `0 ≥ 3 lg n,\r\nit can be seen by induction that `i ≥ 3 · 4\r\n−i\r\nlg n. Indeed, `i+1 ≥ 3 · 4\r\n−i\r\n·\r\n1\r\n2\r\nlg n −\r\n3\r\n4\r\n4\r\n−i\r\nlg n ≥\r\n3 · 4\r\n−(i+1) lg n. By the above, (9.2) is satisfied for T ≤ Θ(lg lg n). Finally, note that condition\r\n(9.1) is equivalent to:\r\nlg ti ≥\r\n1\r\nh\r\nlg n\r\nki\r\n+\r\na\r\nh\r\n+ Θ(lg w + lg2`)\r\n⇔\r\n3\r\n4\r\n4\r\n−i\r\nlg n ≥\r\n1\r\n2\r\n4\r\n−i\r\nlg n +\r\na\r\n2\r\n+ Θ(lg2lg n)\r\n⇔ T ≤ Θ\r\n\u0012\r\nlg min \u001a\r\nlg n\r\na\r\n,\r\nlg n\r\nlg2lg n\r\n\u001b\u0013 = Θ \u0012\r\nlg lg n\r\na\r\n\u0013\r\nSince (9.1) and (9.2) are satisfied, we can apply Lemma 9.7 with i = 0 and the initial\r\nparameters `0 = w, α0 = 1, k0 = 1. We conclude that there is a difficult distribution D0 for\r\nP(n, `) with no solution getting accept probability 1 using n · 2\r\na\r\nspace, 0 published bits, and\r\nT cell probes. Thus we have proved:\r\nTheorem 9.8. In any solution to the static colored predecessor problem on n `-bit keys, if\r\n` = γ lg n for constant γ ≥ 3, and we are allowed n · 2\r\na\r\nspace, then there are data instances\r\nfor which some queries take Ω\r\n\r\nlg lg n\r\na\r\n\u0001\r\ncell probes.\r\n123",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f81a348c-f99d-4dd4-bb2e-5c8863fbcc0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6efb27329a33c1aa7686ad0bc3df354e7401d2c4d5b25983f88e82087b1448cf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 667
      },
      {
        "segments": [
          {
            "segment_id": "f81a348c-f99d-4dd4-bb2e-5c8863fbcc0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 123,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. The proof is by induction over T − i. A distribution that defies a good solution as in\r\nthe lemma is called difficult. In the base case i = T, the space doesn’t matter, and we get\r\nthe difficult distribution directly from (9.2) and Lemma 9.6. Inductively, we use a difficult\r\ndistribution Di to construct a difficult distribution Di−1.\r\nRecall that ki = ki−1ti−1. Given our difficult distribution Di, we use the problem am\u0002plification in Lemma 9.5, to construct a distribution D\r\n∗ti−1\r\ni\r\nfor P(\r\nn\r\nki\r\n· ti−1, `i + lg ti−1) =\r\nP(\r\nn\r\nki−1\r\n, `i + lg ti−1), which guarantees that no solution for Lki−1 P(\r\nn\r\nki−1\r\n, `i + lg ti−1) can have\r\naccept probability αi over Lki−1 D\r\n∗ti−1\r\ni using n · 2\r\na\r\nspace, 1\r\nC\r\n(\r\nαi\r\nh\r\n)\r\n3ki published bits, and T − i\r\ncell probes.\r\nRecall that (9.1) implies ki−1\r\n√h σi−1 · Cw2 ≤\r\n1\r\nC\r\n(\r\nαi\r\nh\r\n)\r\n3ki\r\n, hence that ki−1\r\n√h σi−1 is less than\r\nthe number of bits allowed published for our difficult distribution D\r\n∗ti−1\r\ni\r\n. Also, recall that\r\nσjkj = n · 2\r\na\r\nfor all j. We can therefore use the cell probe elimination in Lemma 9.3,\r\nto construct a distribution \u0010D\r\n∗ti−1\r\ni\r\n\u0011(h)\r\nfor P(\r\nn\r\nki−1\r\n, `i + lg ti−1)\r\n(h)\r\nso that no solution for\r\nLki−1 P(\r\nn\r\nki−1\r\n, `i + lg ti−1)\r\n(h)\r\ncan have accept probability αi−1 ≥ hαi over Lki−1\r\n\u0010\r\nD\r\n∗ti−1\r\ni\r\n\u0011(h)\r\nusing n · 2\r\na\r\nspace, 1\r\nC\r\n(\r\nαi−1\r\nh\r\n)\r\n3ki−1 published bits, and T − i + 1 cell probes. Finally, us\u0002ing Lemma 9.4, we use \u0010\r\nD\r\n∗ti−1\r\ni\r\n\u0011(h)\r\nto construct the desired difficult distribution Di−1 for\r\nP(\r\nn\r\nki−1\r\n, h(`i + lg ti−1)) = P(\r\nn\r\nki−1\r\n, `i−1).\r\nWe now show how to choose h and tiin order to maximize the lower bound T, under the\r\nconditions of (9.1) and (9.2). We only consider the case ` = w = γ lg n, for constant γ ≥ 3.\r\nIn this case, it is enough to set h = 2 and ti = ( n\r\nki\r\n)\r\n3/4\r\n. Then, n\r\nki+1\r\n= ( n\r\nki\r\n)\r\n1/4\r\n, so lg n\r\nki\r\n= 4−ilg n\r\nand lg ti =\r\n3\r\n4\r\n4\r\n−i\r\nlg n. By our recursion for `i, we have `i+1 =\r\n`i\r\n2 −\r\n3\r\n4\r\n4\r\n−i\r\nlg n. Given `0 ≥ 3 lg n,\r\nit can be seen by induction that `i ≥ 3 · 4\r\n−i\r\nlg n. Indeed, `i+1 ≥ 3 · 4\r\n−i\r\n·\r\n1\r\n2\r\nlg n −\r\n3\r\n4\r\n4\r\n−i\r\nlg n ≥\r\n3 · 4\r\n−(i+1) lg n. By the above, (9.2) is satisfied for T ≤ Θ(lg lg n). Finally, note that condition\r\n(9.1) is equivalent to:\r\nlg ti ≥\r\n1\r\nh\r\nlg n\r\nki\r\n+\r\na\r\nh\r\n+ Θ(lg w + lg2`)\r\n⇔\r\n3\r\n4\r\n4\r\n−i\r\nlg n ≥\r\n1\r\n2\r\n4\r\n−i\r\nlg n +\r\na\r\n2\r\n+ Θ(lg2lg n)\r\n⇔ T ≤ Θ\r\n\u0012\r\nlg min \u001a\r\nlg n\r\na\r\n,\r\nlg n\r\nlg2lg n\r\n\u001b\u0013 = Θ \u0012\r\nlg lg n\r\na\r\n\u0013\r\nSince (9.1) and (9.2) are satisfied, we can apply Lemma 9.7 with i = 0 and the initial\r\nparameters `0 = w, α0 = 1, k0 = 1. We conclude that there is a difficult distribution D0 for\r\nP(n, `) with no solution getting accept probability 1 using n · 2\r\na\r\nspace, 0 published bits, and\r\nT cell probes. Thus we have proved:\r\nTheorem 9.8. In any solution to the static colored predecessor problem on n `-bit keys, if\r\n` = γ lg n for constant γ ≥ 3, and we are allowed n · 2\r\na\r\nspace, then there are data instances\r\nfor which some queries take Ω\r\n\r\nlg lg n\r\na\r\n\u0001\r\ncell probes.\r\n123",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f81a348c-f99d-4dd4-bb2e-5c8863fbcc0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6efb27329a33c1aa7686ad0bc3df354e7401d2c4d5b25983f88e82087b1448cf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 667
      },
      {
        "segments": [
          {
            "segment_id": "1284419e-9bea-4f7c-b1d9-68f1a669973c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 124,
            "page_width": 612,
            "page_height": 792,
            "content": "9.3 Proof of Cell-Probe Elimination\r\nWe assume a solution to Lkf\r\n(h)\r\n, and use it to construct a solution to Lkf. The new\r\nsolution uses the query algorithm of the old solution, but skips the first cell probe made by\r\nthis algorithm. A central component of our construction is a structural property about any\r\nquery algorithm for Lk\r\nf\r\n(h) with the input distribution Lk D(h)\r\n. We now define and claim\r\nthis property. In §9.3.1 uses it to construct a solution for Lkf, while §9.3.2 gives the proof.\r\nWe first introduce some convenient notation. Remember that the data structure’s input\r\nfor Lk\r\nf\r\n(h)\r\nconsists of a vector (d\r\n1\r\n, . . . , dk) ∈ Dk, a vector selecting the interesting segments\r\n(r\r\n1\r\n, . . . , rk) ∈ [h]\r\nk and the query prefixes Qi\r\nj\r\nfor all j ∈ [r\r\ni − 1]. Denote by d, r and\r\nQ the random variables giving these three components of the input. Also let p be the\r\nrandom variable representing the bits published by the data structure. Note that p can\r\nalso be understood as a function p(d, r, Q). The query consists of an index i selecting the\r\ninteresting subproblem, and a vector (q1, . . . , qh) with a query to that subproblem. Denote\r\nby i and q these random variables. Note that in our probability space Lk\r\nf\r\n(h)\r\n, we have\r\nqj = Qi\r\nj\r\n,(∀)j < r\r\ni\r\n.\r\nFix some instance p of the published bits and a subproblem index i ∈ [k]. Consider a\r\nprefix (q1, . . . , qj ) for a query to this subproblem. Depending on qj+1, . . . , qh, the query algo\u0002rithm might begin by probing different cells, or might reject the query. Let Γi\r\n(p; q1, . . . , qj )\r\nbe the set of cells that could be inspected by the first cell probe. Note that this set could be\r\n∅, if all queries are rejected.\r\nNow define:\r\nδ\r\ni\r\n(p) = (\r\n0, iff Γi(p; Qi) = ∅\r\nPr h|Γ\r\ni\r\n(p; q1, . . . , qr\r\ni )| ≥ min{σ,|Γ\r\ni\r\n(p;Qi)|}\r\n√h σ\r\n| i = i\r\ni\r\notherwise (9.3)\r\nThe probability space is that defined by Lk D(h) when the query is to subproblem i. In\r\nparticular, such a query will satisfy qj = Qi\r\nj\r\n,(∀)j < r\r\ni\r\n, because the prefix is known to the\r\ndata structure. Note that this definition completely ignores the suffix qr\r\ni+1, . . . , qh of the\r\nquery. The intuition behind this is that for any choice of the suffix, the correct answer to the\r\nquery is the same, so this suffix can be “manufactured” at will. Indeed, an arbitrary choice\r\nof the suffix is buried in the definition of Γi.\r\nWith these observations, it is easier to understand (9.3). If the data structure knows\r\nthat no query to subproblem i will be accepted, δi = 0. Otherwise, we compare two sets of\r\ncells. The first contains the cells that the querier might probe given what the data structure\r\nknows: Γi(p, Qi) contains all cells that could be probed for various q\r\ni\r\nr\r\ni and various suffixes.\r\nThe second contains the cells that the querier could choose to probe considering its given\r\ninput q\r\ni\r\nr\r\ni (the querier is only free to choose the suffix). Obviously, the second set is a subset\r\nof the first. The good case, whose probability is measured by δi, is when it is a rather large\r\nsubset, or at least large compared to σ.\r\nFor convenience, we define δ\r\n∗\r\n(p) = Ei[δ\r\ni\r\n(p)] = 1\r\nk\r\nP\r\ni\r\nδ\r\ni\r\n(p). Using standard notation from\r\nprobability theory, we write δ\r\ni\r\n(p | E), when we condition on some event E in the probability\r\n124",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/1284419e-9bea-4f7c-b1d9-68f1a669973c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71392699a1bde7effa2dfefd9b3c6898511619d807e3b44f3af93c0d662d477b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 652
      },
      {
        "segments": [
          {
            "segment_id": "1284419e-9bea-4f7c-b1d9-68f1a669973c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 124,
            "page_width": 612,
            "page_height": 792,
            "content": "9.3 Proof of Cell-Probe Elimination\r\nWe assume a solution to Lkf\r\n(h)\r\n, and use it to construct a solution to Lkf. The new\r\nsolution uses the query algorithm of the old solution, but skips the first cell probe made by\r\nthis algorithm. A central component of our construction is a structural property about any\r\nquery algorithm for Lk\r\nf\r\n(h) with the input distribution Lk D(h)\r\n. We now define and claim\r\nthis property. In §9.3.1 uses it to construct a solution for Lkf, while §9.3.2 gives the proof.\r\nWe first introduce some convenient notation. Remember that the data structure’s input\r\nfor Lk\r\nf\r\n(h)\r\nconsists of a vector (d\r\n1\r\n, . . . , dk) ∈ Dk, a vector selecting the interesting segments\r\n(r\r\n1\r\n, . . . , rk) ∈ [h]\r\nk and the query prefixes Qi\r\nj\r\nfor all j ∈ [r\r\ni − 1]. Denote by d, r and\r\nQ the random variables giving these three components of the input. Also let p be the\r\nrandom variable representing the bits published by the data structure. Note that p can\r\nalso be understood as a function p(d, r, Q). The query consists of an index i selecting the\r\ninteresting subproblem, and a vector (q1, . . . , qh) with a query to that subproblem. Denote\r\nby i and q these random variables. Note that in our probability space Lk\r\nf\r\n(h)\r\n, we have\r\nqj = Qi\r\nj\r\n,(∀)j < r\r\ni\r\n.\r\nFix some instance p of the published bits and a subproblem index i ∈ [k]. Consider a\r\nprefix (q1, . . . , qj ) for a query to this subproblem. Depending on qj+1, . . . , qh, the query algo\u0002rithm might begin by probing different cells, or might reject the query. Let Γi\r\n(p; q1, . . . , qj )\r\nbe the set of cells that could be inspected by the first cell probe. Note that this set could be\r\n∅, if all queries are rejected.\r\nNow define:\r\nδ\r\ni\r\n(p) = (\r\n0, iff Γi(p; Qi) = ∅\r\nPr h|Γ\r\ni\r\n(p; q1, . . . , qr\r\ni )| ≥ min{σ,|Γ\r\ni\r\n(p;Qi)|}\r\n√h σ\r\n| i = i\r\ni\r\notherwise (9.3)\r\nThe probability space is that defined by Lk D(h) when the query is to subproblem i. In\r\nparticular, such a query will satisfy qj = Qi\r\nj\r\n,(∀)j < r\r\ni\r\n, because the prefix is known to the\r\ndata structure. Note that this definition completely ignores the suffix qr\r\ni+1, . . . , qh of the\r\nquery. The intuition behind this is that for any choice of the suffix, the correct answer to the\r\nquery is the same, so this suffix can be “manufactured” at will. Indeed, an arbitrary choice\r\nof the suffix is buried in the definition of Γi.\r\nWith these observations, it is easier to understand (9.3). If the data structure knows\r\nthat no query to subproblem i will be accepted, δi = 0. Otherwise, we compare two sets of\r\ncells. The first contains the cells that the querier might probe given what the data structure\r\nknows: Γi(p, Qi) contains all cells that could be probed for various q\r\ni\r\nr\r\ni and various suffixes.\r\nThe second contains the cells that the querier could choose to probe considering its given\r\ninput q\r\ni\r\nr\r\ni (the querier is only free to choose the suffix). Obviously, the second set is a subset\r\nof the first. The good case, whose probability is measured by δi, is when it is a rather large\r\nsubset, or at least large compared to σ.\r\nFor convenience, we define δ\r\n∗\r\n(p) = Ei[δ\r\ni\r\n(p)] = 1\r\nk\r\nP\r\ni\r\nδ\r\ni\r\n(p). Using standard notation from\r\nprobability theory, we write δ\r\ni\r\n(p | E), when we condition on some event E in the probability\r\n124",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/1284419e-9bea-4f7c-b1d9-68f1a669973c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71392699a1bde7effa2dfefd9b3c6898511619d807e3b44f3af93c0d662d477b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 652
      },
      {
        "segments": [
          {
            "segment_id": "7a1cefa8-26b8-4b81-943e-e629442b4fe2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 125,
            "page_width": 612,
            "page_height": 792,
            "content": "of (9.3). We also write δ\r\ni\r\n(p | X) when we condition on some random variable X, i.e. δ\r\ni\r\n(p | X)\r\nis a function x 7→ δ\r\ni\r\n(p | X = x). We are now ready to state our claim, to be proven in §9.3.2.\r\nLemma 9.9. There exist r and Q, such that:\r\nEd[δ\r\n∗\r\n(p(r, Q, d) | r = r, Q = Q, d)] ≥\r\nα\r\n2h\r\n9.3.1 The Solution for Lkf\r\nAs mentioned before, we use the solution for Lk\r\nf\r\n(h)\r\n, and try to skip the first cell probe. To\r\nuse this strategy, we need to extend an instance of Lkf to an instance of Lkf\r\n(h)\r\n. This is\r\ndone using the r and Q values whose existence is guaranteed by Lemma 9.9. The extended\r\ndata structure’s input consists of the vector (d\r\n1\r\n, . . . , dk) given to Lkf, and the vectors r\r\nand Q. A query’s input for Lk\r\nf is a problem index i ∈ [k] and a q ∈ Q. We extend this to\r\n(q1, . . . , qh) by letting qj = Qi\r\nj\r\n,(∀)j < r\r\ni\r\n, and qr\r\ni = q, and manufacturing a suffix qr\r\ni+1, . . . , qh\r\nas described below.\r\nFirst note that extending an input of Lkf to an input of Lkf\r\n(h) by this strategy\r\npreserves the desired answer to a query (in particular, the suffix is irrelevant to the answer).\r\nAlso, this transformation is well defined because r and Q are “constants”, defined by the\r\ninput distribution Lk D(h). Since our model is nonuniform, we only care about the existence\r\nof r and Q, and not about computational aspects.\r\nTo fully describe a solution to Lkf, we must specify how to obtain the data structure’s\r\nrepresentation and the published bits, and how the query algorithm works. The data struc\u0002ture’s representation is identical to the representation for Lk\r\nf\r\n(h)\r\n, given the extended input.\r\nThe published bits for Lkf consist of the published bits for Lkf\r\n(h)\r\n, plus a number of\r\npublished cells from the data structure’s representation. Which cells are published will be\r\ndetailed below. We publish the cell address together with its contents, so that the query\r\nalgorithm can tell whether a particular cell is available.\r\nThe query algorithm is now simple to describe. Remember that q1, . . . , qr\r\ni−1 are pre\u0002scribed by Q, and qr\r\ni = q is the original input of Lk\r\nf. We now iterate through all possible\r\nquery suffixes. For each possibility, we simulate the extended query using the algorithm\r\nfor Lkf\r\n(h)\r\n. If this algorithm rejects the query, or the first probed cell is not among the\r\npublished cells, we continue trying suffixes. Otherwise, we stop, obtain the value for the\r\nfirst cell probe from the published cells and continue to simulate this query using actual cell\r\nprobes. If we don’t find any good suffix, we reject the query. It is essential that we can\r\nrecognize accepts in the old algorithm by looking just at published bits. Then, searching for\r\na suffix that would not be rejected is free, as it does not involve any cell probes.\r\nPublishing Cells\r\nIt remains to describe which cells the data structure chooses to publish, in order to make\r\nL\r\nthe query algorithm accept with the desired probability. Let p be the bits published by the\r\nk\r\nf\r\n(h)\r\nsolution. Note that in order for query (i, q) to be accepted, we must publish one\r\n125",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7a1cefa8-26b8-4b81-943e-e629442b4fe2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f2457a923b9c2f6eae44a4443f827e79c2e9037497de97aaebd8bebc9e671426",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 596
      },
      {
        "segments": [
          {
            "segment_id": "7a1cefa8-26b8-4b81-943e-e629442b4fe2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 125,
            "page_width": 612,
            "page_height": 792,
            "content": "of (9.3). We also write δ\r\ni\r\n(p | X) when we condition on some random variable X, i.e. δ\r\ni\r\n(p | X)\r\nis a function x 7→ δ\r\ni\r\n(p | X = x). We are now ready to state our claim, to be proven in §9.3.2.\r\nLemma 9.9. There exist r and Q, such that:\r\nEd[δ\r\n∗\r\n(p(r, Q, d) | r = r, Q = Q, d)] ≥\r\nα\r\n2h\r\n9.3.1 The Solution for Lkf\r\nAs mentioned before, we use the solution for Lk\r\nf\r\n(h)\r\n, and try to skip the first cell probe. To\r\nuse this strategy, we need to extend an instance of Lkf to an instance of Lkf\r\n(h)\r\n. This is\r\ndone using the r and Q values whose existence is guaranteed by Lemma 9.9. The extended\r\ndata structure’s input consists of the vector (d\r\n1\r\n, . . . , dk) given to Lkf, and the vectors r\r\nand Q. A query’s input for Lk\r\nf is a problem index i ∈ [k] and a q ∈ Q. We extend this to\r\n(q1, . . . , qh) by letting qj = Qi\r\nj\r\n,(∀)j < r\r\ni\r\n, and qr\r\ni = q, and manufacturing a suffix qr\r\ni+1, . . . , qh\r\nas described below.\r\nFirst note that extending an input of Lkf to an input of Lkf\r\n(h) by this strategy\r\npreserves the desired answer to a query (in particular, the suffix is irrelevant to the answer).\r\nAlso, this transformation is well defined because r and Q are “constants”, defined by the\r\ninput distribution Lk D(h). Since our model is nonuniform, we only care about the existence\r\nof r and Q, and not about computational aspects.\r\nTo fully describe a solution to Lkf, we must specify how to obtain the data structure’s\r\nrepresentation and the published bits, and how the query algorithm works. The data struc\u0002ture’s representation is identical to the representation for Lk\r\nf\r\n(h)\r\n, given the extended input.\r\nThe published bits for Lkf consist of the published bits for Lkf\r\n(h)\r\n, plus a number of\r\npublished cells from the data structure’s representation. Which cells are published will be\r\ndetailed below. We publish the cell address together with its contents, so that the query\r\nalgorithm can tell whether a particular cell is available.\r\nThe query algorithm is now simple to describe. Remember that q1, . . . , qr\r\ni−1 are pre\u0002scribed by Q, and qr\r\ni = q is the original input of Lk\r\nf. We now iterate through all possible\r\nquery suffixes. For each possibility, we simulate the extended query using the algorithm\r\nfor Lkf\r\n(h)\r\n. If this algorithm rejects the query, or the first probed cell is not among the\r\npublished cells, we continue trying suffixes. Otherwise, we stop, obtain the value for the\r\nfirst cell probe from the published cells and continue to simulate this query using actual cell\r\nprobes. If we don’t find any good suffix, we reject the query. It is essential that we can\r\nrecognize accepts in the old algorithm by looking just at published bits. Then, searching for\r\na suffix that would not be rejected is free, as it does not involve any cell probes.\r\nPublishing Cells\r\nIt remains to describe which cells the data structure chooses to publish, in order to make\r\nL\r\nthe query algorithm accept with the desired probability. Let p be the bits published by the\r\nk\r\nf\r\n(h)\r\nsolution. Note that in order for query (i, q) to be accepted, we must publish one\r\n125",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7a1cefa8-26b8-4b81-943e-e629442b4fe2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f2457a923b9c2f6eae44a4443f827e79c2e9037497de97aaebd8bebc9e671426",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 596
      },
      {
        "segments": [
          {
            "segment_id": "f3fe5583-ade7-4596-a82e-1cb0d4cd4813",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 126,
            "page_width": 612,
            "page_height": 792,
            "content": "cell from Γi\r\n(p; Qi, q). Here, we slightly abuse notation by letting Qi, q denote the r\r\ni\r\nentries\r\nof the prefix Qi, followed by q. We will be able to achieve this for all (i, q) satisfying:\r\nΓ\r\ni\r\n(p; Qi) 6= ∅, |Γ\r\ni\r\n(p; Qi, q)| ≥ min{σ, |Γ\r\ni\r\n(p; Qi)|}\r\n√h σ\r\n(9.4)\r\nComparing to (9.3), this means the accept probability is at least δ\r\n∗\r\n(p | r = r, Q = Q, d =\r\n(d1, . . . , dk)). Then on average over possible inputs (d1, . . . , dk) to Lkf, the accept proba\u0002bility will be at least α\r\n2h\r\n, as guaranteed by Lemma 9.9.\r\nWe will need the following standard result:\r\nLemma 9.10. Consider a universe U 6= ∅ and a family of sets F such that (∀)S ∈ F\r\nwe have S ⊂ U and |S| ≥ |U|\r\nB\r\n. Then there exists a set T ⊂ U, |T| ≤ B ln |F| such that\r\n(∀)S ∈ F, S ∩ T 6= ∅.\r\nProof. Choose B ln |F| elements of U with replacement. For a fixed S ∈ F, an element is\r\noutside S with probability at most 1 −\r\n1\r\nB\r\n. The probability all elements are outside S is at\r\nmost (1 −\r\n1\r\nB\r\n)\r\nB ln |F| < e− ln |F| <\r\n1\r\n|F| . By the union bound, all sets in F are hit at least once\r\nwith positive probability, so a good T exists.\r\nWe distinguish three types of subproblems, parallel to (9.4). If Γi(p; Qi) = ∅, we make\r\nno claim (the accept probability can be zero). Otherwise, if |Γ\r\ni\r\n(p; Qi)| < σ, we handle\r\nsubproblem i using a local strategy. Consider all q such that |Γ\r\ni\r\n(p; Qi, q)| ≥ |Γ\r\ni\r\n(p;Qi)|\r\n√h σ\r\n. We now\r\napply Lemma 9.10 with the universe Γi(p; Qi) and the family Γi(p; Qi, q), for all interesting\r\nq’s. There are at most 2w choices of q, bounding the size of the family. Then, the lemma\r\nguarantees that the data structure can publish a set of O(\r\n√h σ · w) cells which contains at\r\nleast one cell from each interesting set. This means that each interesting q can be handled\r\nby the algorithm.\r\nWe handle the third type of subproblems, namely those with |Γ\r\ni\r\n(p; Qi)| ≥ σ, in a global\r\nfashion. Consider all “interesting” pairs (i, q) with |Γ\r\ni\r\n(p; Qi, q)| ≥ σ\r\n1−1/h. We now apply\r\nLemma 9.10 with the universe consisting of all kσ cells, and the family being Γi(p; Qi, q),\r\nfor interesting (i, q). The cardinality of the family is at most 2w, since i and q form a query,\r\nwhich takes at most one word. Then by Lemma 9.10, the data structure can publish a set\r\nof O(k\r\n√h σ · w) cells, which contains at least one cell from each interesting set. With these\r\ncells, the algorithm can handle all interesting (i, q) queries.\r\nThe total number of cells that we publish is O(k\r\n√h σ · w). Thus, we publish O(k\r\n√h σ · w\r\n2\r\n)\r\nnew bits, plus O(k) bits from the assumed solution to Lkf\r\n(h)\r\n. For big enough C, this is at\r\nmost k\r\n√h σ · Cw2\r\n.\r\n9.3.2 An Analysis of Lkf\r\n(h)\r\n: Proof of Lemma 9.9\r\nOur analysis has two parts. First, we ignore the help given by the published bits, by assuming\r\nthey are constantly set to some value p. As r\r\ni and Qi are chosen randomly, we show that\r\n126",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f3fe5583-ade7-4596-a82e-1cb0d4cd4813.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=96e3dd72a06a917c38f81d241b69f506860bfdd4af19d94fc738321064dd14e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 606
      },
      {
        "segments": [
          {
            "segment_id": "f3fe5583-ade7-4596-a82e-1cb0d4cd4813",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 126,
            "page_width": 612,
            "page_height": 792,
            "content": "cell from Γi\r\n(p; Qi, q). Here, we slightly abuse notation by letting Qi, q denote the r\r\ni\r\nentries\r\nof the prefix Qi, followed by q. We will be able to achieve this for all (i, q) satisfying:\r\nΓ\r\ni\r\n(p; Qi) 6= ∅, |Γ\r\ni\r\n(p; Qi, q)| ≥ min{σ, |Γ\r\ni\r\n(p; Qi)|}\r\n√h σ\r\n(9.4)\r\nComparing to (9.3), this means the accept probability is at least δ\r\n∗\r\n(p | r = r, Q = Q, d =\r\n(d1, . . . , dk)). Then on average over possible inputs (d1, . . . , dk) to Lkf, the accept proba\u0002bility will be at least α\r\n2h\r\n, as guaranteed by Lemma 9.9.\r\nWe will need the following standard result:\r\nLemma 9.10. Consider a universe U 6= ∅ and a family of sets F such that (∀)S ∈ F\r\nwe have S ⊂ U and |S| ≥ |U|\r\nB\r\n. Then there exists a set T ⊂ U, |T| ≤ B ln |F| such that\r\n(∀)S ∈ F, S ∩ T 6= ∅.\r\nProof. Choose B ln |F| elements of U with replacement. For a fixed S ∈ F, an element is\r\noutside S with probability at most 1 −\r\n1\r\nB\r\n. The probability all elements are outside S is at\r\nmost (1 −\r\n1\r\nB\r\n)\r\nB ln |F| < e− ln |F| <\r\n1\r\n|F| . By the union bound, all sets in F are hit at least once\r\nwith positive probability, so a good T exists.\r\nWe distinguish three types of subproblems, parallel to (9.4). If Γi(p; Qi) = ∅, we make\r\nno claim (the accept probability can be zero). Otherwise, if |Γ\r\ni\r\n(p; Qi)| < σ, we handle\r\nsubproblem i using a local strategy. Consider all q such that |Γ\r\ni\r\n(p; Qi, q)| ≥ |Γ\r\ni\r\n(p;Qi)|\r\n√h σ\r\n. We now\r\napply Lemma 9.10 with the universe Γi(p; Qi) and the family Γi(p; Qi, q), for all interesting\r\nq’s. There are at most 2w choices of q, bounding the size of the family. Then, the lemma\r\nguarantees that the data structure can publish a set of O(\r\n√h σ · w) cells which contains at\r\nleast one cell from each interesting set. This means that each interesting q can be handled\r\nby the algorithm.\r\nWe handle the third type of subproblems, namely those with |Γ\r\ni\r\n(p; Qi)| ≥ σ, in a global\r\nfashion. Consider all “interesting” pairs (i, q) with |Γ\r\ni\r\n(p; Qi, q)| ≥ σ\r\n1−1/h. We now apply\r\nLemma 9.10 with the universe consisting of all kσ cells, and the family being Γi(p; Qi, q),\r\nfor interesting (i, q). The cardinality of the family is at most 2w, since i and q form a query,\r\nwhich takes at most one word. Then by Lemma 9.10, the data structure can publish a set\r\nof O(k\r\n√h σ · w) cells, which contains at least one cell from each interesting set. With these\r\ncells, the algorithm can handle all interesting (i, q) queries.\r\nThe total number of cells that we publish is O(k\r\n√h σ · w). Thus, we publish O(k\r\n√h σ · w\r\n2\r\n)\r\nnew bits, plus O(k) bits from the assumed solution to Lkf\r\n(h)\r\n. For big enough C, this is at\r\nmost k\r\n√h σ · Cw2\r\n.\r\n9.3.2 An Analysis of Lkf\r\n(h)\r\n: Proof of Lemma 9.9\r\nOur analysis has two parts. First, we ignore the help given by the published bits, by assuming\r\nthey are constantly set to some value p. As r\r\ni and Qi are chosen randomly, we show that\r\n126",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f3fe5583-ade7-4596-a82e-1cb0d4cd4813.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=96e3dd72a06a917c38f81d241b69f506860bfdd4af19d94fc738321064dd14e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 606
      },
      {
        "segments": [
          {
            "segment_id": "339a1f8c-5827-43b2-955d-5bacdc867dfd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 127,
            "page_width": 612,
            "page_height": 792,
            "content": "the conditions of (9.3) are met with probability at least 1\r\nh\r\ntimes the accept probability for\r\nsubproblem i. This is essentially a lower bound on δ\r\ni\r\n, and hence on δ\r\n∗\r\n.\r\nSecondly, we show that the published bits do not really affect this lower bound on δ\r\n∗\r\n. The\r\nintuition is that there are two few published bits (much fewer than k) so for most subproblems\r\nthey are providing no information at all. That is, the behavior for that subproblem is\r\nstatistically close to when the published bits would not be used. Formally, this takes no\r\nmore than a (subtle) application of Chernoff bounds. The gist of the idea is to consider\r\nsome setting p for the published bits, and all possible inputs (not just those leading to p\r\nbeing published). In this probability space, δ\r\ni are independent for different i, so the average\r\nis close to δ\r\n∗ with overwhelmingly high probability. Now pessimistically assume all inputs\r\nwhere the average of δ\r\ni\r\nis not close to δ\r\n∗ are possible inputs, i.e. input for which p would be\r\nthe real published bits. However, the probability of this event is so small, that even after a\r\nunion bound for all p, it is still negligible.\r\nWe now proceed to the first part of the analysis. Let α\r\ni\r\n(p) be the probability that\r\nthe query algorithm accepts when receiving a random query for subproblem i. Formally,\r\nα\r\ni\r\n(p) = Pr[Γi(p; q) 6= ∅ | i = i]. We define α\r\ni\r\n(p | E), αi(p | X) and α\r\n∗\r\n(·) similar to\r\nthe functions associated to δ\r\ni\r\n. Observe that the probability of correctness guaranteed by\r\nassumption is α = Er,Q,d[α\r\n∗\r\n(p(r, Q, d) | r, Q, d)].\r\nLemma 9.11. For any i and p, we have δ\r\ni\r\n(p) ≥\r\nα\r\ni\r\n(p)\r\nh\r\n.\r\nProof. Let us first recall the random experiment defining δ\r\ni\r\n(p). We select a uniformly random\r\nr ∈ [h] and random q1, . . . , qr−1. First we ask whether Γi(p; q1, . . . , qr−1) = ∅. If not, we ask\r\nabout the probability that a random qr is good, in the sense of (9.3). Now let us rephrase\r\nthe probability space as follows: first select q1, . . . , qh at random; then select r ∈ [h] and use\r\njust q1, . . . , qr as above. The probability that the query (q1, . . . , qh) is accepted is precisely\r\nα\r\ni\r\n(p). Let’s assume it doesn’t. Then, for any r, Γi(p; q1, . . . , qr−1) 6= ∅ because there is at\r\nleast one suffix which is accepted. We will now show that there is at least one choice of r\r\nsuch that qr is good when the prefix is q1, . . . , qr−1. When averaged over q1, . . . , qr−1, this\r\ngives a probability of at least α\r\ni\r\n(p)\r\nh\r\nTo show one good r, let φr = min{|Γ\r\ni\r\n(p; q1, . . . , qr−1)|, σ}. Now observe that φ1\r\nφ2\r\n·\r\nφ2\r\nφ3\r\n·\r\n· · · ·\r\nφh−1\r\nφh\r\n=\r\nφ1\r\nφh\r\n≤ φ1 ≤ σ. By the pigeonhole principle, (∃)r :\r\nφr\r\nφr+1\r\n≤ σ\r\n1/h. This implies\r\n|Γ\r\ni\r\n(p; q1, . . . , qr)| ≥ min{σ,|Γ\r\ni\r\n(p;q1,...,qr−1)|\r\n√h σ\r\n, as desired.\r\nNote that if the algorithm uses zero published bits, we are done. Thus, for the rest of the\r\nanalysis we may assume 1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k ≥ 1. We now proceed to the second part of the analysis,\r\nshowing that δ\r\n∗\r\nis close to the lower bound of the previous lemma, even after a union bound\r\nover all possible published bits.\r\nLemma 9.12. With probability at least 1−\r\nα\r\n8h\r\nover random r, Q and d, we have (∀)p : δ\r\n∗\r\n(p |\r\nr, Q, d) ≥\r\nα\r\n∗(p)\r\nh −\r\nα\r\n4h\r\n127",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/339a1f8c-5827-43b2-955d-5bacdc867dfd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fa9d5e81c6676aa4eab7996ad3c2cd3bc9b883fb0b288375175ce697498f701e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 674
      },
      {
        "segments": [
          {
            "segment_id": "339a1f8c-5827-43b2-955d-5bacdc867dfd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 127,
            "page_width": 612,
            "page_height": 792,
            "content": "the conditions of (9.3) are met with probability at least 1\r\nh\r\ntimes the accept probability for\r\nsubproblem i. This is essentially a lower bound on δ\r\ni\r\n, and hence on δ\r\n∗\r\n.\r\nSecondly, we show that the published bits do not really affect this lower bound on δ\r\n∗\r\n. The\r\nintuition is that there are two few published bits (much fewer than k) so for most subproblems\r\nthey are providing no information at all. That is, the behavior for that subproblem is\r\nstatistically close to when the published bits would not be used. Formally, this takes no\r\nmore than a (subtle) application of Chernoff bounds. The gist of the idea is to consider\r\nsome setting p for the published bits, and all possible inputs (not just those leading to p\r\nbeing published). In this probability space, δ\r\ni are independent for different i, so the average\r\nis close to δ\r\n∗ with overwhelmingly high probability. Now pessimistically assume all inputs\r\nwhere the average of δ\r\ni\r\nis not close to δ\r\n∗ are possible inputs, i.e. input for which p would be\r\nthe real published bits. However, the probability of this event is so small, that even after a\r\nunion bound for all p, it is still negligible.\r\nWe now proceed to the first part of the analysis. Let α\r\ni\r\n(p) be the probability that\r\nthe query algorithm accepts when receiving a random query for subproblem i. Formally,\r\nα\r\ni\r\n(p) = Pr[Γi(p; q) 6= ∅ | i = i]. We define α\r\ni\r\n(p | E), αi(p | X) and α\r\n∗\r\n(·) similar to\r\nthe functions associated to δ\r\ni\r\n. Observe that the probability of correctness guaranteed by\r\nassumption is α = Er,Q,d[α\r\n∗\r\n(p(r, Q, d) | r, Q, d)].\r\nLemma 9.11. For any i and p, we have δ\r\ni\r\n(p) ≥\r\nα\r\ni\r\n(p)\r\nh\r\n.\r\nProof. Let us first recall the random experiment defining δ\r\ni\r\n(p). We select a uniformly random\r\nr ∈ [h] and random q1, . . . , qr−1. First we ask whether Γi(p; q1, . . . , qr−1) = ∅. If not, we ask\r\nabout the probability that a random qr is good, in the sense of (9.3). Now let us rephrase\r\nthe probability space as follows: first select q1, . . . , qh at random; then select r ∈ [h] and use\r\njust q1, . . . , qr as above. The probability that the query (q1, . . . , qh) is accepted is precisely\r\nα\r\ni\r\n(p). Let’s assume it doesn’t. Then, for any r, Γi(p; q1, . . . , qr−1) 6= ∅ because there is at\r\nleast one suffix which is accepted. We will now show that there is at least one choice of r\r\nsuch that qr is good when the prefix is q1, . . . , qr−1. When averaged over q1, . . . , qr−1, this\r\ngives a probability of at least α\r\ni\r\n(p)\r\nh\r\nTo show one good r, let φr = min{|Γ\r\ni\r\n(p; q1, . . . , qr−1)|, σ}. Now observe that φ1\r\nφ2\r\n·\r\nφ2\r\nφ3\r\n·\r\n· · · ·\r\nφh−1\r\nφh\r\n=\r\nφ1\r\nφh\r\n≤ φ1 ≤ σ. By the pigeonhole principle, (∃)r :\r\nφr\r\nφr+1\r\n≤ σ\r\n1/h. This implies\r\n|Γ\r\ni\r\n(p; q1, . . . , qr)| ≥ min{σ,|Γ\r\ni\r\n(p;q1,...,qr−1)|\r\n√h σ\r\n, as desired.\r\nNote that if the algorithm uses zero published bits, we are done. Thus, for the rest of the\r\nanalysis we may assume 1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k ≥ 1. We now proceed to the second part of the analysis,\r\nshowing that δ\r\n∗\r\nis close to the lower bound of the previous lemma, even after a union bound\r\nover all possible published bits.\r\nLemma 9.12. With probability at least 1−\r\nα\r\n8h\r\nover random r, Q and d, we have (∀)p : δ\r\n∗\r\n(p |\r\nr, Q, d) ≥\r\nα\r\n∗(p)\r\nh −\r\nα\r\n4h\r\n127",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/339a1f8c-5827-43b2-955d-5bacdc867dfd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fa9d5e81c6676aa4eab7996ad3c2cd3bc9b883fb0b288375175ce697498f701e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 674
      },
      {
        "segments": [
          {
            "segment_id": "385f74ea-527a-4d72-82d1-a8a819e69487",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 128,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. Fix p arbitrarily. By definition, δ\r\n∗\r\n(p | r, Q, d) = 1\r\nk\r\nP\r\ni\r\nδ\r\ni\r\n(p | r, Q, d). By Lemma 9.11,\r\nE[δ\r\ni\r\n(p | r, Q, d)] = δ\r\ni\r\n(p) ≥\r\nα\r\ni\r\n(p)\r\nh\r\n, which implies δ\r\n∗\r\n(p) ≥\r\nα\r\n∗(p)\r\nh\r\n. Thus, our condition can be\r\nrephrased as:\r\n1\r\nk\r\nX\r\ni\r\nδ\r\ni\r\n(p | r, Q, d) ≥ E\r\n\"\r\n1\r\nk\r\nX\r\ni\r\nδ\r\ni\r\n(p | r, Q, d)\r\n#\r\n−\r\nα\r\n4h\r\nNow note that δ\r\ni\r\n(p | r, Q, d) only depends on r\r\ni\r\n, Qi and d\r\ni\r\n, since we are looking at the\r\nbehavior of a query to subproblem i for a fixed value of the published bits; see the definition\r\nof δ\r\ni\r\nin (9.3). Since (r\r\ni\r\n, Qi, d\r\ni\r\n) are independent for different i, it follows that δ\r\ni\r\n(p | r, Q, d) are\r\nalso independent. Then we can apply a Chernoff bound to analyze the mean δ\r\n∗\r\n(p | r, Q, d)\r\nof these independent random variables. We use an additive Chernoff bound [4]:\r\nPr\r\nr,Q,d\r\nh\r\nδ\r\n∗\r\n(p | r, Q, d) < δ∗(p) −\r\nα\r\n4h\r\ni\r\n< e−Ω(k(\r\nα\r\nh\r\n)\r\n2\r\n)\r\nNow we take a union bound over all possible choices p for the published bits. The proba\u0002bility of the bad event becomes 2 1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k\r\ne\r\n−Ω(( α\r\nh\r\n)\r\n2k)\r\n. For large enough C, this is exp(−Ω((α\r\nh\r\n)\r\n2k)),\r\nfor any α and h. Now we use that 1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k ≥ 1, from the condition that there is at lest one\r\npublished bit, so this probability is at most e\r\n−Ω(Ch/α)\r\n. Given that h\r\nα ≥ 1, this is at most α8h\r\nfor large enough C.\r\nUnfortunately, this lemma is not exactly what we would want, since it provides a lower\r\nbound in terms of α\r\n∗\r\n(p). This accept probability is measured in the original probability\r\nspace. As we condition on r, Q and d, the probability space can be quite different. However,\r\nwe show next that in fact α\r\n∗\r\ncannot change too much. As before, the intuition is that\r\nthere are too few published bits, so for most subproblems they are not changing the query\r\ndistribution significantly.\r\nLemma 9.13. With probability at least 1−\r\nα\r\n8\r\nover random r, Q and d, we have: (∀)p : α\r\n∗\r\n(p |\r\nr, Q, d) ≤ α\r\n∗\r\n(p) + α\r\n4\r\nProof. The proof is very similar to that of Lemma 9.12. Fix p arbitrarily. By definition,\r\nα\r\n∗\r\n(p | r, Q, d) is the average of α\r\ni\r\n(p | r, Q, d). Note that for fixed p, α\r\ni depends only on\r\nr\r\ni\r\n, Qi and d\r\ni\r\n. Hence, the α\r\ni values are independent for different i, and we can apply a\r\nChernoff bound to say the mean is close to its expectation. The rest of the calculation is\r\nparallel to that of Lemma 9.12.\r\nWe combine Lemmas 9.12 and 9.13 by a union bound. We conclude that with probability\r\nat least 1 −\r\nα\r\n4\r\nover random r, Q and d, we have that (∀)p:\r\nδ\r\n∗\r\n(p | r, Q, d) ≥\r\nα\r\n∗(p)\r\nh −\r\nα\r\n4h\r\nα\r\n∗\r\n(p | r, Q, d) ≤ α\r\n∗\r\n(p) + α\r\n4\r\n)\r\n⇒ δ\r\n∗\r\n(p | r, Q, d) −\r\nα\r\n∗\r\n(p | r, Q, d)\r\nh\r\n≥ −\r\nα\r\n2h\r\n(9.5)\r\nSince this holds for all p, it also holds for p = p, i.e. the actual bits p(r, Q, d) published\r\nby the data structure given its input. Now we want to take the expectation over r, Q and\r\n128",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/385f74ea-527a-4d72-82d1-a8a819e69487.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=902f6d31e5197be3b326f713087c98c1730d5dde42de6f73896ca7ddf9189559",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 649
      },
      {
        "segments": [
          {
            "segment_id": "385f74ea-527a-4d72-82d1-a8a819e69487",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 128,
            "page_width": 612,
            "page_height": 792,
            "content": "Proof. Fix p arbitrarily. By definition, δ\r\n∗\r\n(p | r, Q, d) = 1\r\nk\r\nP\r\ni\r\nδ\r\ni\r\n(p | r, Q, d). By Lemma 9.11,\r\nE[δ\r\ni\r\n(p | r, Q, d)] = δ\r\ni\r\n(p) ≥\r\nα\r\ni\r\n(p)\r\nh\r\n, which implies δ\r\n∗\r\n(p) ≥\r\nα\r\n∗(p)\r\nh\r\n. Thus, our condition can be\r\nrephrased as:\r\n1\r\nk\r\nX\r\ni\r\nδ\r\ni\r\n(p | r, Q, d) ≥ E\r\n\"\r\n1\r\nk\r\nX\r\ni\r\nδ\r\ni\r\n(p | r, Q, d)\r\n#\r\n−\r\nα\r\n4h\r\nNow note that δ\r\ni\r\n(p | r, Q, d) only depends on r\r\ni\r\n, Qi and d\r\ni\r\n, since we are looking at the\r\nbehavior of a query to subproblem i for a fixed value of the published bits; see the definition\r\nof δ\r\ni\r\nin (9.3). Since (r\r\ni\r\n, Qi, d\r\ni\r\n) are independent for different i, it follows that δ\r\ni\r\n(p | r, Q, d) are\r\nalso independent. Then we can apply a Chernoff bound to analyze the mean δ\r\n∗\r\n(p | r, Q, d)\r\nof these independent random variables. We use an additive Chernoff bound [4]:\r\nPr\r\nr,Q,d\r\nh\r\nδ\r\n∗\r\n(p | r, Q, d) < δ∗(p) −\r\nα\r\n4h\r\ni\r\n< e−Ω(k(\r\nα\r\nh\r\n)\r\n2\r\n)\r\nNow we take a union bound over all possible choices p for the published bits. The proba\u0002bility of the bad event becomes 2 1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k\r\ne\r\n−Ω(( α\r\nh\r\n)\r\n2k)\r\n. For large enough C, this is exp(−Ω((α\r\nh\r\n)\r\n2k)),\r\nfor any α and h. Now we use that 1\r\nC\r\n(\r\nα\r\nh\r\n)\r\n3k ≥ 1, from the condition that there is at lest one\r\npublished bit, so this probability is at most e\r\n−Ω(Ch/α)\r\n. Given that h\r\nα ≥ 1, this is at most α8h\r\nfor large enough C.\r\nUnfortunately, this lemma is not exactly what we would want, since it provides a lower\r\nbound in terms of α\r\n∗\r\n(p). This accept probability is measured in the original probability\r\nspace. As we condition on r, Q and d, the probability space can be quite different. However,\r\nwe show next that in fact α\r\n∗\r\ncannot change too much. As before, the intuition is that\r\nthere are too few published bits, so for most subproblems they are not changing the query\r\ndistribution significantly.\r\nLemma 9.13. With probability at least 1−\r\nα\r\n8\r\nover random r, Q and d, we have: (∀)p : α\r\n∗\r\n(p |\r\nr, Q, d) ≤ α\r\n∗\r\n(p) + α\r\n4\r\nProof. The proof is very similar to that of Lemma 9.12. Fix p arbitrarily. By definition,\r\nα\r\n∗\r\n(p | r, Q, d) is the average of α\r\ni\r\n(p | r, Q, d). Note that for fixed p, α\r\ni depends only on\r\nr\r\ni\r\n, Qi and d\r\ni\r\n. Hence, the α\r\ni values are independent for different i, and we can apply a\r\nChernoff bound to say the mean is close to its expectation. The rest of the calculation is\r\nparallel to that of Lemma 9.12.\r\nWe combine Lemmas 9.12 and 9.13 by a union bound. We conclude that with probability\r\nat least 1 −\r\nα\r\n4\r\nover random r, Q and d, we have that (∀)p:\r\nδ\r\n∗\r\n(p | r, Q, d) ≥\r\nα\r\n∗(p)\r\nh −\r\nα\r\n4h\r\nα\r\n∗\r\n(p | r, Q, d) ≤ α\r\n∗\r\n(p) + α\r\n4\r\n)\r\n⇒ δ\r\n∗\r\n(p | r, Q, d) −\r\nα\r\n∗\r\n(p | r, Q, d)\r\nh\r\n≥ −\r\nα\r\n2h\r\n(9.5)\r\nSince this holds for all p, it also holds for p = p, i.e. the actual bits p(r, Q, d) published\r\nby the data structure given its input. Now we want to take the expectation over r, Q and\r\n128",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/385f74ea-527a-4d72-82d1-a8a819e69487.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=902f6d31e5197be3b326f713087c98c1730d5dde42de6f73896ca7ddf9189559",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 649
      },
      {
        "segments": [
          {
            "segment_id": "124a637b-ee33-4c5b-9616-f78147f100d7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 129,
            "page_width": 612,
            "page_height": 792,
            "content": "d. Because δ\r\n∗\r\n(·), α∗(·) ∈ [0, 1], we have δ\r\n∗\r\n(·) −\r\n1\r\nh\r\nα\r\n∗\r\n(·) ≥ −1\r\nh\r\n. We use this as a pessimistic\r\nestimate for the cases of r, Q and d where (9.5) does not hold. We obtain:\r\nE\r\n\u0014\r\nδ\r\n∗\r\n(p | r, Q, d) −\r\nα\r\n∗\r\n(p | r, Q, d)\r\nh\r\n\u0015\r\n≥ −\r\nα\r\n2h\r\n−\r\nα\r\n4\r\n·\r\n1\r\nh\r\n⇒ E\r\n\u0002\r\nδ\r\n∗\r\n(p | r, Q, d)\r\n\u0003\r\n≥\r\n1\r\nh\r\nE\r\n\u0002\r\nα\r\n∗\r\n(p | r, Q, d)\r\n\u0003\r\n−\r\n3α\r\n4h\r\n=\r\n1\r\nh\r\nα −\r\n3α\r\n4h\r\n=\r\nα\r\n4h\r\n129",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/124a637b-ee33-4c5b-9616-f78147f100d7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fe65efa12cbe6d5ecfa19ab1641008fca75a84000beefeefc38889958a30b72b",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "e9ea0a80-9e7e-4d4d-ab42-7f95f8d108a4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 130,
            "page_width": 612,
            "page_height": 792,
            "content": "130",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/e9ea0a80-9e7e-4d4d-ab42-7f95f8d108a4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1185db74af63f029dd53d68e3c529c57cb150bdc1eded7c1bdeb0ca1139850f2",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "11115e2e-def0-4df4-b4ac-bc47fd2c7109",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 131,
            "page_width": 612,
            "page_height": 792,
            "content": "Bibliography\r\n[1] Yehuda Afek, Anat Bremler-Barr, and Sariel Har-Peled. Routing with a clue.\r\nIEEE/ACM Transactions on Networking, 9(6):693–705, 2001. See also SIGCOMM’99.\r\n[2] Pankaj K. Agarwal. Range searching. In Jacob E. Goodman and Joseph O’Rourke,\r\neditors, Handbook of Discrete and Computational Geometry (2nd edition). Chapman\r\n& Hall/CRC, 2004.\r\n[3] Mikl´os Ajtai. A lower bound for finding predecessors in Yao’s cell probe model. Com\u0002binatorica, 8(3):235–247, 1988.\r\n[4] Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 2nd edition, 2000.\r\n[5] Stephen Alstrup, Amir M. Ben-Amram, and Theis Rauhe. Worst-case and amortised\r\noptimality in union-find. In Proc. 31st ACM Symposium on Theory of Computing\r\n(STOC), pages 499–506, 1999.\r\n[6] Stephen Alstrup, Gerth S. Brodal, Inge Li Gørtz, and Theis Rauhe. Time and space\r\nefficient multi-method dispatching. In Proc. 8th Scandinavian Workshop on Algorithm\r\nTheory (SWAT), pages 20–29, 2002.\r\n[7] Stephen Alstrup, Gerth S. Brodal, and Theis Rauhe. New data structures for orthog\u0002onal range searching. In Proc. 41st IEEE Symposium on Foundations of Computer\r\nScience (FOCS), pages 198–207, 2000.\r\n[8] Stephen Alstrup, Thore Husfeldt, and Theis Rauhe. Marked ancestor problems. In\r\nProc. 39th IEEE Symposium on Foundations of Computer Science (FOCS), pages 534–\r\n543, 1998.\r\n[9] Stephen Alstrup, Thore Husfeldt, and Theis Rauhe. A cell probe lower bound for dy\u0002namic nearest-neighbor searching. In Proc. 12th ACM/SIAM Symposium on Discrete\r\nAlgorithms (SODA), pages 779–780, 2001.\r\n[10] Arne Andersson, Peter Bro Miltersen, Søren Riis, and Mikkel Thorup. Static dictio\u0002naries on AC0 RAMs: Query time Θ(p\r\nlog n/ log log n) is necessary and sufficient.\r\nIn Proc. 37th IEEE Symposium on Foundations of Computer Science (FOCS), pages\r\n441–450, 1996.\r\n131",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/11115e2e-def0-4df4-b4ac-bc47fd2c7109.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=345a2b662353181961da28235c93f45ab2572fdac879dbd22639410917ebf762",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 379
      },
      {
        "segments": [
          {
            "segment_id": "f92e3ab9-7ffb-4160-a513-8339fdb05f84",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 132,
            "page_width": 612,
            "page_height": 792,
            "content": "[11] Arne Andersson, Peter Bro Miltersen, and Mikkel Thorup. Fusion trees can be imple\u0002mented with AC0\r\ninstructions only. Theoretical Computer Science, 215(1-2):337–344,\r\n1999.\r\n[12] Arne Andersson and Mikkel Thorup. Dynamic ordered sets with exponential search\r\ntrees. Journal of the ACM, 54(3), 2007. See also FOCS’96, STOC’00.\r\n[13] Alexandr Andoni, Dorian Croitoru, and Mihai Pˇatra¸scu. Hardness of nearest-neighbor\r\nsearch under l-infinity. In Proc. 49th IEEE Symposium on Foundations of Computer\r\nScience (FOCS), 2008.\r\n[14] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate\r\nnearest neighbor in high dimensions. Communications of the ACM, 51(1):117–122,\r\n2008. See also FOCS’06.\r\n[15] Alexandr Andoni, Piotr Indyk, and Robert Krauthgamer. Overcoming the `1 non\u0002embeddability barrier: Algorithms for product metrics. Manuscript, 2008.\r\n[16] Alexandr Andoni, Piotr Indyk, and Mihai Pˇatra¸scu. On the optimality of the di\u0002mensionality reduction method. In Proc. 47th IEEE Symposium on Foundations of\r\nComputer Science (FOCS), pages 449–458, 2006.\r\n[17] David Applegate, Gruia Calinescu, David S. Johnson, Howard J. Karloff, Katrina\r\nLigett, and Jia Wang. Compressing rectilinear pictures and minimizing access control\r\nlists. In Proc. 18th ACM/SIAM Symposium on Discrete Algorithms (SODA), pages\r\n1066–1075, 2007.\r\n[18] Lars Arge. The buffer tree: A technique for designing batched external data structures.\r\nAlgorithmica, 37(1):1–24, 2003. See also WADS’95.\r\n[19] Ziv Bar-Yossef, T. S. Jayram, Ravi Kumar, and D. Sivakumar. An information statis\u0002tics approach to data stream and communication complexity. Journal of Computer\r\nand System Sciences, 68(4):702–732, 2004. See also FOCS’02.\r\n[20] Omer Barkol and Yuval Rabani. Tighter lower bounds for nearest neighbor search and\r\nrelated problems in the cell probe model. Journal of Computer and System Sciences,\r\n64(4):873–896, 2002. See also STOC’00.\r\n[21] Paul Beame and Faith E. Fich. Optimal bounds for the predecessor problem and\r\nrelated problems. Journal of Computer and System Sciences, 65(1):38–72, 2002. See\r\nalso STOC’99.\r\n[22] Amir M. Ben-Amram and Zvi Galil. A generalization of a lower bound technique due\r\nto Fredman and Saks. Algorithmica, 30(1):34–66, 2001. See also FOCS’91.\r\n[23] Amir M. Ben-Amram and Zvi Galil. Lower bounds for dynamic data structures on\r\nalgebraic RAMs. Algorithmica, 32(3):364–395, 2002. See also FOCS’91.\r\n132",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/f92e3ab9-7ffb-4160-a513-8339fdb05f84.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2dbafa5f50ea6d7d1b4dc0d64f8fde8f6a480175bf7384121ad3c67b9126b9b2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 341
      },
      {
        "segments": [
          {
            "segment_id": "791ad4e2-cc2d-4aeb-b78e-5a66108089fb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 133,
            "page_width": 612,
            "page_height": 792,
            "content": "[24] Michael A. Bender and Martin Farach-Colton. The lca problem revisited. In Proc. 4th\r\nLatin American Theoretical Informatics (LATIN), pages 88–94, 2000.\r\n[25] Allan Borodin, Rafail Ostrovsky, and Yuval Rabani. Lower bounds for high dimensional\r\nnearest neighbor search and related problems. In Proc. 31st ACM Symposium on\r\nTheory of Computing (STOC), pages 312–321, 1999.\r\n[26] Amit Chakrabarti, Bernard Chazelle, Benjamin Gum, and Alexey Lvov. A lower bound\r\non the complexity of approximate nearest-neighbor searching on the hamming cube. In\r\nProc. 31st ACM Symposium on Theory of Computing (STOC), pages 305–311, 1999.\r\n[27] Amit Chakrabarti and Oded Regev. An optimal randomised cell probe lower bound for\r\napproximate nearest neighbour searching. In Proc. 45th IEEE Symposium on Founda\u0002tions of Computer Science (FOCS), pages 473–482, 2004.\r\n[28] Timothy M. Chan. Closest-point problems simplified on the RAM. In Proc. 13th\r\nACM/SIAM Symposium on Discrete Algorithms (SODA), pages 472–473, 2002.\r\n[29] Moses Charikar, Piotr Indyk, and Rina Panigrahy. New algorithms for subset query,\r\npartial match, orthogonal range searching, and related problems. In Proc. 29th In\u0002ternational Colloquium on Automata, Languages and Programming (ICALP), pages\r\n451–462, 2002.\r\n[30] Bernard Chazelle. A functional approach to data structures and its use in multidimen\u0002sional searching. SIAM Journal on Computing, 17:427–462, 1988. See also FOCS’85.\r\n[31] Bernard Chazelle. Lower bounds for orthogonal range searching II. The arithmetic\r\nmodel. Journal of the ACM, 37(3):439–463, 1990. See also FOCS’86.\r\n[32] Bernard Chazelle. Lower bounds for off-line range searching. Discrete & Computational\r\nGeometry, 17(1):53–65, 1997. See also STOC’95.\r\n[33] Richard Cole, Lee-Ad Gottlieb, and Moshe Lewenstein. Dictionary matching and\r\nindexing with errors and don’t cares. In Proc. 36th ACM Symposium on Theory of\r\nComputing (STOC), pages 91–100, 2004.\r\n[34] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. Locality-sensitive\r\nhashing scheme based on p-stable distributions. In Proc. 20th ACM Symposium on\r\nComputational Geometry (SoCG), pages 253–262, 2004.\r\n[35] Mark de Berg, Marc J. van Kreveld, and Jack Snoeyink. Two- and three-dimensional\r\npoint location in rectangular subdivisions. Journal of Algorithms, 18(2):256–277, 1995.\r\nSee also SWAT’92.\r\n[36] Mikael Degermark, Andrej Brodnik, Svante Carlsson, and Stephen Pink. Small for\u0002warding tables for fast routing lookups. In Proc. ACM SIGCOMM, pages 3–14, 1997.\r\n133",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/791ad4e2-cc2d-4aeb-b78e-5a66108089fb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a03f121be18a9a2b6d009304ce88edaa47b4903f4f620057cd47b43bac1ebd69",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 356
      },
      {
        "segments": [
          {
            "segment_id": "95494e14-6f28-4dae-ae92-edc0bceb1cc9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 134,
            "page_width": 612,
            "page_height": 792,
            "content": "[37] Erik D. Demaine, Friedhelm Meyer auf der Heide, Rasmus Pagh, and Mihai Pˇatra¸scu.\r\nDe dictionariis dynamicis pauco spatio utentibus (lat. on dynamic dictionaries using\r\nlittle space). In Proc. Latin American Theoretical Informatics (LATIN), pages 349–\r\n361, 2006.\r\n[38] Paul F. Dietz. Optimal algorithms for list indexing and subset rank. In Proc. 1st\r\nWorkshop on Algorithms and Data Structures (WADS), pages 39–46, 1989.\r\n[39] Martin Dietzfelbinger and Friedhelm Meyer auf der Heide. A new universal class of hash\r\nfunctions and dynamic hashing in real time. In Proc. 17th International Colloquium\r\non Automata, Languages and Programming (ICALP), pages 6–19, 1990.\r\n[40] David Eppstein, Giuseppe F. Italiano, Roberto Tamassia, Robert E. Tarjan, Jeffery R.\r\nWestbrook, and Moti Yung. Maintenance of a minimum spanning forest in a dynamic\r\nplanar graph. Journal of Algorithms, 13(1):33–54, 1992. See also SODA’90.\r\n[41] David Eppstein and S. Muthukrishnan. Internet packet filter management and rectan\u0002gle geometry. In Proc. 12th ACM/SIAM Symposium on Discrete Algorithms (SODA),\r\npages 827–835, 2001.\r\n[42] Ronald Fagin. Combining fuzzy information from multiple systems. Journal of Com\u0002puter and System Sciences, 58(1):83–99, 1999. See also PODS’96, PODS’98.\r\n[43] Martin Farach-Colton and Piotr Indyk. Approximate nearest neighbor algorithms for\r\nhausdorff metrics via embeddings. In Proc. 40th IEEE Symposium on Foundations of\r\nComputer Science (FOCS), pages 171–180, 1999.\r\n[44] Anja Feldmann and S. Muthukrishnan. Tradeoffs for packet classification. In Proc.\r\nIEEE INFOCOM, pages 1193–1202, 2000.\r\n[45] Paolo Ferragina and S. Muthukrishnan. Efficient dynamic method-lookup for object\r\noriented languages. In Proc. 4th European Symposium on Algorithms (ESA), pages\r\n107–120, 1996.\r\n[46] Paolo Ferragina, S. Muthukrishnan, and Mark de Berg. Multi-method dispatching: A\r\ngeometric approach with applications to string matching problems. In Proc. 31st ACM\r\nSymposium on Theory of Computing (STOC), pages 483–491, 1999.\r\n[47] Michael L. Fredman. A lower bound on the complexity of orthogonal range queries.\r\nJournal of the ACM, 28:696–705, 1981.\r\n[48] Michael L. Fredman. The complexity of maintaining an array and computing its partial\r\nsums. Journal of the ACM, 29(1):250–260, 1982.\r\n[49] Michael L. Fredman and Monika Rauch Henzinger. Lower bounds for fully dynamic\r\nconnectivity problems in graphs. Algorithmica, 22(3):351–362, 1998.\r\n134",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/95494e14-6f28-4dae-ae92-edc0bceb1cc9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0e7ece6cedb038d08a00aed4c0d4734bedf0647e76d5c47aaa8f248211e75b6a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 346
      },
      {
        "segments": [
          {
            "segment_id": "d729330c-f1b4-48a5-a3f8-9dc34f2643a1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 135,
            "page_width": 612,
            "page_height": 792,
            "content": "[50] Michael L. Fredman, J´anos Koml´os, and Endre Szemer´edi. Storing a sparse table\r\nwith 0(1) worst case access time. Journal of the ACM, 31(3):538–544, 1984. See also\r\nFOCS’82.\r\n[51] Michael L. Fredman and Michael E. Saks. The cell probe complexity of dynamic data\r\nstructures. In Proc. 21st ACM Symposium on Theory of Computing (STOC), pages\r\n345–354, 1989.\r\n[52] Michael L. Fredman and Dan E. Willard. Surpassing the information theoretic bound\r\nwith fusion trees. Journal of Computer and System Sciences, 47(3):424–436, 1993. See\r\nalso STOC’90.\r\n[53] Michael L. Fredman and Dan E. Willard. Trans-dichotomous algorithms for mini\u0002mum spanning trees and shortest paths. Journal of Computer and System Sciences,\r\n48(3):533–551, 1994. See also FOCS’90.\r\n[54] Haripriyan Hampapuram and Michael L. Fredman. Optimal biweighted binary trees\r\nand the complexity of maintaining partial sums. SIAM Journal on Computing, 28(1):1–\r\n9, 1998. See also FOCS’93.\r\n[55] Monika Rauch Henzinger and Valerie King. Randomized fully dynamic graph algo\u0002rithms with polylogarithmic time per operation. Journal of the ACM, 46(4):502–516,\r\n1999. See also STOC’95.\r\n[56] Monika Rauch Henzinger and Mikkel Thorup. Sampling to provide or to bound: With\r\napplications to fully dynamic graph algorithms. Random Structures and Algorithms,\r\n11(4):369–379, 1997. See also ICALP’96.\r\n[57] Jacob Holm, Kristian de Lichtenberg, and Mikkel Thorup. Poly-logarithmic determin\u0002istic fully-dynamic algorithms for connectivity, minimum spanning tree, 2-edge, and\r\nbiconnectivity. Journal of the ACM, 48(4):723–760, 2001. See also STOC’98.\r\n[58] Thore Husfeldt and Theis Rauhe. New lower bound techniques for dynamic partial\r\nsums and related problems. SIAM Journal on Computing, 32(3):736–753, 2003. See\r\nalso ICALP’98.\r\n[59] Thore Husfeldt, Theis Rauhe, and Søren Skyum. Lower bounds for dynamic transitive\r\nclosure, planar point location, and parentheses matching. In Proc. 5th Scandinavian\r\nWorkshop on Algorithm Theory (SWAT), pages 198–211, 1996.\r\n[60] Piotr Indyk. Approximate algorithms for high-dimensional geometric problems. Invited\r\ntalk at DIMACS Workshop on Computational Geometry’02. http://people.csail.\r\nmit.edu/indyk/high.ps, 2001.\r\n[61] Piotr Indyk. On approximate nearest neighbors under `∞ norm. Journal of Computer\r\nand System Sciences, 63(4):627–638, 2001. See also FOCS’98.\r\n135",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/d729330c-f1b4-48a5-a3f8-9dc34f2643a1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=57c3a59126e9c406a085dfda5b5b21ba73dd955fe1c3936231b337fe8618dbcf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 325
      },
      {
        "segments": [
          {
            "segment_id": "d7f6663e-7175-460c-84d1-f8a0122eff2a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 136,
            "page_width": 612,
            "page_height": 792,
            "content": "[62] Piotr Indyk. Approximate nearest neighbor algorithms for Frechet metric via product\r\nmetrics. In Proc. 18th ACM Symposium on Computational Geometry (SoCG), pages\r\n102–106, 2002.\r\n[63] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing\r\nthe curse of dimensionality. In Proc. 30th ACM Symposium on Theory of Computing\r\n(STOC), pages 604–613, 1998.\r\n[64] T. S. Jayram, Subhash Khot, Ravi Kumar, and Yuval Rabani. Cell-probe lower bounds\r\nfor the partial match problem. Journal of Computer and System Sciences, 69(3):435–\r\n447, 2004. See also STOC’03.\r\n[65] Bala Kalyanasundaram and Georg Schnitger. The probabilistic communication com\u0002plexity of set intersection. SIAM Journal on Discrete Mathematics, 5(4):545–557, 1992.\r\nSee also Structures’87.\r\n[66] Robert Krauthgamer, James R. Lee, Manor Mendel, and Assaf Naor. Measured de\u0002scent: A new embedding method for finite metrics. Geometric And Functional Analysis,\r\n15(4):839–858, 2005. See also FOCS’04.\r\n[67] Eyal Kushilevitz, Rafail Ostrovsky, and Yuval Rabani. Efficient search for approximate\r\nnearest neighbor in high dimensional spaces. SIAM Journal on Computing, 30(2):457–\r\n474, 2000. See also STOC’98.\r\n[68] Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs and some\r\nof its algorithmic applications. Combinatorica, 15(2):215–245, 1995. See also FOCS’94.\r\n[69] Ding Liu. A strong lower bound for approximate nearest neighbor searching. Infor\u0002mation Processing Letters, 92(1):23–29, 2004.\r\n[70] Peter Bro Miltersen. The bit probe complexity measure revisited. In Proc. 10th Sym\u0002posium on Theoretical Aspects of Computer Science (STACS), pages 662–671, 1993.\r\n[71] Peter Bro Miltersen. Lower bounds for Union-Split-Find related problems on random\r\naccess machines. In Proc. 26th ACM Symposium on Theory of Computing (STOC),\r\npages 625–634, 1994.\r\n[72] Peter Bro Miltersen. Cell probe complexity - a survey. In Proc. 19th Conference on\r\nthe Foundations of Software Technology and Theoretical Computer Science (FSTTCS),\r\n1999. Advances in Data Structures Workshop.\r\n[73] Peter Bro Miltersen, Noam Nisan, Shmuel Safra, and Avi Wigderson. On data struc\u0002tures and asymmetric communication complexity. Journal of Computer and System\r\nSciences, 57(1):37–49, 1998. See also STOC’95.\r\n136",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/d7f6663e-7175-460c-84d1-f8a0122eff2a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9749034c750e9e2a61c6be0df5a63ccbe377a9ee43c29d0b966478c5ac239630",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 317
      },
      {
        "segments": [
          {
            "segment_id": "11950039-693c-40d2-9b7e-19ab04bba506",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 137,
            "page_width": 612,
            "page_height": 792,
            "content": "[74] Peter Bro Miltersen, Sairam Subramanian, Jeffrey S. Vitter, and Roberto Tamas\u0002sia. Complexity models for incremental computation. Theoretical Computer Science,\r\n130(1):203–236, 1994. See also STACS’93.\r\n[75] Christian Worm Mortensen. Fully dynamic orthogonal range reporting on ram. SIAM\r\nJournal on Computing, 35(6):1494–1525, 2006. See also SODA’03.\r\n[76] Christian Worm Mortensen, Rasmus Pagh, and Mihai Pˇatra¸scu. On dynamic range\r\nreporting in one dimension. In Proc. 37th ACM Symposium on Theory of Computing\r\n(STOC), pages 104–111, 2005.\r\n[77] Rajeev Motwani, Assaf Naor, and Rina Panigrahy. Lower bounds on locality sensi\u0002tive hashing. SIAM Journal on Discrete Mathematics, 21(4):930–935, 2007. See also\r\nSoCG’06.\r\n[78] S. Muthukrishnan and Martin M¨uller. Time and space efficient method-lookup for\r\nobject-oriented programs. In Proc. 7th ACM/SIAM Symposium on Discrete Algorithms\r\n(SODA), pages 42–51, 1996.\r\n[79] Yakov Nekrich. A data structure for multi-dimensional range reporting. In Proc. 23rd\r\nACM Symposium on Computational Geometry (SoCG), pages 344–353, 2007.\r\n[80] Rina Panigrahy. Entropy based nearest neighbor search in high dimensions. In Proc.\r\n17th ACM/SIAM Symposium on Discrete Algorithms (SODA), pages 1186–1195, 2006.\r\n[81] Mihai Pˇatra¸scu. Lower bounds for 2-dimensional range counting. In Proc. 39th ACM\r\nSymposium on Theory of Computing (STOC), pages 40–46, 2007.\r\n[82] Mihai Pˇatra¸scu. (data) structures. In Proc. 49th IEEE Symposium on Foundations of\r\nComputer Science (FOCS), 2008.\r\n[83] Mihai Pˇatra¸scu and Erik D. Demaine. Lower bounds for dynamic connectivity. In\r\nProc. 36th ACM Symposium on Theory of Computing (STOC), pages 546–553, 2004.\r\n[84] Mihai Pˇatra¸scu and Erik D. Demaine. Tight bounds for the partial-sums problem.\r\nIn Proc. 15th ACM/SIAM Symposium on Discrete Algorithms (SODA), pages 20–29,\r\n2004.\r\n[85] Mihai Pˇatra¸scu and Mikkel Thorup. Planning for fast connectivity updates. In Proc.\r\n48th IEEE Symposium on Foundations of Computer Science (FOCS), pages 263–271,\r\n2007.\r\n[86] Mihai Pˇatra¸scu and Erik D. Demaine. Logarithmic lower bounds in the cell-probe\r\nmodel. SIAM Journal on Computing, 35(4):932–963, 2006. See also SODA’04 and\r\nSTOC’04.\r\n137",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/11950039-693c-40d2-9b7e-19ab04bba506.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=23f755f71b324656cc21274c5e121f94f67abcf814c94b58e094b66ec61477a2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 309
      },
      {
        "segments": [
          {
            "segment_id": "7496dd73-5157-4aa3-adde-888fcf9cab22",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 138,
            "page_width": 612,
            "page_height": 792,
            "content": "[87] Mihai Pˇatra¸scu and Corina Tarnit¸ˇa. On dynamic bit-probe complexity. Theoretical\r\nComputer Science, 380:127–142, 2007. See also ICALP’05.\r\n[88] Mihai Pˇatra¸scu and Mikkel Thorup. Higher lower bounds for near-neighbor and further\r\nrich problems. In Proc. 47th IEEE Symposium on Foundations of Computer Science\r\n(FOCS), pages 646–654, 2006.\r\n[89] Mihai Pˇatra¸scu and Mikkel Thorup. Time-space trade-offs for predecessor search. In\r\nProc. 38th ACM Symposium on Theory of Computing (STOC), pages 232–240, 2006.\r\n[90] Mihai Pˇatra¸scu and Mikkel Thorup. Randomization does not help searching predeces\u0002sors. In Proc. 18th ACM/SIAM Symposium on Discrete Algorithms (SODA), pages\r\n555–564, 2007.\r\n[91] Satish Rao. Small distortion and volume preserving embeddings for planar and Eu\u0002clidean metrics. In Proc. 15th ACM Symposium on Computational Geometry (SoCG),\r\npages 300–306, 1999.\r\n[92] Alexander A. Razborov. On the distributional complexity of disjointness. Theoretical\r\nComputer Science, 106(2):385–390, 1992.\r\n[93] Ronald L. Rivest. Partial-match retrieval algorithms. SIAM Journal on Computing,\r\n5(1):19–50, 1976. See also FOCS’74 and Stanford PhD thesis.\r\n[94] Hanan Samet. Foundations of Multidimensional and Metric Data Structures. Elsevier,\r\n2006.\r\n[95] Pranab Sen and Srinivasan Venkatesh. Lower bounds for predecessor searching in the\r\ncell probe model. Journal of Computer and System Sciences, 74(3):364–385, 2008. See\r\nalso ICALP’01, CCC’03.\r\n[96] Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk, editors. Nearest Neighbor\r\nMethods in Learning and Vision. Neural Processing Information Series, MIT Press,\r\n2006.\r\n[97] Daniel D. Sleator and Robert E. Tarjan. A data structure for dynamic trees. Journal\r\nof Computer and System Sciences, 26(3):362–391, 1983. See also STOC’81.\r\n[98] Kunal Talwar, Rina Panigrahy, and Udi Wieder. A geometric approach to lower bounds\r\nfor approximate near-neighbor search and partial match. In Proc. 49th IEEE Sympo\u0002sium on Foundations of Computer Science (FOCS), 2008.\r\n[99] Mikkel Thorup. Near-optimal fully-dynamic graph connectivity. In Proc. 32nd ACM\r\nSymposium on Theory of Computing (STOC), pages 343–350, 2000.\r\n[100] Mikkel Thorup. Space efficient dynamic stabbing with fast queries. In Proc. 35th ACM\r\nSymposium on Theory of Computing (STOC), pages 649–658, 2003.\r\n138",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/7496dd73-5157-4aa3-adde-888fcf9cab22.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=65e4dff0db031f58932edfb49f864c0d1cebc7689f611919ee710039874e6462",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "33145986-460c-46e6-81eb-4e3b162d95a6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 139,
            "page_width": 612,
            "page_height": 792,
            "content": "[101] Peter van Emde Boas, R. Kaas, and E. Zijlstra. Design and implementation of an\r\nefficient priority queue. Mathematical Systems Theory, 10:99–127, 1977. Conference\r\nversion by van Emde Boas alone in FOCS’75.\r\n[102] Marcel Waldvogel, George Varghese, Jonathan S. Turner, and Bernhard Plattner. Scal\u0002able high speed ip routing lookups. In Proc. ACM SIGCOMM, pages 25–36, 1997.\r\n[103] Dan E. Willard. Log-logarithmic worst-case range queries are possible in space Θ(N).\r\nInformation Processing Letters, 17(2):81–84, 1983.\r\n[104] Dan E. Willard. Examining computational geometry, van Emde Boas trees, and hash\u0002ing from the perspective of the fusion tree. SIAM Journal on Computing, 29(3):1030–\r\n1049, 2000. See also SODA’92.\r\n[105] Bing Xiao. New bounds in cell probe model. PhD thesis, University of California at\r\nSan Diego, 1992.\r\n[106] Andrew Chi-Chih Yao. Should tables be sorted? Journal of the ACM, 28(3):615–628,\r\n1981. See also FOCS’78.\r\n[107] Andrew Chi-Chih Yao. On the complexity of maintaining partial sums. SIAM Journal\r\non Computing, 14:277–288, 1985.\r\n139",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/8b39a6c6-0476-4ac9-b569-7da6a71c8ad7/images/33145986-460c-46e6-81eb-4e3b162d95a6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041551Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a4f18a66505aff148fcaf1e5e722ec0076b19fe0693620b471beda7b2bc81bf0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 478
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\n \"title\": \"Lower Bound Techniques for Data Structures\"\n}\n```"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Mihai Pˇatra¸scu\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "No response"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "New York\nMIT\n"
        }
      ]
    }
  }
}