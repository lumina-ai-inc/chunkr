{
  "file_name": "Learning-based Query Performance Modeling and Prediction (qperf-tr).pdf",
  "task_id": "895280f6-e9d2-478b-a97e-588da031e221",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "c21f275b-81d7-47f9-8312-aeecd691e934",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Learning-based Query Performance\r\nModeling and Prediction\r\nMert Akdere\r\nBrown University\r\nmakdere@cs.brown.edu\r\nUgur Çetintemel ˇ\r\nBrown University\r\nugur@cs.brown.edu\r\nABSTRACT\r\nAccurate query performance prediction (QPP) is central to effec\u0002tive resource management, query optimization and user experience\r\nmanagement. Analytical cost models, which are commonly used\r\nby optimizers to compare candidate plan costs, are poor predictors\r\nof execution latency. As a more promising approach to QPP, this\r\npaper studies the practicality and utility of sophisticated learning\u0002based models, which have recently been applied to a variety of pre\u0002dictive tasks with great success.\r\nWe propose and evaluate predictive modeling techniques that learn\r\nquery execution behavior at different granularities, ranging from\r\ncoarse-grained plan-level models to fine-grained operator-level mod\u0002els. We demonstrate that these two extremes offer a tradeoff be\u0002tween high accuracy and generality, respectively, and introduce a\r\nhybrid approach that combines their respective strengths by selec\u0002tively composing them in the process of QPP. We discuss how we\r\ncan use a training workload to (i) pre-build and materialize such\r\nmodels offline, so that they are readily available for future pre\u0002dictions, and (ii) build new models online as new predictions are\r\nneeded. All prediction models are built using only static features\r\n(available prior to query execution) and the performance values ob\u0002tained from the offline execution of the training workload.\r\nWe fully implemented all these techniques and extensions on top\r\nof PostgreSQL and evaluated them experimentally by quantifying\r\ntheir effectiveness over analytical workloads, represented by well\u0002established TPC-H data and queries. The results provide quantita\u0002tive evidence that learning-based modeling for QPP is both feasible\r\nand effective for both static and dynamic workload scenarios.\r\n1. INTRODUCTION\r\nModern database systems can greatly benefit from query perfor\u0002mance prediction (QPP), i.e., predicting the execution latency of a\r\nquery plan on a given hardware and system configuration. For ex\u0002ample, resource managers can utilize QPP to perform workload al\u0002location such that interactive behavior is achieved or specific QoS\r\ntargets are met. Optimizers can choose among alternative plans\r\nbased-on expected execution latency instead of total work incurred.\r\nAccurate QPP is important but also challenging: database systems\r\nare becoming increasingly complex, with several database and op\u0002erating system components interacting in sophisticated and often\r\nunexpected ways. The heterogeneity of the underlying hardware\r\nplatforms adds to this complexity by making it more difficult to\r\nquantify the CPU and I/O costs. Analytical cost models predom\u0002inantly used by the current generation of query optimizers cannot\r\ncapture these interactions and complexity; in fact, they are not de\u0002signed to do so. While they do a good job of comparing the costs of\r\nalternative query plans, they are poor predictors of plan execution\r\nlatency. Recent work [1] showed this result for TPC-DS [17], and\r\nthis paper does same for TPC-H [6] data and queries.\r\nIn this paper, we utilize learning-based modeling and prediction\r\ntechniques to tackle QPP for analytical workloads. Data-driven,\r\nlearning-based modeling is fast emerging as an essential ingredient\r\nof both user-facing applications, such as predictive analytics, and\r\nsystem-facing applications, such as autonomic computing and self\u0002management. Prior work reported evidence that such techniques\r\ncan also be used effectively for QPP, at least in constrained set\u0002tings [1]. Our study substantially improves and generalizes these\r\nresults in a number of new directions, arguing that learning-based\r\ntechniques tailored to database query execution are generally appli\u0002cable to and can be highly effective for QPP.\r\nOne of our key contributions is to show that queries can be modeled\r\nat different granularities, each offering different tradeoffs involving\r\npredictive accuracy and generality. If a representative workload is\r\navailable for training purposes, we can make highly accurate pre\u0002dictions using coarse-grained, plan-level models [1]. Such models,\r\nhowever, do not generalize well, performing poorly for unseen or\r\nchanging workloads. For these cases, fine-grained, operator-level\r\nmodeling performs much better due to its ability to capture the be\u0002havior of arbitrary plans, although they do not perform as well as\r\nplan-level models for fixed workloads. We then propose a hybrid\r\napproach that selectively composes plan- and operator-level models\r\nto achieve high accuracy without sacrificing generality.\r\nAll these modeling techniques require a training query workload to\r\nbe executed, so that appropriate feature and performance values are\r\nextracted and logged. Models can then be built (i.e., trained) over\r\nthese logs in offline mode, online mode, or in conjunction. The\r\nmain advantage of pre-building and materialization is that the mod\u0002els are immediately ready for use in predictions whenever needed.\r\nThe challenge, however, is to decide which models to pre-build,\r\nsince it is clearly not feasible to build all possible models in ad\u0002vance. To guide this decision, we propose heuristics that rely on\r\nestimates for additional accuracy yields and use frequencies. The\r\nonline approach, on the other hand, allows for custom (and po\u0002tentially more accurate) model to be built for a specific prediction\r\ntask, but delays the prediction until an appropriate model is built.\r\nNote that online building proceeds over the already available fea\u0002ture data, and does not require new sample query runs. Finally,\r\nonline and offline modeling can be seamlessly combined, with the\r\ndecision of which online models to create influenced by the pre\u0002built models. We note that these techniques require only static fea-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/c21f275b-81d7-47f9-8312-aeecd691e934.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f5a0eec6c7b5b965892b01e2094c9b3c58f96e9e98d4a32fa9e3a47da770d218",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 842
      },
      {
        "segments": [
          {
            "segment_id": "c21f275b-81d7-47f9-8312-aeecd691e934",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Learning-based Query Performance\r\nModeling and Prediction\r\nMert Akdere\r\nBrown University\r\nmakdere@cs.brown.edu\r\nUgur Çetintemel ˇ\r\nBrown University\r\nugur@cs.brown.edu\r\nABSTRACT\r\nAccurate query performance prediction (QPP) is central to effec\u0002tive resource management, query optimization and user experience\r\nmanagement. Analytical cost models, which are commonly used\r\nby optimizers to compare candidate plan costs, are poor predictors\r\nof execution latency. As a more promising approach to QPP, this\r\npaper studies the practicality and utility of sophisticated learning\u0002based models, which have recently been applied to a variety of pre\u0002dictive tasks with great success.\r\nWe propose and evaluate predictive modeling techniques that learn\r\nquery execution behavior at different granularities, ranging from\r\ncoarse-grained plan-level models to fine-grained operator-level mod\u0002els. We demonstrate that these two extremes offer a tradeoff be\u0002tween high accuracy and generality, respectively, and introduce a\r\nhybrid approach that combines their respective strengths by selec\u0002tively composing them in the process of QPP. We discuss how we\r\ncan use a training workload to (i) pre-build and materialize such\r\nmodels offline, so that they are readily available for future pre\u0002dictions, and (ii) build new models online as new predictions are\r\nneeded. All prediction models are built using only static features\r\n(available prior to query execution) and the performance values ob\u0002tained from the offline execution of the training workload.\r\nWe fully implemented all these techniques and extensions on top\r\nof PostgreSQL and evaluated them experimentally by quantifying\r\ntheir effectiveness over analytical workloads, represented by well\u0002established TPC-H data and queries. The results provide quantita\u0002tive evidence that learning-based modeling for QPP is both feasible\r\nand effective for both static and dynamic workload scenarios.\r\n1. INTRODUCTION\r\nModern database systems can greatly benefit from query perfor\u0002mance prediction (QPP), i.e., predicting the execution latency of a\r\nquery plan on a given hardware and system configuration. For ex\u0002ample, resource managers can utilize QPP to perform workload al\u0002location such that interactive behavior is achieved or specific QoS\r\ntargets are met. Optimizers can choose among alternative plans\r\nbased-on expected execution latency instead of total work incurred.\r\nAccurate QPP is important but also challenging: database systems\r\nare becoming increasingly complex, with several database and op\u0002erating system components interacting in sophisticated and often\r\nunexpected ways. The heterogeneity of the underlying hardware\r\nplatforms adds to this complexity by making it more difficult to\r\nquantify the CPU and I/O costs. Analytical cost models predom\u0002inantly used by the current generation of query optimizers cannot\r\ncapture these interactions and complexity; in fact, they are not de\u0002signed to do so. While they do a good job of comparing the costs of\r\nalternative query plans, they are poor predictors of plan execution\r\nlatency. Recent work [1] showed this result for TPC-DS [17], and\r\nthis paper does same for TPC-H [6] data and queries.\r\nIn this paper, we utilize learning-based modeling and prediction\r\ntechniques to tackle QPP for analytical workloads. Data-driven,\r\nlearning-based modeling is fast emerging as an essential ingredient\r\nof both user-facing applications, such as predictive analytics, and\r\nsystem-facing applications, such as autonomic computing and self\u0002management. Prior work reported evidence that such techniques\r\ncan also be used effectively for QPP, at least in constrained set\u0002tings [1]. Our study substantially improves and generalizes these\r\nresults in a number of new directions, arguing that learning-based\r\ntechniques tailored to database query execution are generally appli\u0002cable to and can be highly effective for QPP.\r\nOne of our key contributions is to show that queries can be modeled\r\nat different granularities, each offering different tradeoffs involving\r\npredictive accuracy and generality. If a representative workload is\r\navailable for training purposes, we can make highly accurate pre\u0002dictions using coarse-grained, plan-level models [1]. Such models,\r\nhowever, do not generalize well, performing poorly for unseen or\r\nchanging workloads. For these cases, fine-grained, operator-level\r\nmodeling performs much better due to its ability to capture the be\u0002havior of arbitrary plans, although they do not perform as well as\r\nplan-level models for fixed workloads. We then propose a hybrid\r\napproach that selectively composes plan- and operator-level models\r\nto achieve high accuracy without sacrificing generality.\r\nAll these modeling techniques require a training query workload to\r\nbe executed, so that appropriate feature and performance values are\r\nextracted and logged. Models can then be built (i.e., trained) over\r\nthese logs in offline mode, online mode, or in conjunction. The\r\nmain advantage of pre-building and materialization is that the mod\u0002els are immediately ready for use in predictions whenever needed.\r\nThe challenge, however, is to decide which models to pre-build,\r\nsince it is clearly not feasible to build all possible models in ad\u0002vance. To guide this decision, we propose heuristics that rely on\r\nestimates for additional accuracy yields and use frequencies. The\r\nonline approach, on the other hand, allows for custom (and po\u0002tentially more accurate) model to be built for a specific prediction\r\ntask, but delays the prediction until an appropriate model is built.\r\nNote that online building proceeds over the already available fea\u0002ture data, and does not require new sample query runs. Finally,\r\nonline and offline modeling can be seamlessly combined, with the\r\ndecision of which online models to create influenced by the pre\u0002built models. We note that these techniques require only static fea-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/c21f275b-81d7-47f9-8312-aeecd691e934.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f5a0eec6c7b5b965892b01e2094c9b3c58f96e9e98d4a32fa9e3a47da770d218",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 842
      },
      {
        "segments": [
          {
            "segment_id": "796dc6fb-cfa6-4da7-8354-e4788f556f77",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "tures (i.e., compile-time features which are available prior to query\r\nexecution) for performance prediction.\r\nFinally, we describe how all these techniques can be used in com\u0002bination to provide progressively improved predictions. When a\r\nnew QPP is needed, we can immediately use the pre-built models\r\nto come up with an initial prediction, which we can then continue\r\nto improve over time by building better models online optionally\r\nwith run-time features.\r\nWhile we study the utility of learning-based models for query ex\u0002ecution latency as the performance metric of interest, the proposed\r\ntechniques are general, and thus can be used in the prediction of\r\nother metrics such as throughput. We should also note that this pa\u0002per does not consider QPP in the presence of concurrent execution,\r\nwhich is an important and challenging problem to address, but is\r\noutside the scope of this paper.\r\nWe fully implemented these techniques and report experimental re\u0002sults that quantify their cost and effectiveness for a variety of usage\r\nscenarios on top of PostgreSQL/TPC-H. The results reveal that our\r\nnovel learning-based modeling techniques can serve as an effec\u0002tive QPP solution for analytical workloads, substantially improving\r\nupon the existing solutions.\r\nThe rest of the paper is organized as follows: we start with back\u0002ground information on data-driven model-based prediction in Sec\u0002tion 2. In Section 3, we first describe our general approach to using\r\nstatistical learning techniques for QPP. Plan and operator -level per\u0002formance prediction methods are described in Section 3.1 and Sec\u0002tion 3.2, respectively. Next, in Section 3.4 we introduce the hybrid\r\nprediction method. Online modeling techniques which build pre\u0002diction models at query execution time are discussed in Section 4.\r\nWe present experimental results using the TPC-H query workload\r\nin Section 5. We then end the paper with related work and conclu\u0002sion remarks in Sections 6 and 7.\r\n2. BACKGROUND: MODEL-BASED\r\nPREDICTION\r\nWe use the term model to refer to any predictive function such as\r\nMultiple Regression, Bayesian Nets, and Support Vector Machines.\r\nTraining a model involves using historical data sets to determine the\r\nbest model instance that explains the available data. For example,\r\nfitting a function to a time series may yield a specific polynomial\r\ninstance that can be used to predict future values.\r\nModel training (or building) requires selecting (i) the feature at\u0002tributes, a subset of all attributes in the data set, and (ii) a predic\u0002tion model, e.g., Linear Regression and Support Vector Machines\r\n(SVMs), to be used for modeling. In general, we cannot know\r\nwhich model type and feature set will produce the most accurate\r\nmodel for a given data set without building and testing multiple\r\nmodels. In some cases, a domain expert can manually specify the\r\nfeature attributes. In other cases, this step is trivial as the prediction\r\nattribute(s) directly determine the feature attribute(s), e.g., in auto\u0002regressive models. Alternatively, feature attributes can be learned\r\nautomatically; however, given a set of n attributes, trying the power\r\nset is prohibitively expensive if n is not small or training is expen\u0002sive [4, 3, 2] thereby requiring heuristic solutions.\r\nMost approaches rank the candidate attributes (often based on their\r\ncorrelation to the prediction attribute using metrics such as infor\u0002mation gain or correlation coefficients) and use this ranking to guide\r\na heuristic search [4] to identify most predictive attributes tested\r\nover a disjoint test data set. In this paper, we will use a similar For\u0002ward Feature Selection algorithm based on linear correlation coef\u0002ficients [4]. This algorithm essentially performs a best-first search\r\nin the model space. It starts with building models using small num\u0002ber of features and iteratively builds more complex and accurate\r\nmodels by using more features. The features are considered with\r\nrespect to their correlation with the target/prediction attribute. The\r\ntraining data set may be sampled to speed up the process.\r\nWhile we use a feature selection algorithm for building accurate\r\nmodels using relevant features, we do not consider building multi\u0002ple models of different types for solving the model selection prob\u0002lem. Instead in each one of our experiments we use a single type of\r\nprediction model, either Linear Regression or SVMs, that performs\r\nwell.\r\nHypothesis testing and confidence interval estimations are two com\u0002mon techniques for determining predictive accuracy [2]. As men\u0002tioned, it is not possible to estimate a priori what model would be\r\nmost predictive for a given data set without training/testing it. One\r\nform of hypothesis testing that is commonly used is K-Fold Cross\r\nValidation (K-CV). K-CV divides the observed data up into k non\u0002overlapping partitions. One of the partitions is used as validation\r\ndata while the other k − 1 partitions are used to train the model\r\nand to predict the data in the validation interval. In this study, we\r\nwill use cross-validation techniques to estimate the accuracy of our\r\nprediction models.\r\n3. MODELING QUERY EXECUTIONS\r\nIn this study, we describe QPP methods based on statistical learning\r\nmodels. As is usual in most learning approaches, all of our mod\u0002eling techniques consist of two main phases: training and testing.\r\nThe high-level operations involved in these phases are explained in\r\nFigure 2.\r\nIn the training phase, prediction models are derived from a train\u0002ing data set that contains previously executed queries (i.e., train\u0002ing workload) and the observed performance values. In this phase,\r\nqueries are represented as a set of features (i.e., predictive vari\u0002ables) with corresponding performance values (i.e., target variables)\r\nand the goal is to create an accurate and concise operational sum\u0002mary of the mapping between the feature values and the observed\r\nperformance data points. The prediction models are then used to\r\npredict the performance of unforeseen queries in the test phase. In\r\nmore complex QPP methods, the training and testing phases can be\r\nperformed continuously for improved accuracy and adaptivity.\r\nFigure 1: Statistical Modeling Approach to Query Performance Pre\u0002diction.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/796dc6fb-cfa6-4da7-8354-e4788f556f77.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4b02b862993470797c34978c46f66332ffe5448042d490a3d902a7c8e633ef2c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 948
      },
      {
        "segments": [
          {
            "segment_id": "796dc6fb-cfa6-4da7-8354-e4788f556f77",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "tures (i.e., compile-time features which are available prior to query\r\nexecution) for performance prediction.\r\nFinally, we describe how all these techniques can be used in com\u0002bination to provide progressively improved predictions. When a\r\nnew QPP is needed, we can immediately use the pre-built models\r\nto come up with an initial prediction, which we can then continue\r\nto improve over time by building better models online optionally\r\nwith run-time features.\r\nWhile we study the utility of learning-based models for query ex\u0002ecution latency as the performance metric of interest, the proposed\r\ntechniques are general, and thus can be used in the prediction of\r\nother metrics such as throughput. We should also note that this pa\u0002per does not consider QPP in the presence of concurrent execution,\r\nwhich is an important and challenging problem to address, but is\r\noutside the scope of this paper.\r\nWe fully implemented these techniques and report experimental re\u0002sults that quantify their cost and effectiveness for a variety of usage\r\nscenarios on top of PostgreSQL/TPC-H. The results reveal that our\r\nnovel learning-based modeling techniques can serve as an effec\u0002tive QPP solution for analytical workloads, substantially improving\r\nupon the existing solutions.\r\nThe rest of the paper is organized as follows: we start with back\u0002ground information on data-driven model-based prediction in Sec\u0002tion 2. In Section 3, we first describe our general approach to using\r\nstatistical learning techniques for QPP. Plan and operator -level per\u0002formance prediction methods are described in Section 3.1 and Sec\u0002tion 3.2, respectively. Next, in Section 3.4 we introduce the hybrid\r\nprediction method. Online modeling techniques which build pre\u0002diction models at query execution time are discussed in Section 4.\r\nWe present experimental results using the TPC-H query workload\r\nin Section 5. We then end the paper with related work and conclu\u0002sion remarks in Sections 6 and 7.\r\n2. BACKGROUND: MODEL-BASED\r\nPREDICTION\r\nWe use the term model to refer to any predictive function such as\r\nMultiple Regression, Bayesian Nets, and Support Vector Machines.\r\nTraining a model involves using historical data sets to determine the\r\nbest model instance that explains the available data. For example,\r\nfitting a function to a time series may yield a specific polynomial\r\ninstance that can be used to predict future values.\r\nModel training (or building) requires selecting (i) the feature at\u0002tributes, a subset of all attributes in the data set, and (ii) a predic\u0002tion model, e.g., Linear Regression and Support Vector Machines\r\n(SVMs), to be used for modeling. In general, we cannot know\r\nwhich model type and feature set will produce the most accurate\r\nmodel for a given data set without building and testing multiple\r\nmodels. In some cases, a domain expert can manually specify the\r\nfeature attributes. In other cases, this step is trivial as the prediction\r\nattribute(s) directly determine the feature attribute(s), e.g., in auto\u0002regressive models. Alternatively, feature attributes can be learned\r\nautomatically; however, given a set of n attributes, trying the power\r\nset is prohibitively expensive if n is not small or training is expen\u0002sive [4, 3, 2] thereby requiring heuristic solutions.\r\nMost approaches rank the candidate attributes (often based on their\r\ncorrelation to the prediction attribute using metrics such as infor\u0002mation gain or correlation coefficients) and use this ranking to guide\r\na heuristic search [4] to identify most predictive attributes tested\r\nover a disjoint test data set. In this paper, we will use a similar For\u0002ward Feature Selection algorithm based on linear correlation coef\u0002ficients [4]. This algorithm essentially performs a best-first search\r\nin the model space. It starts with building models using small num\u0002ber of features and iteratively builds more complex and accurate\r\nmodels by using more features. The features are considered with\r\nrespect to their correlation with the target/prediction attribute. The\r\ntraining data set may be sampled to speed up the process.\r\nWhile we use a feature selection algorithm for building accurate\r\nmodels using relevant features, we do not consider building multi\u0002ple models of different types for solving the model selection prob\u0002lem. Instead in each one of our experiments we use a single type of\r\nprediction model, either Linear Regression or SVMs, that performs\r\nwell.\r\nHypothesis testing and confidence interval estimations are two com\u0002mon techniques for determining predictive accuracy [2]. As men\u0002tioned, it is not possible to estimate a priori what model would be\r\nmost predictive for a given data set without training/testing it. One\r\nform of hypothesis testing that is commonly used is K-Fold Cross\r\nValidation (K-CV). K-CV divides the observed data up into k non\u0002overlapping partitions. One of the partitions is used as validation\r\ndata while the other k − 1 partitions are used to train the model\r\nand to predict the data in the validation interval. In this study, we\r\nwill use cross-validation techniques to estimate the accuracy of our\r\nprediction models.\r\n3. MODELING QUERY EXECUTIONS\r\nIn this study, we describe QPP methods based on statistical learning\r\nmodels. As is usual in most learning approaches, all of our mod\u0002eling techniques consist of two main phases: training and testing.\r\nThe high-level operations involved in these phases are explained in\r\nFigure 2.\r\nIn the training phase, prediction models are derived from a train\u0002ing data set that contains previously executed queries (i.e., train\u0002ing workload) and the observed performance values. In this phase,\r\nqueries are represented as a set of features (i.e., predictive vari\u0002ables) with corresponding performance values (i.e., target variables)\r\nand the goal is to create an accurate and concise operational sum\u0002mary of the mapping between the feature values and the observed\r\nperformance data points. The prediction models are then used to\r\npredict the performance of unforeseen queries in the test phase. In\r\nmore complex QPP methods, the training and testing phases can be\r\nperformed continuously for improved accuracy and adaptivity.\r\nFigure 1: Statistical Modeling Approach to Query Performance Pre\u0002diction.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/796dc6fb-cfa6-4da7-8354-e4788f556f77.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4b02b862993470797c34978c46f66332ffe5448042d490a3d902a7c8e633ef2c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 948
      },
      {
        "segments": [
          {
            "segment_id": "f56162bf-b719-4c31-9b3d-59b73ff4a8ba",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Our approach to QPP relies on models that use only static, compile\u0002time features, which allow us to produce predictions before the ex\u0002ecution of queries. There are several static information sources,\r\nsuch as the query text and execution plans, from which query fea\u0002tures can be extracted prior to execution. In this study, we use\r\nfeatures that can be obtained from the information provided by the\r\nquery optimizer. Many DBMS provide optimizer calls that expose\r\nquery-plan information and statistical estimates such as the opti\u0002mized query-plan structure and operator selectivities (for example,\r\nEXPLAIN in PostgreSQL).\r\nThis paper shows that it is possible to create models at varying\r\ngranularities for query performance prediction. As in [1], one coarse\r\nmodeling method is to create a single, plan-level prediction model\r\nthat utilizes query plan features for modeling the execution times\r\nof queries. We discuss this approach in Section 3.1. A finer grained\r\napproach would be to model each operator type separately and use\r\nthem collectively through selective composition to model entire\r\nquery plans. We describe this method in Section 3.2 and compare\r\nthe relative advantages and drawbacks of the two approaches in\r\nSection 3.3. Next, in Section 3.4, we introduce a “hybrid” model\u0002ing approach that combines the fine and coarse grained modeling\r\nmethods to form a highly accurate and general QPP approach.\r\n3.1 Plan-level Modeling\r\nIn the plan-level modeling approach, the performance of a query\r\nis predicted using a single prediction model. We use the features\r\npresented in Table 1 for building plan-level models. This set of\r\nfeatures contains query optimizer estimates such as operator cardi\u0002nalities and plan execution costs together with the occurrence count\r\nof each operator type in the query plan.\r\nFeature Name Description\r\np_tot_cost Estimated plan total cost\r\np_st_cost Estimated plan start cost\r\np_rows Estimated number of output tuples\r\np_width Estimated average width of an out\u0002put tuple (in bytes)\r\nop_count Number of query operators in the\r\nplan\r\nrow_count Estimated total number of tuples in\u0002put and output to/from each opera\u0002tor\r\nbyte_count Estimated total size (in bytes) of all\r\ntuples input and output\r\n<operator_name>_cnt The number of <operator_name>\r\noperators in the query\r\n<operator_name>_rows The total number of tuples output\r\nfrom <operator_name> operators\r\nTable 1: Features for plan-level models. p_st_cost refers to the\r\ncost of query execution until the first output tuple. <opera\u0002tor_name> refers to the query operators such as Limit, Materi\u0002alize and Sort.\r\nAs mentioned in Section 2, we need to address two challenges\r\nwhen using model-based learning techniques. The first problem,\r\nfeature selection, deals with the issue of choosing the most predic\u0002tive features from the available set of features. In our experiments,\r\nwe frequently observed that models using the full set of features\r\ngiven in Table 1 performed less accurately than with smaller num\u0002ber of features. We use a best-first search based, forward feature\r\nselection algorithm [4], described in Section 2 to perform feature\r\nselection. This algorithm starts by building models using a small\r\nnumber of features, and iteratively creates more complex and ac\u0002curate models by adding features in order of correlation with the\r\ntarget variable (i.e., query execution time).\r\nThe second problem is model selection, the process of picking the\r\nright prediction model for the given task and data set. As dis\u0002cussed in Section 2, it is not possible in general to identify the\r\nmost accurate prediction model without training and testing mul\u0002tiple models. We use a regression variant of Support Vector Ma\u0002chines (SVMs) [5] for plan-level modeling, which provided high\r\naccuracy in our experiments. However, we note that all of the ap\u0002proaches we present here are model-agnostic and can readily work\r\nwith different model types.\r\nOnce a plan-level prediction model is built and stored (i.e., mate\u0002rialized), it can then be used to estimate the performance of new\r\nincoming queries based on the query-plan feature values that can\r\nbe obtained from the query optimizer without executing the query.\r\n3.2 Operator-level Modeling\r\nWe now introduce a finer-grained operator-level modeling approach.\r\nUnlike the plan-level approach, which uses a single prediction model,\r\nthe operator-level technique relies on a collection of models that are\r\nselectively composed for end-to-end query performance prediction.\r\nIn the operator-level modeling approach, two separate prediction\r\nmodels are built for each query operator type:\r\n• A start-time prediction model is used for estimating the time\r\nspent during the execution of an operator (and in the sub\u0002query plan rooted at this operator) until it produces its first\r\noutput tuple. This model captures the (non-)blocking behav\u0002ior of individual operators and their interaction with pipelined\r\nquery execution.\r\n• A run-time prediction model is used for modeling the total\r\nexecution time of query operators (and the sub-plans rooted\r\nat these operators). Therefore, the run-time estimate of the\r\nroot operator of a given query plan is the estimated execution\r\ntime for the corresponding query.\r\nTo illustrate the semantics and the use of the start-time model, we\r\nconsider the Materialize operator, which materializes its input tu\u0002ples either to disk or memory. Assume that in a query tree, the Ma\u0002terialize operator is the inner child operator of a Nested Loop join.\r\nAlthough the materialization operation is performed only once, the\r\njoin operator may scan the materialized relation multiple times. In\r\nthis case, the start-time of the Materialize operator would corre\u0002spond to the actual materialization operation, whereas the run-time\r\nwould represent the total execution time for the materialization and\r\nscan operations. In this manner, the parent Nested Loop operator\r\ncan use the start-time and run-time estimates to form an accurate\r\nmodel of its own execution time. This technique also allows us to\r\ntransparently and automatically capture the cumulative effects of\r\nblocking operations and other operational semantics on the execu\u0002tion time.\r\nWe used a single, fixed collection of features to create models for\r\neach query operator. The complete list of features is given in Ta\u0002ble 2. This list includes a generic set of features that are applicable\r\nto almost all query operators. They can also be easily acquired from",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/f56162bf-b719-4c31-9b3d-59b73ff4a8ba.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=90cb54b52acbd01c3b06becae5de81a271c6a87582c51c23ccfd70489d1b8b6b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 976
      },
      {
        "segments": [
          {
            "segment_id": "f56162bf-b719-4c31-9b3d-59b73ff4a8ba",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Our approach to QPP relies on models that use only static, compile\u0002time features, which allow us to produce predictions before the ex\u0002ecution of queries. There are several static information sources,\r\nsuch as the query text and execution plans, from which query fea\u0002tures can be extracted prior to execution. In this study, we use\r\nfeatures that can be obtained from the information provided by the\r\nquery optimizer. Many DBMS provide optimizer calls that expose\r\nquery-plan information and statistical estimates such as the opti\u0002mized query-plan structure and operator selectivities (for example,\r\nEXPLAIN in PostgreSQL).\r\nThis paper shows that it is possible to create models at varying\r\ngranularities for query performance prediction. As in [1], one coarse\r\nmodeling method is to create a single, plan-level prediction model\r\nthat utilizes query plan features for modeling the execution times\r\nof queries. We discuss this approach in Section 3.1. A finer grained\r\napproach would be to model each operator type separately and use\r\nthem collectively through selective composition to model entire\r\nquery plans. We describe this method in Section 3.2 and compare\r\nthe relative advantages and drawbacks of the two approaches in\r\nSection 3.3. Next, in Section 3.4, we introduce a “hybrid” model\u0002ing approach that combines the fine and coarse grained modeling\r\nmethods to form a highly accurate and general QPP approach.\r\n3.1 Plan-level Modeling\r\nIn the plan-level modeling approach, the performance of a query\r\nis predicted using a single prediction model. We use the features\r\npresented in Table 1 for building plan-level models. This set of\r\nfeatures contains query optimizer estimates such as operator cardi\u0002nalities and plan execution costs together with the occurrence count\r\nof each operator type in the query plan.\r\nFeature Name Description\r\np_tot_cost Estimated plan total cost\r\np_st_cost Estimated plan start cost\r\np_rows Estimated number of output tuples\r\np_width Estimated average width of an out\u0002put tuple (in bytes)\r\nop_count Number of query operators in the\r\nplan\r\nrow_count Estimated total number of tuples in\u0002put and output to/from each opera\u0002tor\r\nbyte_count Estimated total size (in bytes) of all\r\ntuples input and output\r\n<operator_name>_cnt The number of <operator_name>\r\noperators in the query\r\n<operator_name>_rows The total number of tuples output\r\nfrom <operator_name> operators\r\nTable 1: Features for plan-level models. p_st_cost refers to the\r\ncost of query execution until the first output tuple. <opera\u0002tor_name> refers to the query operators such as Limit, Materi\u0002alize and Sort.\r\nAs mentioned in Section 2, we need to address two challenges\r\nwhen using model-based learning techniques. The first problem,\r\nfeature selection, deals with the issue of choosing the most predic\u0002tive features from the available set of features. In our experiments,\r\nwe frequently observed that models using the full set of features\r\ngiven in Table 1 performed less accurately than with smaller num\u0002ber of features. We use a best-first search based, forward feature\r\nselection algorithm [4], described in Section 2 to perform feature\r\nselection. This algorithm starts by building models using a small\r\nnumber of features, and iteratively creates more complex and ac\u0002curate models by adding features in order of correlation with the\r\ntarget variable (i.e., query execution time).\r\nThe second problem is model selection, the process of picking the\r\nright prediction model for the given task and data set. As dis\u0002cussed in Section 2, it is not possible in general to identify the\r\nmost accurate prediction model without training and testing mul\u0002tiple models. We use a regression variant of Support Vector Ma\u0002chines (SVMs) [5] for plan-level modeling, which provided high\r\naccuracy in our experiments. However, we note that all of the ap\u0002proaches we present here are model-agnostic and can readily work\r\nwith different model types.\r\nOnce a plan-level prediction model is built and stored (i.e., mate\u0002rialized), it can then be used to estimate the performance of new\r\nincoming queries based on the query-plan feature values that can\r\nbe obtained from the query optimizer without executing the query.\r\n3.2 Operator-level Modeling\r\nWe now introduce a finer-grained operator-level modeling approach.\r\nUnlike the plan-level approach, which uses a single prediction model,\r\nthe operator-level technique relies on a collection of models that are\r\nselectively composed for end-to-end query performance prediction.\r\nIn the operator-level modeling approach, two separate prediction\r\nmodels are built for each query operator type:\r\n• A start-time prediction model is used for estimating the time\r\nspent during the execution of an operator (and in the sub\u0002query plan rooted at this operator) until it produces its first\r\noutput tuple. This model captures the (non-)blocking behav\u0002ior of individual operators and their interaction with pipelined\r\nquery execution.\r\n• A run-time prediction model is used for modeling the total\r\nexecution time of query operators (and the sub-plans rooted\r\nat these operators). Therefore, the run-time estimate of the\r\nroot operator of a given query plan is the estimated execution\r\ntime for the corresponding query.\r\nTo illustrate the semantics and the use of the start-time model, we\r\nconsider the Materialize operator, which materializes its input tu\u0002ples either to disk or memory. Assume that in a query tree, the Ma\u0002terialize operator is the inner child operator of a Nested Loop join.\r\nAlthough the materialization operation is performed only once, the\r\njoin operator may scan the materialized relation multiple times. In\r\nthis case, the start-time of the Materialize operator would corre\u0002spond to the actual materialization operation, whereas the run-time\r\nwould represent the total execution time for the materialization and\r\nscan operations. In this manner, the parent Nested Loop operator\r\ncan use the start-time and run-time estimates to form an accurate\r\nmodel of its own execution time. This technique also allows us to\r\ntransparently and automatically capture the cumulative effects of\r\nblocking operations and other operational semantics on the execu\u0002tion time.\r\nWe used a single, fixed collection of features to create models for\r\neach query operator. The complete list of features is given in Ta\u0002ble 2. This list includes a generic set of features that are applicable\r\nto almost all query operators. They can also be easily acquired from",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/f56162bf-b719-4c31-9b3d-59b73ff4a8ba.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=90cb54b52acbd01c3b06becae5de81a271c6a87582c51c23ccfd70489d1b8b6b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 976
      },
      {
        "segments": [
          {
            "segment_id": "db75b5c1-e284-4301-b65b-d993ba27f2a6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "most, if not all, existing DBMSs. As in the case of plan-level mod\u0002eling approach, we use the forward feature selection algorithm, to\r\nbuild accurate prediction models with the relevant set of features.\r\nFeature Name Description\r\nnp Estimated I/O (in number of pages)\r\nnt Estimated number of output tuples\r\nnt1 Estimated number of input tuples\r\n(from left child operator)\r\nnt2 Estimated number of input tuples\r\n(from left right operator)\r\nsel Estimated operator selectivity\r\nst1 Start-time of left child operator\r\nrt1 Run-time of left child operator\r\nst2 Start-time of right child operator\r\nrt2 Run-time of right child operator\r\nTable 2: Features for the operator-level models. Start time\r\nrefers to the time spent in query execution until the first out\u0002put tuple.\r\nThe individual operator models are collectively used to estimate the\r\nexecution latency of a given query by selectively composing them\r\nin a hierarchical manner akin to how optimizers derive query costs\r\nfrom the costs of individual operators. That is, by appropriately\r\nconnecting the inputs and outputs of prediction models following\r\nthe structure of query plans, it is possible to produce predictors for\r\narbitrary queries.\r\nIn Figure 2, we illustrate this process for a simple query plan con\u0002sisting of three operators. The performance prediction operation\r\nworks in a bottom-up manner: each query operator uses its predic\u0002tion models and feature values to produce its start-time and run\u0002time estimates. The estimates produced by an operator are then fed\r\nto the parent operator, which uses them for its own performance\r\nprediction.\r\nFigure 2: Operator-level query performance prediction: operator\r\nmodels use operator-level features together with the predictions of child\r\noperators for performance prediction.\r\n3.3 Plan- versus Operator-level Modeling\r\nThe premise of the plan-level approach is that queries with similar\r\nfeature vectors will have similar query plans and plan statistics, and\r\ntherefore are likely to exhibit similar behavior and performance.\r\nSuch an approach is specifically targeted to scenarios in which the\r\nqueries in the training and test phases have similar execution plans\r\n(e.g., generated from the same query templates or from the same\r\nuser program).\r\nFurthermore, this approach is based on the correlation of the query\r\nexecution plans and statistics with the query execution times. This\r\ncorrelation is used directly in mapping query-plan based features\r\nto execution performance. The high-level modeling approach used\r\nin this case therefore offers the ability to capture the cumulative\r\neffects of a set of lower level underlying factors, such as opera\u0002tor interactions during query processing, on the execution time for\r\neach distinct query plan (in the training data) with a low complexity\r\nmodel.\r\nThe plan-level approach, however, is prone to failure in some com\u0002mon real-world scenarios. A significant problem exists in the case\r\nof dynamic query workloads where queries with unforeseen exe\u0002cution plans are frequently observed. Even worse, there can be\r\nproblems even with static workloads. As the feature values only\r\nrepresent a limited view of a query plan and its execution, it is\r\npossible that different queries can be mapped to very similar fea\u0002ture values and therefore be inaccurately modeled. While it is un\u0002likely for completely different queries to be mapped to identical\r\nfeatures, similar queries can sometimes have different execution\r\nperformance. For instance, increasing the number of time consum\u0002ing aggregate operations in a query will not significantly change its\r\nfeature vector, but may highly increase its execution time. Adding\r\nmore features (e.g., number of aggregates and constraints) to the\r\nmodel would alleviate such issues, however, each added feature\r\nwould also increase the size of the required training data.\r\nBy using multiple prediction models collectively in an hierarchi\u0002cal manner, the operator-level prediction method is able to pro\u0002duce performance predictions for arbitrary queries. Therefore, it\r\nis a more general approach compared to the plan-level method and\r\nhas the potential to be more effective for dynamic query workloads\r\nwhere unforeseen query plan structures are common.\r\nOn the downside, the operator-level prediction method may suffer\r\nfrom drawbacks similar to those that affect analytical cost estima\u0002tion methods (as both methods rely on low-level operator-based\r\nmodels). A key problem is that the prediction errors in the lower\r\nlevels of a query plan are propagated to the upper levels and may\r\nsignificantly degrade the end prediction accuracy.\r\nAnother potential problem is that the concurrent use of multiple\r\nresources such as CPU and disk may not be correctly reflected in\r\nthe operator-level (or the analytical) models. For instance, a query\r\ncould be simply performing an aggregate computation on the rows\r\nof a table that it sequentially scans from the disk. If the per-tuple\r\nprocessing takes less time than reading a tuple from the disk, then\r\nthe query execution time is approximately the same as the sequen\u0002tial scan time. However, if the processing of a tuple takes longer\r\nthan reading it from the disk, then the execution time will be closer\r\nto the processing time. As such, the interactions of the query exe\u0002cution system and the underlying hardware/software platforms can\r\nget quite complex. In such cases, simple operator-level modeling\r\napproaches may fall short of accurately representing this sophisti\u0002cated behavior. Therefore, in static query workloads where training\r\nand testing queries have similar plan structures we expect the high\u0002level information available in the plan-level approach to result in\r\nmore accurate predictions.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/db75b5c1-e284-4301-b65b-d993ba27f2a6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb382a2035feb4f35d1e4b89748c67961cad88e6b41b18a5d9039a36311224ea",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 859
      },
      {
        "segments": [
          {
            "segment_id": "db75b5c1-e284-4301-b65b-d993ba27f2a6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "most, if not all, existing DBMSs. As in the case of plan-level mod\u0002eling approach, we use the forward feature selection algorithm, to\r\nbuild accurate prediction models with the relevant set of features.\r\nFeature Name Description\r\nnp Estimated I/O (in number of pages)\r\nnt Estimated number of output tuples\r\nnt1 Estimated number of input tuples\r\n(from left child operator)\r\nnt2 Estimated number of input tuples\r\n(from left right operator)\r\nsel Estimated operator selectivity\r\nst1 Start-time of left child operator\r\nrt1 Run-time of left child operator\r\nst2 Start-time of right child operator\r\nrt2 Run-time of right child operator\r\nTable 2: Features for the operator-level models. Start time\r\nrefers to the time spent in query execution until the first out\u0002put tuple.\r\nThe individual operator models are collectively used to estimate the\r\nexecution latency of a given query by selectively composing them\r\nin a hierarchical manner akin to how optimizers derive query costs\r\nfrom the costs of individual operators. That is, by appropriately\r\nconnecting the inputs and outputs of prediction models following\r\nthe structure of query plans, it is possible to produce predictors for\r\narbitrary queries.\r\nIn Figure 2, we illustrate this process for a simple query plan con\u0002sisting of three operators. The performance prediction operation\r\nworks in a bottom-up manner: each query operator uses its predic\u0002tion models and feature values to produce its start-time and run\u0002time estimates. The estimates produced by an operator are then fed\r\nto the parent operator, which uses them for its own performance\r\nprediction.\r\nFigure 2: Operator-level query performance prediction: operator\r\nmodels use operator-level features together with the predictions of child\r\noperators for performance prediction.\r\n3.3 Plan- versus Operator-level Modeling\r\nThe premise of the plan-level approach is that queries with similar\r\nfeature vectors will have similar query plans and plan statistics, and\r\ntherefore are likely to exhibit similar behavior and performance.\r\nSuch an approach is specifically targeted to scenarios in which the\r\nqueries in the training and test phases have similar execution plans\r\n(e.g., generated from the same query templates or from the same\r\nuser program).\r\nFurthermore, this approach is based on the correlation of the query\r\nexecution plans and statistics with the query execution times. This\r\ncorrelation is used directly in mapping query-plan based features\r\nto execution performance. The high-level modeling approach used\r\nin this case therefore offers the ability to capture the cumulative\r\neffects of a set of lower level underlying factors, such as opera\u0002tor interactions during query processing, on the execution time for\r\neach distinct query plan (in the training data) with a low complexity\r\nmodel.\r\nThe plan-level approach, however, is prone to failure in some com\u0002mon real-world scenarios. A significant problem exists in the case\r\nof dynamic query workloads where queries with unforeseen exe\u0002cution plans are frequently observed. Even worse, there can be\r\nproblems even with static workloads. As the feature values only\r\nrepresent a limited view of a query plan and its execution, it is\r\npossible that different queries can be mapped to very similar fea\u0002ture values and therefore be inaccurately modeled. While it is un\u0002likely for completely different queries to be mapped to identical\r\nfeatures, similar queries can sometimes have different execution\r\nperformance. For instance, increasing the number of time consum\u0002ing aggregate operations in a query will not significantly change its\r\nfeature vector, but may highly increase its execution time. Adding\r\nmore features (e.g., number of aggregates and constraints) to the\r\nmodel would alleviate such issues, however, each added feature\r\nwould also increase the size of the required training data.\r\nBy using multiple prediction models collectively in an hierarchi\u0002cal manner, the operator-level prediction method is able to pro\u0002duce performance predictions for arbitrary queries. Therefore, it\r\nis a more general approach compared to the plan-level method and\r\nhas the potential to be more effective for dynamic query workloads\r\nwhere unforeseen query plan structures are common.\r\nOn the downside, the operator-level prediction method may suffer\r\nfrom drawbacks similar to those that affect analytical cost estima\u0002tion methods (as both methods rely on low-level operator-based\r\nmodels). A key problem is that the prediction errors in the lower\r\nlevels of a query plan are propagated to the upper levels and may\r\nsignificantly degrade the end prediction accuracy.\r\nAnother potential problem is that the concurrent use of multiple\r\nresources such as CPU and disk may not be correctly reflected in\r\nthe operator-level (or the analytical) models. For instance, a query\r\ncould be simply performing an aggregate computation on the rows\r\nof a table that it sequentially scans from the disk. If the per-tuple\r\nprocessing takes less time than reading a tuple from the disk, then\r\nthe query execution time is approximately the same as the sequen\u0002tial scan time. However, if the processing of a tuple takes longer\r\nthan reading it from the disk, then the execution time will be closer\r\nto the processing time. As such, the interactions of the query exe\u0002cution system and the underlying hardware/software platforms can\r\nget quite complex. In such cases, simple operator-level modeling\r\napproaches may fall short of accurately representing this sophisti\u0002cated behavior. Therefore, in static query workloads where training\r\nand testing queries have similar plan structures we expect the high\u0002level information available in the plan-level approach to result in\r\nmore accurate predictions.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/db75b5c1-e284-4301-b65b-d993ba27f2a6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb382a2035feb4f35d1e4b89748c67961cad88e6b41b18a5d9039a36311224ea",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 859
      },
      {
        "segments": [
          {
            "segment_id": "482ea7ba-7c21-4bc0-8896-fee55e0ab7db",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "3.4 Hybrid Modeling\r\nIn hybrid query performance prediction, we combine the operator\u0002and plan- level modeling techniques to obtain an accurate and gen\u0002erally applicable QPP solution. As discussed, this is a general so\u0002lution that works for both static and dynamic workloads. Thus, as\r\nlong as the predictive accuracy is acceptable, operator-level mod\u0002eling is effective. For queries with low operator-level prediction\r\naccuracy, on the other hand, we learn models for specific query sub\u0002plans using plan-level modeling and compose both types of models\r\nto predict the performance of the entire plan. We argue, and later\r\nalso experimentally demonstrate, that this hybrid solution indeed\r\ncombines the relative benefits of the operator-level and plan-level\r\napproaches by not only retaining the generality of the former but\r\nalso yielding predictive accuracy values comparable or much bet\u0002ter than those of the latter.\r\nHybrid QPP Example: To illustrate the hybrid method, we con\u0002sider the performance prediction of an example TPC-H query (gen\u0002erated from TPC-H template-13), whose execution plan is given\r\nin Figure 3. This plan is obtained from a 10GB TPC-H database\r\ninstalled on PostgreSQL. As we describe in detail in the Experi\u0002ments section, we build operator-level models on a training data set\r\nconsisting of example TPC-H query executions. When we use the\r\noperator-level models for performance prediction in this example\r\nquery, we obtain a prediction error (i.e., |true value - estimate| /\r\ntrue value ) of 114%. Upon analysis of the individual prediction\r\nerrors for each operator in the query plan, we realized that the sub\u0002plan rooted at the Materialize operator (highlighted sub-plan in the\r\nfigure) is the root cause of the prediction errors in the upper level\r\nquery operators. The operator-level model based prediction error\r\nfor the materialization sub-plan is 97%.\r\nFigure 3: Hybrid QPP example: plan-level prediction is used for\r\nthe highlighted sub-plan together with operator-level prediction for the\r\nrest of the operators to produce the end query performance prediction.\r\nIn the hybrid approach, we build a separate plan-level model for\r\nthis sub-plan. The model is trained using the occurrences of the\r\nhighlighted sub-plan in the training data. The hybrid method uses\r\nthe plan-level model to directly predict the execution performance\r\nof the materialization sub-plan, while the rest of the prediction op\u0002erations is unchanged, i.e., performed with the operator-level mod\u0002els. The prediction errors obtained with the hybrid approach are\r\nshown with the red values in the figure. The new overall prediction\r\nerror for this example query drops down to 14%.\r\nGiven a training data set consisting of example query executions,\r\nthe goal of the hybrid method is to accurately model the perfor\u0002mance of all queries in the data set using operator-level models\r\ntogether with a minimal number of plan-level models. In this way,\r\nwe maximize the applicability of the operator-level models in query\r\nperformance prediction and maintain high prediction accuracy with\r\nthe integration of plan-level models.\r\nThe hybrid performance prediction method is described in Algo\u0002rithm 1. The algorithm starts by building prediction models for\r\neach query operator based on the provided training data. The ac\u0002curacy of operator-level prediction is then estimated by application\r\non the training data (e.g., either through cross-validation or hold\u0002out test data). Next, the algorithm tries to increase the performance\r\nprediction accuracy by building plan-level models.\r\nEach plan-level model is used for directly modeling the perfor\u0002mance of a separate query plan (or sub-plan). In a query plan\r\nwith N operators, there is a maximum of N − 1 sub-plans (e.g.,\r\nin a chain of operators) for plan-level modeling. Then a training\r\ndata set with M queries can have O(MN) candidate sub-plans for\r\nmodeling.\r\nIn theory, we could build and test plan-level models for each dis\u0002tinct sub-plan (with at least a minimum number of occurrences in\r\nthe training data set) and try to find a minimal subset of these mod\u0002els for which the prediction accuracy is sufficiently high. However,\r\nthis would require a large amount of time since (i) we need to build\r\nand test models for all candidate sub-plans, and (ii) the prediction\r\naccuracy of each subset of models (in increasing sizes) needs to be\r\nseparately estimated with testing.\r\nInstead, we propose heuristics that iteratively build a collection of\r\nplan-level models to maximize the expected predictive accuracy.\r\nIn each iteration, a new plan-level model is built, tested and added\r\nto the model set, if it improves the overall prediction accuracy (by\r\nmore than a threshold value, ǫ). The models are chosen, built and\r\ntested according to plan ordering strategies. We consider the fol\u0002lowing strategies for the hybrid approach:\r\n• Size-based: order the plans in increasing number of op\u0002erators\r\nThe size-based strategy considers generating models for smaller\r\nplans before larger ones. This strategy is based on the fact\r\nthat smaller plans occur more frequently (since by defini\u0002tion all sub-plans of a large plan are at least as frequent) in\r\nany data set, and therefore models for smaller plans are more\r\nlikely to appear in future queries. In case of a tie involving\r\ntwo plans with the same size, the more frequently occurring\r\nplan is given priority.\r\n• Frequency-based: order the plans in decreasing occur\u0002rence frequency across the queries",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/482ea7ba-7c21-4bc0-8896-fee55e0ab7db.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=df4c33d7b0f7033f044f38cfc81919052bb274fa8061382a1806a25069748546",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 844
      },
      {
        "segments": [
          {
            "segment_id": "482ea7ba-7c21-4bc0-8896-fee55e0ab7db",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "3.4 Hybrid Modeling\r\nIn hybrid query performance prediction, we combine the operator\u0002and plan- level modeling techniques to obtain an accurate and gen\u0002erally applicable QPP solution. As discussed, this is a general so\u0002lution that works for both static and dynamic workloads. Thus, as\r\nlong as the predictive accuracy is acceptable, operator-level mod\u0002eling is effective. For queries with low operator-level prediction\r\naccuracy, on the other hand, we learn models for specific query sub\u0002plans using plan-level modeling and compose both types of models\r\nto predict the performance of the entire plan. We argue, and later\r\nalso experimentally demonstrate, that this hybrid solution indeed\r\ncombines the relative benefits of the operator-level and plan-level\r\napproaches by not only retaining the generality of the former but\r\nalso yielding predictive accuracy values comparable or much bet\u0002ter than those of the latter.\r\nHybrid QPP Example: To illustrate the hybrid method, we con\u0002sider the performance prediction of an example TPC-H query (gen\u0002erated from TPC-H template-13), whose execution plan is given\r\nin Figure 3. This plan is obtained from a 10GB TPC-H database\r\ninstalled on PostgreSQL. As we describe in detail in the Experi\u0002ments section, we build operator-level models on a training data set\r\nconsisting of example TPC-H query executions. When we use the\r\noperator-level models for performance prediction in this example\r\nquery, we obtain a prediction error (i.e., |true value - estimate| /\r\ntrue value ) of 114%. Upon analysis of the individual prediction\r\nerrors for each operator in the query plan, we realized that the sub\u0002plan rooted at the Materialize operator (highlighted sub-plan in the\r\nfigure) is the root cause of the prediction errors in the upper level\r\nquery operators. The operator-level model based prediction error\r\nfor the materialization sub-plan is 97%.\r\nFigure 3: Hybrid QPP example: plan-level prediction is used for\r\nthe highlighted sub-plan together with operator-level prediction for the\r\nrest of the operators to produce the end query performance prediction.\r\nIn the hybrid approach, we build a separate plan-level model for\r\nthis sub-plan. The model is trained using the occurrences of the\r\nhighlighted sub-plan in the training data. The hybrid method uses\r\nthe plan-level model to directly predict the execution performance\r\nof the materialization sub-plan, while the rest of the prediction op\u0002erations is unchanged, i.e., performed with the operator-level mod\u0002els. The prediction errors obtained with the hybrid approach are\r\nshown with the red values in the figure. The new overall prediction\r\nerror for this example query drops down to 14%.\r\nGiven a training data set consisting of example query executions,\r\nthe goal of the hybrid method is to accurately model the perfor\u0002mance of all queries in the data set using operator-level models\r\ntogether with a minimal number of plan-level models. In this way,\r\nwe maximize the applicability of the operator-level models in query\r\nperformance prediction and maintain high prediction accuracy with\r\nthe integration of plan-level models.\r\nThe hybrid performance prediction method is described in Algo\u0002rithm 1. The algorithm starts by building prediction models for\r\neach query operator based on the provided training data. The ac\u0002curacy of operator-level prediction is then estimated by application\r\non the training data (e.g., either through cross-validation or hold\u0002out test data). Next, the algorithm tries to increase the performance\r\nprediction accuracy by building plan-level models.\r\nEach plan-level model is used for directly modeling the perfor\u0002mance of a separate query plan (or sub-plan). In a query plan\r\nwith N operators, there is a maximum of N − 1 sub-plans (e.g.,\r\nin a chain of operators) for plan-level modeling. Then a training\r\ndata set with M queries can have O(MN) candidate sub-plans for\r\nmodeling.\r\nIn theory, we could build and test plan-level models for each dis\u0002tinct sub-plan (with at least a minimum number of occurrences in\r\nthe training data set) and try to find a minimal subset of these mod\u0002els for which the prediction accuracy is sufficiently high. However,\r\nthis would require a large amount of time since (i) we need to build\r\nand test models for all candidate sub-plans, and (ii) the prediction\r\naccuracy of each subset of models (in increasing sizes) needs to be\r\nseparately estimated with testing.\r\nInstead, we propose heuristics that iteratively build a collection of\r\nplan-level models to maximize the expected predictive accuracy.\r\nIn each iteration, a new plan-level model is built, tested and added\r\nto the model set, if it improves the overall prediction accuracy (by\r\nmore than a threshold value, ǫ). The models are chosen, built and\r\ntested according to plan ordering strategies. We consider the fol\u0002lowing strategies for the hybrid approach:\r\n• Size-based: order the plans in increasing number of op\u0002erators\r\nThe size-based strategy considers generating models for smaller\r\nplans before larger ones. This strategy is based on the fact\r\nthat smaller plans occur more frequently (since by defini\u0002tion all sub-plans of a large plan are at least as frequent) in\r\nany data set, and therefore models for smaller plans are more\r\nlikely to appear in future queries. In case of a tie involving\r\ntwo plans with the same size, the more frequently occurring\r\nplan is given priority.\r\n• Frequency-based: order the plans in decreasing occur\u0002rence frequency across the queries",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/482ea7ba-7c21-4bc0-8896-fee55e0ab7db.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=df4c33d7b0f7033f044f38cfc81919052bb274fa8061382a1806a25069748546",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 844
      },
      {
        "segments": [
          {
            "segment_id": "70014f81-6aa9-4dc1-a631-e1a7d2a7a6de",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "The frequency-based strategy is similar to the size-based strat\u0002egy except that it directly uses the occurrence frequency of\r\na plan from the training data for ranking. In case the occur\u0002rence frequency is the same for two plans, smaller plans are\r\nconsidered first. An important difference from the size-based\r\nstrategy is that when a large plan has a high occurrence fre\u0002quency, the frequency-based strategy will consider modeling\r\nits sub-plans sequentially before switching to other plans.\r\n• Error-based: order the plans in decreasing value of oc\u0002currence frequency × average prediction error\r\nThe error-based strategy considers plans with respect to their\r\ntotal prediction error across all queries in the training data.\r\nThe assumption is that more accurate modeling of such high\r\nerror plans will more rapidly reduce the overall prediction\r\nerror.\r\nIn all the above strategies, the plans for which (i) the average pre\u0002diction accuracy with the existing models is already above a thresh\u0002old, or (ii) the occurrence frequency is too low are not considered\r\nin model generation.\r\nAlgorithm 1 Hybrid Model Building Algorithm\r\nInput: data = example query executions\r\nInput: strategy = plan selection strategy\r\nInput: target_accuracy = target prediction accuracy\r\nOutput: models = prediction models\r\nOutput: accuracy = estimated prediction accuracy\r\n1. models = build_operator_models(data)\r\n2. [predictions, accuracy] = apply_models(data, models)\r\n3. candidate_plans = get_plan_list(strategy, data, predictions)\r\n4. while accuracy ≤ target_accuracy do\r\n5. plan = get_next(strategy, candidate_plans)\r\n6. plan_model = build_plan_model(data, plan)\r\n7. [predictions, new_accuracy] = apply_models(data, models\r\n∪ plan_model)\r\n8. if new_accuracy − ǫ ≤ accuracy then\r\n9. candidate_plans.remove(plan)\r\n10. else\r\n11. models = models ∪ plan_model\r\n12. candidate_plans.update(predictions, plan_model)\r\n13. accuracy = new_accuracy\r\nIn order to create the list of candidate plans (i.e., candidate_plans)\r\nfor modeling, we traverse the plans of all queries in the training\r\ndata in a depth-first manner in function get_plan_list. Dur\u0002ing the traversal, this function builds a hash-based index using keys\r\nbased on plan tree structures. In this way, all occurrences of a plan\r\nstructure are hashed to the same value and metrics required by the\r\nheuristic strategies such as the occurrence frequency and average\r\nprediction error can be easily computed.\r\nWhen a new plan-level model is added to the set of chosen models\r\n(i.e., models), the candidate plan list is updated with the new pre\u0002diction errors and occurrence frequencies for all plans. The occur\u0002rence frequency of a plan p will change with the addition of a model\r\nwhen the plan for the added model contains p as a sub-plan (since\r\nsuch occurrences of p are consumed by the newly added model).\r\nWe can efficiently identify the set of plans for which the prediction\r\nerrors or the occurrence frequencies might change with the addition\r\nof a model as follows: For each plan-level model, we need to keep\r\ntrack of the set of queries to which they can be applied, and in turn,\r\nfor each query we need to keep track of the set of applicable plan\u0002level models. When a new model is added, the only plans that need\r\nto be updated are the plans that can be applied to one or more of\r\nthe queries that the newly added plan is also applicable.\r\nFinally, in cases where the target accuracy is unachievable, a max\u0002imum number of iterations can be used to terminate the algorithm.\r\nOther variations for the stopping condition, such as setting a maxi\u0002mum number of iterations without accuracy improvement, are also\r\npossible but not evaluated in this study.\r\n4. ONLINE MODEL BUILDING\r\nIn dynamic query workloads, where queries with unforeseen plan\r\nstructures are present, the plan-level performance prediction method\r\nperforms poorly due to lack of good training data. The operator\u0002level and the hybrid prediction methods are designed to be much\r\nmore applicable to unforeseen plan structures. In addition, the hy\u0002brid method will utilize its plan-level models as much as possible to\r\nprovide accuracy levels much higher than those achievable through\r\npure operator-level modeling.\r\nThe prediction accuracy of the hybrid approach in dynamic work\u0002load scenarios depends on the applicability of its plan-level mod\u0002els in future queries. As a case study, we analyze the generated\r\nexecution plans for the TPC-H query workload on a 10GB TPC-H\r\ndatabase running on PostgreSQL. In Figure 4(b), we show the most\r\ncommon sub-plans within the execution plans of queries generated\r\nfrom 14 TPC-H templates for which we could use operator-level\r\nprediction techniques in our experiments (See Experiments Section\r\nfor more details.). Our key observations for this data set include:\r\n(1) Smaller sub-plans are more common across the TPC-H query\r\nplans (see Figure 4(a)).\r\n(2) The plans for the queries of each TPC-H template (except\r\ntemplate-6) share common sub-plans with the plans of queries\r\nof at least one other TPC-H template (see Figure 4(c)).\r\nThese observations suggest that for the TPC-H workload: (i) it is\r\npossible to create plan-level models based on the execution plans\r\nfor the queries of a TPC-H template and utilize them in the perfor\u0002mance prediction of queries from other TPC-H templates, and (ii)\r\nthe size-based plan ordering strategy discussed in Section 3.4 will\r\nlikely achieve higher applicability compared to the other strategies\r\nin the dynamic workload case.\r\nHowever, the hybrid approach may fail to increase the prediction\r\naccuracy for dynamic workloads in some cases. For example, the\r\nprediction errors for some unforeseen query plans may not origi\u0002nate from the common sub-plans, and as a result, plan-level mod\u0002els from the training data cannot reduce the error. In other cases,\r\nthe common sub-plans could actually be the source of prediction\r\nerrors, but the plan-ordering strategies may not necessarily choose\r\nto build plan-level models for them. For instance, some applicable\r\nplan-level models may be discarded, because they did not improve\r\nthe prediction accuracy in training.\r\nIn the online modeling technique, we build new plan-level models\r\nfor performance prediction at run-time upon the receipt of a query.\r\nWe initially produce predictions with the set of existing models,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/70014f81-6aa9-4dc1-a631-e1a7d2a7a6de.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6b4329eb17f894dfd359bb1900db5d440aa733f7eff0cd3fb81957bedd0ae6aa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 968
      },
      {
        "segments": [
          {
            "segment_id": "70014f81-6aa9-4dc1-a631-e1a7d2a7a6de",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "The frequency-based strategy is similar to the size-based strat\u0002egy except that it directly uses the occurrence frequency of\r\na plan from the training data for ranking. In case the occur\u0002rence frequency is the same for two plans, smaller plans are\r\nconsidered first. An important difference from the size-based\r\nstrategy is that when a large plan has a high occurrence fre\u0002quency, the frequency-based strategy will consider modeling\r\nits sub-plans sequentially before switching to other plans.\r\n• Error-based: order the plans in decreasing value of oc\u0002currence frequency × average prediction error\r\nThe error-based strategy considers plans with respect to their\r\ntotal prediction error across all queries in the training data.\r\nThe assumption is that more accurate modeling of such high\r\nerror plans will more rapidly reduce the overall prediction\r\nerror.\r\nIn all the above strategies, the plans for which (i) the average pre\u0002diction accuracy with the existing models is already above a thresh\u0002old, or (ii) the occurrence frequency is too low are not considered\r\nin model generation.\r\nAlgorithm 1 Hybrid Model Building Algorithm\r\nInput: data = example query executions\r\nInput: strategy = plan selection strategy\r\nInput: target_accuracy = target prediction accuracy\r\nOutput: models = prediction models\r\nOutput: accuracy = estimated prediction accuracy\r\n1. models = build_operator_models(data)\r\n2. [predictions, accuracy] = apply_models(data, models)\r\n3. candidate_plans = get_plan_list(strategy, data, predictions)\r\n4. while accuracy ≤ target_accuracy do\r\n5. plan = get_next(strategy, candidate_plans)\r\n6. plan_model = build_plan_model(data, plan)\r\n7. [predictions, new_accuracy] = apply_models(data, models\r\n∪ plan_model)\r\n8. if new_accuracy − ǫ ≤ accuracy then\r\n9. candidate_plans.remove(plan)\r\n10. else\r\n11. models = models ∪ plan_model\r\n12. candidate_plans.update(predictions, plan_model)\r\n13. accuracy = new_accuracy\r\nIn order to create the list of candidate plans (i.e., candidate_plans)\r\nfor modeling, we traverse the plans of all queries in the training\r\ndata in a depth-first manner in function get_plan_list. Dur\u0002ing the traversal, this function builds a hash-based index using keys\r\nbased on plan tree structures. In this way, all occurrences of a plan\r\nstructure are hashed to the same value and metrics required by the\r\nheuristic strategies such as the occurrence frequency and average\r\nprediction error can be easily computed.\r\nWhen a new plan-level model is added to the set of chosen models\r\n(i.e., models), the candidate plan list is updated with the new pre\u0002diction errors and occurrence frequencies for all plans. The occur\u0002rence frequency of a plan p will change with the addition of a model\r\nwhen the plan for the added model contains p as a sub-plan (since\r\nsuch occurrences of p are consumed by the newly added model).\r\nWe can efficiently identify the set of plans for which the prediction\r\nerrors or the occurrence frequencies might change with the addition\r\nof a model as follows: For each plan-level model, we need to keep\r\ntrack of the set of queries to which they can be applied, and in turn,\r\nfor each query we need to keep track of the set of applicable plan\u0002level models. When a new model is added, the only plans that need\r\nto be updated are the plans that can be applied to one or more of\r\nthe queries that the newly added plan is also applicable.\r\nFinally, in cases where the target accuracy is unachievable, a max\u0002imum number of iterations can be used to terminate the algorithm.\r\nOther variations for the stopping condition, such as setting a maxi\u0002mum number of iterations without accuracy improvement, are also\r\npossible but not evaluated in this study.\r\n4. ONLINE MODEL BUILDING\r\nIn dynamic query workloads, where queries with unforeseen plan\r\nstructures are present, the plan-level performance prediction method\r\nperforms poorly due to lack of good training data. The operator\u0002level and the hybrid prediction methods are designed to be much\r\nmore applicable to unforeseen plan structures. In addition, the hy\u0002brid method will utilize its plan-level models as much as possible to\r\nprovide accuracy levels much higher than those achievable through\r\npure operator-level modeling.\r\nThe prediction accuracy of the hybrid approach in dynamic work\u0002load scenarios depends on the applicability of its plan-level mod\u0002els in future queries. As a case study, we analyze the generated\r\nexecution plans for the TPC-H query workload on a 10GB TPC-H\r\ndatabase running on PostgreSQL. In Figure 4(b), we show the most\r\ncommon sub-plans within the execution plans of queries generated\r\nfrom 14 TPC-H templates for which we could use operator-level\r\nprediction techniques in our experiments (See Experiments Section\r\nfor more details.). Our key observations for this data set include:\r\n(1) Smaller sub-plans are more common across the TPC-H query\r\nplans (see Figure 4(a)).\r\n(2) The plans for the queries of each TPC-H template (except\r\ntemplate-6) share common sub-plans with the plans of queries\r\nof at least one other TPC-H template (see Figure 4(c)).\r\nThese observations suggest that for the TPC-H workload: (i) it is\r\npossible to create plan-level models based on the execution plans\r\nfor the queries of a TPC-H template and utilize them in the perfor\u0002mance prediction of queries from other TPC-H templates, and (ii)\r\nthe size-based plan ordering strategy discussed in Section 3.4 will\r\nlikely achieve higher applicability compared to the other strategies\r\nin the dynamic workload case.\r\nHowever, the hybrid approach may fail to increase the prediction\r\naccuracy for dynamic workloads in some cases. For example, the\r\nprediction errors for some unforeseen query plans may not origi\u0002nate from the common sub-plans, and as a result, plan-level mod\u0002els from the training data cannot reduce the error. In other cases,\r\nthe common sub-plans could actually be the source of prediction\r\nerrors, but the plan-ordering strategies may not necessarily choose\r\nto build plan-level models for them. For instance, some applicable\r\nplan-level models may be discarded, because they did not improve\r\nthe prediction accuracy in training.\r\nIn the online modeling technique, we build new plan-level models\r\nfor performance prediction at run-time upon the receipt of a query.\r\nWe initially produce predictions with the set of existing models,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/70014f81-6aa9-4dc1-a631-e1a7d2a7a6de.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6b4329eb17f894dfd359bb1900db5d440aa733f7eff0cd3fb81957bedd0ae6aa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 968
      },
      {
        "segments": [
          {
            "segment_id": "1ca060b4-30aa-410b-8b18-88071b2f8a4b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "2 4 6 8 10 12 14 16\r\n0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\n0.5\r\n0.6\r\n0.7\r\n0.8\r\n0.9\r\n1\r\nCommon Sub−plan Size\r\nF(x)\r\n(a) CDF for common sub-plan sizes\r\n(#query operators)\r\n(b) 6 most common sub-plans across queries\r\nof 14 TPC-H Templates\r\n1 3 4 5 6 7 8 9 10 12 13 14 18 19\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nTPC−H Template\r\n#Templates with Common Plans\r\n(c) #templates the queries of a TPC-H tem\u0002plate shares common sub-plans with\r\nFigure 4: Analysis of common sub-plans for the execution plans of queries generated from 14 TPC-H Templates.\r\nand then update our results after new plan-level models are built\r\nfor the received query.\r\nOnline model building is performed similarly to offline model build\u0002ing described for the hybrid method. However, in the online case,\r\nthe set of candidate plans are generated based on the set of sub\u0002plans of the execution plan for the newly received query. The on\u0002line building of plan-level models guarantee that if the execution\r\nplan for a test query has a common sub-plan (with high prediction\r\nerror) with the queries in the training data, then a plan-level model\r\nwill be built and used for its prediction (if a plan-level model with\r\nbetter estimated accuracy than the operator-level prediction method\r\nexists).\r\n5. EXPERIMENTS\r\n5.1 Setup\r\nOur experimental study uses the TPC-H decision support bench\u0002mark [6] implemented on top of PostgreSQL. The details are pre\u0002sented below.\r\nDatabase Management System. We use an instrumented version\r\nof PostgreSQL 8.4.1. The instrumentation code monitored fea\u0002tures and performance metrics from query executions; i.e., for each\r\nquery, the execution plan, the optimizer estimates and the actual\r\nvalues of features as well as the performance metrics were logged.\r\nData sets and workload. We created 10GB and 1GB TPC-H databases\r\naccording to the specification. The primary key indices as indicated\r\nin the TPC-H specification were created for both databases. We\r\nenforced a limit of one hour execution time to keep the overall ex\u0002perimentation duration under control. This resulted in 18 of the 22\r\nTPC-H templates being used, as the remaining 4 templates always\r\ntook longer than 1 hour to execute in the 10GB case.\r\nThere are approximately 55 queries from each template in both\r\ndatabases. With the 1GB database, all queries finish under an hour\r\nand the data set contains 1000 queries. On the other hand, with\r\nthe 10GB database only 17 of the queries from template-9 finished\r\nwithin an hour, so we have 17 template-9 queries in the 10GB\r\ndata set. Thus, the resulting 10GB data set we used contains 960\r\nqueries.\r\nHardware. The queries were executed on a single commodity server\r\nwith 4GB RAM running Ubuntu with kernel 2.6.28. The database\r\nbuffer pool size was set to 1GB (25% of the total RAM as the rule\r\nof thumb). All queries were executed sequentially with cold start\r\n(i.e., both filesystem and DB buffers were flushed before the start\r\nof each query).\r\nPredictive models. We used Support Vector Machines (available\r\nfrom the libsvm library [5]) with the nu-SVR kernel for support\u0002vector based regression and linear regression models (available from\r\nthe Shark machine learning library [7]) for plan- and operator-level\r\nmodeling, respectively. Both models were integrated to the database\r\nas user defined functions. Our algorithms were implemented as a\r\ncombination of C-based user-defined functions in PostgreSQL and\r\nas external applications written in C++ and Python. A forward fea\u0002ture selection algorithm, described in Section 2, was used to build\r\naccurate prediction models using a small number of features.\r\nMetrics and validation. We use the mean relative error as our error\r\nmetric:\r\n1\r\nN\r\nXN\r\ni=1\r\n|actuali − estimatei|\r\nactuali\r\nThis error is useful when we would like to minimize the relative\r\nprediction error in all queries regardless of their execution time.\r\nNon-relative error metrics such as square error would be better for\r\nminimizing the absolute difference (or its square) in actual and pre\u0002dicted execution times. Other types of metrics include R\r\n2\r\nor pre\u0002dictive risk [1]. These metrics measure the performance of the esti\u0002mates with respect to a point estimate (i.e., the mean). As such, in\r\nmany cases, they can have deceptively low error values even though\r\nthe actual estimates have high error, as these metrics depend on the\r\nscale and statistical characteristics of the entire data set.\r\nOur results, except for the dynamic workload cases are based on\r\n5-fold cross validation. That is, the data is divided into 5 equal\r\nparts, and 4 parts are used to build models for prediction on the\r\nremaining part. This process is repeated 5 times, i.e., all parts are\r\nused in testing. The reported prediction accuracy is the average\r\nof the individual accuracy values from the testing of each cross\u0002validation part. We used stratified sampling for dividing the data\r\ninto 5 parts to ensure that each part contains roughly equal number",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/1ca060b4-30aa-410b-8b18-88071b2f8a4b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=dc59d83e0603cdbf3e593403a8b6f02f6619d30286a778074d934cdc5f7de0cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 798
      },
      {
        "segments": [
          {
            "segment_id": "1ca060b4-30aa-410b-8b18-88071b2f8a4b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "2 4 6 8 10 12 14 16\r\n0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\n0.5\r\n0.6\r\n0.7\r\n0.8\r\n0.9\r\n1\r\nCommon Sub−plan Size\r\nF(x)\r\n(a) CDF for common sub-plan sizes\r\n(#query operators)\r\n(b) 6 most common sub-plans across queries\r\nof 14 TPC-H Templates\r\n1 3 4 5 6 7 8 9 10 12 13 14 18 19\r\n0\r\n2\r\n4\r\n6\r\n8\r\n10\r\nTPC−H Template\r\n#Templates with Common Plans\r\n(c) #templates the queries of a TPC-H tem\u0002plate shares common sub-plans with\r\nFigure 4: Analysis of common sub-plans for the execution plans of queries generated from 14 TPC-H Templates.\r\nand then update our results after new plan-level models are built\r\nfor the received query.\r\nOnline model building is performed similarly to offline model build\u0002ing described for the hybrid method. However, in the online case,\r\nthe set of candidate plans are generated based on the set of sub\u0002plans of the execution plan for the newly received query. The on\u0002line building of plan-level models guarantee that if the execution\r\nplan for a test query has a common sub-plan (with high prediction\r\nerror) with the queries in the training data, then a plan-level model\r\nwill be built and used for its prediction (if a plan-level model with\r\nbetter estimated accuracy than the operator-level prediction method\r\nexists).\r\n5. EXPERIMENTS\r\n5.1 Setup\r\nOur experimental study uses the TPC-H decision support bench\u0002mark [6] implemented on top of PostgreSQL. The details are pre\u0002sented below.\r\nDatabase Management System. We use an instrumented version\r\nof PostgreSQL 8.4.1. The instrumentation code monitored fea\u0002tures and performance metrics from query executions; i.e., for each\r\nquery, the execution plan, the optimizer estimates and the actual\r\nvalues of features as well as the performance metrics were logged.\r\nData sets and workload. We created 10GB and 1GB TPC-H databases\r\naccording to the specification. The primary key indices as indicated\r\nin the TPC-H specification were created for both databases. We\r\nenforced a limit of one hour execution time to keep the overall ex\u0002perimentation duration under control. This resulted in 18 of the 22\r\nTPC-H templates being used, as the remaining 4 templates always\r\ntook longer than 1 hour to execute in the 10GB case.\r\nThere are approximately 55 queries from each template in both\r\ndatabases. With the 1GB database, all queries finish under an hour\r\nand the data set contains 1000 queries. On the other hand, with\r\nthe 10GB database only 17 of the queries from template-9 finished\r\nwithin an hour, so we have 17 template-9 queries in the 10GB\r\ndata set. Thus, the resulting 10GB data set we used contains 960\r\nqueries.\r\nHardware. The queries were executed on a single commodity server\r\nwith 4GB RAM running Ubuntu with kernel 2.6.28. The database\r\nbuffer pool size was set to 1GB (25% of the total RAM as the rule\r\nof thumb). All queries were executed sequentially with cold start\r\n(i.e., both filesystem and DB buffers were flushed before the start\r\nof each query).\r\nPredictive models. We used Support Vector Machines (available\r\nfrom the libsvm library [5]) with the nu-SVR kernel for support\u0002vector based regression and linear regression models (available from\r\nthe Shark machine learning library [7]) for plan- and operator-level\r\nmodeling, respectively. Both models were integrated to the database\r\nas user defined functions. Our algorithms were implemented as a\r\ncombination of C-based user-defined functions in PostgreSQL and\r\nas external applications written in C++ and Python. A forward fea\u0002ture selection algorithm, described in Section 2, was used to build\r\naccurate prediction models using a small number of features.\r\nMetrics and validation. We use the mean relative error as our error\r\nmetric:\r\n1\r\nN\r\nXN\r\ni=1\r\n|actuali − estimatei|\r\nactuali\r\nThis error is useful when we would like to minimize the relative\r\nprediction error in all queries regardless of their execution time.\r\nNon-relative error metrics such as square error would be better for\r\nminimizing the absolute difference (or its square) in actual and pre\u0002dicted execution times. Other types of metrics include R\r\n2\r\nor pre\u0002dictive risk [1]. These metrics measure the performance of the esti\u0002mates with respect to a point estimate (i.e., the mean). As such, in\r\nmany cases, they can have deceptively low error values even though\r\nthe actual estimates have high error, as these metrics depend on the\r\nscale and statistical characteristics of the entire data set.\r\nOur results, except for the dynamic workload cases are based on\r\n5-fold cross validation. That is, the data is divided into 5 equal\r\nparts, and 4 parts are used to build models for prediction on the\r\nremaining part. This process is repeated 5 times, i.e., all parts are\r\nused in testing. The reported prediction accuracy is the average\r\nof the individual accuracy values from the testing of each cross\u0002validation part. We used stratified sampling for dividing the data\r\ninto 5 parts to ensure that each part contains roughly equal number",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/1ca060b4-30aa-410b-8b18-88071b2f8a4b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=dc59d83e0603cdbf3e593403a8b6f02f6619d30286a778074d934cdc5f7de0cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 798
      },
      {
        "segments": [
          {
            "segment_id": "db4fb8c3-b517-4d16-a932-da4f8d8e16a0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "of queries from each template.\r\n5.2 Prediction with Optimizer Cost Models\r\nWe start with results showing predictions on top of analytical cost\r\nmodels used by conventional optimizers are non-starters for QPP.\r\nSpecifically, we built a linear regression model to predict the query\r\nexecution times based on the query optimizer cost estimates. Over\u0002all, the maximum relative error is 1744%, the minimum relative\r\nerror is 30% and the mean relative error is 120%1.\r\nTo provide more intuition into the reasons, we show the optimizer\r\ncosts versus the query execution times for a subset of the queries\r\n(a stratified sample) on the 10GB TPC-H data set in Figure 5. Ob\u0002serve that the lower left and lower right data points correspond to\r\nqueries with roughly the same execution times, even though their\r\ncost estimates have a magnitude of difference.\r\nIn this setup, most queries are I/O intensive. We expect this to be\r\nthe ideal case for predicting with analytical cost models. The rea\u0002son is that optimizer cost models generally rely on the assumption\r\nthat I/O is the most time consuming operation. Therefore, for CPU\r\nintensive workloads, we would expect to see even lower accuracy\r\nvalues.\r\nAs a concrete example, consider TPC-H template-1, which includes\r\nan aggregate over numeric types. We observed that evaluating\r\nsuch aggregates can easily become the bottleneck, because arith\u0002metic operations are performed in software rather than hardware.\r\nAs such, introducing additional aggregates to a query will signifi\u0002cantly alter the execution time even though the volume of I/O (and\r\nhence the predictions with the cost model) will remain approxi\u0002mately constant.\r\n106107\r\n101\r\n102\r\n103\r\n104\r\nOptimizer Cost Estimate\r\nQuery Execution Time (sec)\r\nLeast Squares Fit Line\r\nFigure 5: Optimizer Cost vs Query Execution Time\r\n5.3 Predicting for Static Workloads\r\nResults for the plan-level and operator-level prediction methods\r\nare given in Figure 6 both for the 10GB and 1GB TPC-H scenar\u0002ios. These results were obtained using estimate-based features for\r\n1\r\nIn this case, the predictive risk [1] is about .93, which is close to\r\n1. This result suggests that it performs much better compared to a\r\npoint estimate, although the actual relative errors per query as we\r\nreported are high.\r\nbuilding models in training and for prediction in testing. The use of\r\nactual (observed) values for features is discussed in Section 5.3.3.\r\n5.3.1 Plan-level Modeling\r\nThe plan-level prediction results contain values for all the 18 TPC\u0002H templates. Overall, the average relative prediction errors are\r\n6.75% and 17.43% for the 10GB and 1GB databases, respectively\r\n(Figure 6(a)-(c)), implying that plan-level modeling can be very ef\u0002fective for static workloads. The difference between the results for\r\nthe two databases can be explained by the respective ratios of the\r\nstandard deviation to the average execution time of queries, which\r\nis about 2.63 times greater in the 1GB database case. This charac\u0002teristics makes the 1GB case fundamentally more difficult to pre\u0002dict.\r\nIn both cases, queries from template-9 stand out as the worst pre\u0002dicted set of queries. We note that template-9 queries take much\r\nlonger than the queries of the other templates. As the number of\r\ninstances of template 9, and therefore of longer running queries,\r\nis relatively few in both data sets, the prediction models do not fit\r\nwell. To alleviate this problem, we built a separate prediction model\r\nfor template-9 for the 10GB case, which reduced its error down to\r\n7%.\r\n5.3.2 Operator-level Modeling\r\nWe now show operator-level prediction results on 14 of the 18 TPC\u0002H templates2\r\n.\r\nFor the 10GB case, in 11 of the 14 templates the operator-level\r\nprediction method performed better than 20% error (Figure 6(d)).\r\nFor these 11 templates the average error is 7.30%. The error, how\u0002ever, goes up to 53.92% when we consider all the 14 templates, a\r\nsignificant degradation.\r\nFor the 1GB scenario, we show the results of operator-level predic\u0002tion for the 14 TPC-H templates in Figure 6(f). In this case, for 8 of\r\nthe templates the average error is below 25% and the mean error is\r\n16.45%. However, the mean error for all the 14 TPC-H templates\r\nis 59.57% (slightly larger than the 10GB case).\r\nWe see that operator-level prediction produces modest errors for\r\nmany cases, but also does perform poorly for some. We analyzed\r\nthe set of templates that belongs to the latter case, and noticed that\r\nthey commonly exhibit one or more of the following properties:\r\n• (Estimation errors) the optimizer statistic estimates are sig\u0002nificantly inaccurate.\r\n• (I/O-compute overlap) there is significant computation and\r\nI/O overlap in the query. The end-effect of such concur\u0002rent behavior on execution time is difficult to capture due\r\nto pipelining.\r\n• (Operator interactions) The operators of the same query heav\u0002ily interact with each other (e.g., multiple scans on the same\r\ntable that use the same cached data).\r\n2The execution plans for the queries of the remaining 4 templates\r\ncontain PostgreSQL-specific structures, namely INITPLAN and\r\nSUBQUERY, which lead to non-standard (i.e., non tree-based) ex\u0002ecution plans with which our current operator-level models cannot\r\ncope at present.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/db4fb8c3-b517-4d16-a932-da4f8d8e16a0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=614c9bff94a33e4c45d50cb8b68e3d039ae21bd43bf6c78ad444b008ce511e32",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 821
      },
      {
        "segments": [
          {
            "segment_id": "db4fb8c3-b517-4d16-a932-da4f8d8e16a0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "of queries from each template.\r\n5.2 Prediction with Optimizer Cost Models\r\nWe start with results showing predictions on top of analytical cost\r\nmodels used by conventional optimizers are non-starters for QPP.\r\nSpecifically, we built a linear regression model to predict the query\r\nexecution times based on the query optimizer cost estimates. Over\u0002all, the maximum relative error is 1744%, the minimum relative\r\nerror is 30% and the mean relative error is 120%1.\r\nTo provide more intuition into the reasons, we show the optimizer\r\ncosts versus the query execution times for a subset of the queries\r\n(a stratified sample) on the 10GB TPC-H data set in Figure 5. Ob\u0002serve that the lower left and lower right data points correspond to\r\nqueries with roughly the same execution times, even though their\r\ncost estimates have a magnitude of difference.\r\nIn this setup, most queries are I/O intensive. We expect this to be\r\nthe ideal case for predicting with analytical cost models. The rea\u0002son is that optimizer cost models generally rely on the assumption\r\nthat I/O is the most time consuming operation. Therefore, for CPU\r\nintensive workloads, we would expect to see even lower accuracy\r\nvalues.\r\nAs a concrete example, consider TPC-H template-1, which includes\r\nan aggregate over numeric types. We observed that evaluating\r\nsuch aggregates can easily become the bottleneck, because arith\u0002metic operations are performed in software rather than hardware.\r\nAs such, introducing additional aggregates to a query will signifi\u0002cantly alter the execution time even though the volume of I/O (and\r\nhence the predictions with the cost model) will remain approxi\u0002mately constant.\r\n106107\r\n101\r\n102\r\n103\r\n104\r\nOptimizer Cost Estimate\r\nQuery Execution Time (sec)\r\nLeast Squares Fit Line\r\nFigure 5: Optimizer Cost vs Query Execution Time\r\n5.3 Predicting for Static Workloads\r\nResults for the plan-level and operator-level prediction methods\r\nare given in Figure 6 both for the 10GB and 1GB TPC-H scenar\u0002ios. These results were obtained using estimate-based features for\r\n1\r\nIn this case, the predictive risk [1] is about .93, which is close to\r\n1. This result suggests that it performs much better compared to a\r\npoint estimate, although the actual relative errors per query as we\r\nreported are high.\r\nbuilding models in training and for prediction in testing. The use of\r\nactual (observed) values for features is discussed in Section 5.3.3.\r\n5.3.1 Plan-level Modeling\r\nThe plan-level prediction results contain values for all the 18 TPC\u0002H templates. Overall, the average relative prediction errors are\r\n6.75% and 17.43% for the 10GB and 1GB databases, respectively\r\n(Figure 6(a)-(c)), implying that plan-level modeling can be very ef\u0002fective for static workloads. The difference between the results for\r\nthe two databases can be explained by the respective ratios of the\r\nstandard deviation to the average execution time of queries, which\r\nis about 2.63 times greater in the 1GB database case. This charac\u0002teristics makes the 1GB case fundamentally more difficult to pre\u0002dict.\r\nIn both cases, queries from template-9 stand out as the worst pre\u0002dicted set of queries. We note that template-9 queries take much\r\nlonger than the queries of the other templates. As the number of\r\ninstances of template 9, and therefore of longer running queries,\r\nis relatively few in both data sets, the prediction models do not fit\r\nwell. To alleviate this problem, we built a separate prediction model\r\nfor template-9 for the 10GB case, which reduced its error down to\r\n7%.\r\n5.3.2 Operator-level Modeling\r\nWe now show operator-level prediction results on 14 of the 18 TPC\u0002H templates2\r\n.\r\nFor the 10GB case, in 11 of the 14 templates the operator-level\r\nprediction method performed better than 20% error (Figure 6(d)).\r\nFor these 11 templates the average error is 7.30%. The error, how\u0002ever, goes up to 53.92% when we consider all the 14 templates, a\r\nsignificant degradation.\r\nFor the 1GB scenario, we show the results of operator-level predic\u0002tion for the 14 TPC-H templates in Figure 6(f). In this case, for 8 of\r\nthe templates the average error is below 25% and the mean error is\r\n16.45%. However, the mean error for all the 14 TPC-H templates\r\nis 59.57% (slightly larger than the 10GB case).\r\nWe see that operator-level prediction produces modest errors for\r\nmany cases, but also does perform poorly for some. We analyzed\r\nthe set of templates that belongs to the latter case, and noticed that\r\nthey commonly exhibit one or more of the following properties:\r\n• (Estimation errors) the optimizer statistic estimates are sig\u0002nificantly inaccurate.\r\n• (I/O-compute overlap) there is significant computation and\r\nI/O overlap in the query. The end-effect of such concur\u0002rent behavior on execution time is difficult to capture due\r\nto pipelining.\r\n• (Operator interactions) The operators of the same query heav\u0002ily interact with each other (e.g., multiple scans on the same\r\ntable that use the same cached data).\r\n2The execution plans for the queries of the remaining 4 templates\r\ncontain PostgreSQL-specific structures, namely INITPLAN and\r\nSUBQUERY, which lead to non-standard (i.e., non tree-based) ex\u0002ecution plans with which our current operator-level models cannot\r\ncope at present.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/db4fb8c3-b517-4d16-a932-da4f8d8e16a0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=614c9bff94a33e4c45d50cb8b68e3d039ae21bd43bf6c78ad444b008ce511e32",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 821
      },
      {
        "segments": [
          {
            "segment_id": "1c1c63db-b278-4cd2-b4df-33a5cdf987f4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 19 22\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n80.1\r\nTPC−H Template\r\nRelative Error (%)\r\nTemplate Error\r\nAvg. Error\r\n(a) Plan-level, Errors by Template (10GB)\r\n101102103104\r\n101\r\n102\r\n103\r\n104\r\nQuery Execution Time (sec)\r\nQuery Execution Time (sec)\r\nTrue Value\r\nEstimate\r\n(b) Plan-level Prediction (10GB)\r\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 19 22\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n75.5 89.7\r\nTPC−H Template\r\nRelative Error (%)\r\nTemplate Error\r\nAvg. Error\r\n(c) Plan-level, Errors by Template (1GB)\r\n1 3 4 5 6 7 8 9 10 12 13 14 18 19\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n380 114 163\r\nTPC−H Template\r\nRelative Error (%)\r\n(d) Operator-level, Errors by Template\r\n(10GB)\r\n102.4 102.6 102.8\r\n102.4\r\n102.5\r\n102.6\r\n102.7\r\n102.8\r\n102.9\r\nQuery Execution Time (sec)\r\nQuery Execution Time (sec)\r\nTrue Value\r\nEstimate\r\n(e) Operator-level Prediction (10GB)\r\n1 3 4 5 6 7 8 9 10 12 13 14 18 19\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n74 115 59 269 89 100\r\nTPC−H Template\r\nRelative Error (%)\r\n(f) Operator-level, Errors by Template (1GB)\r\nFigure 6: Static workload experiments with plan-level and operator-level prediction methods in 1GB and 10GB TPC-H databases.\r\nThe error values in bar-plots are capped at 50. Error values beyond the limits of the plots are printed next to their corresponding\r\nbars.\r\nNext, we discuss the practical impact of statistics estimation errors\r\non model accuracy. We then turn to the latter two issues that rep\u0002resent the fundamental limitations of operator-level modeling; that\r\nis, such models learn operator behavior “in isolation” without rep\u0002resenting the context within which they are occur.\r\n5.3.3 Impact of Estimation Errors\r\nWe tried all the combinations of actual and estimate feature values\r\nfor training and testing for plan-level and operator-level prediction.\r\nThe results are given in Figure 7(a) for the 10GB scenario. For fur\u0002ther detail, we also show the prediction errors grouped by TPC-H\r\ntemplates in Figure 7(b) for the actual/actual case and plan-level\r\nprediction (over the 10GB scenario). These results are to be com\u0002pared with those in Figure 6(a).\r\nUnsurprisingly, the best results are obtained in the actual/actual\r\ncase (i.e., training and testing with actual feature values), which is\r\nnot a viable option in practice due to the unavailability of the actual\r\nfeature values without running the queries. The next best results are\r\nobtained with the estimate/estimate option (i.e., training and testing\r\nwith estimated feature values), the option that we used in the rest\r\nof the paper. Finally, the results obtained with actual/estimate (i.e.,\r\ntraining on actual values and testing on estimates) are much worse\r\nthan the other two, primarily due to optimizer estimation errors that\r\nare not taken into account during training.\r\nTo provide a sense of the magnitude of the estimation errors made\r\nby the optimizer, consider template-18, which is one of the tem\u0002plates that exhibit the biggest error in operator-level prediction with\r\nactual/estimate model building. Instances of template-18 include\r\n(a) Prediction with Actual\r\nValues vs Estimates\r\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 19 22\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n54.4\r\nTPC−H Template\r\nRelative Error (%)\r\nTemplate Error\r\nAvg. Error\r\n(b) Plan-level Modeling with Actual\r\nValues (10GB)\r\nFigure 7: Impact of Estimation Errors on Prediction Accuracy\r\nin Static Workload Experiments\r\nthe following group by clause on table lineitem:\r\ngroup by l_orderkey having sum(l_quantity) > 314\r\nThere are 15 million distinct l_orderkey values in lineitem\r\n(out of approximately 60 million tuples). The estimated number of\r\ngroups satisfying sum(l_quantity) > 314 is 399521, whereas\r\nthe actual number is 84. The PostgreSQL query optimizer com\u0002putes this estimate using histograms (with 100 bins) for each col\u0002umn based on the attribute independence assumption. The results",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/1c1c63db-b278-4cd2-b4df-33a5cdf987f4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9916f56cecdc1bad2a579ab5d7959e8e590d03592e745c4b071643bd7373e947",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 652
      },
      {
        "segments": [
          {
            "segment_id": "1c1c63db-b278-4cd2-b4df-33a5cdf987f4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 19 22\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n80.1\r\nTPC−H Template\r\nRelative Error (%)\r\nTemplate Error\r\nAvg. Error\r\n(a) Plan-level, Errors by Template (10GB)\r\n101102103104\r\n101\r\n102\r\n103\r\n104\r\nQuery Execution Time (sec)\r\nQuery Execution Time (sec)\r\nTrue Value\r\nEstimate\r\n(b) Plan-level Prediction (10GB)\r\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 19 22\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n75.5 89.7\r\nTPC−H Template\r\nRelative Error (%)\r\nTemplate Error\r\nAvg. Error\r\n(c) Plan-level, Errors by Template (1GB)\r\n1 3 4 5 6 7 8 9 10 12 13 14 18 19\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n380 114 163\r\nTPC−H Template\r\nRelative Error (%)\r\n(d) Operator-level, Errors by Template\r\n(10GB)\r\n102.4 102.6 102.8\r\n102.4\r\n102.5\r\n102.6\r\n102.7\r\n102.8\r\n102.9\r\nQuery Execution Time (sec)\r\nQuery Execution Time (sec)\r\nTrue Value\r\nEstimate\r\n(e) Operator-level Prediction (10GB)\r\n1 3 4 5 6 7 8 9 10 12 13 14 18 19\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n74 115 59 269 89 100\r\nTPC−H Template\r\nRelative Error (%)\r\n(f) Operator-level, Errors by Template (1GB)\r\nFigure 6: Static workload experiments with plan-level and operator-level prediction methods in 1GB and 10GB TPC-H databases.\r\nThe error values in bar-plots are capped at 50. Error values beyond the limits of the plots are printed next to their corresponding\r\nbars.\r\nNext, we discuss the practical impact of statistics estimation errors\r\non model accuracy. We then turn to the latter two issues that rep\u0002resent the fundamental limitations of operator-level modeling; that\r\nis, such models learn operator behavior “in isolation” without rep\u0002resenting the context within which they are occur.\r\n5.3.3 Impact of Estimation Errors\r\nWe tried all the combinations of actual and estimate feature values\r\nfor training and testing for plan-level and operator-level prediction.\r\nThe results are given in Figure 7(a) for the 10GB scenario. For fur\u0002ther detail, we also show the prediction errors grouped by TPC-H\r\ntemplates in Figure 7(b) for the actual/actual case and plan-level\r\nprediction (over the 10GB scenario). These results are to be com\u0002pared with those in Figure 6(a).\r\nUnsurprisingly, the best results are obtained in the actual/actual\r\ncase (i.e., training and testing with actual feature values), which is\r\nnot a viable option in practice due to the unavailability of the actual\r\nfeature values without running the queries. The next best results are\r\nobtained with the estimate/estimate option (i.e., training and testing\r\nwith estimated feature values), the option that we used in the rest\r\nof the paper. Finally, the results obtained with actual/estimate (i.e.,\r\ntraining on actual values and testing on estimates) are much worse\r\nthan the other two, primarily due to optimizer estimation errors that\r\nare not taken into account during training.\r\nTo provide a sense of the magnitude of the estimation errors made\r\nby the optimizer, consider template-18, which is one of the tem\u0002plates that exhibit the biggest error in operator-level prediction with\r\nactual/estimate model building. Instances of template-18 include\r\n(a) Prediction with Actual\r\nValues vs Estimates\r\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 19 22\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n54.4\r\nTPC−H Template\r\nRelative Error (%)\r\nTemplate Error\r\nAvg. Error\r\n(b) Plan-level Modeling with Actual\r\nValues (10GB)\r\nFigure 7: Impact of Estimation Errors on Prediction Accuracy\r\nin Static Workload Experiments\r\nthe following group by clause on table lineitem:\r\ngroup by l_orderkey having sum(l_quantity) > 314\r\nThere are 15 million distinct l_orderkey values in lineitem\r\n(out of approximately 60 million tuples). The estimated number of\r\ngroups satisfying sum(l_quantity) > 314 is 399521, whereas\r\nthe actual number is 84. The PostgreSQL query optimizer com\u0002putes this estimate using histograms (with 100 bins) for each col\u0002umn based on the attribute independence assumption. The results",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/1c1c63db-b278-4cd2-b4df-33a5cdf987f4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9916f56cecdc1bad2a579ab5d7959e8e590d03592e745c4b071643bd7373e947",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 652
      },
      {
        "segments": [
          {
            "segment_id": "d71d823c-17df-457e-98d9-6555fafa8185",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "are later fed into a Hash-Semi-Join, whose cost estimate is corre\u0002spondingly very much off the mark.\r\nComparing the actual/actual against the estimate/estimate results,\r\nwe observe that optimization estimate errors lead to, perhaps sur\u0002prisingly, only a modest degradation in prediction accuracy. This\r\nresult is due to the ability of the models to also integrate error cor\u0002rections during learning. Thus, while better estimations generally\r\nmean better results, it is possible to produce highly accurate pre\u0002dictions even with rather mediocre estimations (as in the case of\r\nPostgreSQL).\r\n5.3.4 Hybrid Prediction Method\r\nWe now present comparative results of the three plan ordering strate\u0002gies (see Section 3.4) discussed for offline hybrid model selection.\r\nThe results, shown in Figure 8, were obtained with the 14 TPC-H\r\ntemplates used in operator-level modeling and the 10 GB database.\r\n0 5 10 15 20 25 30\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n55\r\nIteration Number\r\nRelative Error (%)\r\nerror−based\r\nsize−based\r\nfrequency−based\r\nFigure 8: Hybrid Prediction Plan Ordering Strategies\r\nAs described earlier, we first create an ordered list of query sub\u0002plans based on the chosen plan ordering strategy, leaving out sub\u0002plans with average error lower than a given threshold (.1 in this ex\u0002periment) for the size-based and frequency-based strategies. Then,\r\nat each iteration (x-axis), we create a model for the next plan in\r\nthe ordered list, add this model to the current model set and then\r\nre-evaluate predictive error on the test workload (y-axis). The step\r\nbehavior is observed when a newly created model decreases the\r\nerror.\r\nWe observe that the size-based and error-based strategies quickly\r\nreduce the error rate. The size-based strategy takes longer to reach\r\nthe minimum error level, as in some cases larger sub-plans should\r\nbe modeled for reducing the error and it takes time for this strategy\r\nto reach those plans.\r\nThe frequency-based strategy initially takes longer to reduce the\r\nerror. The reason is that this strategy can easily get stuck in a rela\u0002tively large sub-plan that has a high occurrence rate, since it needs\r\nto explore all the sub-plans involved in the larger sub-plan (start\u0002ing from the smallest sub-plan) until it decreases the error rate. As\r\ndiscussed earlier, all such sub-plans are by definition at least as fre\u0002quent, hence need to be explored with this heuristic. Overall, the\r\nerror-based strategy provides a well balanced solution, quickly and\r\ndramatically reducing the prediction errors only with a small num\u0002ber of additional models.\r\n5.4 Predicting for Dynamic Workload\r\nThe results so far have shown that for known, static workloads,\r\nplan-level modeling performs well. They have also revealed that\r\nhybrid models offer similar, and sometimes even better accuracy\r\nthan plan-level models. Next, we present results demonstrating that\r\nplan-level modeling has serious limitations for unknown or chang\u0002ing workloads, whereas hybrid modeling still continues to provide\r\nhigh accuracy. We also report comparative results for online model\r\nbuilding (Section 4) that creates custom hybrid models for a given\r\nquery from the available training data.\r\nFor this experiment, we used the 12 templates shown in Figure 9,\r\nwith 11 of them used in training and the remaining for testing. That\r\nis for each template we build and test separate prediction models\r\nbased on the training data of the other templates. The two other\r\nTPC-H templates were excluded because they include specific op\u0002erators exclusively found in those templates, and thus cannot be\r\nmodeled with our current setup. We show results for plan-level,\r\noperator-level, hybrid (with error-based and size-based strategies),\r\nand online modeling algorithms.\r\nAs expected, plan-level models perform poorly across the board\r\nand thus do not offer much value in the presence of dynamic work\u0002loads. We also observe that the online (hybrid) modeling algorithm\r\nperforms best in all cases, except for template-7. Further investi\u0002gation reveals that the training data lacks a specific sub-plan that\r\nis the root cause of the error on template-7. These results confirm\r\nthe ability of online modeling to identify the models that are very\r\nlikely to help by utilizing the knowledge of a given query plan.\r\nSuch models can be eliminated by offline strategies if they do not\r\nhelp improve training accuracy.\r\nAnother interesting observation is that the size-based hybrid strat\u0002egy performs somewhat better than the error-based strategy in these\r\nexperiments. This can be explained by the ability of the former to\r\nfavor models for smaller sub-plans that are more likely to occur in\r\nunseen queries.\r\n1 3 4 5 6 7 8 9 10 12 14 19\r\n0\r\n10\r\n20\r\n30\r\n40\r\n50\r\n60\r\n70\r\n80\r\n90\r\n100\r\nTPC−H Template\r\nRelative Error (%)\r\nPlan−level\r\nOp−level\r\nError−based\r\nSize−based\r\nOnline\r\nFigure 9: Dynamic Workload Prediction Results\r\n6. RELATED WORK",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/d71d823c-17df-457e-98d9-6555fafa8185.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a2dc3c3fe03ca4a527239405a2f071c0ea2c2b5fedd910dff1aa2b688350f56f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 762
      },
      {
        "segments": [
          {
            "segment_id": "d71d823c-17df-457e-98d9-6555fafa8185",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "are later fed into a Hash-Semi-Join, whose cost estimate is corre\u0002spondingly very much off the mark.\r\nComparing the actual/actual against the estimate/estimate results,\r\nwe observe that optimization estimate errors lead to, perhaps sur\u0002prisingly, only a modest degradation in prediction accuracy. This\r\nresult is due to the ability of the models to also integrate error cor\u0002rections during learning. Thus, while better estimations generally\r\nmean better results, it is possible to produce highly accurate pre\u0002dictions even with rather mediocre estimations (as in the case of\r\nPostgreSQL).\r\n5.3.4 Hybrid Prediction Method\r\nWe now present comparative results of the three plan ordering strate\u0002gies (see Section 3.4) discussed for offline hybrid model selection.\r\nThe results, shown in Figure 8, were obtained with the 14 TPC-H\r\ntemplates used in operator-level modeling and the 10 GB database.\r\n0 5 10 15 20 25 30\r\n5\r\n10\r\n15\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n55\r\nIteration Number\r\nRelative Error (%)\r\nerror−based\r\nsize−based\r\nfrequency−based\r\nFigure 8: Hybrid Prediction Plan Ordering Strategies\r\nAs described earlier, we first create an ordered list of query sub\u0002plans based on the chosen plan ordering strategy, leaving out sub\u0002plans with average error lower than a given threshold (.1 in this ex\u0002periment) for the size-based and frequency-based strategies. Then,\r\nat each iteration (x-axis), we create a model for the next plan in\r\nthe ordered list, add this model to the current model set and then\r\nre-evaluate predictive error on the test workload (y-axis). The step\r\nbehavior is observed when a newly created model decreases the\r\nerror.\r\nWe observe that the size-based and error-based strategies quickly\r\nreduce the error rate. The size-based strategy takes longer to reach\r\nthe minimum error level, as in some cases larger sub-plans should\r\nbe modeled for reducing the error and it takes time for this strategy\r\nto reach those plans.\r\nThe frequency-based strategy initially takes longer to reduce the\r\nerror. The reason is that this strategy can easily get stuck in a rela\u0002tively large sub-plan that has a high occurrence rate, since it needs\r\nto explore all the sub-plans involved in the larger sub-plan (start\u0002ing from the smallest sub-plan) until it decreases the error rate. As\r\ndiscussed earlier, all such sub-plans are by definition at least as fre\u0002quent, hence need to be explored with this heuristic. Overall, the\r\nerror-based strategy provides a well balanced solution, quickly and\r\ndramatically reducing the prediction errors only with a small num\u0002ber of additional models.\r\n5.4 Predicting for Dynamic Workload\r\nThe results so far have shown that for known, static workloads,\r\nplan-level modeling performs well. They have also revealed that\r\nhybrid models offer similar, and sometimes even better accuracy\r\nthan plan-level models. Next, we present results demonstrating that\r\nplan-level modeling has serious limitations for unknown or chang\u0002ing workloads, whereas hybrid modeling still continues to provide\r\nhigh accuracy. We also report comparative results for online model\r\nbuilding (Section 4) that creates custom hybrid models for a given\r\nquery from the available training data.\r\nFor this experiment, we used the 12 templates shown in Figure 9,\r\nwith 11 of them used in training and the remaining for testing. That\r\nis for each template we build and test separate prediction models\r\nbased on the training data of the other templates. The two other\r\nTPC-H templates were excluded because they include specific op\u0002erators exclusively found in those templates, and thus cannot be\r\nmodeled with our current setup. We show results for plan-level,\r\noperator-level, hybrid (with error-based and size-based strategies),\r\nand online modeling algorithms.\r\nAs expected, plan-level models perform poorly across the board\r\nand thus do not offer much value in the presence of dynamic work\u0002loads. We also observe that the online (hybrid) modeling algorithm\r\nperforms best in all cases, except for template-7. Further investi\u0002gation reveals that the training data lacks a specific sub-plan that\r\nis the root cause of the error on template-7. These results confirm\r\nthe ability of online modeling to identify the models that are very\r\nlikely to help by utilizing the knowledge of a given query plan.\r\nSuch models can be eliminated by offline strategies if they do not\r\nhelp improve training accuracy.\r\nAnother interesting observation is that the size-based hybrid strat\u0002egy performs somewhat better than the error-based strategy in these\r\nexperiments. This can be explained by the ability of the former to\r\nfavor models for smaller sub-plans that are more likely to occur in\r\nunseen queries.\r\n1 3 4 5 6 7 8 9 10 12 14 19\r\n0\r\n10\r\n20\r\n30\r\n40\r\n50\r\n60\r\n70\r\n80\r\n90\r\n100\r\nTPC−H Template\r\nRelative Error (%)\r\nPlan−level\r\nOp−level\r\nError−based\r\nSize−based\r\nOnline\r\nFigure 9: Dynamic Workload Prediction Results\r\n6. RELATED WORK",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/d71d823c-17df-457e-98d9-6555fafa8185.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a2dc3c3fe03ca4a527239405a2f071c0ea2c2b5fedd910dff1aa2b688350f56f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 762
      },
      {
        "segments": [
          {
            "segment_id": "1fc8f62e-e238-45b3-889e-16373de29b8c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "Query-plan-level predictions have recently been studied [1]. In [1],\r\nauthors consider plan-level query performance prediction for the\r\nfollowing static query workloads: the TPC-DS query benchmark\r\nand a query workload obtained from a customer database. They\r\nreport that they can predict individual query execution times within\r\n20% of the actual time for 85% of their test queries. In addition to\r\nthe query execution time, estimation of other performance metrics\r\nsuch as disk I/O and message bytes is also considered. In this paper,\r\nwe focused on the execution time performance metric. While we\r\ncan apply our techniques separately for each performance metric,\r\nwe plan to consider the extension to joint prediction of multiple\r\nmetrics in future work.\r\nIn previous work, machine learning techniques have been used in\r\nthe context of the query optimizer [8, 9, 10]. In the learning op\u0002timizer (LEO) [8, 9] project, model-based techniques are used to\r\ncreate a self-tuning database query optimizer. The goal in [8, 9] is\r\nto produce better cost estimates for use in query optimization. The\r\napproach taken is to compare the estimates of the query optimizer\r\nwith the actual values observed during query execution to repair\r\nthe inaccurate estimates. In [10], a statistical modeling technique\r\ncalled transform regression is used to create cost models for XML\r\nquery operators. In addition, new training data can be efficiently\r\nintegrated into their existing cost models for adapting to changing\r\nworkloads.\r\nRecently, there have been successful applications of machine learn\u0002ing techniques in system self-management problems. In [11], au\u0002thors present a statistics-driven modeling framework for data-intensive\r\nCloud applications. Kernel Canonical Correlation Analysis (KCCA)\r\nmodeling techniques are used to make predictions for the execution\r\nperformance of Hadoop jobs. In [14], a statistics-driven workload\r\ngeneration framework is presented for the purpose of identifying\r\nsuggestions (e.g., scheduling and configuration) to improve the en\u0002ergy efficiency of MapReduce systems.\r\nIn [12, 13] authors describe a successfull experimental modeling\r\napproach for capturing interactions in query mixes, i.e., sets of con\u0002currently running queries. Given a query workload, the goal is to\r\ncome up with a query execution schedule (in terms of query mixes)\r\nthat minimizes the total execution time. The query interactions are\r\nmodeled using statistical models based on selectively chosen sam\u0002ple executions of query mixes. In our study, we have not yet con\u0002sidered performance prediction in concurrent query workloads.\r\nFinally, there has also been work on query progress indicators [15,\r\n16]. Query progress indicators provide estimations for the comple\u0002tion degrees of running queries. Such studies assume that the work\r\ndone by individual query operators are transparent, i.e., externally\r\nvisible. While these studies are also related to query execution per\u0002formance, they do not provide predictions for the execution time of\r\nqueries.\r\n7. CONCLUSIONS\r\nThis paper studied techniques for learning-based modeling of query\r\nexecution for QPP over analytical workloads. We proposed novel\r\nquery modeling techniques and demonstrated their general appli\u0002cability and effectiveness with implementation on PostgreSQL and\r\nTPC-H data and queries. We provide the most comprehensive work\r\non this topic to date, and show results that significantly improve\r\nupon the existing solutions in terms of generality and predictive\r\naccuracy.\r\nLearning-based QPP is a fertile research area, with many open op\u0002portunities and challenges to be explored. One immediate idea is to\r\nsupplement the static models studied in this paper with additional\r\nrun-time features. The values for such features can be obtained dur\u0002ing the early stages of query execution, and used to create richer\r\nmodels that yield higher predictive accuracy with modest delays\r\nin prediction. A generalization of this approach will lead us to an\r\nonline, progressive prediction model, where predictions are contin\u0002ually updated during query execution, in a manner similar to online\r\naggregation [20].\r\nAs mentioned earlier, this paper does not address QPP in the pres\u0002ence of concurrent query execution. There is already some promis\u0002ing work addressing this problem [12, 13], and we believe the tech\u0002niques proposed here can be extended to provide an alternative per\u0002spective to this challenge. As yet another direction, our techniques\r\ncan be adapted to work for other data processing platforms such as\r\nMapReduce/Hadoop [18] and Dryad [19].\r\nAs database systems and the underlying software-hardware plat\u0002forms become increasingly sophisticated, it is becoming increas\u0002ingly infeasible to manually develop and maintain accurate mod\u0002els for system behavior. As such, learning-based modeling will\r\nbecome increasingly more useful, prevalent, and eventually indis\u0002pensable. Our work and results form another promising step to\u0002wards facilitating this vision.\r\n8. REFERENCES\r\n[1] Ganapathi, A., Kuno, H., Dayal, U., Wiener, J.,Fox, A.,\r\nJordan, M., and Patterson, D. Predicting multiple\r\nperformance metrics for queries: Better decisions enabled by\r\nmachine learning. In International.Conf. on Data\r\nEngineering (ICDE) 2009.\r\n[2] Makridakis, S., Wheelwright S., and Hyndman, R.\r\nForecasting Methods and Applications. Third Edition. John\r\nWiley & Sons, Inc. 1998.\r\n[3] M. Hall and G. Holmes. Benchmarking attribute selection\r\ntechniques for discrete class data mining. IEEE Trans. on\r\nKnowledge and Data Engineering, 15(3), Nov. 2003.\r\n[4] I. H. Witten and E. Frank. Data Mining: Practical Machine\r\nLearning Tools and Techniques. Morgan Kaufmann, second\r\nedition, June 2005.\r\n[5] Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library\r\nfor support vector machines, 2001. Software available at\r\nhttp://www.csie.ntu.edu.tw/ cjlin/libsvm.\r\n[6] TPC-H benchmark specification, http://www.tpc.org/tpch/\r\n[7] Christian Igel, Verena Heidrich-Meisner, and Tobias\r\nGlasmachers. Shark. Journal of Machine Learning Research\r\n9, pp. 993-996, 2008.\r\n[8] Volker Markl, G. M. Lohman and V. Raman. LEO: An\r\nAutonomic Query Optimizer for DB2. IBM Systems Journal\r\nSpecial Issue on Autonomic Computing, January 2001.\r\n[9] M. Stillger, G. M. Lohman, V. Markl and M. Kandil. LEO -\r\nDB2’s LEarning Optimizer. Proc. Intl. Conf. Very Large\r\nData Bases (VLDB). 2001.\r\n[10] Zhang, N., Haas, P. J., Josifovski, V., Lohman, G. M., and\r\nZhang, C. 2005. Statistical learning techniques for costing\r\nXML queries. In Proceedings of the 31st international\r\nConference on Very Large Data Bases (Trondheim, Norway,\r\nAugust 30 - September 02, 2005). Very Large Data Bases.\r\nVLDB Endowment, 289-300.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/1fc8f62e-e238-45b3-889e-16373de29b8c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bca4161f5b9fd27dd3ee849ad603cf73ced5839387e5fde84d4cd1e2f05dd206",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 969
      },
      {
        "segments": [
          {
            "segment_id": "1fc8f62e-e238-45b3-889e-16373de29b8c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "Query-plan-level predictions have recently been studied [1]. In [1],\r\nauthors consider plan-level query performance prediction for the\r\nfollowing static query workloads: the TPC-DS query benchmark\r\nand a query workload obtained from a customer database. They\r\nreport that they can predict individual query execution times within\r\n20% of the actual time for 85% of their test queries. In addition to\r\nthe query execution time, estimation of other performance metrics\r\nsuch as disk I/O and message bytes is also considered. In this paper,\r\nwe focused on the execution time performance metric. While we\r\ncan apply our techniques separately for each performance metric,\r\nwe plan to consider the extension to joint prediction of multiple\r\nmetrics in future work.\r\nIn previous work, machine learning techniques have been used in\r\nthe context of the query optimizer [8, 9, 10]. In the learning op\u0002timizer (LEO) [8, 9] project, model-based techniques are used to\r\ncreate a self-tuning database query optimizer. The goal in [8, 9] is\r\nto produce better cost estimates for use in query optimization. The\r\napproach taken is to compare the estimates of the query optimizer\r\nwith the actual values observed during query execution to repair\r\nthe inaccurate estimates. In [10], a statistical modeling technique\r\ncalled transform regression is used to create cost models for XML\r\nquery operators. In addition, new training data can be efficiently\r\nintegrated into their existing cost models for adapting to changing\r\nworkloads.\r\nRecently, there have been successful applications of machine learn\u0002ing techniques in system self-management problems. In [11], au\u0002thors present a statistics-driven modeling framework for data-intensive\r\nCloud applications. Kernel Canonical Correlation Analysis (KCCA)\r\nmodeling techniques are used to make predictions for the execution\r\nperformance of Hadoop jobs. In [14], a statistics-driven workload\r\ngeneration framework is presented for the purpose of identifying\r\nsuggestions (e.g., scheduling and configuration) to improve the en\u0002ergy efficiency of MapReduce systems.\r\nIn [12, 13] authors describe a successfull experimental modeling\r\napproach for capturing interactions in query mixes, i.e., sets of con\u0002currently running queries. Given a query workload, the goal is to\r\ncome up with a query execution schedule (in terms of query mixes)\r\nthat minimizes the total execution time. The query interactions are\r\nmodeled using statistical models based on selectively chosen sam\u0002ple executions of query mixes. In our study, we have not yet con\u0002sidered performance prediction in concurrent query workloads.\r\nFinally, there has also been work on query progress indicators [15,\r\n16]. Query progress indicators provide estimations for the comple\u0002tion degrees of running queries. Such studies assume that the work\r\ndone by individual query operators are transparent, i.e., externally\r\nvisible. While these studies are also related to query execution per\u0002formance, they do not provide predictions for the execution time of\r\nqueries.\r\n7. CONCLUSIONS\r\nThis paper studied techniques for learning-based modeling of query\r\nexecution for QPP over analytical workloads. We proposed novel\r\nquery modeling techniques and demonstrated their general appli\u0002cability and effectiveness with implementation on PostgreSQL and\r\nTPC-H data and queries. We provide the most comprehensive work\r\non this topic to date, and show results that significantly improve\r\nupon the existing solutions in terms of generality and predictive\r\naccuracy.\r\nLearning-based QPP is a fertile research area, with many open op\u0002portunities and challenges to be explored. One immediate idea is to\r\nsupplement the static models studied in this paper with additional\r\nrun-time features. The values for such features can be obtained dur\u0002ing the early stages of query execution, and used to create richer\r\nmodels that yield higher predictive accuracy with modest delays\r\nin prediction. A generalization of this approach will lead us to an\r\nonline, progressive prediction model, where predictions are contin\u0002ually updated during query execution, in a manner similar to online\r\naggregation [20].\r\nAs mentioned earlier, this paper does not address QPP in the pres\u0002ence of concurrent query execution. There is already some promis\u0002ing work addressing this problem [12, 13], and we believe the tech\u0002niques proposed here can be extended to provide an alternative per\u0002spective to this challenge. As yet another direction, our techniques\r\ncan be adapted to work for other data processing platforms such as\r\nMapReduce/Hadoop [18] and Dryad [19].\r\nAs database systems and the underlying software-hardware plat\u0002forms become increasingly sophisticated, it is becoming increas\u0002ingly infeasible to manually develop and maintain accurate mod\u0002els for system behavior. As such, learning-based modeling will\r\nbecome increasingly more useful, prevalent, and eventually indis\u0002pensable. Our work and results form another promising step to\u0002wards facilitating this vision.\r\n8. REFERENCES\r\n[1] Ganapathi, A., Kuno, H., Dayal, U., Wiener, J.,Fox, A.,\r\nJordan, M., and Patterson, D. Predicting multiple\r\nperformance metrics for queries: Better decisions enabled by\r\nmachine learning. In International.Conf. on Data\r\nEngineering (ICDE) 2009.\r\n[2] Makridakis, S., Wheelwright S., and Hyndman, R.\r\nForecasting Methods and Applications. Third Edition. John\r\nWiley & Sons, Inc. 1998.\r\n[3] M. Hall and G. Holmes. Benchmarking attribute selection\r\ntechniques for discrete class data mining. IEEE Trans. on\r\nKnowledge and Data Engineering, 15(3), Nov. 2003.\r\n[4] I. H. Witten and E. Frank. Data Mining: Practical Machine\r\nLearning Tools and Techniques. Morgan Kaufmann, second\r\nedition, June 2005.\r\n[5] Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library\r\nfor support vector machines, 2001. Software available at\r\nhttp://www.csie.ntu.edu.tw/ cjlin/libsvm.\r\n[6] TPC-H benchmark specification, http://www.tpc.org/tpch/\r\n[7] Christian Igel, Verena Heidrich-Meisner, and Tobias\r\nGlasmachers. Shark. Journal of Machine Learning Research\r\n9, pp. 993-996, 2008.\r\n[8] Volker Markl, G. M. Lohman and V. Raman. LEO: An\r\nAutonomic Query Optimizer for DB2. IBM Systems Journal\r\nSpecial Issue on Autonomic Computing, January 2001.\r\n[9] M. Stillger, G. M. Lohman, V. Markl and M. Kandil. LEO -\r\nDB2’s LEarning Optimizer. Proc. Intl. Conf. Very Large\r\nData Bases (VLDB). 2001.\r\n[10] Zhang, N., Haas, P. J., Josifovski, V., Lohman, G. M., and\r\nZhang, C. 2005. Statistical learning techniques for costing\r\nXML queries. In Proceedings of the 31st international\r\nConference on Very Large Data Bases (Trondheim, Norway,\r\nAugust 30 - September 02, 2005). Very Large Data Bases.\r\nVLDB Endowment, 289-300.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/1fc8f62e-e238-45b3-889e-16373de29b8c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bca4161f5b9fd27dd3ee849ad603cf73ced5839387e5fde84d4cd1e2f05dd206",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 969
      },
      {
        "segments": [
          {
            "segment_id": "d00f2010-ea4c-4583-83b1-32a9654f7159",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "[11] Ganapathi, A., Yanpei Chen, Fox, A., Katz, R., Patterson, D.\r\nStatistics-driven workload modeling for the Cloud. Data\r\nEngineering Workshops (ICDEW), 2010 pp.87-92, 1-6\r\nMarch 2010.\r\n[12] Ahmad, M., Aboulnaga, A., Babu, S., and Munagala, K.\r\nModeling and exploiting query interactions in database\r\nsystems. In Proceeding of the 17th ACM Conference on\r\nInformation and Knowledge Management (Napa Valley,\r\nCalifornia, USA, October 26 - 30, 2008). CIKM ’08. ACM,\r\nNew York, NY, 183-192.\r\n[13] M. Ahmad, A. Aboulnaga, S. Babu, and K. Munagala.\r\nQshuffler: Getting the query mix right. In ICDE, 2008.\r\n[14] Chen, Y., Ganapathi, A. S., Fox, A., Katz, R. H., Patterson,\r\nD. A. Statistical workloads for energy efficient mapreduce.\r\nTech. Rep. UCB/EECS-2010-6, EECS Department,\r\nUniversity of California, Berkeley, Jan 2010.\r\n[15] Chaudhuri, S., Narasayya, V., and Ramamurthy, R.\r\nEstimating progress of execution for SQL queries. In\r\nProceedings of the 2004 ACM SIGMOD international\r\nConference on Management of Data (Paris, France, June 13 -\r\n18, 2004). SIGMOD ’04\r\n[16] Luo, G., Naughton, J. F., Ellmann, C. J., and Watzke, M. W.\r\nIncreasing the Accuracy and Coverage of SQL Progress\r\nIndicators. In Proceedings of the 21st international\r\nConference on Data Engineering (April 05 - 08, 2005).\r\nICDE.\r\n[17] R. Othayoth and M. Poess, The making of tpc-ds, in VLDB\r\n06: Proceedings of the 32nd international conference on Very\r\nlarge data bases. VLDB Endowment, 2006, pp. 1049Ð1058.\r\n[18] Hadoop MapReduce web site.\r\nhttp://hadoop.apache.org/mapreduce/\r\n[19] Dryad: Distributed Data-Parallel Programs from Sequential\r\nBuilding Blocks Michael Isard, Mihai Budiu, Yuan Yu,\r\nAndrew Birrell, and Dennis Fetterly European Conference\r\non Computer Systems (EuroSys), Lisbon, Portugal, March\r\n21-23, 2007\r\n[20] \"Online Aggregation.\" J. M. Hellerstein, P. J. Haas, and H. J.\r\nWang. Proc. 1997 ACM SIGMOD Intl. Conf. Management\r\nof Data, 171-182.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/895280f6-e9d2-478b-a97e-588da031e221/images/d00f2010-ea4c-4583-83b1-32a9654f7159.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041858Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=269d7d0809813ec324b9b34117879d41ed5ee97c28cea671a6a4b90ae2a6efc3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 286
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "\"Learning-based Query Performance Modeling and Prediction\"\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Ganapathi, A., Chen, Y., Fox, A., Katz, R., Patterson, D.\nAhmad, M., Aboulnaga, A., Babu, S., Munagala, K.\nAhmad, M., Aboulnaga, A., Babu, S., Munagala, K.\nChen, Y., Ganapathi, A. S., Fox, A., Katz, R. H., Patterson, D. A.\nChaudhuri, S., Narasayya, V., Ramamurthy, R.\nLuo, G., Naughton, J. F., Ellmann, C. J., Watzke, M. W.\nOthayoth, R., Poess, M.\nIsard, M., Budiu, M., Yu, Y., Birrell, A., Fetterly, D.\nHellerstein, J. M., Haas, P. J., Wang, H. J.\nGanapathi, A., Kuno, H., Dayal, U., Wiener, J., Fox, A., Jordan, M., Patterson, D.\nMakridakis, S., Wheelwright, S., Hyndman, R.\nHall, M., Holmes, G.\nWitten, I. H., Frank, E.\nChang, C., Lin, C.\nIgel, C., Heidrich-Meisner, V., Glasmachers, T.\nMarkl, V., Lohman, G. M., Raman, V.\nStillger, M., Lohman, G. M., Markl, V., Kandil, M.\nZhang, N., Haas, P. J., Josifovski, V., Lohman, G. M., Zhang, C.\nAkdere, M., Çetintemel, U.\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "March 2010"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "Napa Valley, California, USA\nParis, France\nTrondheim, Norway\nLisbon, Portugal"
        }
      ]
    }
  }
}