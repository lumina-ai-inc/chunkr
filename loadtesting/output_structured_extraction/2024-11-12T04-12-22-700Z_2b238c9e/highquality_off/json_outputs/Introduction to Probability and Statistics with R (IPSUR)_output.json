{
  "file_name": "Introduction to Probability and Statistics with R (IPSUR).pdf",
  "task_id": "202e8752-2474-4b02-9a37-c25701197a8b",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "cdafa3e5-393e-4307-9249-879fc2cac23f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 1,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Introduction to Probability\r\nand Statistics Using R\r\nG. Jay Kerns\r\nFirst Edition",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/cdafa3e5-393e-4307-9249-879fc2cac23f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cd1a06deca40e4fa28dcb1b6b88b8794298c2ad394ab94f94716ae9d16f1fe03",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "976e60e1-e20b-4030-8607-cba458587a8d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 2,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "ii\r\nIPSUR: Introduction to Probability and Statistics Using R\r\nCopyright © 2010 G. Jay Kerns\r\nISBN: 978-0-557-24979-4\r\nPermission is granted to copy, distribute and/or modify this document under the terms of the GNU\r\nFree Documentation License, Version 1.3 or any later version published by the Free Software Foun\u0002dation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the\r\nlicense is included in the section entitled “GNU Free Documentation License”.\r\nDate: March 24, 2011",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/976e60e1-e20b-4030-8607-cba458587a8d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8f386b5bd2460e2c8907e36b0a0857a30d63100f08edaa31c78e320f59f6a413",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 90
      },
      {
        "segments": [
          {
            "segment_id": "524e67b8-c59a-40ae-8307-c8f694bead60",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 3,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Contents\r\nPreface vii\r\nList of Figures xiii\r\nList of Tables xv\r\n1 An Introduction to Probability and Statistics 1\r\n1.1 Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\r\n1.2 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\r\n2 An Introduction to R 3\r\n2.1 Downloading and Installing R . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\r\n2.2 Communicating with R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\r\n2.3 Basic R Operations and Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . 6\r\n2.4 Getting Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\r\n2.5 External Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\r\n2.6 Other Tips . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n3 Data Description 17\r\n3.1 Types of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\r\n3.2 Features of Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\r\n3.3 Descriptive Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\r\n3.4 Exploratory Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\r\n3.5 Multivariate Data and Data Frames . . . . . . . . . . . . . . . . . . . . . . . . . . 45\r\n3.6 Comparing Populations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\r\n4 Probability 67\r\n4.1 Sample Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\r\n4.2 Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\r\n4.3 Model Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\r\n4.4 Properties of Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\r\n4.5 Counting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\r\n4.6 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\r\niii",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/524e67b8-c59a-40ae-8307-c8f694bead60.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7c9abcc5db588f6755bdba6425661ee5eac1931e3142109a2edfbeddb8edbd7c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 917
      },
      {
        "segments": [
          {
            "segment_id": "524e67b8-c59a-40ae-8307-c8f694bead60",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 3,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Contents\r\nPreface vii\r\nList of Figures xiii\r\nList of Tables xv\r\n1 An Introduction to Probability and Statistics 1\r\n1.1 Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\r\n1.2 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\r\n2 An Introduction to R 3\r\n2.1 Downloading and Installing R . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\r\n2.2 Communicating with R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\r\n2.3 Basic R Operations and Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . 6\r\n2.4 Getting Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\r\n2.5 External Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\r\n2.6 Other Tips . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n3 Data Description 17\r\n3.1 Types of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\r\n3.2 Features of Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\r\n3.3 Descriptive Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\r\n3.4 Exploratory Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\r\n3.5 Multivariate Data and Data Frames . . . . . . . . . . . . . . . . . . . . . . . . . . 45\r\n3.6 Comparing Populations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\r\n4 Probability 67\r\n4.1 Sample Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\r\n4.2 Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\r\n4.3 Model Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\r\n4.4 Properties of Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\r\n4.5 Counting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\r\n4.6 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\r\niii",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/524e67b8-c59a-40ae-8307-c8f694bead60.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7c9abcc5db588f6755bdba6425661ee5eac1931e3142109a2edfbeddb8edbd7c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 917
      },
      {
        "segments": [
          {
            "segment_id": "ed2de92d-1022-46d5-b33d-9427c6ec189a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 4,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "iv CONTENTS\r\n4.7 Independent Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\r\n4.8 Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\r\n4.9 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r\n5 Discrete Distributions 111\r\n5.1 Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\r\n5.2 The Discrete Uniform Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 114\r\n5.3 The Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\r\n5.4 Expectation and Moment Generating Functions . . . . . . . . . . . . . . . . . . . 122\r\n5.5 The Empirical Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\r\n5.6 Other Discrete Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\r\n5.7 Functions of Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . 136\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\r\n6 Continuous Distributions 143\r\n6.1 Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\r\n6.2 The Continuous Uniform Distribution . . . . . . . . . . . . . . . . . . . . . . . . 148\r\n6.3 The Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\r\n6.4 Functions of Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . 153\r\n6.5 Other Continuous Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\r\n7 Multivariate Distributions 165\r\n7.1 Joint and Marginal Probability Distributions . . . . . . . . . . . . . . . . . . . . . 166\r\n7.2 Joint and Marginal Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\r\n7.3 Conditional Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\r\n7.4 Independent Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\r\n7.5 Exchangeable Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\r\n7.6 The Bivariate Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 179\r\n7.7 Bivariate Transformations of Random Variables . . . . . . . . . . . . . . . . . . . 181\r\n7.8 Remarks for the Multivariate Case . . . . . . . . . . . . . . . . . . . . . . . . . . 184\r\n7.9 The Multinomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\r\n8 Sampling Distributions 191\r\n8.1 Simple Random Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\r\n8.2 Sampling from a Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . 193\r\n8.3 The Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\r\n8.4 Sampling Distributions of Two-Sample Statistics . . . . . . . . . . . . . . . . . . 197\r\n8.5 Simulated Sampling Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 200\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\r\n9 Estimation 205\r\n9.1 Point Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\r\n9.2 Confidence Intervals for Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\r\n9.3 Confidence Intervals for Differences of Means . . . . . . . . . . . . . . . . . . . . 221",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ed2de92d-1022-46d5-b33d-9427c6ec189a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4c1532e332fd7bcb4698c31a6d937579cd1881a7788cb6cee6470c921ce788e6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1280
      },
      {
        "segments": [
          {
            "segment_id": "ed2de92d-1022-46d5-b33d-9427c6ec189a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 4,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "iv CONTENTS\r\n4.7 Independent Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\r\n4.8 Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\r\n4.9 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r\n5 Discrete Distributions 111\r\n5.1 Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\r\n5.2 The Discrete Uniform Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 114\r\n5.3 The Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\r\n5.4 Expectation and Moment Generating Functions . . . . . . . . . . . . . . . . . . . 122\r\n5.5 The Empirical Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\r\n5.6 Other Discrete Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\r\n5.7 Functions of Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . 136\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\r\n6 Continuous Distributions 143\r\n6.1 Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\r\n6.2 The Continuous Uniform Distribution . . . . . . . . . . . . . . . . . . . . . . . . 148\r\n6.3 The Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\r\n6.4 Functions of Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . 153\r\n6.5 Other Continuous Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\r\n7 Multivariate Distributions 165\r\n7.1 Joint and Marginal Probability Distributions . . . . . . . . . . . . . . . . . . . . . 166\r\n7.2 Joint and Marginal Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\r\n7.3 Conditional Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\r\n7.4 Independent Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\r\n7.5 Exchangeable Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\r\n7.6 The Bivariate Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 179\r\n7.7 Bivariate Transformations of Random Variables . . . . . . . . . . . . . . . . . . . 181\r\n7.8 Remarks for the Multivariate Case . . . . . . . . . . . . . . . . . . . . . . . . . . 184\r\n7.9 The Multinomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\r\n8 Sampling Distributions 191\r\n8.1 Simple Random Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\r\n8.2 Sampling from a Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . 193\r\n8.3 The Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\r\n8.4 Sampling Distributions of Two-Sample Statistics . . . . . . . . . . . . . . . . . . 197\r\n8.5 Simulated Sampling Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 200\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\r\n9 Estimation 205\r\n9.1 Point Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\r\n9.2 Confidence Intervals for Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\r\n9.3 Confidence Intervals for Differences of Means . . . . . . . . . . . . . . . . . . . . 221",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ed2de92d-1022-46d5-b33d-9427c6ec189a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4c1532e332fd7bcb4698c31a6d937579cd1881a7788cb6cee6470c921ce788e6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1280
      },
      {
        "segments": [
          {
            "segment_id": "d0a25fff-dde6-4ec9-b852-6e09614fd8f8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 5,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "CONTENTS v\r\n9.4 Confidence Intervals for Proportions . . . . . . . . . . . . . . . . . . . . . . . . . 223\r\n9.5 Confidence Intervals for Variances . . . . . . . . . . . . . . . . . . . . . . . . . . 225\r\n9.6 Fitting Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\r\n9.7 Sample Size and Margin of Error . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\r\n9.8 Other Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\r\n10 Hypothesis Testing 229\r\n10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\r\n10.2 Tests for Proportions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\r\n10.3 One Sample Tests for Means and Variances . . . . . . . . . . . . . . . . . . . . . 235\r\n10.4 Two-Sample Tests for Means and Variances . . . . . . . . . . . . . . . . . . . . . 239\r\n10.5 Other Hypothesis Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\r\n10.6 Analysis of Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\r\n10.7 Sample Size and Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\r\n11 Simple Linear Regression 249\r\n11.1 Basic Philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\r\n11.2 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\r\n11.3 Model Utility and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262\r\n11.4 Residual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\r\n11.5 Other Diagnostic Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\r\n12 Multiple Linear Regression 285\r\n12.1 The Multiple Linear Regression Model . . . . . . . . . . . . . . . . . . . . . . . . 285\r\n12.2 Estimation and Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\r\n12.3 Model Utility and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\r\n12.4 Polynomial Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\r\n12.5 Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\r\n12.6 Qualitative Explanatory Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 307\r\n12.7 Partial F Statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\r\n12.8 Residual Analysis and Diagnostic Tools . . . . . . . . . . . . . . . . . . . . . . . 312\r\n12.9 Additional Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\r\n13 Resampling Methods 319\r\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\r\n13.2 Bootstrap Standard Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\r\n13.3 Bootstrap Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\r\n13.4 Resampling in Hypothesis Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\r\n14 Categorical Data Analysis 333\r\n15 Nonparametric Statistics 335",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d0a25fff-dde6-4ec9-b852-6e09614fd8f8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=524d42ccfeef11ce5e20823d8faa215905c4215dbd05bf93ae78370461cb2604",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1320
      },
      {
        "segments": [
          {
            "segment_id": "d0a25fff-dde6-4ec9-b852-6e09614fd8f8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 5,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "CONTENTS v\r\n9.4 Confidence Intervals for Proportions . . . . . . . . . . . . . . . . . . . . . . . . . 223\r\n9.5 Confidence Intervals for Variances . . . . . . . . . . . . . . . . . . . . . . . . . . 225\r\n9.6 Fitting Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\r\n9.7 Sample Size and Margin of Error . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\r\n9.8 Other Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\r\n10 Hypothesis Testing 229\r\n10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\r\n10.2 Tests for Proportions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\r\n10.3 One Sample Tests for Means and Variances . . . . . . . . . . . . . . . . . . . . . 235\r\n10.4 Two-Sample Tests for Means and Variances . . . . . . . . . . . . . . . . . . . . . 239\r\n10.5 Other Hypothesis Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\r\n10.6 Analysis of Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\r\n10.7 Sample Size and Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\r\n11 Simple Linear Regression 249\r\n11.1 Basic Philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\r\n11.2 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\r\n11.3 Model Utility and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262\r\n11.4 Residual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\r\n11.5 Other Diagnostic Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\r\n12 Multiple Linear Regression 285\r\n12.1 The Multiple Linear Regression Model . . . . . . . . . . . . . . . . . . . . . . . . 285\r\n12.2 Estimation and Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\r\n12.3 Model Utility and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\r\n12.4 Polynomial Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\r\n12.5 Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\r\n12.6 Qualitative Explanatory Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 307\r\n12.7 Partial F Statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\r\n12.8 Residual Analysis and Diagnostic Tools . . . . . . . . . . . . . . . . . . . . . . . 312\r\n12.9 Additional Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\r\n13 Resampling Methods 319\r\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\r\n13.2 Bootstrap Standard Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\r\n13.3 Bootstrap Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\r\n13.4 Resampling in Hypothesis Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\r\nChapter Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\r\n14 Categorical Data Analysis 333\r\n15 Nonparametric Statistics 335",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d0a25fff-dde6-4ec9-b852-6e09614fd8f8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=524d42ccfeef11ce5e20823d8faa215905c4215dbd05bf93ae78370461cb2604",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 1320
      },
      {
        "segments": [
          {
            "segment_id": "29cf0103-e0f1-4ff8-963c-d030bef1aad5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 6,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "vi CONTENTS\r\n16 Time Series 337\r\nA R Session Information 339\r\nB GNU Free Documentation License 341\r\nC History 349\r\nD Data 351\r\nD.1 Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\r\nD.2 Importing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\r\nD.3 Creating New Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\r\nD.4 Editing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\r\nD.5 Exporting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\r\nD.6 Reshaping Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\r\nE Mathematical Machinery 361\r\nE.1 Set Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\r\nE.2 Differential and Integral Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\r\nE.3 Sequences and Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365\r\nE.4 The Gamma Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\r\nE.5 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\r\nE.6 Multivariable Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\r\nF Writing Reports with R 373\r\nF.1 What to Write . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\r\nF.2 How to Write It with R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\r\nF.3 Formatting Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\r\nF.4 Other Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\r\nG Instructions for Instructors 379\r\nG.1 Generating This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\r\nG.2 How to Use This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\r\nG.3 Ancillary Materials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\r\nG.4 Modifying This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\r\nH RcmdrTestDrive Story 383\r\nBibliography 389\r\nIndex 395",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/29cf0103-e0f1-4ff8-963c-d030bef1aad5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5677fb4bfc1c7c2760e12fd88e82dd4b1c27345119c60fdbc1b8a2867d77542e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 820
      },
      {
        "segments": [
          {
            "segment_id": "29cf0103-e0f1-4ff8-963c-d030bef1aad5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 6,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "vi CONTENTS\r\n16 Time Series 337\r\nA R Session Information 339\r\nB GNU Free Documentation License 341\r\nC History 349\r\nD Data 351\r\nD.1 Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\r\nD.2 Importing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\r\nD.3 Creating New Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\r\nD.4 Editing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\r\nD.5 Exporting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\r\nD.6 Reshaping Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\r\nE Mathematical Machinery 361\r\nE.1 Set Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\r\nE.2 Differential and Integral Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\r\nE.3 Sequences and Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365\r\nE.4 The Gamma Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\r\nE.5 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\r\nE.6 Multivariable Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\r\nF Writing Reports with R 373\r\nF.1 What to Write . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\r\nF.2 How to Write It with R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\r\nF.3 Formatting Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\r\nF.4 Other Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\r\nG Instructions for Instructors 379\r\nG.1 Generating This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\r\nG.2 How to Use This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\r\nG.3 Ancillary Materials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\r\nG.4 Modifying This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\r\nH RcmdrTestDrive Story 383\r\nBibliography 389\r\nIndex 395",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/29cf0103-e0f1-4ff8-963c-d030bef1aad5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5677fb4bfc1c7c2760e12fd88e82dd4b1c27345119c60fdbc1b8a2867d77542e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 820
      },
      {
        "segments": [
          {
            "segment_id": "d16ecb65-c5cf-40a2-907f-0eec101f9411",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 7,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Preface\r\nThis book was expanded from lecture materials I use in a one semester upper-division undergradu\u0002ate course entitled Probability and Statistics at Youngstown State University. Those lecture mate\u0002rials, in turn, were based on notes that I transcribed as a graduate student at Bowling Green State\r\nUniversity. The course for which the materials were written is 50-50 Probability and Statistics, and\r\nthe attendees include mathematics, engineering, and computer science majors (among others). The\r\ncatalog prerequisites for the course are a full year of calculus.\r\nThe book can be subdivided into three basic parts. The first part includes the introductions and\r\nelementary descriptive statistics; I want the students to be knee-deep in data right out of the gate.\r\nThe second part is the study of probability, which begins at the basics of sets and the equally likely\r\nmodel, journeys past discrete/continuous random variables, and continues through to multivariate\r\ndistributions. The chapter on sampling distributions paves the way to the third part, which is in\u0002ferential statistics. This last part includes point and interval estimation, hypothesis testing, and\r\nfinishes with introductions to selected topics in applied statistics.\r\nI usually only have time in one semester to cover a small subset of this book. I cover the material\r\nin Chapter 2 in a class period that is supplemented by a take-home assignment for the students. I\r\nspend a lot of time on Data Description, Probability, Discrete, and Continuous Distributions. I\r\nmention selected facts from Multivariate Distributions in passing, and discuss the meaty parts of\r\nSampling Distributions before moving right along to Estimation (which is another chapter I dwell\r\non considerably). Hypothesis Testing goes faster after all of the previous work, and by that time\r\nthe end of the semester is in sight. I normally choose one or two final chapters (sometimes three)\r\nfrom the remaining to survey, and regret at the end that I did not have the chance to cover more.\r\nIn an attempt to be correct I have included material in this book which I would normally not\r\nmention during the course of a standard lecture. For instance, I normally do not highlight the\r\nintricacies of measure theory or integrability conditions when speaking to the class. Moreover, I\r\noften stray from the matrix approach to multiple linear regression because many of my students\r\nhave not yet been formally trained in linear algebra. That being said, it is important to me for\r\nthe students to hold something in their hands which acknowledges the world of mathematics and\r\nstatistics beyond the classroom, and which may be useful to them for many semesters to come. It\r\nalso mirrors my own experience as a student.\r\nThe vision for this document is a more or less self contained, essentially complete, correct,\r\nintroductory textbook. There should be plenty of exercises for the student, with full solutions for\r\nsome, and no solutions for others (so that the instructor may assign them for grading). By Sweave’s\r\ndynamic nature it is possible to write randomly generated exercises and I had planned to implement\r\nthis idea already throughout the book. Alas, there are only 24 hours in a day. Look for more in\r\nfuture editions.\r\nvii",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d16ecb65-c5cf-40a2-907f-0eec101f9411.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=902f77e6d2c0f962f264fc31e9c03a1861713bd9976debb6dc814dbfd5c77e9d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 523
      },
      {
        "segments": [
          {
            "segment_id": "d16ecb65-c5cf-40a2-907f-0eec101f9411",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 7,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Preface\r\nThis book was expanded from lecture materials I use in a one semester upper-division undergradu\u0002ate course entitled Probability and Statistics at Youngstown State University. Those lecture mate\u0002rials, in turn, were based on notes that I transcribed as a graduate student at Bowling Green State\r\nUniversity. The course for which the materials were written is 50-50 Probability and Statistics, and\r\nthe attendees include mathematics, engineering, and computer science majors (among others). The\r\ncatalog prerequisites for the course are a full year of calculus.\r\nThe book can be subdivided into three basic parts. The first part includes the introductions and\r\nelementary descriptive statistics; I want the students to be knee-deep in data right out of the gate.\r\nThe second part is the study of probability, which begins at the basics of sets and the equally likely\r\nmodel, journeys past discrete/continuous random variables, and continues through to multivariate\r\ndistributions. The chapter on sampling distributions paves the way to the third part, which is in\u0002ferential statistics. This last part includes point and interval estimation, hypothesis testing, and\r\nfinishes with introductions to selected topics in applied statistics.\r\nI usually only have time in one semester to cover a small subset of this book. I cover the material\r\nin Chapter 2 in a class period that is supplemented by a take-home assignment for the students. I\r\nspend a lot of time on Data Description, Probability, Discrete, and Continuous Distributions. I\r\nmention selected facts from Multivariate Distributions in passing, and discuss the meaty parts of\r\nSampling Distributions before moving right along to Estimation (which is another chapter I dwell\r\non considerably). Hypothesis Testing goes faster after all of the previous work, and by that time\r\nthe end of the semester is in sight. I normally choose one or two final chapters (sometimes three)\r\nfrom the remaining to survey, and regret at the end that I did not have the chance to cover more.\r\nIn an attempt to be correct I have included material in this book which I would normally not\r\nmention during the course of a standard lecture. For instance, I normally do not highlight the\r\nintricacies of measure theory or integrability conditions when speaking to the class. Moreover, I\r\noften stray from the matrix approach to multiple linear regression because many of my students\r\nhave not yet been formally trained in linear algebra. That being said, it is important to me for\r\nthe students to hold something in their hands which acknowledges the world of mathematics and\r\nstatistics beyond the classroom, and which may be useful to them for many semesters to come. It\r\nalso mirrors my own experience as a student.\r\nThe vision for this document is a more or less self contained, essentially complete, correct,\r\nintroductory textbook. There should be plenty of exercises for the student, with full solutions for\r\nsome, and no solutions for others (so that the instructor may assign them for grading). By Sweave’s\r\ndynamic nature it is possible to write randomly generated exercises and I had planned to implement\r\nthis idea already throughout the book. Alas, there are only 24 hours in a day. Look for more in\r\nfuture editions.\r\nvii",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d16ecb65-c5cf-40a2-907f-0eec101f9411.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=902f77e6d2c0f962f264fc31e9c03a1861713bd9976debb6dc814dbfd5c77e9d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 523
      },
      {
        "segments": [
          {
            "segment_id": "011f76bf-8aeb-438d-ab43-5333056f7b37",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 8,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "viii CONTENTS\r\nSeasoned readers will be able to detect my origins: Probability and Statistical Inference by\r\nHogg and Tanis [44], Statistical Inference by Casella and Berger [13], and Theory of Point Estima\u0002tion/Testing Statistical Hypotheses by Lehmann [59, 58]. I highly recommend each of those books\r\nto every reader of this one. Some R books with “introductory” in the title that I recommend are\r\nIntroductory Statistics with R by Dalgaard [19] and Using R for Introductory Statistics by Verzani\r\n[87]. Surely there are many, many other good introductory books about R, but frankly, I have tried\r\nto steer clear of them for the past year or so to avoid any undue influence on my own writing.\r\nI would like to make special mention of two other books: Introduction to Statistical Thought\r\nby Michael Lavine [56] and Introduction to Probability by Grinstead and Snell [37]. Both of these\r\nbooks are free and are what ultimately convinced me to release IPSUR under a free license, too.\r\nPlease bear in mind that the title of this book is “Introduction to Probability and Statistics\r\nUsing R”, and not “Introduction to R Using Probability and Statistics”, nor even “Introduction\r\nto Probability and Statistics and R Using Words”. The people at the party are Probability and\r\nStatistics; the handshake is R. There are several important topics about R which some individuals\r\nwill feel are underdeveloped, glossed over, or wantonly omitted. Some will feel the same way\r\nabout the probabilistic and/or statistical content. Still others will just want to learn R and skip all\r\nof the mathematics.\r\nDespite any misgivings: here it is, warts and all. I humbly invite said individuals to take this\r\nbook, with the GNU Free Documentation License (GNU-FDL) in hand, and make it better. In that\r\nspirit there are at least a few ways in my view in which this book could be improved.\r\nBetter data. The data analyzed in this book are almost entirely from the datasets package in\r\nbase R, and here is why:\r\n1. I made a conscious effort to minimize dependence on contributed packages,\r\n2. The data are instantly available, already in the correct format, so we need not take time\r\nto manage them, and\r\n3. The data are real.\r\nI made no attempt to choose data sets that would be interesting to the students; rather, data\r\nwere chosen for their potential to convey a statistical point. Many of the data sets are decades\r\nold or more (for instance, the data used to introduce simple linear regression are the speeds\r\nand stopping distances of cars in the 1920’s).\r\nIn a perfect world with infinite time I would research and contribute recent, real data in a\r\ncontext crafted to engage the students in every example. One day I hope to stumble over said\r\ntime. In the meantime, I will add new data sets incrementally as time permits.\r\nMore proofs. I would like to include more proofs for the sake of completeness (I understand that\r\nsome people would not consider more proofs to be improvement). Many proofs have been\r\nskipped entirely, and I am not aware of any rhyme or reason to the current omissions. I will\r\nadd more when I get a chance.\r\nMore and better graphics: I have not used the ggplot2 package [90] because I do not know how\r\nto use it yet. It is on my to-do list.\r\nMore and better exercises: There are only a few exercises in the first edition simply because I\r\nhave not had time to write more. I have toyed with the exams package [38] and I believe that",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/011f76bf-8aeb-438d-ab43-5333056f7b37.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=26de005ddaf7b849b617c7b426c859f03af018b50bdb9c2ecfc62f7568916277",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 596
      },
      {
        "segments": [
          {
            "segment_id": "011f76bf-8aeb-438d-ab43-5333056f7b37",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 8,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "viii CONTENTS\r\nSeasoned readers will be able to detect my origins: Probability and Statistical Inference by\r\nHogg and Tanis [44], Statistical Inference by Casella and Berger [13], and Theory of Point Estima\u0002tion/Testing Statistical Hypotheses by Lehmann [59, 58]. I highly recommend each of those books\r\nto every reader of this one. Some R books with “introductory” in the title that I recommend are\r\nIntroductory Statistics with R by Dalgaard [19] and Using R for Introductory Statistics by Verzani\r\n[87]. Surely there are many, many other good introductory books about R, but frankly, I have tried\r\nto steer clear of them for the past year or so to avoid any undue influence on my own writing.\r\nI would like to make special mention of two other books: Introduction to Statistical Thought\r\nby Michael Lavine [56] and Introduction to Probability by Grinstead and Snell [37]. Both of these\r\nbooks are free and are what ultimately convinced me to release IPSUR under a free license, too.\r\nPlease bear in mind that the title of this book is “Introduction to Probability and Statistics\r\nUsing R”, and not “Introduction to R Using Probability and Statistics”, nor even “Introduction\r\nto Probability and Statistics and R Using Words”. The people at the party are Probability and\r\nStatistics; the handshake is R. There are several important topics about R which some individuals\r\nwill feel are underdeveloped, glossed over, or wantonly omitted. Some will feel the same way\r\nabout the probabilistic and/or statistical content. Still others will just want to learn R and skip all\r\nof the mathematics.\r\nDespite any misgivings: here it is, warts and all. I humbly invite said individuals to take this\r\nbook, with the GNU Free Documentation License (GNU-FDL) in hand, and make it better. In that\r\nspirit there are at least a few ways in my view in which this book could be improved.\r\nBetter data. The data analyzed in this book are almost entirely from the datasets package in\r\nbase R, and here is why:\r\n1. I made a conscious effort to minimize dependence on contributed packages,\r\n2. The data are instantly available, already in the correct format, so we need not take time\r\nto manage them, and\r\n3. The data are real.\r\nI made no attempt to choose data sets that would be interesting to the students; rather, data\r\nwere chosen for their potential to convey a statistical point. Many of the data sets are decades\r\nold or more (for instance, the data used to introduce simple linear regression are the speeds\r\nand stopping distances of cars in the 1920’s).\r\nIn a perfect world with infinite time I would research and contribute recent, real data in a\r\ncontext crafted to engage the students in every example. One day I hope to stumble over said\r\ntime. In the meantime, I will add new data sets incrementally as time permits.\r\nMore proofs. I would like to include more proofs for the sake of completeness (I understand that\r\nsome people would not consider more proofs to be improvement). Many proofs have been\r\nskipped entirely, and I am not aware of any rhyme or reason to the current omissions. I will\r\nadd more when I get a chance.\r\nMore and better graphics: I have not used the ggplot2 package [90] because I do not know how\r\nto use it yet. It is on my to-do list.\r\nMore and better exercises: There are only a few exercises in the first edition simply because I\r\nhave not had time to write more. I have toyed with the exams package [38] and I believe that",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/011f76bf-8aeb-438d-ab43-5333056f7b37.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=26de005ddaf7b849b617c7b426c859f03af018b50bdb9c2ecfc62f7568916277",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 596
      },
      {
        "segments": [
          {
            "segment_id": "3881b5c7-b1f7-46e6-9618-aba808995ba1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 9,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "CONTENTS ix\r\nit is a right way to move forward. As I learn more about what the package can do I would\r\nlike to incorporate it into later editions of this book.\r\nAbout This Document\r\nIPSUR contains many interrelated parts: the Document, the Program, the Package, and the An\u0002cillaries. In short, the Document is what you are reading right now. The Program provides an\r\nefficient means to modify the Document. The Package is an R package that houses the Program\r\nand the Document. Finally, the Ancillaries are extra materials that reside in the Package and were\r\nproduced by the Program to supplement use of the Document. We briefly describe each of them in\r\nturn.\r\nThe Document\r\nThe Document is that which you are reading right now – IPSUR’s raison d’être. There are trans\u0002parent copies (nonproprietary text files) and opaque copies (everything else). See the GNU-FDL in\r\nAppendix B for more precise language and details.\r\nIPSUR.tex is a transparent copy of the Document to be typeset with a LATEX distribution such as\r\nMikTEX or TEX Live. Any reader is free to modify the Document and release the modified\r\nversion in accordance with the provisions of the GNU-FDL. Note that this file cannot be\r\nused to generate a randomized copy of the Document. Indeed, in its released form it is\r\nonly capable of typesetting the exact version of IPSUR which you are currently reading.\r\nFurthermore, the .tex file is unable to generate any of the ancillary materials.\r\nIPSUR-xxx.eps, IPSUR-xxx.pdf are the image files for every graph in the Document. These\r\nare needed when typesetting with LATEX.\r\nIPSUR.pdf is an opaque copy of the Document. This is the file that instructors would likely want\r\nto distribute to students.\r\nIPSUR.dvi is another opaque copy of the Document in a different file format.\r\nThe Program\r\nThe Program includes IPSUR.lyx and its nephew IPSUR.Rnw; the purpose of each is to give\r\nindividuals a way to quickly customize the Document for their particular purpose(s).\r\nIPSUR.lyx is the source LYX file for the Program, released under the GNU General Public Li\u0002cense (GNU GPL) Version 3. This file is opened, modified, and compiled with LYX, a\r\nsophisticated open-source document processor, and may be used (together with Sweave) to\r\ngenerate a randomized, modified copy of the Document with brand new data sets for some of\r\nthe exercises and the solution manuals (in the Second Edition). Additionally, LYX can easily\r\nactivate/deactivate entire blocks of the document, e.g. the proofs of the theorems, the student\r\nsolutions to the exercises, or the instructor answers to the problems, so that the new author\r\nmay choose which sections (s)he would like to include in the final Document (again, Second\r\nEdition). The IPSUR.lyx file is all that a person needs (in addition to a properly configured",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3881b5c7-b1f7-46e6-9618-aba808995ba1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1669d58b67e75285abce0fb521e5cbb3c18dc4769df2558a4751129beb2a5065",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 460
      },
      {
        "segments": [
          {
            "segment_id": "193e270e-5807-494f-9108-0a24bd825d6e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 10,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "x CONTENTS\r\nsystem – see Appendix G) to generate/compile/export to all of the other formats described\r\nabove and below, which includes the ancillary materials IPSUR.Rdata and IPSUR.R.\r\nIPSUR.Rnw is another form of the source code for the Program, also released under the GNU GPL\r\nVersion 3. It was produced by exporting IPSUR.lyx into R/Sweave format (.Rnw). This file\r\nmay be processed with Sweave to generate a randomized copy of IPSUR.tex – a transparent\r\ncopy of the Document – together with the ancillary materials IPSUR.Rdata and IPSUR.R.\r\nPlease note, however, that IPSUR.Rnw is just a simple text file which does not support many\r\nof the extra features that LYX offers such as WYSIWYM editing, instantly (de)activating\r\nbranches of the manuscript, and more.\r\nThe Package\r\nThere is a contributed package on CRAN, called IPSUR. The package affords many advantages, one\r\nbeing that it houses the Document in an easy-to-access medium. Indeed, a student can have the\r\nDocument at his/her fingertips with only three commands:\r\n> install.packages(\"IPSUR\")\r\n> library(IPSUR)\r\n> read(IPSUR)\r\nAnother advantage goes hand in hand with the Program’s license; since IPSUR is free, the source\r\ncode must be freely available to anyone that wants it. A package hosted on CRAN allows the author\r\nto obey the license by default.\r\nA much more important advantage is that the excellent facilities at R-Forge are building and\r\nchecking the package daily against patched and development versions of the absolute latest pre\u0002release of R. If any problems surface then I will know about it within 24 hours.\r\nAnd finally, suppose there is some sort of problem. The package structure makes it incredibly\r\neasy for me to distribute bug-fixes and corrected typographical errors. As an author I can make my\r\ncorrections, upload them to the repository at R-Forge, and they will be reflected worldwide within\r\nhours. We aren’t in Kansas anymore, Toto.\r\nAncillary Materials\r\nThese are extra materials that accompany IPSUR. They reside in the /etc subdirectory of the\r\npackage source.\r\nIPSUR.R is the exported R code from IPSUR.Rnw. With this script, literally every R command\r\nfrom the entirety of IPSUR can be resubmitted at the command line.\r\nNotation\r\nWe use the notation x or stem.leaf notation to denote objects, functions, etc.. The sequence\r\n“Statistics . Summaries . Active Dataset” means to click the Statistics menu item, next click the\r\nSummaries submenu item, and finally click Active Dataset.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/193e270e-5807-494f-9108-0a24bd825d6e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5e069a0e87f246e9178081166394398dc41c7b688787029d280ace72a1fd41ba",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 392
      },
      {
        "segments": [
          {
            "segment_id": "2e8a3e0c-0cc7-419d-9516-e28928721823",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 11,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "CONTENTS xi\r\nAcknowledgements\r\nThis book would not have been possible without the firm mathematical and statistical foundation\r\nprovided by the professors at Bowling Green State University, including Drs. Gábor Székely, Craig\r\nZirbel, Arjun K. Gupta, Hanfeng Chen, Truc Nguyen, and James Albert. I would also like to thank\r\nDrs. Neal Carothers and Kit Chan.\r\nI would also like to thank my colleagues at Youngstown State University for their support.\r\nIn particular, I would like to thank Dr. G. Andy Chang for showing me what it means to be a\r\nstatistician.\r\nI would like to thank Richard Heiberger for his insightful comments and improvements to\r\nseveral points and displays in the manuscript.\r\nFinally, and most importantly, I would like to thank my wife for her patience and understanding\r\nwhile I worked hours, days, months, and years on a free book. In retrospect, I can’t believe I ever\r\ngot away with it.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/2e8a3e0c-0cc7-419d-9516-e28928721823.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0cc7d40668b4681b79047940eccb0282966b4bc8a912dba941709f20d06763e2",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "346abbaf-b6d5-4ff7-8e43-a85272d3689a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 12,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "xii CONTENTS",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/346abbaf-b6d5-4ff7-8e43-a85272d3689a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=67147fbca8e8a663fd3d608c9ef50484551718a8aa880ff402976e1803da10e1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 152
      },
      {
        "segments": [
          {
            "segment_id": "377c2d65-2ae7-4679-8cf1-69afe236c442",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 13,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "List of Figures\r\n3.1.1 Strip charts of the precip, rivers, and discoveries data . . . . . . . . . . 20\r\n3.1.2 (Relative) frequency histograms of the precip data . . . . . . . . . . . . . . 22\r\n3.1.3 More histograms of the precip data . . . . . . . . . . . . . . . . . . . . . . 23\r\n3.1.4 Index plots of the LakeHuron data . . . . . . . . . . . . . . . . . . . . . . . 26\r\n3.1.5 Bar graphs of the state.region data . . . . . . . . . . . . . . . . . . . . . 29\r\n3.1.6 Pareto chart of the state.division data . . . . . . . . . . . . . . . . . . . 30\r\n3.1.7 Dot chart of the state.region data . . . . . . . . . . . . . . . . . . . . . . 31\r\n3.6.1 Boxplots of weight by feed type in the chickwts data . . . . . . . . . . . . 51\r\n3.6.2 Histograms of age by education level from the infert data . . . . . . . . . 52\r\n3.6.3 An xyplot of Petal.Length versus Petal.Width by Species in the iris\r\ndata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\r\n3.6.4 A coplot of conc versus uptake by Type and Treatment in the CO2 data . 54\r\n4.0.1 Two types of experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\r\n4.5.1 The birthday problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\r\n5.3.1 Graph of the binom(size = 3, prob = 1/2) CDF . . . . . . . . . . . . . . . 119\r\n5.3.2 The binom(size = 3, prob = 0.5) distribution from the distr package . . . . 121\r\n5.5.1 The empirical CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\r\n6.5.1 Chi square distribution for various degrees of freedom . . . . . . . . . . . . . 159\r\n6.5.2 Plot of the gamma(shape = 13, rate = 1) MGF . . . . . . . . . . . . . . . 163\r\n7.6.1 Graph of a bivariate normal PDF . . . . . . . . . . . . . . . . . . . . . . . . 182\r\n7.9.1 Plot of a multinomial PMF . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\r\n8.2.1 Student’s t distribution for various degrees of freedom . . . . . . . . . . . . . 195\r\n8.5.1 Plot of simulated IQRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\r\n8.5.2 Plot of simulated MADs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\r\n9.1.1 Capture-recapture experiment . . . . . . . . . . . . . . . . . . . . . . . . . . 207\r\n9.1.2 Assorted likelihood functions for fishing, part two . . . . . . . . . . . . . . . 209\r\n9.1.3 Species maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . 210\r\n9.2.1 Simulated confidence intervals . . . . . . . . . . . . . . . . . . . . . . . . . 216\r\n9.2.2 Confidence interval plot for the PlantGrowth data . . . . . . . . . . . . . . . 219\r\n10.2.1 Hypothesis test plot based on normal.and.t.dist from the HH package . . . 236\r\nxiii",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/377c2d65-2ae7-4679-8cf1-69afe236c442.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8be91d48d926cc08748f32aacc52956897e89e0adfc18dc5976c636c2c72a914",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 825
      },
      {
        "segments": [
          {
            "segment_id": "377c2d65-2ae7-4679-8cf1-69afe236c442",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 13,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "List of Figures\r\n3.1.1 Strip charts of the precip, rivers, and discoveries data . . . . . . . . . . 20\r\n3.1.2 (Relative) frequency histograms of the precip data . . . . . . . . . . . . . . 22\r\n3.1.3 More histograms of the precip data . . . . . . . . . . . . . . . . . . . . . . 23\r\n3.1.4 Index plots of the LakeHuron data . . . . . . . . . . . . . . . . . . . . . . . 26\r\n3.1.5 Bar graphs of the state.region data . . . . . . . . . . . . . . . . . . . . . 29\r\n3.1.6 Pareto chart of the state.division data . . . . . . . . . . . . . . . . . . . 30\r\n3.1.7 Dot chart of the state.region data . . . . . . . . . . . . . . . . . . . . . . 31\r\n3.6.1 Boxplots of weight by feed type in the chickwts data . . . . . . . . . . . . 51\r\n3.6.2 Histograms of age by education level from the infert data . . . . . . . . . 52\r\n3.6.3 An xyplot of Petal.Length versus Petal.Width by Species in the iris\r\ndata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\r\n3.6.4 A coplot of conc versus uptake by Type and Treatment in the CO2 data . 54\r\n4.0.1 Two types of experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\r\n4.5.1 The birthday problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\r\n5.3.1 Graph of the binom(size = 3, prob = 1/2) CDF . . . . . . . . . . . . . . . 119\r\n5.3.2 The binom(size = 3, prob = 0.5) distribution from the distr package . . . . 121\r\n5.5.1 The empirical CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\r\n6.5.1 Chi square distribution for various degrees of freedom . . . . . . . . . . . . . 159\r\n6.5.2 Plot of the gamma(shape = 13, rate = 1) MGF . . . . . . . . . . . . . . . 163\r\n7.6.1 Graph of a bivariate normal PDF . . . . . . . . . . . . . . . . . . . . . . . . 182\r\n7.9.1 Plot of a multinomial PMF . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\r\n8.2.1 Student’s t distribution for various degrees of freedom . . . . . . . . . . . . . 195\r\n8.5.1 Plot of simulated IQRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\r\n8.5.2 Plot of simulated MADs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\r\n9.1.1 Capture-recapture experiment . . . . . . . . . . . . . . . . . . . . . . . . . . 207\r\n9.1.2 Assorted likelihood functions for fishing, part two . . . . . . . . . . . . . . . 209\r\n9.1.3 Species maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . 210\r\n9.2.1 Simulated confidence intervals . . . . . . . . . . . . . . . . . . . . . . . . . 216\r\n9.2.2 Confidence interval plot for the PlantGrowth data . . . . . . . . . . . . . . . 219\r\n10.2.1 Hypothesis test plot based on normal.and.t.dist from the HH package . . . 236\r\nxiii",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/377c2d65-2ae7-4679-8cf1-69afe236c442.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8be91d48d926cc08748f32aacc52956897e89e0adfc18dc5976c636c2c72a914",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 825
      },
      {
        "segments": [
          {
            "segment_id": "6434fd94-83f9-4719-9763-c40634c219bc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 14,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "xiv LIST OF FIGURES\r\n10.3.1 Hypothesis test plot based on normal.and.t.dist from the HH package . . . 238\r\n10.6.1 Between group versus within group variation . . . . . . . . . . . . . . . . . . 244\r\n10.6.2 Between group versus within group variation . . . . . . . . . . . . . . . . . . 245\r\n10.6.3 Some F plots from the HH package . . . . . . . . . . . . . . . . . . . . . . . 246\r\n10.7.1 Plot of significance level and power . . . . . . . . . . . . . . . . . . . . . . . 247\r\n11.1.1 Philosophical foundations of SLR . . . . . . . . . . . . . . . . . . . . . . . . 251\r\n11.1.2 Scatterplot of dist versus speed for the cars data . . . . . . . . . . . . . . 252\r\n11.2.1 Scatterplot with added regression line for the cars data . . . . . . . . . . . . 255\r\n11.2.2 Scatterplot with confidence/prediction bands for the cars data . . . . . . . . 263\r\n11.4.1 Normal q-q plot of the residuals for the cars data . . . . . . . . . . . . . . . 269\r\n11.4.2 Plot of standardized residuals against the fitted values for the cars data . . . . 271\r\n11.4.3 Plot of the residuals versus the fitted values for the cars data . . . . . . . . . 273\r\n11.5.1 Cook’s distances for the cars data . . . . . . . . . . . . . . . . . . . . . . . 280\r\n11.5.2 Diagnostic plots for the cars data . . . . . . . . . . . . . . . . . . . . . . . . 281\r\n12.1.1 Scatterplot matrix of trees data . . . . . . . . . . . . . . . . . . . . . . . . 287\r\n12.1.2 3D scatterplot with regression plane for the trees data . . . . . . . . . . . . 289\r\n12.4.1 Scatterplot of Volume versus Girth for the trees data . . . . . . . . . . . . 300\r\n12.4.2 A quadratic model for the trees data . . . . . . . . . . . . . . . . . . . . . . 303\r\n12.6.1 A dummy variable model for the trees data . . . . . . . . . . . . . . . . . . 309\r\n13.2.1 Bootstrapping the standard error of the mean, simulated data . . . . . . . . . 322\r\n13.2.2 Bootstrapping the standard error of the median for the rivers data . . . . . . 324",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6434fd94-83f9-4719-9763-c40634c219bc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2dd3da2d4f4eb2282d2511d255b42ef0e20feda5ee69f962fbc5d853f6eaec06",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "6434fd94-83f9-4719-9763-c40634c219bc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 14,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "xiv LIST OF FIGURES\r\n10.3.1 Hypothesis test plot based on normal.and.t.dist from the HH package . . . 238\r\n10.6.1 Between group versus within group variation . . . . . . . . . . . . . . . . . . 244\r\n10.6.2 Between group versus within group variation . . . . . . . . . . . . . . . . . . 245\r\n10.6.3 Some F plots from the HH package . . . . . . . . . . . . . . . . . . . . . . . 246\r\n10.7.1 Plot of significance level and power . . . . . . . . . . . . . . . . . . . . . . . 247\r\n11.1.1 Philosophical foundations of SLR . . . . . . . . . . . . . . . . . . . . . . . . 251\r\n11.1.2 Scatterplot of dist versus speed for the cars data . . . . . . . . . . . . . . 252\r\n11.2.1 Scatterplot with added regression line for the cars data . . . . . . . . . . . . 255\r\n11.2.2 Scatterplot with confidence/prediction bands for the cars data . . . . . . . . 263\r\n11.4.1 Normal q-q plot of the residuals for the cars data . . . . . . . . . . . . . . . 269\r\n11.4.2 Plot of standardized residuals against the fitted values for the cars data . . . . 271\r\n11.4.3 Plot of the residuals versus the fitted values for the cars data . . . . . . . . . 273\r\n11.5.1 Cook’s distances for the cars data . . . . . . . . . . . . . . . . . . . . . . . 280\r\n11.5.2 Diagnostic plots for the cars data . . . . . . . . . . . . . . . . . . . . . . . . 281\r\n12.1.1 Scatterplot matrix of trees data . . . . . . . . . . . . . . . . . . . . . . . . 287\r\n12.1.2 3D scatterplot with regression plane for the trees data . . . . . . . . . . . . 289\r\n12.4.1 Scatterplot of Volume versus Girth for the trees data . . . . . . . . . . . . 300\r\n12.4.2 A quadratic model for the trees data . . . . . . . . . . . . . . . . . . . . . . 303\r\n12.6.1 A dummy variable model for the trees data . . . . . . . . . . . . . . . . . . 309\r\n13.2.1 Bootstrapping the standard error of the mean, simulated data . . . . . . . . . 322\r\n13.2.2 Bootstrapping the standard error of the median for the rivers data . . . . . . 324",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6434fd94-83f9-4719-9763-c40634c219bc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2dd3da2d4f4eb2282d2511d255b42ef0e20feda5ee69f962fbc5d853f6eaec06",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "e0693549-654a-4342-bfa0-44d11b2cd2ff",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 15,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "List of Tables\r\n4.1 Sampling k from n objects with urnsamples . . . . . . . . . . . . . . . . . . . . 89\r\n4.2 Rolling two dice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\r\n5.1 Correspondence between stats and distr . . . . . . . . . . . . . . . . . . . . . 121\r\n7.1 Maximum U and sum V of a pair of dice rolls (X, Y) . . . . . . . . . . . . . . . . 168\r\n7.2 Joint values of U = max(X, Y) and V = X + Y . . . . . . . . . . . . . . . . . . . . 168\r\n7.3 The joint PMF of (U, V) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\r\nE.1 Set operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\r\nE.2 Differentiation rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\r\nE.3 Some derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\r\nE.4 Some integrals (constants of integration omitted) . . . . . . . . . . . . . . . . . . 364\r\nxv",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e0693549-654a-4342-bfa0-44d11b2cd2ff.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ef47fe1f25e68fc00473157347764118ba64f0d15f4a9b855a18a6410977d751",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "4cfc1fe1-923b-4f8c-a4ec-447b36ed0098",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 16,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "xvi LIST OF TABLES",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4cfc1fe1-923b-4f8c-a4ec-447b36ed0098.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=292ebdcd141813ca3353e26d61573a46d57ca4e18c306e68215f7de7ac439af7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 355
      },
      {
        "segments": [
          {
            "segment_id": "8f3a122b-7909-49e1-99c3-46d15f1bca1b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 17,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 1\r\nAn Introduction to Probability and\r\nStatistics\r\nThis chapter has proved to be the hardest to write, by far. The trouble is that there is so much to\r\nsay – and so many people have already said it so much better than I could. When I get something I\r\nlike I will release it here.\r\nIn the meantime, there is a lot of information already available to a person with an Internet\r\nconnection. I recommend to start at Wikipedia, which is not a flawless resource but it has the main\r\nideas with links to reputable sources.\r\nIn my lectures I usually tell stories about Fisher, Galton, Gauss, Laplace, Quetelet, and the\r\nChevalier de Mere.\r\n1.1 Probability\r\nThe common folklore is that probability has been around for millennia but did not gain the attention\r\nof mathematicians until approximately 1654 when the Chevalier de Mere had a question regarding\r\nthe fair division of a game’s payoff to the two players, if the game had to end prematurely.\r\n1.2 Statistics\r\nStatistics concerns data; their collection, analysis, and interpretation. In this book we distinguish\r\nbetween two types of statistics: descriptive and inferential.\r\nDescriptive statistics concerns the summarization of data. We have a data set and we would like\r\nto describe the data set in multiple ways. Usually this entails calculating numbers from the data,\r\ncalled descriptive measures, such as percentages, sums, averages, and so forth.\r\nInferential statistics does more. There is an inference associated with the data set, a conclusion\r\ndrawn about the population from which the data originated.\r\nI would like to mention that there are two schools of thought of statistics: frequentist and\r\nbayesian. The difference between the schools is related to how the two groups interpret the under\u0002lying probability (see Section 4.3). The frequentist school gained a lot of ground among statisti\u0002cians due in large part to the work of Fisher, Neyman, and Pearson in the early twentieth century.\r\n1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8f3a122b-7909-49e1-99c3-46d15f1bca1b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d77512d84b5d73cb8512ece81a2314e159d950bc0bc0c5cbfc8a1ff0e86c1002",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "4169e9d1-c223-4d05-bf93-53992eda638a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 18,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2 CHAPTER 1. AN INTRODUCTION TO PROBABILITY AND STATISTICS\r\nThat dominance lasted until inexpensive computing power became widely available; nowadays the\r\nbayesian school is garnering more attention and at an increasing rate.\r\nThis book is devoted mostly to the frequentist viewpoint because that is how I was trained, with\r\nthe conspicuous exception of Sections 4.8 and 7.3. I plan to add more bayesian material in later\r\neditions of this book.\r\nChapter Exercises",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4169e9d1-c223-4d05-bf93-53992eda638a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1c1c4364b816c29f83feb86ec3d29d72b0174e93492727bddb5d9592388f8825",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 392
      },
      {
        "segments": [
          {
            "segment_id": "0ea4f3c5-d7cc-47b5-9455-6fb1d0e781eb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 19,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 2\r\nAn Introduction to R\r\n2.1 Downloading and Installing R\r\nThe instructions for obtaining R largely depend on the user’s hardware and operating system. The R\r\nProject has written an R Installation and Administration manual with complete, precise instructions\r\nabout what to do, together with all sorts of additional information. The following is just a primer\r\nto get a person started.\r\n2.1.1 Installing R\r\nVisit one of the links below to download the latest version of R for your operating system:\r\nMicrosoft Windows: http://cran.r-project.org/bin/windows/base/\r\nMacOS: http://cran.r-project.org/bin/macosx/\r\nLinux: http://cran.r-project.org/bin/linux/\r\nOn Microsoft Windows, click the R-x.y.z.exe installer to start installation. When it asks for\r\n\"Customized startup options\", specify Yes. In the next window, be sure to select the SDI (single\r\ndocument interface) option; this is useful later when we discuss three dimensional plots with the\r\nrgl package [1].\r\nInstalling R on a USB drive (Windows) With this option you can use R portably and without\r\nadministrative privileges. There is an entry in the R for Windows FAQ about this. Here is the\r\nprocedure I use:\r\n1. Download the Windows installer above and start installation as usual. When it asks where to\r\ninstall, navigate to the top-level directory of the USB drive instead of the default C drive.\r\n2. When it asks whether to modify the Windows registry, uncheck the box; we do NOT want to\r\ntamper with the registry.\r\n3. After installation, change the name of the folder from R-x.y.z to just plain R. (Even quicker:\r\ndo this in step 1.)\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0ea4f3c5-d7cc-47b5-9455-6fb1d0e781eb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=20e24916b4afa015ff684f0d0e7f0e44a0e968b5e4436b292bb0f51037e601a7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 251
      },
      {
        "segments": [
          {
            "segment_id": "a1cc1762-2fa4-4ead-82d1-2c6fa6e8cd22",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 20,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4 CHAPTER 2. AN INTRODUCTION TO R\r\n4. Download the following shortcut to the top-level directory of the USB drive, right beside the\r\nR folder, not inside the folder.\r\nhttp://ipsur.r-forge.r-project.org/book/download/R.exe\r\nUse the downloaded shortcut to run R.\r\nSteps 3 and 4 are not required but save you the trouble of navigating to the R-x.y.z/bin directory\r\nto double-click Rgui.exe every time you want to run the program. It is useless to create your own\r\nshortcut to Rgui.exe. Windows does not allow shortcuts to have relative paths; they always have a\r\ndrive letter associated with them. So if you make your own shortcut and plug your USB drive into\r\nsome other machine that happens to assign your drive a different letter, then your shortcut will no\r\nlonger be pointing to the right place.\r\n2.1.2 Installing and Loading Add-on Packages\r\nThere are base packages (which come with R automatically), and contributed packages (which\r\nmust be downloaded for installation). For example, on the version of R being used for this docu\u0002ment the default base packages loaded at startup are\r\n> getOption(\"defaultPackages\")\r\n[1] \"datasets\" \"utils\" \"grDevices\" \"graphics\" \"stats\" \"methods\"\r\nThe base packages are maintained by a select group of volunteers, called “R Core”. In addi\u0002tion to the base packages, there are literally thousands of additional contributed packages written\r\nby individuals all over the world. These are stored worldwide on mirrors of the Comprehensive\r\nR Archive Network, or CRAN for short. Given an active Internet connection, anybody is free to\r\ndownload and install these packages and even inspect the source code.\r\nTo install a package named foo, open up R and type install.packages(\"foo\"). To in\u0002stall foo and additionally install all of the other packages on which foo depends, instead type\r\ninstall.packages(\"foo\", depends = TRUE).\r\nThe general command install.packages() will (on most operating systems) open a window\r\ncontaining a huge list of available packages; simply choose one or more to install.\r\nNo matter how many packages are installed onto the system, each one must first be loaded\r\nfor use with the library function. For instance, the foreign package [18] contains all sorts of\r\nfunctions needed to import data sets into R from other software such as SPSS, SAS, etc.. But none\r\nof those functions will be available until the command library(foreign) is issued.\r\nType library() at the command prompt (described below) to see a list of all available pack\u0002ages in your library.\r\nFor complete, precise information regarding installation of R and add-on packages, see the R\r\nInstallation and Administration manual, http://cran.r-project.org/manuals.html.\r\n2.2 Communicating with R\r\nOne line at a time This is the most basic method and is the first one that beginners will use.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a1cc1762-2fa4-4ead-82d1-2c6fa6e8cd22.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4d534106c5083a845f78958dc581e0bd4b351ac32ca4e949cf47d02b1721ccf9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 438
      },
      {
        "segments": [
          {
            "segment_id": "3d1af61f-0e7d-4fcf-b47c-605f8a6cd1a4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 21,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2.2. COMMUNICATING WITH R 5\r\nRGui (Microsoftr Windows)\r\nTerminal\r\nEmacs/ESS, XEmacs\r\nJGR\r\nMultiple lines at a time For longer programs (called scripts) there is too much code to write\r\nall at once at the command prompt. Furthermore, for longer scripts it is convenient to be able\r\nto only modify a certain piece of the script and run it again in R. Programs called script editors\r\nare specially designed to aid the communication and code writing process. They have all sorts of\r\nhelpful features including R syntax highlighting, automatic code completion, delimiter matching,\r\nand dynamic help on the R functions as they are being written. Even more, they often have all of\r\nthe text editing features of programs like Microsoftr Word. Lastly, most script editors are fully\r\ncustomizable in the sense that the user can customize the appearance of the interface to choose\r\nwhat colors to display, when to display them, and how to display them.\r\nR Editor (Windows): In Microsoftr Windows, RGui has its own built-in script editor, called R\r\nEditor. From the console window, select File . New Script. A script window opens, and the\r\nlines of code can be written in the window. When satisfied with the code, the user highlights\r\nall of the commands and presses Ctrl+R. The commands are automatically run at once in R\r\nand the output is shown. To save the script for later, click File . Save as... in R Editor. The\r\nscript can be reopened later with File . Open Script... in RGui. Note that R Editor does not\r\nhave the fancy syntax highlighting that the others do.\r\nRWinEdt: This option is coordinated with WinEdt for LATEX and has additional features such as\r\ncode highlighting, remote sourcing, and a ton of other things. However, one first needs to\r\ndownload and install a shareware version of another program, WinEdt, which is only free for\r\na while – pop-up windows will eventually appear that ask for a registration code. RWinEdt\r\nis nevertheless a very fine choice if you already own WinEdt or are planning to purchase it\r\nin the near future.\r\nTinn-R/Sciviews-K: This one is completely free and has all of the above mentioned options and\r\nmore. It is simple enough to use that the user can virtually begin working with it immediately\r\nafter installation. But Tinn-R proper is only available for Microsoftr Windows operating\r\nsystems. If you are on MacOS or Linux, a comparable alternative is Sci-Views - Komodo\r\nEdit.\r\nEmacs/ESS: Emacs is an all purpose text editor. It can do absolutely anything with respect to\r\nmodifying, searching, editing, and manipulating, text. And if Emacs can’t do it, then you\r\ncan write a program that extends Emacs to do it. Once such extension is called ESS, which\r\nstands for Emacs Speaks Statistics. With ESS a person can speak to R, do all of the tricks that\r\nthe other script editors offer, and much, much, more. Please see the following for installation\r\ndetails, documentation, reference cards, and a whole lot more:\r\nhttp://ess.r-project.org",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3d1af61f-0e7d-4fcf-b47c-605f8a6cd1a4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3f68aff40c06d242691780862e02080d8ec73f75d95c023b4eed7487eeb7f218",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 498
      },
      {
        "segments": [
          {
            "segment_id": "601a6bf0-3468-405b-84b2-cd92b428dafa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 22,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6 CHAPTER 2. AN INTRODUCTION TO R\r\nFair warning: if you want to try Emacs and if you grew up with Microsoftr Windows or\r\nMacintosh, then you are going to need to relearn everything you thought you knew about computers\r\nyour whole life. (Or, since Emacs is completely customizable, you can reconfigure Emacs to behave\r\nthe way you want.) I have personally experienced this transformation and I will never go back.\r\nJGR (read “Jaguar”): This one has the bells and whistles of RGui plus it is based on Java, so\r\nit works on multiple operating systems. It has its own script editor like R Editor but with\r\nadditional features such as syntax highlighting and code-completion. If you do not use\r\nMicrosoftr Windows (or even if you do) you definitely want to check out this one.\r\nKate, Bluefish, etc. There are literally dozens of other text editors available, many of them free,\r\nand each has its own (dis)advantages. I only have mentioned the ones with which I have\r\nhad substantial personal experience and have enjoyed at some point. Play around, and let me\r\nknow what you find.\r\nGraphical User Interfaces (GUIs) By the word “GUI” I mean an interface in which the user\r\ncommunicates with R by way of points-and-clicks in a menu of some sort. Again, there are many,\r\nmany options and I only mention ones that I have used and enjoyed. Some of the other more\r\npopular script editors can be downloaded from the R-Project website at http://www.sciviews.\r\norg/_rgui/. On the left side of the screen (under Projects) there are several choices available.\r\nR Commander provides a point-and-click interface to many basic statistical tasks. It is called the\r\n“Commander” because every time one makes a selection from the menus, the code corre\u0002sponding to the task is listed in the output window. One can take this code, copy-and-paste\r\nit to a text file, then re-run it again at a later time without the R Commander’s assistance.\r\nIt is well suited for the introductory level. Rcmdr also allows for user-contributed “Plugins”\r\nwhich are separate packages on CRAN that add extra functionality to the Rcmdr package. The\r\nplugins are typically named with the prefix RcmdrPlugin to make them easy to identify in\r\nthe CRAN package list. One such plugin is the\r\nRcmdrPlugin.IPSUR package which accompanies this text.\r\nPoor Man’s GUI is an alternative to the Rcmdr which is based on GTk instead of Tcl/Tk. It has\r\nbeen a while since I used it but I remember liking it very much when I did. One thing\r\nthat stood out was that the user could drag-and-drop data sets for plots. See here for more\r\ninformation: http://wiener.math.csi.cuny.edu/pmg/.\r\nRattle is a data mining toolkit which was designed to manage/analyze very large data sets, but\r\nit provides enough other general functionality to merit mention here. See [91] for more\r\ninformation.\r\nDeducer is relatively new and shows promise from what I have seen, but I have not actually used\r\nit in the classroom yet.\r\n2.3 Basic R Operations and Concepts\r\nThe R developers have written an introductory document entitled “An Introduction to R”. There is\r\na sample session included which shows what basic interaction with R looks like. I recommend that",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/601a6bf0-3468-405b-84b2-cd92b428dafa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6c4dc2ffd89782e4484a7f199a7357742f0087cae9207f8634392aa5b6d70af1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 533
      },
      {
        "segments": [
          {
            "segment_id": "601a6bf0-3468-405b-84b2-cd92b428dafa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 22,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6 CHAPTER 2. AN INTRODUCTION TO R\r\nFair warning: if you want to try Emacs and if you grew up with Microsoftr Windows or\r\nMacintosh, then you are going to need to relearn everything you thought you knew about computers\r\nyour whole life. (Or, since Emacs is completely customizable, you can reconfigure Emacs to behave\r\nthe way you want.) I have personally experienced this transformation and I will never go back.\r\nJGR (read “Jaguar”): This one has the bells and whistles of RGui plus it is based on Java, so\r\nit works on multiple operating systems. It has its own script editor like R Editor but with\r\nadditional features such as syntax highlighting and code-completion. If you do not use\r\nMicrosoftr Windows (or even if you do) you definitely want to check out this one.\r\nKate, Bluefish, etc. There are literally dozens of other text editors available, many of them free,\r\nand each has its own (dis)advantages. I only have mentioned the ones with which I have\r\nhad substantial personal experience and have enjoyed at some point. Play around, and let me\r\nknow what you find.\r\nGraphical User Interfaces (GUIs) By the word “GUI” I mean an interface in which the user\r\ncommunicates with R by way of points-and-clicks in a menu of some sort. Again, there are many,\r\nmany options and I only mention ones that I have used and enjoyed. Some of the other more\r\npopular script editors can be downloaded from the R-Project website at http://www.sciviews.\r\norg/_rgui/. On the left side of the screen (under Projects) there are several choices available.\r\nR Commander provides a point-and-click interface to many basic statistical tasks. It is called the\r\n“Commander” because every time one makes a selection from the menus, the code corre\u0002sponding to the task is listed in the output window. One can take this code, copy-and-paste\r\nit to a text file, then re-run it again at a later time without the R Commander’s assistance.\r\nIt is well suited for the introductory level. Rcmdr also allows for user-contributed “Plugins”\r\nwhich are separate packages on CRAN that add extra functionality to the Rcmdr package. The\r\nplugins are typically named with the prefix RcmdrPlugin to make them easy to identify in\r\nthe CRAN package list. One such plugin is the\r\nRcmdrPlugin.IPSUR package which accompanies this text.\r\nPoor Man’s GUI is an alternative to the Rcmdr which is based on GTk instead of Tcl/Tk. It has\r\nbeen a while since I used it but I remember liking it very much when I did. One thing\r\nthat stood out was that the user could drag-and-drop data sets for plots. See here for more\r\ninformation: http://wiener.math.csi.cuny.edu/pmg/.\r\nRattle is a data mining toolkit which was designed to manage/analyze very large data sets, but\r\nit provides enough other general functionality to merit mention here. See [91] for more\r\ninformation.\r\nDeducer is relatively new and shows promise from what I have seen, but I have not actually used\r\nit in the classroom yet.\r\n2.3 Basic R Operations and Concepts\r\nThe R developers have written an introductory document entitled “An Introduction to R”. There is\r\na sample session included which shows what basic interaction with R looks like. I recommend that",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/601a6bf0-3468-405b-84b2-cd92b428dafa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6c4dc2ffd89782e4484a7f199a7357742f0087cae9207f8634392aa5b6d70af1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 533
      },
      {
        "segments": [
          {
            "segment_id": "efff1ea5-95c0-4cb8-bfde-4b0a91084712",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 23,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2.3. BASIC R OPERATIONS AND CONCEPTS 7\r\nall new users of R read that document, but bear in mind that there are concepts mentioned which\r\nwill be unfamiliar to the beginner.\r\nBelow are some of the most basic operations that can be done with R. Almost every book about\r\nR begins with a section like the one below; look around to see all sorts of things that can be done\r\nat this most basic level.\r\n2.3.1 Arithmetic\r\n> 2 + 3 # add\r\n[1] 5\r\n> 4 * 5 / 6 # multiply and divide\r\n[1] 3.333333\r\n> 7^8 # 7 to the 8th power\r\n[1] 5764801\r\nNotice the comment character #. Anything typed after a # symbol is ignored by R. We know\r\nthat 20/6 is a repeating decimal, but the above example shows only 7 digits. We can change the\r\nnumber of digits displayed with options:\r\n> options(digits = 16)\r\n> 10/3 # see more digits\r\n[1] 3.333333333333333\r\n> sqrt(2) # square root\r\n[1] 1.414213562373095\r\n> exp(1) # Euler's constant, e\r\n[1] 2.718281828459045\r\n> pi\r\n[1] 3.141592653589793\r\n> options(digits = 7) # back to default\r\nNote that it is possible to set digits up to 22, but setting them over 16 is not recommended\r\n(the extra significant digits are not necessarily reliable). Above notice the sqrt function for square\r\nroots and the exp function for powers of e, Euler’s number.\r\n2.3.2 Assignment, Object names, and Data types\r\nIt is often convenient to assign numbers and values to variables (objects) to be used later. The\r\nproper way to assign values to a variable is with the <- operator (with a space on either side). The\r\n= symbol works too, but it is recommended by the R masters to reserve = for specifying arguments\r\nto functions (discussed later). In this book we will follow their advice and use <- for assignment.\r\nOnce a variable is assigned, its value can be printed by simply entering the variable name by itself.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/efff1ea5-95c0-4cb8-bfde-4b0a91084712.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f7d733acf8d64f9d975d38bb48252842410dd02c056a0fdff9519e4cf416b4be",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 329
      },
      {
        "segments": [
          {
            "segment_id": "e16219ce-ce16-4c13-bfdd-4b31f6fa4acc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 24,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "8 CHAPTER 2. AN INTRODUCTION TO R\r\n> x <- 7*41/pi # don't see the calculated value\r\n> x # take a look\r\n[1] 91.35494\r\nWhen choosing a variable name you can use letters, numbers, dots “.”, or underscore “_”\r\ncharacters. You cannot use mathematical operators, and a leading dot may not be followed by\r\na number. Examples of valid names are: x, x1, y.value, and y_hat. (More precisely, the set\r\nof allowable characters in object names depends on one’s particular system and locale; see An\r\nIntroduction to R for more discussion on this.)\r\nObjects can be of many types, modes, and classes. At this level, it is not necessary to investigate\r\nall of the intricacies of the respective types, but there are some with which you need to become\r\nfamiliar:\r\ninteger: the values 0, ±1, ±2, . . . ; these are represented exactly by R.\r\ndouble: real numbers (rational and irrational); these numbers are not represented exactly (save\r\nintegers or fractions with a denominator that is a power of 2, see [85]).\r\ncharacter: elements that are wrapped with pairs of \" or ';\r\nlogical: includes TRUE, FALSE, and NA (which are reserved words); the NA stands for “not avail\u0002able”, i.e., a missing value.\r\nYou can determine an object’s type with the typeof function. In addition to the above, there is the\r\ncomplex data type:\r\n> sqrt(-1) # isn't defined\r\n[1] NaN\r\n> sqrt(-1+0i) # is defined\r\n[1] 0+1i\r\n> sqrt(as.complex(-1)) # same thing\r\n[1] 0+1i\r\n> (0 + 1i)^2 # should be -1\r\n[1] -1+0i\r\n> typeof((0 + 1i)^2)\r\n[1] \"complex\"\r\nNote that you can just type (1i)^2 to get the same answer. The NaN stands for “not a number”;\r\nit is represented internally as double.\r\n2.3.3 Vectors\r\nAll of this time we have been manipulating vectors of length 1. Now let us move to vectors with\r\nmultiple entries.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e16219ce-ce16-4c13-bfdd-4b31f6fa4acc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f75450bd76f519a8ac310b5fecd31829c08c744d2b9d50598efbb4fcf20c2326",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 311
      },
      {
        "segments": [
          {
            "segment_id": "3cece006-cdfd-46f0-a663-384b1f203635",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 25,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2.3. BASIC R OPERATIONS AND CONCEPTS 9\r\nEntering data vectors\r\n1. c: If you would like to enter the data 74,31,95,61,76,34,23,54,96 into R, you may\r\ncreate a data vector with the c function (which is short for concatenate).\r\n> x <- c(74, 31, 95, 61, 76, 34, 23, 54, 96)\r\n> x\r\n[1] 74 31 95 61 76 34 23 54 96\r\nThe elements of a vector are usually coerced by R to the the most general type of any of the\r\nelements, so if you do c(1, \"2\") then the result will be c(\"1\", \"2\").\r\n2. scan: This method is useful when the data are stored somewhere else. For instance, you\r\nmay type x <- scan() at the command prompt and R will display 1: to indicate that it is\r\nwaiting for the first data value. Type a value and press Enter, at which point R will display\r\n2:, and so forth. Note that entering an empty line stops the scan. This method is especially\r\nhandy when you have a column of values, say, stored in a text file or spreadsheet. You may\r\ncopy and paste them all at the 1: prompt, and R will store all of the values instantly in the\r\nvector x.\r\n3. repeated data; regular patterns: the seq function will generate all sorts of sequences of num\u0002bers. It has the arguments from, to, by, and length.out which can be set in concert with\r\none another. We will do a couple of examples to show you how it works.\r\n> seq(from = 1, to = 5)\r\n[1] 1 2 3 4 5\r\n> seq(from = 2, by = -0.1, length.out = 4)\r\n[1] 2.0 1.9 1.8 1.7\r\nNote that we can get the first line much quicker with the colon operator :\r\n> 1:5\r\n[1] 1 2 3 4 5\r\nThe vector LETTERS has the 26 letters of the English alphabet in uppercase and letters has\r\nall of them in lowercase.\r\nIndexing data vectors Sometimes we do not want the whole vector, but just a piece of it. We\r\ncan access the intermediate parts with the [] operator. Observe (with x defined above)\r\n> x[1]\r\n[1] 74\r\n> x[2:4]",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3cece006-cdfd-46f0-a663-384b1f203635.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d47b60d29fe3d42c70e06cac21f355ab57e8b82f529e3dd8cfea8fd7401072f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 363
      },
      {
        "segments": [
          {
            "segment_id": "00b058bd-97e0-4ee6-9bdb-7c020d6cb819",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 26,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10 CHAPTER 2. AN INTRODUCTION TO R\r\n[1] 31 95 61\r\n> x[c(1, 3, 4, 8)]\r\n[1] 74 95 61 54\r\n> x[-c(1, 3, 4, 8)]\r\n[1] 31 76 34 23 96\r\nNotice that we used the minus sign to specify those elements that we do not want.\r\n> LETTERS[1:5]\r\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\r\n> letters[-(6:24)]\r\n[1] \"a\" \"b\" \"c\" \"d\" \"e\" \"y\" \"z\"\r\n2.3.4 Functions and Expressions\r\nA function takes arguments as input and returns an object as output. There are functions to do all\r\nsorts of things. We show some examples below.\r\n> x <- 1:5\r\n> sum(x)\r\n[1] 15\r\n> length(x)\r\n[1] 5\r\n> min(x)\r\n[1] 1\r\n> mean(x) # sample mean\r\n[1] 3\r\n> sd(x) # sample standard deviation\r\n[1] 1.581139\r\nIt will not be long before the user starts to wonder how a particular function is doing its job, and\r\nsince R is open-source, anybody is free to look under the hood of a function to see how things are\r\ncalculated. For detailed instructions see the article “Accessing the Sources” by Uwe Ligges [60].\r\nIn short:\r\n1. Type the name of the function without any parentheses or arguments. If you are lucky then\r\nthe code for the entire function will be printed, right there looking at you. For instance,\r\nsuppose that we would like to see how the intersect function works:\r\n> intersect",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/00b058bd-97e0-4ee6-9bdb-7c020d6cb819.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cd4525a18539b28cc5afb142e1913e8fc8fa26931a2d467e1ada44b97b321ce0",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "b48dc09a-d8e8-4f3f-b530-b81532e84c37",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 27,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2.3. BASIC R OPERATIONS AND CONCEPTS 11\r\nfunction (x, y)\r\n{\r\ny <- as.vector(y)\r\nunique(y[match(as.vector(x), y, 0L)])\r\n}\r\n<environment: namespace:base>\r\n2. If instead it shows UseMethod(\"something\") then you will need to choose the class of the\r\nobject to be inputted and next look at the method that will be dispatched to the object. For\r\ninstance, typing rev says\r\n> rev\r\nfunction (x)\r\nUseMethod(\"rev\")\r\n<environment: namespace:base>\r\nThe output is telling us that there are multiple methods associated with the rev function. To\r\nsee what these are, type\r\n> methods(rev)\r\n[1] rev.default rev.dendrogram*\r\nNon-visible functions are asterisked\r\nNow we learn that there are two different rev(x) functions, only one of which being chosen\r\nat each call depending on what x is. There is one for dendrogram objects and a default\r\nmethod for everything else. Simply type the name to see what each method does. For\r\nexample, the default method can be viewed with\r\n> rev.default\r\nfunction (x)\r\nif (length(x)) x[length(x):1L] else x\r\n<environment: namespace:base>\r\n3. Some functions are hidden by a namespace (see An Introduction to R [85]), and are not\r\nvisible on the first try. For example, if we try to look at the code for wilcox.test (see\r\nChapter 15) we get the following:\r\n> wilcox.test\r\nfunction (x, ...)\r\nUseMethod(\"wilcox.test\")\r\n<environment: namespace:stats>",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b48dc09a-d8e8-4f3f-b530-b81532e84c37.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f73f61a027d99f3cbd83d793322e53105c59c4dea8601eb0fb287dec5d536f41",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 441
      },
      {
        "segments": [
          {
            "segment_id": "3d7276d4-c172-4b88-a497-e722af47ae7d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 28,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12 CHAPTER 2. AN INTRODUCTION TO R\r\n> methods(wilcox.test)\r\n[1] wilcox.test.default* wilcox.test.formula*\r\nNon-visible functions are asterisked\r\nIf we were to try wilcox.test.default we would get a “not found” error, because it is\r\nhidden behind the namespace for the package stats (shown in the last line when we tried\r\nwilcox.test). In cases like these we prefix the package name to the front of the func\u0002tion name with three colons; the command stats:::wilcox.test.default will show the\r\nsource code, omitted here for brevity.\r\n4. If it shows .Internal(something) or .Primitive(\"something\"), then it will be necessary\r\nto download the source code of R (which is not a binary version with an .exe extension)\r\nand search inside the code there. See Ligges [60] for more discussion on this. An example\r\nis exp:\r\n> exp\r\nfunction (x) .Primitive(\"exp\")\r\nBe warned that most of the .Internal functions are written in other computer languages\r\nwhich the beginner may not understand, at least initially.\r\n2.4 Getting Help\r\nWhen you are using R, it will not take long before you find yourself needing help. Fortunately,\r\nR has extensive help resources and you should immediately become familiar with them. Begin by\r\nclicking Help on Rgui. The following options are available.\r\n• Console: gives useful shortcuts, for instance, Ctrl+L, to clear the R console screen.\r\n• FAQ on R: frequently asked questions concerning general R operation.\r\n• FAQ on R for Windows: frequently asked questions about R, tailored to the Microsoft\r\nWindows operating system.\r\n• Manuals: technical manuals about all features of the R system including installation, the\r\ncomplete language definition, and add-on packages.\r\n• R functions (text). . . : use this if you know the exact name of the function you want to know\r\nmore about, for example, mean or plot. Typing mean in the window is equivalent to typing\r\nhelp(\"mean\") at the command line, or more simply, ?mean. Note that this method only\r\nworks if the function of interest is contained in a package that is already loaded into the\r\nsearch path with library.\r\n• HTML Help: use this to browse the manuals with point-and-click links. It also has a Search\r\nEngine & Keywords for searching the help page titles, with point-and-click links for the\r\nsearch results. This is possibly the best help method for beginners. It can be started from the\r\ncommand line with the command help.start().",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3d7276d4-c172-4b88-a497-e722af47ae7d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=038ec02ff84ddee7eea82bf74586abe0c87b8fb4ec95ab3b950890d6b2f1ea2c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 389
      },
      {
        "segments": [
          {
            "segment_id": "a707e360-5ffd-4b2c-a7d9-94ee648be9b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 29,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2.4. GETTING HELP 13\r\n• Search help. . . : use this if you do not know the exact name of the function of interest, or if\r\nthe function is in a package that has not been loaded yet. For example, you may enter plo\r\nand a text window will return listing all the help files with an alias, concept, or title matching\r\n‘plo’ using regular expression matching; it is equivalent to typing help.search(\"plo\")\r\nat the command line. The advantage is that you do not need to know the exact name of\r\nthe function; the disadvantage is that you cannot point-and-click the results. Therefore, one\r\nmay wish to use the HTML Help search engine instead. An equivalent way is ??plo at the\r\ncommand line.\r\n• search.r-project.org. . . : this will search for words in help lists and email archives of the R\r\nProject. It can be very useful for finding other questions that other users have asked.\r\n• Apropos. . . : use this for more sophisticated partial name matching of functions. See ?apropos\r\nfor details.\r\nOn the help pages for a function there are sometimes “Examples” listed at the bottom of the page,\r\nwhich will work if copy-pasted at the command line (unless marked otherwise). The example\r\nfunction will run the code automatically, skipping the intermediate step. For instance, we may try\r\nexample(mean) to see a few examples of how the mean function works.\r\n2.4.1 R Help Mailing Lists\r\nThere are several mailing lists associated with R, and there is a huge community of people that\r\nread and answer questions related to R. See here http://www.r-project.org/mail.html for\r\nan idea of what is available. Particularly pay attention to the bottom of the page which lists several\r\nspecial interest groups (SIGs) related to R.\r\nBear in mind that R is free software, which means that it was written by volunteers, and the\r\npeople that frequent the mailing lists are also volunteers who are not paid by customer support fees.\r\nConsequently, if you want to use the mailing lists for free advice then you must adhere to some\r\nbasic etiquette, or else you may not get a reply, or even worse, you may receive a reply which is a\r\nbit less cordial than you are used to. Below are a few considerations:\r\n1. Read the FAQ (http://cran.r-project.org/faqs.html). Note that there are different\r\nFAQs for different operating systems. You should read these now, even without a question at\r\nthe moment, to learn a lot about the idiosyncrasies of R.\r\n2. Search the archives. Even if your question is not a FAQ, there is a very high likelihood that\r\nyour question has been asked before on the mailing list. If you want to know about topic foo,\r\nthen you can do RSiteSearch(\"foo\") to search the mailing list archives (and the online\r\nhelp) for it.\r\n3. Do a Google search and an RSeek.org search.\r\nIf your question is not a FAQ, has not been asked on R-help before, and does not yield to a Google\r\n(or alternative) search, then, and only then, should you even consider writing to R-help. Below are\r\na few additional considerations.\r\n1. Read the posting guide (http://www.r-project.org/posting-guide.html) before\r\nposting. This will save you a lot of trouble and pain.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a707e360-5ffd-4b2c-a7d9-94ee648be9b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8201f8d70e124b62e5a20f229f876af6cd80635e03d726276a03c8f9c512f3c0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "a707e360-5ffd-4b2c-a7d9-94ee648be9b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 29,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2.4. GETTING HELP 13\r\n• Search help. . . : use this if you do not know the exact name of the function of interest, or if\r\nthe function is in a package that has not been loaded yet. For example, you may enter plo\r\nand a text window will return listing all the help files with an alias, concept, or title matching\r\n‘plo’ using regular expression matching; it is equivalent to typing help.search(\"plo\")\r\nat the command line. The advantage is that you do not need to know the exact name of\r\nthe function; the disadvantage is that you cannot point-and-click the results. Therefore, one\r\nmay wish to use the HTML Help search engine instead. An equivalent way is ??plo at the\r\ncommand line.\r\n• search.r-project.org. . . : this will search for words in help lists and email archives of the R\r\nProject. It can be very useful for finding other questions that other users have asked.\r\n• Apropos. . . : use this for more sophisticated partial name matching of functions. See ?apropos\r\nfor details.\r\nOn the help pages for a function there are sometimes “Examples” listed at the bottom of the page,\r\nwhich will work if copy-pasted at the command line (unless marked otherwise). The example\r\nfunction will run the code automatically, skipping the intermediate step. For instance, we may try\r\nexample(mean) to see a few examples of how the mean function works.\r\n2.4.1 R Help Mailing Lists\r\nThere are several mailing lists associated with R, and there is a huge community of people that\r\nread and answer questions related to R. See here http://www.r-project.org/mail.html for\r\nan idea of what is available. Particularly pay attention to the bottom of the page which lists several\r\nspecial interest groups (SIGs) related to R.\r\nBear in mind that R is free software, which means that it was written by volunteers, and the\r\npeople that frequent the mailing lists are also volunteers who are not paid by customer support fees.\r\nConsequently, if you want to use the mailing lists for free advice then you must adhere to some\r\nbasic etiquette, or else you may not get a reply, or even worse, you may receive a reply which is a\r\nbit less cordial than you are used to. Below are a few considerations:\r\n1. Read the FAQ (http://cran.r-project.org/faqs.html). Note that there are different\r\nFAQs for different operating systems. You should read these now, even without a question at\r\nthe moment, to learn a lot about the idiosyncrasies of R.\r\n2. Search the archives. Even if your question is not a FAQ, there is a very high likelihood that\r\nyour question has been asked before on the mailing list. If you want to know about topic foo,\r\nthen you can do RSiteSearch(\"foo\") to search the mailing list archives (and the online\r\nhelp) for it.\r\n3. Do a Google search and an RSeek.org search.\r\nIf your question is not a FAQ, has not been asked on R-help before, and does not yield to a Google\r\n(or alternative) search, then, and only then, should you even consider writing to R-help. Below are\r\na few additional considerations.\r\n1. Read the posting guide (http://www.r-project.org/posting-guide.html) before\r\nposting. This will save you a lot of trouble and pain.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a707e360-5ffd-4b2c-a7d9-94ee648be9b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8201f8d70e124b62e5a20f229f876af6cd80635e03d726276a03c8f9c512f3c0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "d422f984-7422-47ce-9f36-b665f0b400c7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 30,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "14 CHAPTER 2. AN INTRODUCTION TO R\r\n2. Get rid of the command prompts (>) from output. Readers of your message will take the text\r\nfrom your mail and copy-paste into an R session. If you make the readers’ job easier then it\r\nwill increase the likelihood of a response.\r\n3. Questions are often related to a specific data set, and the best way to communicate the data is\r\nwith a dump command. For instance, if your question involves data stored in a vector x, you\r\ncan type dump(\"x\",\"\") at the command prompt and copy-paste the output into the body of\r\nyour email message. Then the reader may easily copy-paste the message from your email\r\ninto R and x will be available to him/her.\r\n4. Sometimes the answer the question is related to the operating system used, the attached\r\npackages, or the exact version of R being used. The sessionInfo() command collects\r\nall of this information to be copy-pasted into an email (and the Posting Guide requests this\r\ninformation). See Appendix A for an example.\r\n2.5 External Resources\r\nThere is a mountain of information on the Internet about R. Below are a few of the important ones.\r\nThe R Project for Statistical Computing: (http://www.r-project.org/) Go here first.\r\nThe Comprehensive R Archive Network: (http://cran.r-project.org/) This is where R\r\nis stored along with thousands of contributed packages. There are also loads of contributed\r\ninformation (books, tutorials, etc.). There are mirrors all over the world with duplicate infor\u0002mation.\r\nR-Forge: (http://r-forge.r-project.org/) This is another location where R packages are\r\nstored. Here you can find development code which has not yet been released to CRAN.\r\nR Wiki: (http://wiki.r-project.org/rwiki/doku.php) There are many tips and tricks listed\r\nhere. If you find a trick of your own, login and share it with the world.\r\nOther: the R Graph Gallery (http://addictedtor.free.fr/graphiques/) and R Graphical\r\nManual (http://bm2.genes.nig.ac.jp/RGM2/index.php) have literally thousands of\r\ngraphs to peruse. RSeek (http://www.rseek.org) is a search engine based on Google\r\nspecifically tailored for R queries.\r\n2.6 Other Tips\r\nIt is unnecessary to retype commands repeatedly, since R remembers what you have recently en\u0002tered on the command line. On the Microsoftr Windows RGui, to cycle through the previous\r\ncommands just push the ↑ (up arrow) key. On Emacs/ESS the command is M-p (which means hold\r\ndown the Alt button and press “p”). More generally, the command history() will show a whole\r\nlist of recently entered commands.\r\n• To find out what all variables are in the current work environment, use the commands\r\nobjects() or ls(). These list all available objects in the workspace. If you wish to remove\r\none or more variables, use remove(var1, var2, var3), or more simply use rm(var1,\r\nvar2, var3), and to remove all objects use rm(list = ls()).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d422f984-7422-47ce-9f36-b665f0b400c7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7958d2c927ac99b9a9dd05d522f38a2100220197754c556d5381398d466cbe4e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 454
      },
      {
        "segments": [
          {
            "segment_id": "29fb08b7-4b79-4fb3-ac1f-e44d16a07e2f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 31,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "2.6. OTHER TIPS 15\r\n• Another use of scan is when you have a long list of numbers (separated by spaces or on\r\ndifferent lines) already typed somewhere else, say in a text file. To enter all the data in one\r\nfell swoop, first highlight and copy the list of numbers to the Clipboard with Edit . Copy\r\n(or by right-clicking and selecting Copy). Next type the x <- scan() command in the R\r\nconsole, and paste the numbers at the 1: prompt with Edit . Paste. All of the numbers will\r\nautomatically be entered into the vector x.\r\n• The command Ctrl+l clears the screen in the Microsoftr Windows RGui. The comparable\r\ncommand for Emacs/ESS is\r\n• Once you use R for awhile there may be some commands that you wish to run automatically\r\nwhenever R starts. These commands may be saved in a file called Rprofile.site which\r\nis usually in the etc folder, which lives in the R home directory (which on Microsoftr\r\nWindows usually is C:\\Program Files\\R). Alternatively, you can make a file .Rprofile\r\nto be stored in the user’s home directory, or anywhere R is invoked. This allows for multiple\r\nconfigurations for different projects or users. See “Customizing the Environment” of An\r\nIntroduction to R for more details.\r\n• When exiting R the user is given the option to “save the workspace”. I recommend that\r\nbeginners DO NOT save the workspace when quitting. If Yes is selected, then all of the\r\nobjects and data currently in R’s memory is saved in a file located in the working directory\r\ncalled .RData. This file is then automatically loaded the next time R starts (in which case\r\nR will say [previously saved workspace restored]). This is a valuable feature for\r\nexperienced users of R, but I find that it causes more trouble than it saves with beginners.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/29fb08b7-4b79-4fb3-ac1f-e44d16a07e2f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1571a4d87dafcc86add4ae440d39f00a4c530c8db13ac59a9ed482e9c24e1b27",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "1eded67b-f2d3-45f7-a7cd-ab7786daed33",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 32,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "16 CHAPTER 2. AN INTRODUCTION TO R\r\nChapter Exercises",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1eded67b-f2d3-45f7-a7cd-ab7786daed33.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f96763f6d83718ae43587db6fb14c2df2e3077e6bfbecdf4b87170f0302e3acb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 317
      },
      {
        "segments": [
          {
            "segment_id": "b5257789-b79c-405e-99d5-cb44fe88ec54",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 33,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 3\r\nData Description\r\nIn this chapter we introduce the different types of data that a statistician is likely to encounter, and in\r\neach subsection we give some examples of how to display the data of that particular type. Once we\r\nsee how to display data distributions, we next introduce the basic properties of data distributions.\r\nWe qualitatively explore several data sets. Once that we have intuitive properties of data sets,\r\nwe next discuss how we may numerically measure and describe those properties with descriptive\r\nstatistics.\r\nWhat do I want them to know?\r\n• different data types, such as quantitative versus qualitative, nominal versus ordinal, and dis\u0002crete versus continuous\r\n• basic graphical displays for assorted data types, and some of their (dis)advantages\r\n• fundamental properties of data distributions, including center, spread, shape, and crazy ob\u0002servations\r\n• methods to describe data (visually/numerically) with respect to the properties, and how the\r\nmethods differ depending on the data type\r\n• all of the above in the context of grouped data, and in particular, the concept of a factor\r\n3.1 Types of Data\r\nLoosely speaking, a datum is any piece of collected information, and a data set is a collection of\r\ndata related to each other in some way. We will categorize data into five types and describe each in\r\nturn:\r\nQuantitative data associated with a measurement of some quantity on an observational unit,\r\nQualitative data associated with some quality or property of the observational unit,\r\nLogical data to represent true or false and which play an important role later,\r\n17",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b5257789-b79c-405e-99d5-cb44fe88ec54.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9f49b43a2f491b9af4b9ae411158fb94c1c94f375cd8f647a18ff4716b4b4912",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 257
      },
      {
        "segments": [
          {
            "segment_id": "434afce9-155e-4fe0-bfc2-1839c85c42a6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 34,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "18 CHAPTER 3. DATA DESCRIPTION\r\nMissing data that should be there but are not, and\r\nOther types everything else under the sun.\r\nIn each subsection we look at some examples of the type in question and introduce methods to\r\ndisplay them.\r\n3.1.1 Quantitative data\r\nQuantitative data are any data that measure or are associated with a measurement of the quantity of\r\nsomething. They invariably assume numerical values. Quantitative data can be further subdivided\r\ninto two categories.\r\n• Discrete data take values in a finite or countably infinite set of numbers, that is, all possible\r\nvalues could (at least in principle) be written down in an ordered list. Examples include:\r\ncounts, number of arrivals, or number of successes. They are often represented by integers,\r\nsay, 0, 1, 2, etc..\r\n• Continuous data take values in an interval of numbers. These are also known as scale data,\r\ninterval data, or measurement data. Examples include: height, weight, length, time, etc.\r\nContinuous data are often characterized by fractions or decimals: 3.82, 7.0001, 4 5\r\n8\r\n, etc..\r\nNote that the distinction between discrete and continuous data is not always clear-cut. Sometimes\r\nit is convenient to treat data as if they were continuous, even though strictly speaking they are not\r\ncontinuous. See the examples.\r\nExample 3.1. Annual Precipitation in US Cities. The vector precip contains average amount\r\nof rainfall (in inches) for each of 70 cities in the United States and Puerto Rico. Let us take a look\r\nat the data:\r\n> str(precip)\r\nNamed num [1:70] 67 54.7 7 48.5 14 17.2 20.7 13 43.4 40.2 ...\r\n- attr(*, \"names\")= chr [1:70] \"Mobile\" \"Juneau\" \"Phoenix\" \"Little Rock\" ...\r\n> precip[1:4]\r\nMobile Juneau Phoenix Little Rock\r\n67.0 54.7 7.0 48.5\r\nThe output shows that precip is a numeric vector which has been named, that is, each value\r\nhas a name associated with it (which can be set with the names function). These are quantitative\r\ncontinuous data.\r\nExample 3.2. Lengths of Major North American Rivers. The U.S. Geological Survey recorded\r\nthe lengths (in miles) of several rivers in North America. They are stored in the vector rivers in\r\nthe datasets package (which ships with base R). See ?rivers. Let us take a look at the data with\r\nthe str function.\r\n> str(rivers)\r\nnum [1:141] 735 320 325 392 524 ...",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/434afce9-155e-4fe0-bfc2-1839c85c42a6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d02b2339c23e99a59db96e82a4331d15456743d68f2b258e986b9b04d1823f3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 383
      },
      {
        "segments": [
          {
            "segment_id": "9130e498-8348-4d48-a83e-af2d16925019",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 35,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 19\r\nThe output says that rivers is a numeric vector of length 141, and the first few values are 735,\r\n320, 325, etc. These data are definitely quantitative and it appears that the measurements have been\r\nrounded to the nearest mile. Thus, strictly speaking, these are discrete data. But we will find it\r\nconvenient later to take data like these to be continuous for some of our statistical procedures.\r\nExample 3.3. Yearly Numbers of Important Discoveries. The vector discoveries contains\r\nnumbers of “great” inventions/discoveries in each year from 1860 to 1959, as reported by the 1975\r\nWorld Almanac. Let us take a look at the data:\r\n> str(discoveries)\r\nTime-Series [1:100] from 1860 to 1959: 5 3 0 2 0 3 2 3 6 1 ...\r\n> discoveries[1:4]\r\n[1] 5 3 0 2\r\nThe output is telling us that discoveries is a time series (see Section 3.1.5 for more) of length\r\n100. The entries are integers, and since they represent counts this is a good example of discrete\r\nquantitative data. We will take a closer look in the following sections.\r\nDisplaying Quantitative Data\r\nOne of the first things to do when confronted by quantitative data (or any data, for that matter) is\r\nto make some sort of visual display to gain some insight into the data’s structure. There are almost\r\nas many display types from which to choose as there are data sets to plot. We describe some of the\r\nmore popular alternatives.\r\nStrip charts (also known as Dot plots) These can be used for discrete or continuous data, and\r\nusually look best when the data set is not too large. Along the horizontal axis is a numerical scale\r\nabove which the data values are plotted. We can do it in R with a call to the stripchart function.\r\nThere are three available methods.\r\noverplot plots ties covering each other. This method is good to display only the distinct values\r\nassumed by the data set.\r\njitter adds some noise to the data in the y direction in which case the data values are not covered\r\nup by ties.\r\nstack plots repeated values stacked on top of one another. This method is best used for discrete\r\ndata with a lot of ties; if there are no repeats then this method is identical to overplot.\r\nSee Figure 3.1.1, which is produced by the following code.\r\n> stripchart(precip, xlab = \"rainfall\")\r\n> stripchart(rivers, method = \"jitter\", xlab = \"length\")\r\n> stripchart(discoveries, method = \"stack\", xlab = \"number\")",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9130e498-8348-4d48-a83e-af2d16925019.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1665cd637bc70e49e91c98f1acfa0896e51cffef1ea754e0122c06d989875794",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 417
      },
      {
        "segments": [
          {
            "segment_id": "0785f91c-3a13-41c1-b02e-b558ea741a48",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 36,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "20 CHAPTER 3. DATA DESCRIPTION\r\n10 30 50\r\nrainfall\r\n0 1000 2500\r\nlength\r\n0 2 4 6 8 12\r\nnumber\r\nFigure 3.1.1: Strip charts of the precip, rivers, and discoveries data\r\nThe first graph uses the overplot method, the second the jitter method, and the third the stack method.\r\nThe leftmost graph is a strip chart of the precip data. The graph shows tightly clustered values\r\nin the middle with some others falling balanced on either side, with perhaps slightly more falling\r\nto the left. Later we will call this a symmetric distribution, see Section 3.2.3. The middle graph is\r\nof the rivers data, a vector of length 141. There are several repeated values in the rivers data, and\r\nif we were to use the overplot method we would lose some of them in the display. This plot shows\r\na what we will later call a right-skewed shape with perhaps some extreme values on the far right of\r\nthe display. The third graph strip charts discoveries data which are literally a textbook example\r\nof a right skewed distribution.\r\nThe DOTplot function in the UsingR package [86] is another alternative.\r\nHistogram These are typically used for continuous data. A histogram is constructed by first\r\ndeciding on a set of classes, or bins, which partition the real line into a set of boxes into which the\r\ndata values fall. Then vertical bars are drawn over the bins with height proportional to the number",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0785f91c-3a13-41c1-b02e-b558ea741a48.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=57e4447468eb808d7fd33ab37b5befd6c596a0942db357e0c65de5ea27919e76",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 241
      },
      {
        "segments": [
          {
            "segment_id": "40cca3c4-6b87-4e51-bad8-fe00a7eea0c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 37,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 21\r\nof observations that fell into the bin.\r\nThese are one of the most common summary displays, and they are often misidentified as “Bar\r\nGraphs” (see below.) The scale on the y axis can be frequency, percentage, or density (relative\r\nfrequency). The term histogram was coined by Karl Pearson in 1891, see [66].\r\nExample 3.4. Annual Precipitation in US Cities. We are going to take another look at the precip\r\ndata that we investigated earlier. The strip chart in Figure 3.1.1 suggested a loosely balanced\r\ndistribution; let us now look to see what a histogram says.\r\nThere are many ways to plot histograms in R, and one of the easiest is with the hist function.\r\nThe following code produces the plots in Figure 3.1.2.\r\n> hist(precip, main = \"\")\r\n> hist(precip, freq = FALSE, main = \"\")\r\nNotice the argument main = \"\", which suppresses the main title from being displayed – it\r\nwould have said “Histogram of precip” otherwise. The plot on the left is a frequency histogram\r\n(the default), and the plot on the right is a relative frequency histogram (freq = FALSE).\r\nPlease be careful regarding the biggest weakness of histograms: the graph obtained strongly\r\ndepends on the bins chosen. Choose another set of bins, and you will get a different histogram.\r\nMoreover, there are not any definitive criteria by which bins should be defined; the best choice for\r\na given data set is the one which illuminates the data set’s underlying structure (if any). Luckily for\r\nus there are algorithms to automatically choose bins that are likely to display well, and more often\r\nthan not the default bins do a good job. This is not always the case, however, and a responsible\r\nstatistician will investigate many bin choices to test the stability of the display.\r\nExample 3.5. Recall that the strip chart in Figure 3.1.1 suggested a relatively balanced shape to the\r\nprecip data distribution. Watch what happens when we change the bins slightly (with the breaks\r\nargument to hist). See Figure 3.1.3 which was produced by the following code.\r\n> hist(precip, breaks = 10, main = \"\")\r\n> hist(precip, breaks = 200, main = \"\")\r\nThe leftmost graph (with breaks = 10) shows that the distribution is not balanced at all. There\r\nare two humps: a big one in the middle and a smaller one to the left. Graphs like this often indicate\r\nsome underlying group structure to the data; we could now investigate whether the cities for which\r\nrainfall was measured were similar in some way, with respect to geographic region, for example.\r\nThe rightmost graph in Figure 3.1.3 shows what happens when the number of bins is too large:\r\nthe histogram is too grainy and hides the rounded appearance of the earlier histograms. If we were\r\nto continue increasing the number of bins we would eventually get all observed bins to have exactly\r\none element, which is nothing more than a glorified strip chart.\r\nStemplots (more to be said in Section 3.4) Stemplots have two basic parts: stems and leaves.\r\nThe final digit of the data values is taken to be a leaf, and the leading digit(s) is (are) taken to be\r\nstems. We draw a vertical line, and to the left of the line we list the stems. To the right of the line,\r\nwe list the leaves beside their corresponding stem. There will typically be several leaves for each\r\nstem, in which case the leaves accumulate to the right. It is sometimes necessary to round the data\r\nvalues, especially for larger data sets.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/40cca3c4-6b87-4e51-bad8-fe00a7eea0c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=75d1e4b3b0921d1115c4b6fd9ba5f84c731cf004e511a75e3feddcf33cf8015c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 596
      },
      {
        "segments": [
          {
            "segment_id": "40cca3c4-6b87-4e51-bad8-fe00a7eea0c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 37,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 21\r\nof observations that fell into the bin.\r\nThese are one of the most common summary displays, and they are often misidentified as “Bar\r\nGraphs” (see below.) The scale on the y axis can be frequency, percentage, or density (relative\r\nfrequency). The term histogram was coined by Karl Pearson in 1891, see [66].\r\nExample 3.4. Annual Precipitation in US Cities. We are going to take another look at the precip\r\ndata that we investigated earlier. The strip chart in Figure 3.1.1 suggested a loosely balanced\r\ndistribution; let us now look to see what a histogram says.\r\nThere are many ways to plot histograms in R, and one of the easiest is with the hist function.\r\nThe following code produces the plots in Figure 3.1.2.\r\n> hist(precip, main = \"\")\r\n> hist(precip, freq = FALSE, main = \"\")\r\nNotice the argument main = \"\", which suppresses the main title from being displayed – it\r\nwould have said “Histogram of precip” otherwise. The plot on the left is a frequency histogram\r\n(the default), and the plot on the right is a relative frequency histogram (freq = FALSE).\r\nPlease be careful regarding the biggest weakness of histograms: the graph obtained strongly\r\ndepends on the bins chosen. Choose another set of bins, and you will get a different histogram.\r\nMoreover, there are not any definitive criteria by which bins should be defined; the best choice for\r\na given data set is the one which illuminates the data set’s underlying structure (if any). Luckily for\r\nus there are algorithms to automatically choose bins that are likely to display well, and more often\r\nthan not the default bins do a good job. This is not always the case, however, and a responsible\r\nstatistician will investigate many bin choices to test the stability of the display.\r\nExample 3.5. Recall that the strip chart in Figure 3.1.1 suggested a relatively balanced shape to the\r\nprecip data distribution. Watch what happens when we change the bins slightly (with the breaks\r\nargument to hist). See Figure 3.1.3 which was produced by the following code.\r\n> hist(precip, breaks = 10, main = \"\")\r\n> hist(precip, breaks = 200, main = \"\")\r\nThe leftmost graph (with breaks = 10) shows that the distribution is not balanced at all. There\r\nare two humps: a big one in the middle and a smaller one to the left. Graphs like this often indicate\r\nsome underlying group structure to the data; we could now investigate whether the cities for which\r\nrainfall was measured were similar in some way, with respect to geographic region, for example.\r\nThe rightmost graph in Figure 3.1.3 shows what happens when the number of bins is too large:\r\nthe histogram is too grainy and hides the rounded appearance of the earlier histograms. If we were\r\nto continue increasing the number of bins we would eventually get all observed bins to have exactly\r\none element, which is nothing more than a glorified strip chart.\r\nStemplots (more to be said in Section 3.4) Stemplots have two basic parts: stems and leaves.\r\nThe final digit of the data values is taken to be a leaf, and the leading digit(s) is (are) taken to be\r\nstems. We draw a vertical line, and to the left of the line we list the stems. To the right of the line,\r\nwe list the leaves beside their corresponding stem. There will typically be several leaves for each\r\nstem, in which case the leaves accumulate to the right. It is sometimes necessary to round the data\r\nvalues, especially for larger data sets.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/40cca3c4-6b87-4e51-bad8-fe00a7eea0c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=75d1e4b3b0921d1115c4b6fd9ba5f84c731cf004e511a75e3feddcf33cf8015c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 596
      },
      {
        "segments": [
          {
            "segment_id": "e11dfef1-5740-4aed-b6a9-1976de11a6cd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 38,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "22 CHAPTER 3. DATA DESCRIPTION\r\nprecip\r\nFrequency\r\n0 20 40 60\r\n0\r\n5 10 15 20 25\r\nprecip\r\nDensity\r\n0 20 40 60\r\n0.000 0.010 0.020 0.030\r\nFigure 3.1.2: (Relative) frequency histograms of the precip data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e11dfef1-5740-4aed-b6a9-1976de11a6cd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ee4e9c86e73452d5bc4d8618f09ac23101bf23be315d1a1f9462497dec318ca4",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "4da4ce64-d80b-4f97-8273-05f7768a2355",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 39,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 23\r\nprecip\r\nFrequency\r\n10 30 50 70\r\n0\r\n2\r\n4\r\n6\r\n8 10 12 14\r\nprecip\r\nFrequency\r\n10 30 50\r\n0 1 2\r\n3\r\n4\r\nFigure 3.1.3: More histograms of the precip data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4da4ce64-d80b-4f97-8273-05f7768a2355.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=330bbabecb17fd2ff6c4895f66a56e073e0680a7ee739f93c223f12015c7e154",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "ad202bfa-7180-45ea-bc67-c6b4958ec4c6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 40,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "24 CHAPTER 3. DATA DESCRIPTION\r\nExample 3.6. UKDriverDeaths is a time series that contains the total car drivers killed or se\u0002riously injured in Great Britain monthly from Jan 1969 to Dec 1984. See ?UKDriverDeaths.\r\nCompulsory seat belt use was introduced on January 31, 1983. We construct a stem and leaf dia\u0002gram in R with the stem.leaf function from the aplpack package [92].\r\n> library(aplpack)\r\n> stem.leaf(UKDriverDeaths, depth = FALSE)\r\n1 | 2: represents 120\r\nleaf unit: 10\r\nn: 192\r\n10 | 57\r\n11 | 136678\r\n12 | 123889\r\n13 | 0255666888899\r\n14 | 00001222344444555556667788889\r\n15 | 0000111112222223444455555566677779\r\n16 | 01222333444445555555678888889\r\n17 | 11233344566667799\r\n18 | 00011235568\r\n19 | 01234455667799\r\n20 | 0000113557788899\r\n21 | 145599\r\n22 | 013467\r\n23 | 9\r\n24 | 7\r\nHI: 2654\r\nThe display shows a more or less balanced mound-shaped distribution, with one or maybe two\r\nhumps, a big one and a smaller one just to its right. Note that the data have been rounded to the\r\ntens place so that each datum gets only one leaf to the right of the dividing line.\r\nNotice that the depths have been suppressed. To learn more about this option and many others,\r\nsee Section 3.4. Unlike a histogram, the original data values may be recovered from the stemplot\r\ndisplay – modulo the rounding – that is, starting from the top and working down we can read off\r\nthe data values 1050, 1070, 1110, 1130, etc.\r\nIndex plot Done with the plot function. These are good for plotting data which are ordered,\r\nfor example, when the data are measured over time. That is, the first observation was measured at\r\ntime 1, the second at time 2, etc. It is a two dimensional plot, in which the index (or time) is the\r\nx variable and the measured value is the y variable. There are several plotting methods for index\r\nplots, and we discuss two of them:\r\nspikes: draws a vertical line from the x-axis to the observation height (type = \"h\").\r\npoints: plots a simple point at the observation height (type = \"p\").\r\nExample 3.7. Level of Lake Huron 1875-1972. Brockwell and Davis [11] give the annual mea\u0002surements of the level (in feet) of Lake Huron from 1875–1972. The data are stored in the time\r\nseries LakeHuron. See ?LakeHuron. Figure 3.1.4 was produced with the following code:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ad202bfa-7180-45ea-bc67-c6b4958ec4c6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2b4239dfaf3a0388031bcbbafe470c3a1f06d6d895bc0787a9b07c48f6a17099",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 461
      },
      {
        "segments": [
          {
            "segment_id": "f617aaad-68c0-45f9-b4e6-90bb2e0220c3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 41,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 25\r\n> plot(LakeHuron, type = \"h\")\r\n> plot(LakeHuron, type = \"p\")\r\nThe plots show an overall decreasing trend to the observations, and there appears to be some\r\nseasonal variation that increases over time.\r\nDensity estimates\r\n3.1.2 Qualitative Data, Categorical Data, and Factors\r\nQualitative data are simply any type of data that are not numerical, or do not represent numerical\r\nquantities. Examples of qualitative variables include a subject’s name, gender, race/ethnicity, po\u0002litical party, socioeconomic status, class rank, driver’s license number, and social security number\r\n(SSN).\r\nPlease bear in mind that some data look to be quantitative but are not, because they do not\r\nrepresent numerical quantities and do not obey mathematical rules. For example, a person’s shoe\r\nsize is typically written with numbers: 8, or 9, or 12, or 12 1\r\n2\r\n. Shoe size is not quantitative, however,\r\nbecause if we take a size 8 and combine with a size 9 we do not get a size 17.\r\nSome qualitative data serve merely to identify the observation (such a subject’s name, driver’s\r\nlicense number, or SSN). This type of data does not usually play much of a role in statistics. But\r\nother qualitative variables serve to subdivide the data set into categories; we call these factors. In\r\nthe above examples, gender, race, political party, and socioeconomic status would be considered\r\nfactors (shoe size would be another one). The possible values of a factor are called its levels. For\r\ninstance, the factor gender would have two levels, namely, male and female. Socioeconomic status\r\ntypically has three levels: high, middle, and low.\r\nFactors may be of two types: nominal and ordinal. Nominal factors have levels that correspond\r\nto names of the categories, with no implied ordering. Examples of nominal factors would be hair\r\ncolor, gender, race, or political party. There is no natural ordering to “Democrat” and “Republican”;\r\nthe categories are just names associated with different groups of people.\r\nIn contrast, ordinal factors have some sort of ordered structure to the underlying factor levels.\r\nFor instance, socioeconomic status would be an ordinal categorical variable because the levels cor\u0002respond to ranks associated with income, education, and occupation. Another example of ordinal\r\ncategorical data would be class rank.\r\nFactors have special status in R. They are represented internally by numbers, but even when they\r\nare written numerically their values do not convey any numeric meaning or obey any mathematical\r\nrules (that is, Stage III cancer is not Stage I cancer + Stage II cancer).\r\nExample 3.8. The state.abb vector gives the two letter postal abbreviations for all 50 states.\r\n> str(state.abb)\r\nchr [1:50] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DE\" ...\r\nThese would be ID data. The state.name vector lists all of the complete names and those data\r\nwould also be ID.\r\nExample 3.9. U.S. State Facts and Features. The U.S. Department of Commerce of the U.S. Cen\u0002sus Bureau releases all sorts of information in the Statistical Abstract of the United States, and the",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f617aaad-68c0-45f9-b4e6-90bb2e0220c3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=98f9b640012af68448a5bd62089be31baf9b4adb2dfc2a7afc67d4f98785a3a9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 495
      },
      {
        "segments": [
          {
            "segment_id": "6281f4c4-7a2d-4c3f-bb2b-033dad760da3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 42,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "26 CHAPTER 3. DATA DESCRIPTION\r\nTime\r\nLakeHuron\r\n1880 1900 1920 1940 1960\r\n576 578 580 582\r\n●\r\n●\r\n●●\r\n●\r\n●●\r\n●\r\n●●●\r\n●\r\n●\r\n●\r\n●●\r\n●●\r\n●●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●●\r\n●●●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\nTime\r\nLakeHuron\r\n1880 1900 1920 1940 1960\r\n576 578 580 582\r\nFigure 3.1.4: Index plots of the LakeHuron data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6281f4c4-7a2d-4c3f-bb2b-033dad760da3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4239bf00871d41417df686fa0c32aa5154ffaba601b8ea74835561737bed54a0",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "aa6fd68b-bbfd-402f-ab0d-49977ef6052d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 43,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 27\r\nstate.region data lists each of the 50 states and the region to which it belongs, be it Northeast,\r\nSouth, North Central, or West. See ?state.region.\r\n> str(state.region)\r\nFactor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\r\n> state.region[1:5]\r\n[1] South West West South West\r\nLevels: Northeast South North Central West\r\nThe str output shows that state.region is already stored internally as a factor and it lists a\r\ncouple of the factor levels. To see all of the levels we printed the first five entries of the vector in\r\nthe second line.need to print a piece of the from\r\nDisplaying Qualitative Data\r\nTables One of the best ways to summarize qualitative data is with a table of the data values. We\r\nmay count frequencies with the table function or list proportions with the prop.table function\r\n(whose input is a frequency table). In the R Commander you can do it with Statistics . Frequency\r\nDistribution. . . . Alternatively, to look at tables for all factors in the Active data set you can do\r\nStatistics . Summaries . Active Dataset.\r\n> Tbl <- table(state.division)\r\n> Tbl # frequencies\r\nstate.division\r\nNew England Middle Atlantic South Atlantic\r\n6 3 8\r\nEast South Central West South Central East North Central\r\n4 4 5\r\nWest North Central Mountain Pacific\r\n7 8 5\r\n> Tbl/sum(Tbl) # relative frequencies\r\nstate.division\r\nNew England Middle Atlantic South Atlantic\r\n0.12 0.06 0.16\r\nEast South Central West South Central East North Central\r\n0.08 0.08 0.10\r\nWest North Central Mountain Pacific\r\n0.14 0.16 0.10\r\n> prop.table(Tbl) # same thing\r\nstate.division\r\nNew England Middle Atlantic South Atlantic\r\n0.12 0.06 0.16\r\nEast South Central West South Central East North Central\r\n0.08 0.08 0.10\r\nWest North Central Mountain Pacific\r\n0.14 0.16 0.10",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/aa6fd68b-bbfd-402f-ab0d-49977ef6052d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=73632a455324875f935a54e7f23cf295e1311060e2e3706b97903ae7b0de66d4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 411
      },
      {
        "segments": [
          {
            "segment_id": "4de4f601-009c-4365-bddf-83f11a6366fd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 44,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "28 CHAPTER 3. DATA DESCRIPTION\r\nBar Graphs A bar graph is the analogue of a histogram for categorical data. A bar is displayed\r\nfor each level of a factor, with the heights of the bars proportional to the frequencies of observations\r\nfalling in the respective categories. A disadvantage of bar graphs is that the levels are ordered\r\nalphabetically (by default), which may sometimes obscure patterns in the display.\r\nExample 3.10. U.S. State Facts and Features. The state.region data lists each of the 50\r\nstates and the region to which it belongs, be it Northeast, South, North Central, or West. See\r\n?state.region. It is already stored internally as a factor. We make a bar graph with the barplot\r\nfunction:\r\n> barplot(table(state.region), cex.names = 0.5)\r\n> barplot(prop.table(table(state.region)), cex.names = 0.5)\r\nSee Figure 3.1.5. The display on the left is a frequency bar graph because the y axis shows\r\ncounts, while the display on the left is a relative frequency bar graph. The only difference between\r\nthe two is the scale. Looking at the graph we see that the majority of the fifty states are in the South,\r\nfollowed by West, North Central, and finally Northeast. Over 30% of the states are in the South.\r\nNotice the cex.names argument that we used, above. It shrinks the names on the x axis by\r\n50% which makes them easier to read. See ?par for a detailed list of additional plot parameters.\r\nPareto Diagrams A pareto diagram is a lot like a bar graph except the bars are rearranged such\r\nthat they decrease in height going from left to right. The rearrangement is handy because it can\r\nvisually reveal structure (if any) in how fast the bars decrease – this is much more difficult when\r\nthe bars are jumbled.\r\nExample 3.11. U.S. State Facts and Features. The state.division data record the division\r\n(New England, Middle Atlantic, South Atlantic, East South Central, West South Central, East\r\nNorth Central, West North Central, Mountain, and Pacific) of the fifty states. We can make a pareto\r\ndiagram with either the RcmdrPlugin.IPSUR package or with the pareto.chart function from\r\nthe qcc package [77]. See Figure 3.1.6. The code follows.\r\n> library(qcc)\r\n> pareto.chart(table(state.division), ylab = \"Frequency\")\r\nDot Charts These are a lot like a bar graph that has been turned on its side with the bars replaced\r\nby dots on horizontal lines. They do not convey any more (or less) information than the associated\r\nbar graph, but the strength lies in the economy of the display. Dot charts are so compact that it\r\nis easy to graph very complicated multi-variable interactions together in one graph. See Section\r\n3.6. We will give an example here using the same data as above for comparison. The graph was\r\nproduced by the following code.\r\n> x <- table(state.region)\r\n> dotchart(as.vector(x), labels = names(x))\r\nSee Figure 3.1.7. Compare it to Figure 3.1.5.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4de4f601-009c-4365-bddf-83f11a6366fd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=47737eb30fbab8965240ea2ca2d285e89dece6ccf40858a976c8dbba8d15b94d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 476
      },
      {
        "segments": [
          {
            "segment_id": "37b6df63-fac3-477b-a9e6-06e8811cbcaf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 45,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 29\r\nNortheast South West\r\n0\r\n5 10 15\r\nNortheast South West\r\n0.00 0.10 0.20 0.30\r\nFigure 3.1.5: Bar graphs of the state.region data\r\nThe left graph is a frequency barplot made with table and the right is a relative frequency barplot made with\r\nprop.table.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/37b6df63-fac3-477b-a9e6-06e8811cbcaf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=32aa74c6acef9aa1eae0e379b27086850bc08b00e27ae0f37d9fdc64d828eb29",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "37c017a4-3c7f-4f71-a062-fad2d6cc51b0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 46,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "30 CHAPTER 3. DATA DESCRIPTION\r\nPackage 'qcc', version 2.0.1\r\nType 'citation(\"qcc\")' for citing this R package in publications.\r\nPareto chart analysis for table(state.division)\r\nFrequency Cum.Freq. Percentage Cum.Percent.\r\nMountain 8 8 16 16\r\nSouth Atlantic 8 16 16 32\r\nWest North Central 7 23 14 46\r\nNew England 6 29 12 58\r\nPacific 5 34 10 68\r\nEast North Central 5 39 10 78\r\nWest South Central 4 43 8 86\r\nEast South Central 4 47 8 94\r\nMiddle Atlantic 3 50 6 100 Mountain South Atlantic West North Central New England Pacific East North Central West South Central East South Central Middle Atlantic\r\nPareto Chart for table(state.division)\r\nFrequency\r\n0 10 20 30 40 50\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n0% 25% 75%\r\nCumulative Percentage\r\nFigure 3.1.6: Pareto chart of the state.division data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/37c017a4-3c7f-4f71-a062-fad2d6cc51b0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f49e442455ceaaa65f7d773716022f733b36a85e0a004f59b4245dfb042b531e",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "340fb86c-22c5-4181-967d-22923cfc6ddd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 47,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.1. TYPES OF DATA 31\r\nNortheast\r\nSouth\r\nNorth Central\r\nWest\r\n●\r\n●\r\n●\r\n●\r\n9 10 11 12 13 14 15 16\r\nFigure 3.1.7: Dot chart of the state.region data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/340fb86c-22c5-4181-967d-22923cfc6ddd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fc117433051de0833978ab4e395f2b2868ee8433dd2c0baa97114895523a9192",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 214
      },
      {
        "segments": [
          {
            "segment_id": "e322cf73-6d40-4cd5-b8ff-4b4b18d1f96b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 48,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "32 CHAPTER 3. DATA DESCRIPTION\r\nPie Graphs These can be done with R and the R Commander, but they fallen out of favor in\r\nrecent years because researchers have determined that while the human eye is good at judging\r\nlinear measures, it is notoriously bad at judging relative areas (such as those displayed by a pie\r\ngraph). Pie charts are consequently a very bad way of displaying information. A bar chart or dot\r\nchart is a preferable way of displaying qualitative data. See ?pie for more information.\r\nWe are not going to do any examples of a pie graph and discourage their use elsewhere.\r\n3.1.3 Logical Data\r\nThere is another type of information recognized by R which does not fall into the above categories.\r\nThe value is either TRUE or FALSE (note that equivalently you can use 1 = TRUE, 0 = FALSE).\r\nHere is an example of a logical vector:\r\n> x <- 5:9\r\n> y <- (x < 7.3)\r\n> y\r\n[1] TRUE TRUE TRUE FALSE FALSE\r\nMany functions in R have options that the user may or may not want to activate in the function\r\ncall. For example, the stem.leaf function has the depths argument which is TRUE by default. We\r\nsaw in Section 3.1.1 how to turn the option off, simply enter stem.leaf(x, depths = FALSE)\r\nand they will not be shown on the display.\r\nWe can swap TRUE with FALSE with the exclamation point !.\r\n> !y\r\n[1] FALSE FALSE FALSE TRUE TRUE\r\n3.1.4 Missing Data\r\nMissing data are a persistent and prevalent problem in many statistical analyses, especially those\r\nassociated with the social sciences. R reserves the special symbol NA to representing missing data.\r\nOrdinary arithmetic with NA values give NA’s (addition, subtraction, etc.) and applying a func\u0002tion to a vector that has an NA in it will usually give an NA.\r\n> x <- c(3, 7, NA, 4, 7)\r\n> y <- c(5, NA, 1, 2, 2)\r\n> x + y\r\n[1] 8 NA NA 6 9\r\nSome functions have a na.rm argument which when TRUE will ignore missing data as if it were\r\nnot there (such as mean, var, sd, IQR, mad, . . . ).\r\n> sum(x)\r\n[1] NA\r\n> sum(x, na.rm = TRUE)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e322cf73-6d40-4cd5-b8ff-4b4b18d1f96b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8caf0a7314cb51e03ea88e933d59786012e197334853629c3add10312f9e0e6e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 373
      },
      {
        "segments": [
          {
            "segment_id": "0b39a21a-d2b5-495f-9750-df4ee9d3a29a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 49,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.2. FEATURES OF DATA DISTRIBUTIONS 33\r\n[1] 21\r\nOther functions do not have a na.rm argument and will return NA or an error if the argument\r\nhas NAs. In those cases we can find the locations of any NAs with the is.na function and remove\r\nthose cases with the [] operator.\r\n> is.na(x)\r\n[1] FALSE FALSE TRUE FALSE FALSE\r\n> z <- x[!is.na(x)]\r\n> sum(z)\r\n[1] 21\r\nThe analogue of is.na for rectangular data sets (or data frames) is the complete.cases func\u0002tion. See Appendix D.4.\r\n3.1.5 Other Data Types\r\n3.2 Features of Data Distributions\r\nGiven that the data have been appropriately displayed, the next step is to try to identify salient\r\nfeatures represented in the graph. The acronym to remember is Center, Unusual features, Spread,\r\nand Shape. (CUSS).\r\n3.2.1 Center\r\nOne of the most basic features of a data set is its center. Loosely speaking, the center of a data set\r\nis associated with a number that represents a middle or general tendency of the data. Of course,\r\nthere are usually several values that would serve as a center, and our later tasks will be focused on\r\nchoosing an appropriate one for the data at hand. Judging from the histogram that we saw in Figure\r\n3.1.3, a measure of center would be about 35.\r\n3.2.2 Spread\r\nThe spread of a data set is associated with its variability; data sets with a large spread tend to cover\r\na large interval of values, while data sets with small spread tend to cluster tightly around a central\r\nvalue.\r\n3.2.3 Shape\r\nWhen we speak of the shape of a data set, we are usually referring to the shape exhibited by an\r\nassociated graphical display, such as a histogram. The shape can tell us a lot about any underlying\r\nstructure to the data, and can help us decide which statistical procedure we should use to analyze\r\nthem.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0b39a21a-d2b5-495f-9750-df4ee9d3a29a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d046197b0c6af66424d4f146c74ef79d4bb7b220efdda61b49dcabd31c937128",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 312
      },
      {
        "segments": [
          {
            "segment_id": "9cf804e8-2df6-4cf7-99b9-c230cbcab585",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 50,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "34 CHAPTER 3. DATA DESCRIPTION\r\nSymmetry and Skewness A distribution is said to be right-skewed (or positively skewed) if the\r\nright tail seems to be stretched from the center. A left-skewed (or negatively skewed) distribution\r\nis stretched to the left side. A symmetric distribution has a graph that is balanced about its center,\r\nin the sense that half of the graph may be reflected about a central line of symmetry to match the\r\nother half.\r\nWe have already encountered skewed distributions: both the discoveries data in Figure 3.1.1\r\nand the precip data in Figure 3.1.3 appear right-skewed. The UKDriverDeaths data in Example\r\n3.6 is relatively symmetric (but note the one extreme value 2654 identified at the bottom of the\r\nstemplot).\r\nKurtosis Another component to the shape of a distribution is how “peaked” it is. Some distri\u0002butions tend to have a flat shape with thin tails. These are called platykurtic, and an example of a\r\nplatykurtic distribution is the uniform distribution; see Section 6.2. On the other end of the spec\u0002trum are distributions with a steep peak, or spike, accompanied by heavy tails; these are called\r\nleptokurtic. Examples of leptokurtic distributions are the Laplace distribution and the logistic dis\u0002tribution. See Section 6.5. In between are distributions (called mesokurtic) with a rounded peak\r\nand moderately sized tails. The standard example of a mesokurtic distribution is the famous bell\u0002shaped curve, also known as the Gaussian, or normal, distribution, and the binomial distribution\r\ncan be mesokurtic for specific choices of p. See Sections 5.3 and 6.3.\r\n3.2.4 Clusters and Gaps\r\nClusters or gaps are sometimes observed in quantitative data distributions. They indicate clumping\r\nof the data about distinct values, and gaps may exist between clusters. Clusters often suggest an\r\nunderlying grouping to the data. For example, take a look at the faithful data which contains\r\nthe duration of eruptions and the waiting time between eruptions of the Old Faithful geyser in\r\nYellowstone National Park. (Do not be frightened by the complicated information at the left of the\r\ndisplay for now; we will learn how to interpret it in Section 3.4).\r\n> library(aplpack)\r\n> stem.leaf(faithful$eruptions)\r\n1 | 2: represents 1.2\r\nleaf unit: 0.1\r\nn: 272\r\n12 s | 667777777777\r\n51 1. | 888888888888888888888888888899999999999\r\n71 2* | 00000000000011111111\r\n87 t | 2222222222333333\r\n92 f | 44444\r\n94 s | 66\r\n97 2. | 889\r\n98 3* | 0\r\n102 t | 3333\r\n108 f | 445555\r\n118 s | 6666677777\r\n(16) 3. | 8888888889999999",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9cf804e8-2df6-4cf7-99b9-c230cbcab585.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=646dbec80f9489fb2e3576022d5fb9186e76a5d889c00509e113f5809b42e997",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 408
      },
      {
        "segments": [
          {
            "segment_id": "b3afcc9c-9dbd-43df-aa45-898bddbc924d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 51,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.3. DESCRIPTIVE STATISTICS 35\r\n138 4* | 0000000000000000111111111111111\r\n107 t | 22222222222233333333333333333\r\n78 f | 44444444444445555555555555555555555\r\n43 s | 6666666666677777777777\r\n21 4. | 88888888888899999\r\n4 5* | 0001\r\nThere are definitely two clusters of data here; an upper cluster and a lower cluster.\r\n3.2.5 Extreme Observations and other Unusual Features\r\nExtreme observations fall far from the rest of the data. Such observations are troublesome to many\r\nstatistical procedures; they cause exaggerated estimates and instability. It is important to identify\r\nextreme observations and examine the source of the data more closely. There are many possible\r\nreasons underlying an extreme observation:\r\n• Maybe the value is a typographical error. Especially with large data sets becoming more\r\nprevalent, many of which being recorded by hand, mistakes are a common problem. After\r\ncloser scrutiny, these can often be fixed.\r\n• Maybe the observation was not meant for the study, because it does not belong to the\r\npopulation of interest. For example, in medical research some subjects may have relevant\r\ncomplications in their genealogical history that would rule out their participation in the ex\u0002periment. Or when a manufacturing company investigates the properties of one of its devices,\r\nperhaps a particular product is malfunctioning and is not representative of the majority of the\r\nitems.\r\n• Maybe it indicates a deeper trend or phenomenon. Many of the most influential scientific\r\ndiscoveries were made when the investigator noticed an unexpected result, a value that was\r\nnot predicted by the classical theory. Albert Einstein, Louis Pasteur, and others built their\r\ncareers on exactly this circumstance.\r\n3.3 Descriptive Statistics\r\n3.3.1 Frequencies and Relative Frequencies\r\nThese are used for categorical data. The idea is that there are a number of different categories, and\r\nwe would like to get some idea about how the categories are represented in the population. For\r\nexample, we may want to see how the\r\n3.3.2 Measures of Center\r\nThe sample mean is denoted x (read “x-bar”) and is simply the arithmetic average of the observa\u0002tions:\r\nx =\r\nx1 + x2 + · · · + xn\r\nn\r\n=\r\n1\r\nn\r\nXn\r\ni=1\r\nxi. (3.3.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b3afcc9c-9dbd-43df-aa45-898bddbc924d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=367df6bb002ef967792bc9210fcdbdb1b297bb4dc831c8d0251fd0a502a0c0e5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 347
      },
      {
        "segments": [
          {
            "segment_id": "b7d34e99-30fb-4b67-a00e-97a0d2fe3a57",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 52,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "36 CHAPTER 3. DATA DESCRIPTION\r\n• Good: natural, easy to compute, has nice mathematical properties\r\n• Bad: sensitive to extreme values\r\nIt is appropriate for use with data sets that are not highly skewed without extreme observations.\r\nThe sample median is another popular measure of center and is denoted ˜x. To calculate its\r\nvalue, first sort the data into an increasing sequence of numbers. If the data set has an odd number\r\nof observations then ˜x is the value of the middle observation, which lies in position (n + 1)/2;\r\notherwise, there are two middle observations and ˜x is the average of those middle values.\r\n• Good: resistant to extreme values, easy to describe\r\n• Bad: not as mathematically tractable, need to sort the data to calculate\r\nOne desirable property of the sample median is that it is resistant to extreme observations, in the\r\nsense that the value of ˜x depends only the values of the middle observations, and is quite unaf\u0002fected by the actual values of the outer observations in the ordered list. The same cannot be said\r\nfor the sample mean. Any significant changes in the magnitude of an observation xk results in a\r\ncorresponding change in the value of the mean. Hence, the sample mean is said to be sensitive to\r\nextreme observations.\r\nThe trimmed mean is a measure designed to address the sensitivity of the sample mean to\r\nextreme observations. The idea is to “trim” a fraction (less than 1/2) of the observations off each\r\nend of the ordered list, and then calculate the sample mean of what remains. We will denote it by\r\nxt=0.05.\r\n• Good: resistant to extreme values, shares nice statistical properties\r\n• Bad: need to sort the data\r\n3.3.3 How to do it with R\r\n• You can calculate frequencies or relative frequencies with the table function, and relative\r\nfrequencies with prop.table(table()).\r\n• You can calculate the sample mean of a data vector x with the command mean(x).\r\n• You can calculate the sample median of x with the command median(x).\r\n• You can calculate the trimmed mean with the trim argument; mean(x, trim = 0.05).\r\n3.3.4 Order Statistics and the Sample Quantiles\r\nA common first step in an analysis of a data set is to sort the values. Given a data set x1, x2, . . . ,xn,\r\nwe may sort the values to obtain an increasing sequence\r\nx(1) ≤ x(2) ≤ x(3) ≤ · · · ≤ x(n) (3.3.2)\r\nand the resulting values are called the order statistics. The k\r\nth entry in the list, x(k)\r\n, is the k\r\nth order\r\nstatistic, and approximately 100(k/n)% of the observations fall below x(k). The order statistics give",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b7d34e99-30fb-4b67-a00e-97a0d2fe3a57.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0ffb6979f78649ec5e52ec29f137df037f55ff083467cc15bc56cbc405e2dd15",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 446
      },
      {
        "segments": [
          {
            "segment_id": "5f0995e7-08b5-4d2f-ab3a-4018f5d54cb1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 53,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.3. DESCRIPTIVE STATISTICS 37\r\nan indication of the shape of the data distribution, in the sense that a person can look at the order\r\nstatistics and have an idea about where the data are concentrated, and where they are sparse.\r\nThe sample quantiles are related to the order statistics. Unfortunately, there is not a universally\r\naccepted definition of them. Indeed, R is equipped to calculate quantiles using nine distinct defini\u0002tions! We will describe the default method (type = 7), but the interested reader can see the details\r\nfor the other methods with ?quantile.\r\nSuppose the data set has n observations. Find the sample quantile of order p (0 < p < 1),\r\ndenoted ˜qp , as follows:\r\nFirst step: sort the data to obtain the order statistics x(1), x(2), . . . ,x(n).\r\nSecond step: calculate (n − 1)p + 1 and write it in the form k.d, where k is an integer and d is a\r\ndecimal.\r\nThird step: The sample quantile ˜qp is\r\nq˜ p = x(k) + d(x(k+1) − x(k)). (3.3.3)\r\nThe interpretation of ˜qp is that approximately 100p% of the data fall below the value ˜qp .\r\nKeep in mind that there is not a unique definition of percentiles, quartiles, etc. Open a different\r\nbook, and you’ll find a different procedure. The difference is small and seldom plays a role except\r\nin small data sets with repeated values. In fact, most people do not even notice in common use.\r\nClearly, the most popular sample quantile is ˜q0.50, also known as the sample median, ˜x. The\r\nclosest runners-up are the first quartile q˜0.25 and the third quartile q˜0.75 (the second quartile is the\r\nmedian).\r\n3.3.5 How to do it with R\r\nAt the command prompt We can find the order statistics of a data set stored in a vector x with\r\nthe command sort(x).\r\nYou can calculate the sample quantiles of any order p where 0 < p < 1 for a data set stored\r\nin a data vector x with the quantile function, for instance, the command quantile(x, probs\r\n= c(0, 0.25, 0.37)) will return the smallest observation, the first quartile, ˜q0.25, and the 37th\r\nsample quantile, ˜q0.37. For ˜qp simply change the values in the probs argument to the value p.\r\nWith the R Commander In Rcmdr we can find the order statistics of a variable in the Active\r\ndata set by doing Data . Manage variables in Active data set. . . . Compute new variable. . . .\r\nIn the Expression to compute dialog simply type sort(varname), where varname is the variable\r\nthat it is desired to sort.\r\nIn Rcmdr, we can calculate the sample quantiles for a particular variable with the sequence\r\nStatistics . Summaries . Numerical Summaries. . . . We can automatically calculate the quartiles\r\nfor all variables in the Active data set with the sequence Statistics . Summaries . Active\r\nDataset.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5f0995e7-08b5-4d2f-ab3a-4018f5d54cb1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=233ef0f4e35cbfdc8dcc3d77eaf031302a0d41e39c723aa67309b3f1b6a48e1e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 482
      },
      {
        "segments": [
          {
            "segment_id": "c0fc3973-055a-4836-9a32-da4875c0d23a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 54,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "38 CHAPTER 3. DATA DESCRIPTION\r\n3.3.6 Measures of Spread\r\nSample Variance and Standard Deviation The sample variance is denoted s\r\n2\r\nand is calculated\r\nwith the formula\r\ns\r\n2 =\r\n1\r\nn − 1\r\nXn\r\ni=1\r\n(xi − x)\r\n2\r\n. (3.3.4)\r\nThe sample standard deviation is s =\r\n√\r\ns\r\n2\r\n. Intuitively, the sample variance is approximately the\r\naverage squared distance of the observations from the sample mean. The sample standard deviation\r\nis used to scale the estimate back to the measurement units of the original data.\r\n• Good: tractable, has nice mathematical/statistical properties\r\n• Bad: sensitive to extreme values\r\nWe will spend a lot of time with the variance and standard deviation in the coming chapters. In the\r\nmeantime, the following two rules give some meaning to the standard deviation, in that there are\r\nbounds on how much of the data can fall past a certain distance from the mean.\r\nFact 3.12. Chebychev’s Rule: The proportion of observations within k standard deviations of the\r\nmean is at least 1 − 1/k\r\n2\r\n, i.e., at least 75%, 89%, and 94% of the data are within 2, 3, and 4\r\nstandard deviations of the mean, respectively.\r\nNote that Chebychev’s Rule does not say anything about when k = 1, because 1 − 1/1\r\n2 = 0,\r\nwhich states that at least 0% of the observations are within one standard deviation of the mean\r\n(which is not saying much).\r\nChebychev’s Rule applies to any data distribution, any list of numbers, no matter where it came\r\nfrom or what the histogram looks like. The price for such generality is that the bounds are not very\r\ntight; if we know more about how the data are shaped then we can say more about how much of\r\nthe data can fall a given distance from the mean.\r\nFact 3.13. Empirical Rule: If data follow a bell-shaped curve, then approximately 68%, 95%, and\r\n99.7% of the data are within 1, 2, and 3 standard deviations of the mean, respectively.\r\nInterquartile Range Just as the sample mean is sensitive to extreme values, so the associated\r\nmeasure of spread is similarly sensitive to extremes. Further, the problem is exacerbated by the\r\nfact that the extreme distances are squared. We know that the sample quartiles are resistant to\r\nextremes, and a measure of spread associated with them is the interquartile range (IQR) defined\r\nby IQR = q0.75 − q0.25.\r\n• Good: stable, resistant to outliers, robust to nonnormality, easy to explain\r\n• Bad: not as tractable, need to sort the data, only involves the middle 50% of the data.\r\nMedian Absolute Deviation A measure even more robust than the IQR is the median absolute\r\ndeviation (MAD). To calculate it we first get the median ex, next the absolute deviations |x1 − x˜|,\r\n|x2 − x˜|, . . . , |xn − x˜|, and the MAD is proportional to the median of those deviations:\r\nMAD ∝ median(|x1 − x˜|, |x2 − x˜|, . . . , |xn − x˜|). (3.3.5)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c0fc3973-055a-4836-9a32-da4875c0d23a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1208f33cbc8ea5adb0b5df483d3dfc6734ba32a88c79365c48eb2fd48a61507f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 503
      },
      {
        "segments": [
          {
            "segment_id": "1de4d1eb-ca44-43cb-8c09-438c63b6d337",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 55,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.3. DESCRIPTIVE STATISTICS 39\r\nThat is, the MAD = c · median(|x1 − x˜|, |x2 − x˜|, . . . , |xn − x˜|), where c is a constant chosen so that\r\nthe MAD has nice properties. The value of c in R is by default c = 1.4286. This value is chosen to\r\nensure that the estimator of σ is correct, on the average, under suitable sampling assumptions (see\r\nSection 9.1).\r\n• Good: stable, very robust, even more so than the IQR.\r\n• Bad: not tractable, not well known and less easy to explain.\r\nComparing Apples to Apples\r\nWe have seen three different measures of spread which, for a given data set, will give three different\r\nanswers. Which one should we use? It depends on the data set. If the data are well behaved, with\r\nan approximate bell-shaped distribution, then the sample mean and sample standard deviation are\r\nnatural choices with nice mathematical properties. However, if the data have an unusual or skewed\r\nshape with several extreme values, perhaps the more resistant choices among the IQR or MAD\r\nwould be more appropriate.\r\nHowever, once we are looking at the three numbers it is important to understand that the esti\u0002mators are not all measuring the same quantity, on the average. In particular, it can be shown that\r\nwhen the data follow an approximately bell-shaped distribution, then on the average, the sample\r\nstandard deviation s and the MAD will be the approximately the same value, namely, σ, but the\r\nIQR will be on the average 1.349 times larger than s and the MAD. See 8 for more details.\r\n3.3.7 How to do it with R\r\nAt the command prompt From the console we may compute the sample range with range(x)\r\nand the sample variance with var(x), where x is a numeric vector. The sample standard deviation\r\nis sqrt(var(x)) or just sd(x). The IQR is IQR(x) and the median absolute deviation is mad(x).\r\nIn R Commander In Rcmdr we can calculate the sample standard deviation with the Statistics .\r\nSummaries . Numerical Summaries. . . combination. R Commander does not calculate the IQR\r\nor MAD in any of the menu selections, by default.\r\n3.3.8 Measures of Shape\r\nSample Skewness The sample skewness, denoted by g1, is defined by the formula\r\ng1 =\r\n1\r\nn\r\nPn\r\ni=1\r\n(xi − x)\r\n3\r\ns\r\n3\r\n. (3.3.6)\r\nThe sample skewness can be any value −∞ < g1 < ∞. The sign of g1 indicates the direction\r\nof skewness of the distribution. Samples that have g1 > 0 indicate right-skewed distributions\r\n(or positively skewed), and samples with g1 < 0 indicate left-skewed distributions (or negatively\r\nskewed). Values of g1 near zero indicate a symmetric distribution. These are not hard and fast rules,\r\nhowever. The value of g1 is subject to sampling variability and thus only provides a suggestion to\r\nthe skewness of the underlying distribution.\r\nWe still need to know how big is “big”, that is, how do we judge whether an observed value\r\nof g1 is far enough away from zero for the data set to be considered skewed to the right or left? A",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1de4d1eb-ca44-43cb-8c09-438c63b6d337.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9582082ce4cacbbf05465773409db91108addcf00baf2e4c131df2b2416637f5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 520
      },
      {
        "segments": [
          {
            "segment_id": "1de4d1eb-ca44-43cb-8c09-438c63b6d337",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 55,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.3. DESCRIPTIVE STATISTICS 39\r\nThat is, the MAD = c · median(|x1 − x˜|, |x2 − x˜|, . . . , |xn − x˜|), where c is a constant chosen so that\r\nthe MAD has nice properties. The value of c in R is by default c = 1.4286. This value is chosen to\r\nensure that the estimator of σ is correct, on the average, under suitable sampling assumptions (see\r\nSection 9.1).\r\n• Good: stable, very robust, even more so than the IQR.\r\n• Bad: not tractable, not well known and less easy to explain.\r\nComparing Apples to Apples\r\nWe have seen three different measures of spread which, for a given data set, will give three different\r\nanswers. Which one should we use? It depends on the data set. If the data are well behaved, with\r\nan approximate bell-shaped distribution, then the sample mean and sample standard deviation are\r\nnatural choices with nice mathematical properties. However, if the data have an unusual or skewed\r\nshape with several extreme values, perhaps the more resistant choices among the IQR or MAD\r\nwould be more appropriate.\r\nHowever, once we are looking at the three numbers it is important to understand that the esti\u0002mators are not all measuring the same quantity, on the average. In particular, it can be shown that\r\nwhen the data follow an approximately bell-shaped distribution, then on the average, the sample\r\nstandard deviation s and the MAD will be the approximately the same value, namely, σ, but the\r\nIQR will be on the average 1.349 times larger than s and the MAD. See 8 for more details.\r\n3.3.7 How to do it with R\r\nAt the command prompt From the console we may compute the sample range with range(x)\r\nand the sample variance with var(x), where x is a numeric vector. The sample standard deviation\r\nis sqrt(var(x)) or just sd(x). The IQR is IQR(x) and the median absolute deviation is mad(x).\r\nIn R Commander In Rcmdr we can calculate the sample standard deviation with the Statistics .\r\nSummaries . Numerical Summaries. . . combination. R Commander does not calculate the IQR\r\nor MAD in any of the menu selections, by default.\r\n3.3.8 Measures of Shape\r\nSample Skewness The sample skewness, denoted by g1, is defined by the formula\r\ng1 =\r\n1\r\nn\r\nPn\r\ni=1\r\n(xi − x)\r\n3\r\ns\r\n3\r\n. (3.3.6)\r\nThe sample skewness can be any value −∞ < g1 < ∞. The sign of g1 indicates the direction\r\nof skewness of the distribution. Samples that have g1 > 0 indicate right-skewed distributions\r\n(or positively skewed), and samples with g1 < 0 indicate left-skewed distributions (or negatively\r\nskewed). Values of g1 near zero indicate a symmetric distribution. These are not hard and fast rules,\r\nhowever. The value of g1 is subject to sampling variability and thus only provides a suggestion to\r\nthe skewness of the underlying distribution.\r\nWe still need to know how big is “big”, that is, how do we judge whether an observed value\r\nof g1 is far enough away from zero for the data set to be considered skewed to the right or left? A",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1de4d1eb-ca44-43cb-8c09-438c63b6d337.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9582082ce4cacbbf05465773409db91108addcf00baf2e4c131df2b2416637f5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 520
      },
      {
        "segments": [
          {
            "segment_id": "a91230d0-5d1e-4d5b-abb4-fc5283a4edea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 56,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "40 CHAPTER 3. DATA DESCRIPTION\r\ngood rule of thumb is that data sets with skewness larger than 2 √6/n in magnitude are substantially\r\nskewed, in the direction of the sign of g1. See Tabachnick & Fidell [83] for details.\r\nSample Excess Kurtosis The sample excess kurtosis, denoted by g2, is given by the formula\r\ng2 =\r\n1\r\nn\r\nPn\r\ni=1\r\n(xi − x)\r\n4\r\ns\r\n4\r\n− 3. (3.3.7)\r\nThe sample excess kurtosis takes values −2 ≤ g2 < ∞. The subtraction of 3 may seem mysterious\r\nbut it is done so that mound shaped samples have values of g2 near zero. Samples with g2 > 0 are\r\ncalled leptokurtic, and samples with g2 < 0 are called platykurtic. Samples with g2 ≈ 0 are called\r\nmesokurtic.\r\nAs a rule of thumb, if |g2| > 4\r\n√\r\n6/n then the sample excess kurtosis is substantially different\r\nfrom zero in the direction of the sign of g2. See Tabachnick & Fidell [83] for details.\r\nNotice that both the sample skewness and the sample kurtosis are invariant with respect to\r\nlocation and scale, that is, the values of g1 and g2 do not depend on the measurement units of the\r\ndata.\r\n3.3.9 How to do it with R\r\nThe e1071 package [22] has the skewness function for the sample skewness and the kurtosis\r\nfunction for the sample excess kurtosis. Both functions have a na.rm argument which is FALSE by\r\ndefault.\r\nExample 3.14. We said earlier that the discoveries data looked positively skewed; let’s see what\r\nthe statistics say:\r\n> library(e1071)\r\n> skewness(discoveries)\r\n[1] 1.207600\r\n> 2 * sqrt(6/length(discoveries))\r\n[1] 0.4898979\r\nThe data are definitely skewed to the right. Let us check the sample excess kurtosis of the\r\nUKDriverDeaths data:\r\n> kurtosis(UKDriverDeaths)\r\n[1] 0.07133848\r\n> 4 * sqrt(6/length(UKDriverDeaths))\r\n[1] 0.7071068\r\nso that the UKDriverDeaths data appear to be mesokurtic, or at least not substantially lep\u0002tokurtic.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a91230d0-5d1e-4d5b-abb4-fc5283a4edea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=da12c100ccdbc2812246b1788c385a43801d345e923b389dc3daa264eef7e8b4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 312
      },
      {
        "segments": [
          {
            "segment_id": "35bdf856-1d63-4eff-98f1-0bd2efa0bad1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 57,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.4. EXPLORATORY DATA ANALYSIS 41\r\n3.4 Exploratory Data Analysis\r\nThis field was founded (mostly) by John Tukey (1915-2000). Its tools are useful when not much is\r\nknown regarding the underlying causes associated with the data set, and are often used for checking\r\nassumptions. For example, suppose we perform an experiment and collect some data. . . now what?\r\nWe look at the data using exploratory visual tools.\r\n3.4.1 More About Stemplots\r\nThere are many bells and whistles associated with stemplots, and the stem.leaf function can do\r\nmany of them.\r\nTrim Outliers: Some data sets have observations that fall far from the bulk of the other data (in\r\na sense made more precise in Section 3.4.6). These extreme observations often obscure the\r\nunderlying structure to the data and are best left out of the data display. The trim.outliers\r\nargument (which is TRUE by default) will separate the extreme observations from the others\r\nand graph the stemplot without them; they are listed at the bottom (respectively, top) of the\r\nstemplot with the label HI (respectively LO).\r\nSplit Stems: The standard stemplot has only one line per stem, which means that all observations\r\nwith first digit 3 are plotted on the same line, regardless of the value of the second digit. But\r\nthis gives some stemplots a “skyscraper” appearance, with too many observations stacked\r\nonto the same stem. We can often fix the display by increasing the number of lines available\r\nfor a given stem. For example, we could make two lines per stem, say, 3* and 3.. Obser\u0002vations with second digit 0 through 4 would go on the upper line, while observations with\r\nsecond digit 5 through 9 would go on the lower line. (We could do a similar thing with five\r\nlines per stem, or even ten lines per stem.) The end result is a more spread out stemplot\r\nwhich often looks better. A good example of this was shown on page 34.\r\nDepths: these are used to give insight into the balance of the observations as they accumulate\r\ntoward the median. In a column beside the standard stemplot, the frequency of the stem\r\ncontaining the sample median is shown in parentheses. Next, frequencies are accumulated\r\nfrom the outside inward, including the outliers. Distributions that are more symmetric will\r\nhave better balanced depths on either side of the sample median.\r\n3.4.2 How to do it with R\r\nAt the command prompt The basic command is stem(x) or a more sophisticated version writ\u0002ten by Peter Wolf called stem.leaf(x) in the R Commander. We will describe stem.leaf since\r\nthat is the one used by R Commander.\r\nWith the R Commander WARNING: Sometimes when making a stem plot the result will not\r\nbe what you expected. There are several reasons for this:\r\n• Stemplots by default will trim extreme observations (defined in Section 3.4.6) from the dis\u0002play. This in some cases will result in stemplots that are not as wide as expected.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/35bdf856-1d63-4eff-98f1-0bd2efa0bad1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=968d97f22b151bc0540c06485b9534041ad42b8ee31610add329d857ea1eece4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 488
      },
      {
        "segments": [
          {
            "segment_id": "7a840394-ed60-41cb-a9ac-48c5b1815395",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 58,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "42 CHAPTER 3. DATA DESCRIPTION\r\n• The leafs digit is chosen automatically by stem.leaf according to an algorithm that the\r\ncomputer believes will represent the data well. Depending on the choice of the digit, stem.leaf\r\nmay drop digits from the data or round the values in unexpected ways.\r\nLet us take a look at the rivers data set.\r\n> stem.leaf(rivers)\r\n1 | 2: represents 120\r\nleaf unit: 10\r\nn: 141\r\n1 1 | 3\r\n29 2 | 0111133334555556666778888899\r\n64 3 | 00000111122223333455555666677888999\r\n(18) 4 | 011222233344566679\r\n59 5 | 000222234467\r\n47 6 | 0000112235789\r\n34 7 | 12233368\r\n26 8 | 04579\r\n21 9 | 0008\r\n17 10 | 035\r\n14 11 | 07\r\n12 12 | 047\r\n9 13 | 0\r\nHI: 1450 1459 1770 1885 2315 2348 2533 3710\r\nThe stemplot shows a right-skewed shape to the rivers data distribution. Notice that the last\r\ndigit of each of the data values were dropped from the display. Notice also that there were eight\r\nextreme observations identified by the computer, and their exact values are listed at the bottom\r\nof the stemplot. Look at the scale on the left of the stemplot and try to imagine how ridiculous\r\nthe graph would have looked had we tried to include enough stems to include these other eight\r\nobservations; the stemplot would have stretched over several pages. Notice finally that we can use\r\nthe depths to approximate the sample median for these data. The median lies in the row identified\r\nby (18), which means that the median is the average of the ninth and tenth observation on that row.\r\nThose two values correspond to 43 and 43, so a good guess for the median would be 430. (For the\r\nrecord, the sample median is ex = 425. Recall that stemplots round the data to the nearest stem-leaf\r\npair.)\r\nNext let us see what the precip data look like.\r\n> stem.leaf(precip)\r\n1 | 2: represents 12\r\nleaf unit: 1\r\nn: 70\r\nLO: 7 7.2 7.8 7.8\r\n8 1* | 1344\r\n13 1. | 55677\r\n16 2* | 024\r\n18 2. | 59",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7a840394-ed60-41cb-a9ac-48c5b1815395.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4e243af5b402221444db676eee63ea3aff71efc385934f06bb37f46c7f9d9bbd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 348
      },
      {
        "segments": [
          {
            "segment_id": "f62f28d5-57bd-48d5-a57d-09694d63f711",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 59,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.4. EXPLORATORY DATA ANALYSIS 43\r\n28 3* | 0000111234\r\n(15) 3. | 555566677788899\r\n27 4* | 0000122222334\r\n14 4. | 56688899\r\n6 5* | 44\r\n4 5. | 699\r\nHI: 67\r\nHere is an example of split stems, with two lines per stem. The final digit of each datum has\r\nbeen dropped for the display. The data appear to be left skewed with four extreme values to the\r\nleft and one extreme value to the right. The sample median is approximately 37 (it turns out to be\r\n36.6).\r\n3.4.3 Hinges and the Five Number Summary\r\nGiven a data set x1, x2, . . . , xn, the hinges are found by the following method:\r\n• Find the order statistics x(1), x(2), . . . , x(n).\r\n• The lower hinge hL is in position L = b(n + 3)/2c /2, where the symbol bxc denotes the\r\nlargest integer less than or equal to x. If the position L is not an integer, then the hinge hL is\r\nthe average of the adjacent order statistics.\r\n• The upper hinge hU is in position n + 1 − L.\r\nGiven the hinges, the five number summary (5NS ) is\r\n5NS = (x(1), hL, x˜, hU, x(n)). (3.4.1)\r\nAn advantage of the 5NS is that it reduces a potentially large data set to a shorter list of only five\r\nnumbers, and further, these numbers give insight regarding the shape of the data distribution similar\r\nto the sample quantiles in Section 3.3.4.\r\n3.4.4 How to do it with R\r\nIf the data are stored in a vector x, then you can compute the 5NS with the fivenum function.\r\n3.4.5 Boxplots\r\nA boxplot is essentially a graphical representation of the 5NS . It can be a handy alternative to a\r\nstripchart when the sample size is large.\r\nA boxplot is constructed by drawing a box alongside the data axis with sides located at the\r\nupper and lower hinges. A line is drawn parallel to the sides to denote the sample median. Lastly,\r\nwhiskers are extended from the sides of the box to the maximum and minimum data values (more\r\nprecisely, to the most extreme values that are not potential outliers, defined below).\r\nBoxplots are good for quick visual summaries of data sets, and the relative positions of the\r\nvalues in the 5NS are good at indicating the underlying shape of the data distribution, although\r\nperhaps not as effectively as a histogram. Perhaps the greatest advantage of a boxplot is that it can\r\nhelp to objectively identify extreme observations in the data set as described in the next section.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f62f28d5-57bd-48d5-a57d-09694d63f711.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9f472f0af304fa82a935ded7152198a92c8ec23f39697112c95a5bff619f17b8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 433
      },
      {
        "segments": [
          {
            "segment_id": "ea4551e3-affe-4204-a0b3-b8407697a458",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 60,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "44 CHAPTER 3. DATA DESCRIPTION\r\nBoxplots are also good because one can visually assess multiple features of the data set simul\u0002taneously:\r\nCenter can be estimated by the sample median, ˜x.\r\nSpread can be judged by the width of the box, hU − hL. We know that this will be close to the\r\nIQR, which can be compared to s and the MAD, perhaps after rescaling if appropriate.\r\nShape is indicated by the relative lengths of the whiskers, and the position of the median inside the\r\nbox. Boxes with unbalanced whiskers indicate skewness in the direction of the long whisker.\r\nSkewed distributions often have the median tending in the opposite direction of skewness.\r\nKurtosis can be assessed using the box and whiskers. A wide box with short whiskers will\r\ntend to be platykurtic, while a skinny box with wide whiskers indicates leptokurtic distribu\u0002tions.\r\nExtreme observations are identified with open circles (see below).\r\n3.4.6 Outliers\r\nA potential outlier is any observation that falls beyond 1.5 times the width of the box on either side,\r\nthat is, any observation less than hL − 1.5(hU − hL) or greater than hU + 1.5(hU − hL). A suspected\r\noutlier is any observation that falls beyond 3 times the width of the box on either side. In R, both\r\npotential and suspected outliers (if present) are denoted by open circles; there is no distinction\r\nbetween the two.\r\nWhen potential outliers are present, the whiskers of the boxplot are then shortened to extend to\r\nthe most extreme observation that is not a potential outlier. If an outlier is displayed in a boxplot,\r\nthe index of the observation may be identified in a subsequent plot in Rcmdr by clicking the Identify\r\noutliers with mouse option in the Boxplot dialog.\r\nWhat do we do about outliers? They merit further investigation. The primary goal is to deter\u0002mine why the observation is outlying, if possible. If the observation is a typographical error, then\r\nit should be corrected before continuing. If the observation is from a subject that does not belong\r\nto the population of interest, then perhaps the datum should be removed. Otherwise, perhaps the\r\nvalue is hinting at some hidden structure to the data.\r\n3.4.7 How to do it with R\r\nThe quickest way to visually identify outliers is with a boxplot, described above. Another way is\r\nwith the boxplot.stats function.\r\nExample 3.15. The rivers data. We will look for potential outliers in the rivers data.\r\n> boxplot.stats(rivers)$out\r\n[1] 1459 1450 1243 2348 3710 2315 2533 1306 1270 1885 1770\r\nWe may change the coef argument to 3 (it is 1.5 by default) to identify suspected outliers.\r\n> boxplot.stats(rivers, coef = 3)$out\r\n[1] 2348 3710 2315 2533 1885",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ea4551e3-affe-4204-a0b3-b8407697a458.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=587afaff070d24b3da7799948b7064677fc5e40a1ad60efe96be838bff0d90a9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 448
      },
      {
        "segments": [
          {
            "segment_id": "f57134d6-407f-4858-a295-8573197224ce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 61,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.5. MULTIVARIATE DATA AND DATA FRAMES 45\r\n3.4.8 Standardizing variables\r\nIt is sometimes useful to compare data sets with each other on a scale that is independent of the\r\nmeasurement units. Given a set of observed data x1, x2, . . . , xn we get z scores, denoted z1, z2, . . . ,\r\nzn, by means of the following formula\r\nzi =\r\nxi − x\r\ns\r\n, i = 1, 2, . . . , n.\r\n3.4.9 How to do it with R\r\nThe scale function will rescale a numeric vector (or data frame) by subtracting the sample mean\r\nfrom each value (column) and/or by dividing each observation by the sample standard deviation.\r\n3.5 Multivariate Data and Data Frames\r\nWe have had experience with vectors of data, which are long lists of numbers. Typically, each\r\nentry in the vector is a single measurement on a subject or experimental unit in the study. We saw\r\nin Section 2.3.3 how to form vectors with the c function or the scan function.\r\nHowever, statistical studies often involve experiments where there are two (or more) measure\u0002ments associated with each subject. We display the measured information in a rectangular array\r\nin which each row corresponds to a subject, and the columns contain the measurements for each\r\nrespective variable. For instance, if one were to measure the height and weight and hair color of\r\neach of 11 persons in a research study, the information could be represented with a rectangular\r\narray. There would be 11 rows. Each row would have the person’s height in the first column and\r\nhair color in the second column.\r\nThe corresponding objects in R are called data frames, and they can be constructed with the\r\ndata.frame function. Each row is an observation, and each column is a variable.\r\nExample 3.16. Suppose we have two vectors x and y and we want to make a data frame out of\r\nthem.\r\n> x <- 5:8\r\n> y <- letters[3:6]\r\n> A <- data.frame(v1 = x, v2 = y)\r\nNotice that x and y are the same length. This is necessary. Also notice that x is a numeric\r\nvector and y is a character vector. We may choose numeric and character vectors (or even factors)\r\nfor the columns of the data frame, but each column must be of exactly one type. That is, we can\r\nhave a column for height and a column for gender, but we will get an error if we try to mix\r\nfunction height (numeric) and gender (character or factor) information in the same column.\r\nIndexing of data frames is similar to indexing of vectors. To get the entry in row i and column\r\nj do A[i,j]. We can get entire rows and columns by omitting the other index.\r\n> A[3, ]\r\nv1 v2\r\n3 7 e",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f57134d6-407f-4858-a295-8573197224ce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=984ce6e0c42b71c0b720aac54cc2515fa0fd642a58d265f0eb88b74e30542537",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 469
      },
      {
        "segments": [
          {
            "segment_id": "f9242fc1-a98e-46b3-89fb-a447fd0c2a47",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 62,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "46 CHAPTER 3. DATA DESCRIPTION\r\n> A[1, ]\r\nv1 v2\r\n1 5 c\r\n> A[, 2]\r\n[1] c d e f\r\nLevels: c d e f\r\nThere are several things happening above. Notice that A[3,] gave a data frame (with the same\r\nentries as the third row of A) yet A[1, ] is a numeric vector. A[ ,2] is a factor vector because the\r\ndefault setting for data.frame is stringsAsFactors = TRUE.\r\nData frames have a names attribute and the names may be extracted with the names function.\r\nOnce we have the names we may extract given columns by way of the dollar sign.\r\n> names(A)\r\n[1] \"v1\" \"v2\"\r\n> A$v1\r\n[1] 5 6 7 8\r\nThe above is identical to A[ ,1].\r\n3.5.1 Bivariate Data\r\n• Stacked bar charts\r\n• odds ratio and relative risk\r\n• Introduce the sample correlation coefficient.\r\nThe sample Pearson product-moment correlation coefficient:\r\nr =\r\nPn\r\ni=1\r\n(xi − x)(yi − y)\r\npPn\r\ni=1\r\n(xi − x)\r\npPn\r\ni=1\r\n(yi − y)\r\n• independent of scale\r\n• −1 < r < 1\r\n• measures strength and direction of linear association\r\n• Two-Way Tables. Done with table, or in the R Commander by following Statistics . Con\u0002tingency Tables . Two-way Tables. You can also enter and analyze a two-way table.\r\n◦ table\r\n◦ prop.table\r\n◦ addmargins\r\n◦ rowPercents (Rcmdr)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f9242fc1-a98e-46b3-89fb-a447fd0c2a47.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ec19c5ce7334856e0a0eb1f925f0fcee8d9591955c89e3821d8a0881a8b669e4",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "97501e6a-1ab8-4e64-9298-23f2fab723fe",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 63,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 47\r\n◦ colPercents (Rcmdr)\r\n◦ totPercents(Rcmdr)\r\n◦ A <- xtabs(~ gender + race, data = RcmdrTestDrive)\r\n◦ xtabs( Freq ~ Class + Sex, data = Titanic) # from built in table\r\n◦ barplot(A, legend.text=TRUE)\r\n◦ barplot(t(A), legend.text=TRUE)\r\n◦ barplot(A, legend.text=TRUE, beside = TRUE)\r\n◦ spineplot(gender ~ race, data = RcmdrTestDrive)\r\n◦ Spine plot: plots categorical versus categorical\r\n• Scatterplot: look for linear association and correlation.\r\n◦ carb ~ optden, data = Formaldehyde (boring)\r\n◦ conc ~ rate, data = Puromycin\r\n◦ xyplot(accel ~ dist, data = attenu) nonlinear association\r\n◦ xyplot(eruptions ~ waiting, data = faithful) (linear, two groups)\r\n◦ xyplot(Petal.Width ~ Petal.Length, data = iris)\r\n◦ xyplot(pressure ~ temperature, data = pressure) (exponential growth)\r\n◦ xyplot(weight ~ height, data = women) (strong positive linear)\r\n3.5.2 Multivariate Data\r\nMultivariate Data Display\r\n• Multi-Way Tables. You can do this with table, or in R Commander by following Statistics\r\n. Contingency Tables . Multi-way Tables.\r\n• Scatterplot matrix. used for displaying pairwise scatterplots simultaneously. Again, look for\r\nlinear association and correlation.\r\n• 3D Scatterplot. See Figure 289\r\n• plot(state.region, state.division)\r\n• barplot(table(state.division,state.region), legend.text=TRUE)\r\n3.6 Comparing Populations\r\nSometimes we have data from two or more groups (or populations) and we would like to compare\r\nthem and draw conclusions. Some issues that we would like to address:\r\n• Comparing centers and spreads: variation within versus between groups\r\n• Comparing clusters and gaps\r\n• Comparing outliers and unusual features\r\n• Comparing shapes.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/97501e6a-1ab8-4e64-9298-23f2fab723fe.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5d24b5196f007e06825dfaa599cbb6b4a98c6ab0cfec9100d825de85f421a8e1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 463
      },
      {
        "segments": [
          {
            "segment_id": "b0ecb350-2014-4d1b-a04b-118bce063554",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 64,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "48 CHAPTER 3. DATA DESCRIPTION\r\n3.6.1 Numerically\r\nI am thinking here about the Statistics . Numerical Summaries . Summarize by groups option\r\nor the Statistics . Summaries .Table of Statistics option.\r\n3.6.2 Graphically\r\n• Boxplots\r\n◦ Variable width: the width of the drawn boxplots are proportional to √ni, where niis\r\nthe size of the i\r\nth group. Why? Because many statistics have variability proportional to\r\nthe reciprocal of the square root of the sample size.\r\n◦ Notches: extend to 1.58 · (hU − hL)/\r\n√\r\nn. The idea is to give roughly a 95% confidence\r\ninterval for the difference in two medians. See Chapter 10.\r\n• Stripcharts\r\n◦ stripchart(weight ~ feed, method=\"stack\", data=chickwts)\r\n• Bar Graphs\r\n◦ barplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions)) # stacked bar chart\r\n◦ barplot(xtabs(Freq ~ Admit, data = UCBAdmissions))\r\n◦ barplot(xtabs(Freq ~ Gender + Admit, data = UCBAdmissions), legend = TRUE, be\u0002side = TRUE) # oops, discrimination.\r\n◦ barplot(xtabs(Freq ~ Admit+Dept, data = UCBAdmissions), legend = TRUE, beside =\r\nTRUE) # different departments have different standards\r\n◦ barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside\r\n= TRUE) # men mostly applied to easy departments, women mostly applied to difficult\r\ndepartments\r\n◦ barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside\r\n= TRUE)\r\n◦ barchart(Admit ~ Freq, data = C)\r\n◦ barchart(Admit ~ Freq|Gender, data = C)\r\n◦ barchart(Admit ~ Freq | Dept, groups = Gender, data = C)\r\n◦ barchart(Admit ~ Freq | Dept, groups = Gender, data = C, auto.key = TRUE)\r\n• Histograms\r\n◦ ~ breaks | wool*tension, data = warpbreaks\r\n◦ ~ weight | feed, data = chickwts\r\n◦ ~ weight | group, data = PlantGrowth\r\n◦ ~ count | spray, data = InsectSprays\r\n◦ ~ len | dose, data = ToothGrowth",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b0ecb350-2014-4d1b-a04b-118bce063554.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d4ec529723788d6be049bd499732a86e3a4c13371bddb7f628ed0305d7400cf0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 295
      },
      {
        "segments": [
          {
            "segment_id": "b4daf27a-0d99-4dfa-ba60-eec766f1b687",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 65,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 49\r\n◦ ~ decrease | treatment, data = OrchardSprays (or rowpos or colpos)\r\n• Scatterplots\r\n◦ xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species)\r\n> library(lattice)\r\n> xyplot()\r\n• Scatterplot matrices\r\n◦ splom(~ cbind(GNP.deflator,GNP,Unemployed,Armed.Forces,Population,Year,Employed),\r\ndata = longley)\r\n◦ splom(~ cbind(pop15,pop75,dpi), data = LifeCycleSavings)\r\n◦ splom(~ cbind(Murder, Assault, Rape), data = USArrests)\r\n◦ splom(~ cbind(CONT, INTG, DMNR), data = USJudgeRatings)\r\n◦ splom(~ cbind(area,peri,shape,perm), data = rock)\r\n◦ splom(~ cbind(Air.Flow, Water.Temp, Acid.Conc., stack.loss), data = stackloss)\r\n◦ splom(~ cbind(Fertility,Agriculture,Examination,Education,Catholic,Infant.Mortality),\r\ndata = swiss)\r\n◦ splom(~ cbind(Fertility,Agriculture,Examination), data = swiss) (positive and negative)\r\n• Dot charts\r\n◦ dotchart(USPersonalExpenditure)\r\n◦ dotchart(t(USPersonalExpenditure))\r\n◦ dotchart(WorldPhones) (transpose is no good)\r\n◦ freeny.x is no good, neither is volcano\r\n◦ dotchart(UCBAdmissions[„1])\r\n◦ dotplot(Survived ~ Freq | Class, groups = Sex, data = B)\r\n◦ dotplot(Admit ~ Freq | Dept, groups = Gender, data = C)\r\n• Mosaic plot\r\n◦ mosaic(~ Survived + Class + Age + Sex, data = Titanic) (or just mosaic(Titanic))\r\n◦ mosaic(~ Admit + Dept + Gender, data = UCBAdmissions)\r\n• Spine plots\r\n◦ spineplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions)) # rescaled barplot\r\n• Quantile-quantile plots: There are two ways to do this. One way is to compare two indepen\u0002dent samples (of the same size). qqplot(x,y). Another way is to compare the sample quantiles\r\nof one variable to the theoretical quantiles of another distribution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b4daf27a-0d99-4dfa-ba60-eec766f1b687.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71ad5301342a8efa43449ea483b1edb3a79307643604ebd2dba0be89a0b2bd58",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "3b80f021-cf61-426a-a4a9-18fa61791a33",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 66,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "50 CHAPTER 3. DATA DESCRIPTION\r\nGiven two samples {x1, x2, . . . , xn} and {y1, y2, . . . , yn}, we may find the order statistics x(1) ≤\r\nx(2) ≤ · · · ≤ x(n) and y(1) ≤ y(2) ≤ · · · ≤ y(n). Next, plot the n points (x(1), y(1)), (x(2), y(2))\r\n,. . . ,(x(n), y(n)).\r\nIt is clear that if x(k) = y(k) for all k = 1, 2, . . . , n, then we will have a straight line. It is also\r\nclear that in the real world, a straight line is NEVER observed, and instead we have a scatterplot\r\nthat hopefully had a general linear trend. What do the rules tell us?\r\n• If the y-intercept of the line is greater (less) than zero, then the center of the Y data is greater\r\n(less) than the center of the X data.\r\n• If the slope of the line is greater (less) than one, then the spread of the Y data is greater (less)\r\nthan the spread of the X data..\r\n3.6.3 Lattice Graphics\r\nThe following types of plots are useful when there is one variable of interest and there is a factor in\r\nthe data set by which the variable is categorized.\r\nIt is sometimes nice to set lattice.options(default.theme = \"col.whitebg\")\r\nSide by side boxplots\r\n> library(lattice)\r\n> bwplot(~weight | feed, data = chickwts)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3b80f021-cf61-426a-a4a9-18fa61791a33.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8e8fb0103ab32da96f0b85d76592698a6aea78b4daae1581fbdad031e75ce46a",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "69ecc6c4-0b2a-4c53-b499-2dc0666b0023",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 67,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 51\r\nweight\r\n100 200 300 400\r\n●\r\ncasein\r\n●\r\nhorsebean\r\n100 200 300 400\r\n●\r\nlinseed\r\n●\r\nmeatmeal\r\n100 200 300 400\r\n●\r\nsoybean\r\n● ● ● ●\r\nsunflower\r\nFigure 3.6.1: Boxplots of weight by feed type in the chickwts data\r\nHistograms\r\n> histogram(~age | education, data = infert)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/69ecc6c4-0b2a-4c53-b499-2dc0666b0023.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c463869fb1366031dcae04090b6af7105ef9caac28cb75eee30359acbfecc202",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 510
      },
      {
        "segments": [
          {
            "segment_id": "09141a00-8404-46f7-8675-5d82f45bac79",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 68,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "52 CHAPTER 3. DATA DESCRIPTION\r\nage\r\nPercent of Total\r\n0\r\n10\r\n20\r\n30\r\n20 25 30 35 40 45\r\n0−5yrs 6−11yrs\r\n0\r\n10\r\n20\r\n30\r\n12+ yrs\r\nFigure 3.6.2: Histograms of age by education level from the infert data\r\nScatterplots\r\n> xyplot(Petal.Length ~ Petal.Width | Species, data = iris)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/09141a00-8404-46f7-8675-5d82f45bac79.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cc10b8aa870d4ad972886daa83f358d4316bdb6576851a2d61a6a92da0c20192",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "776bed7a-3938-4fd9-8779-866c0661bfc7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 69,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 53\r\nPetal.Width\r\nPetal.Length\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n0.0 0.5 1.0 1.5 2.0 2.5\r\n●●● ● ●●●● ●● ●● ●●●\r\n●\r\n●●● ●● ●● ● ●●●● ● ● ●●●●\r\nsetosa\r\n●●\r\n●\r\n●\r\n● ●●\r\n●\r\n●\r\n● ●\r\n● ●\r\n●\r\n●\r\n●● ●●\r\n●\r\n●\r\n● ●\r\n●●\r\n● ●\r\n●\r\n●●●●\r\n●\r\n● ●●● ●●● ●\r\n●\r\n●\r\n●●●\r\n●\r\n●\r\nversicolor\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n●\r\n●\r\n● ● ●\r\n●\r\n●\r\n●\r\n● ●\r\n●●\r\n●\r\n● ●● ●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n● ●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n● ● ●\r\n● ●\r\n●● ●● ●\r\nvirginica\r\nFigure 3.6.3: An xyplot of Petal.Length versus Petal.Width by Species in the iris data\r\nCoplots\r\n> coplot(conc ~ uptake | Type * Treatment, data = CO2)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/776bed7a-3938-4fd9-8779-866c0661bfc7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=60cc4192270fea8e0d9a079ec2f9c55257ee28dc3bd3d90617cf301e332605c5",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "70622c04-a2f9-455a-8f04-524e80ebd35c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 70,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "54 CHAPTER 3. DATA DESCRIPTION\r\nNULL\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n200 600 1000\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n10 20 30 40\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n10 20 30 40\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n200 600 1000\r\nuptake\r\nconc\r\nQuebec\r\nMississippi\r\nGiven : Type\r\nnonchilled\r\nchilled\r\nGiven : Treatment\r\nFigure 3.6.4: A coplot of conc versus uptake by Type and Treatment in the CO2 data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/70622c04-a2f9-455a-8f04-524e80ebd35c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=18910246fc43e0b0744cb92781b68351b7782913858ad4395a2536a48544ee80",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 327
      },
      {
        "segments": [
          {
            "segment_id": "ba9d3596-19eb-447c-a388-e7225f4abb68",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 71,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 55\r\nChapter Exercises\r\nDirections: Open R and issue the following commands at the command line to get started. Note\r\nthat you need to have the RcmdrPlugin.IPSUR package installed, and for some exercises you need\r\nthe e1071 package.\r\nlibrary(RcmdrPlugin.IPSUR)\r\ndata(RcmdrTestDrive)\r\nattach(RcmdrTestDrive)\r\nnames(RcmdrTestDrive) # shows names of variables\r\nTo load the data in the R Commander (Rcmdr), click the Data Set button, and select RcmdrTestDrive\r\nas the active data set. To learn more about the data set and where it comes from, type ?RcmdrTestDrive\r\nat the command line.\r\nExercise 3.1. Perform a summary of all variables in RcmdrTestDrive. You can do this with the\r\ncommand\r\nsummary( RcmdrTestDrive )\r\nAlternatively, you can do this in the Rcmdr with the sequence Statistics . Summaries . Active\r\nData Set. Report the values of the summary statistics for each variable.\r\nAnswer:\r\n> summary(RcmdrTestDrive)\r\norder race smoke gender salary\r\nMin. : 1.00 AfAmer: 18 No :134 Female:95 Min. :11.62\r\n1st Qu.: 42.75 Asian : 8 Yes: 34 Male :73 1st Qu.:15.93\r\nMedian : 84.50 Other : 16 Median :17.59\r\nMean : 84.50 White :126 Mean :17.10\r\n3rd Qu.:126.25 3rd Qu.:18.46\r\nMax. :168.00 Max. :21.19\r\nreduction before after parking\r\nMin. :4.904 Min. :51.17 Min. :48.79 Min. : 1.000\r\n1st Qu.:5.195 1st Qu.:63.36 1st Qu.:62.80 1st Qu.: 1.000\r\nMedian :5.501 Median :67.62 Median :66.94 Median : 2.000\r\nMean :5.609 Mean :67.36 Mean :66.85 Mean : 2.524\r\n3rd Qu.:5.989 3rd Qu.:71.28 3rd Qu.:70.88 3rd Qu.: 3.000\r\nMax. :6.830 Max. :89.96 Max. :89.89 Max. :18.000\r\nExercise 3.2. Make a table of the race variable. Do this with Statistics . Summaries . IPSUR -\r\nFrequency Distributions...\r\n1. Which ethnicity has the highest frequency?\r\n2. Which ethnicity has the lowest frequency?\r\n3. Include a bar graph of race. Do this with Graphs . IPSUR - Bar Graph...",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ba9d3596-19eb-447c-a388-e7225f4abb68.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2f4c502cf0deb928f5ad5f63a4a15a3e92a16ab7b5793b24f841b927a4a23ea6",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "88b2feab-ee99-4741-896a-79652ecd8bc4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 72,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "56 CHAPTER 3. DATA DESCRIPTION\r\nSolution: First we will make a table of the race variable with the table function.\r\n> table(race)\r\nrace\r\nAfAmer Asian Other White\r\n18 8 16 126\r\n1. For these data, White has the highest frequency.\r\n2. For these data, Asian has the lowest frequency.\r\n3. The graph is shown below.\r\nAfAmer Asian Other White\r\nrace\r\nFrequency\r\n0 20 40 60 80 120\r\nExercise 3.3. Calculate the average salary by the factor gender. Do this with Statistics . Sum\u0002maries . Table of Statistics...\r\n1. Which gender has the highest mean salary?\r\n2. Report the highest mean salary.\r\n3. Compare the spreads for the genders by calculating the standard deviation of salary by gen\u0002der. Which gender has the biggest standard deviation?",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/88b2feab-ee99-4741-896a-79652ecd8bc4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9aa41435454de8d7fe1c2caed1ed6d140203fc1e2cd1f57059334c5e4419b54d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 421
      },
      {
        "segments": [
          {
            "segment_id": "ddd6d1c7-7943-4123-be8c-0e8fba4e23d5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 73,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 57\r\n4. Make boxplots of salary by gender with the following method:\r\nOn the Rcmdr, click Graphs . IPSUR - Boxplot...\r\nIn the Variable box, select salary.\r\nClick the Plot by groups... box and select gender. Click OK.\r\nClick OK to graph the boxplot.\r\nHow does the boxplot compare to your answers to (1) and (3)?\r\nSolution: We can generate a table listing the average salaries by gender with two methods. The\r\nfirst uses tapply:\r\n> x <- tapply(salary, list(gender = gender), mean)\r\n> x\r\ngender\r\nFemale Male\r\n16.46353 17.93035\r\nThe second method uses the by function:\r\n> by(salary, gender, mean, na.rm = TRUE)\r\ngender: Female\r\n[1] 16.46353\r\n--------------------------------------------------------\r\ngender: Male\r\n[1] 17.93035\r\nNow to answer the questions:\r\n1. Which gender has the highest mean salary?\r\nWe can answer this by looking above. For these data, the gender with the highest mean salary\r\nis Male.\r\n2. Report the highest mean salary.\r\nDepending on our answer above, we would do something like\r\nmean(salary[gender == Male ])\r\nfor example. For these data, the highest mean salary is\r\n> x[which(x == max(x))]\r\nMale\r\n17.93035",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ddd6d1c7-7943-4123-be8c-0e8fba4e23d5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bdea0d2c1edf21ab9a092b34024a06345de9b0b942c7ea72897303146006b4e8",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "fdf2cd17-bafb-4abc-8d0d-6a5b18e84f1d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 74,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "58 CHAPTER 3. DATA DESCRIPTION\r\n3. Compare the spreads for the genders by calculating the standard deviation of salary by gen\u0002der. Which gender has the biggest standard deviation?\r\n> y <- tapply(salary, list(gender = gender), sd)\r\n> y\r\ngender\r\nFemale Male\r\n2.122113 1.077183\r\nFor these data, the the largest standard deviation is approximately 2.12 which was attained\r\nby the Female gender.\r\n4. Make boxplots of salary by gender. How does the boxplot compare to your answers to (1)\r\nand (3)?\r\nThe graph is shown below.\r\nFemale Male\r\n12 14 16 18 20\r\nsalary\r\ngender",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/fdf2cd17-bafb-4abc-8d0d-6a5b18e84f1d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3d1139539dc0bea6d3d4db38e756bc34c799608fa39420af60523b89a05d72cd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 278
      },
      {
        "segments": [
          {
            "segment_id": "5cb22e19-0cf3-4a55-a91a-315c2863b5f3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 75,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 59\r\nAnswers will vary. There should be some remarks that the center of the box is farther to the\r\nright for the Male gender, and some recognition that the box is wider for the Female gender.\r\nExercise 3.4. For this problem we will study the variable reduction.\r\n1. Find the order statistics and store them in a vector x. Hint: x <- sort(reduction)\r\n2. Find x(137), the 137th order statistic.\r\n3. Find the IQR.\r\n4. Find the Five Number Summary (5NS).\r\n5. Use the 5NS to calculate what the width of a boxplot of reduction would be.\r\n6. Compare your answers (3) and (5). Are they the same? If not, are they close?\r\n7. Make a boxplot of reduction, and include the boxplot in your report. You can do this with\r\nthe boxplot function, or in Rcmdr with Graphs . IPSUR - Boxplot...\r\n8. Are there any potential/suspected outliers? If so, list their values. Hint: use your answer to\r\n(a).\r\n9. Using the rules discussed in the text, classify answers to (8), if any, as potential or suspected\r\noutliers.\r\nAnswers:\r\n> x[137]\r\n[1] 6.101618\r\n> IQR(x)\r\n[1] 0.7943932\r\n> fivenum(x)\r\n[1] 4.903922 5.193638 5.501241 5.989846 6.830096\r\n> fivenum(x)[4] - fivenum(x)[2]\r\n[1] 0.796208\r\nCompare your answers (3) and (5). Are they the same? If not, are they close?\r\nYes, they are close, within 0.00181484542950905 of each other.\r\nThe boxplot of reduction is below.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5cb22e19-0cf3-4a55-a91a-315c2863b5f3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=aa3f3f876e8a946193ddf80959415a80db423701629d5aaa4abf0d68a4c6b770",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c20f11e6-15f1-4b8a-8cc9-5e7a5fa81a5c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 76,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "60 CHAPTER 3. DATA DESCRIPTION\r\n5.0 5.5 6.0 6.5\r\nreduction\r\n> temp <- fivenum(x)\r\n> inF <- 1.5 * (temp[4] - temp[2]) + temp[4]\r\n> outF <- 3 * (temp[4] - temp[2]) + temp[4]\r\n> which(x > inF)\r\ninteger(0)\r\n> which(x > outF)\r\ninteger(0)\r\nObservations would be considered potential outliers, while observation(s) would be considered\r\na suspected outlier.\r\nExercise 3.5. In this problem we will compare the variables before and after. Don’t forget library(e1071).\r\n1. Examine the two measures of center for both variables. Judging from these measures, which\r\nvariable has a higher center?\r\n2. Which measure of center is more appropriate for before? (You may want to look at a boxplot.)\r\nWhich measure of center is more appropriate for after?\r\n3. Based on your answer to (2), choose an appropriate measure of spread for each variable,\r\ncalculate it, and report its value. Which variable has the biggest spread? (Note that you need\r\nto make sure that your measures are on the same scale.)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c20f11e6-15f1-4b8a-8cc9-5e7a5fa81a5c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=729e038656f909c3101aa00911751ba699a8f9d7c294bb8cd5800c61a38e590e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 399
      },
      {
        "segments": [
          {
            "segment_id": "e0a6f454-b0c8-43ad-8392-36491f7d73a6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 77,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 61\r\n4. Calculate and report the skewness and kurtosis for before. Based on these values, how would\r\nyou describe the shape of before?\r\n5. Calculate and report the skewness and kurtosis for after. Based on these values, how would\r\nyou describe the shape of after?\r\n6. Plot histograms of before and after and compare them to your answers to (4) and (5).\r\nSolution:\r\n1. Examine the two measures of center for both variables that you found in problem 1. Judging\r\nfrom these measures, which variable has a higher center?\r\nWe may take a look at the summary(RcmdrTestDrive) output from Exercise 3.1. Here we\r\nwill repeat the relevant summary statistics.\r\n> c(mean(before), median(before))\r\n[1] 67.36338 67.61824\r\n> c(mean(after), median(after))\r\n[1] 66.85215 66.93608\r\nThe idea is to look at the two measures and compare them to make a decision. In a nice\r\nworld, both the mean and median of one variable will be larger than the other which sends a\r\nnice message. If We get a mixed message, then we should look for other information, such\r\nas extreme values in one of the variables, which is one of the reasons for the next part of the\r\nproblem.\r\n2. Which measure of center is more appropriate for before? (You may want to look at a boxplot.)\r\nWhich measure of center is more appropriate for after?\r\nThe boxplot of before is shown below.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e0a6f454-b0c8-43ad-8392-36491f7d73a6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4efee9a30fcea1eed69deb19eb9f247d13a9ce849472041a571b5502bb7e5ff9",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "1ff0d302-d87f-4a31-90a0-16837caf6ba1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 78,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "62 CHAPTER 3. DATA DESCRIPTION\r\n● ● ●●\r\n50 60 70 80 90\r\nbefore\r\nWe want to watch out for extreme values (shown as circles separated from the box) or large\r\ndepartures from symmetry. If the distribution is fairly symmetric then the mean and median\r\nshould be approximately the same. But if the distribution is highly skewed with extreme\r\nvalues then we should be skeptical of the sample mean, and fall back to the median which\r\nis resistant to extremes. By design, the before variable is set up to have a fairly symmetric\r\ndistribution.\r\nA boxplot of after is shown next.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1ff0d302-d87f-4a31-90a0-16837caf6ba1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bffad8b96c498456076bfe3bb20d8d3ccce8b8eda35570b278fcc5ed7dcaebb2",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "12fabac8-0d15-4c97-83ed-2d69a6ae0746",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 79,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 63\r\n● ●\r\n50 60 70 80 90\r\nafter\r\nThe same remarks apply to the after variable. The after variable has been designed to be\r\nleft-skewed. . . thus, the median would likely be a good choice for this variable.\r\n3. Based on your answer to (2), choose an appropriate measure of spread for each variable,\r\ncalculate it, and report its value. Which variable has the biggest spread? (Note that you need\r\nto make sure that your measures are on the same scale.)\r\nSince before has a symmetric, mound shaped distribution, an excellent measure of center\r\nwould be the sample standard deviation. And since after is left-skewed, we should use the\r\nmedian absolute deviation. It is also acceptable to use the IQR, but we should rescale it\r\nappropriately, namely, by dividing by 1.349. The exact values are shown below.\r\n> sd(before)\r\n[1] 6.201724\r\n> mad(after)\r\n[1] 6.095189",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/12fabac8-0d15-4c97-83ed-2d69a6ae0746.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4879a41f6007759af6cbd15fdbd09e0de2379a8698493042eb85dab0da4df12d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 483
      },
      {
        "segments": [
          {
            "segment_id": "04f304a0-87fb-4759-b3b2-ca5321091574",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 80,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "64 CHAPTER 3. DATA DESCRIPTION\r\n> IQR(after)/1.349\r\n[1] 5.986954\r\nJudging from the values above, we would decide which variable has the higher spread. Look\r\nat how close the mad and the IQR (after suitable rescaling) are; it goes to show why the\r\nrescaling is important.\r\n4. Calculate and report the skewness and kurtosis for before. Based on these values, how would\r\nyou describe the shape of before?\r\nThe values of these descriptive measures are shown below.\r\n> library(e1071)\r\n> skewness(before)\r\n[1] 0.4016912\r\n> kurtosis(before)\r\n[1] 1.542225\r\nWe should take the sample skewness value and compare it to 2 √6/n ≈0.378 in absolute\r\nvalue to see if it is substantially different from zero. The direction of skewness is decided by\r\nthe sign (positive or negative) of the skewness value.\r\nWe should take the sample kurtosis value and compare it to 2 ·\r\n√\r\n24/168 ≈0.756), in absolute\r\nvalue to see if the excess kurtosis is substantially different from zero. And take a look at the\r\nsign to see whether the distribution is platykurtic or leptokurtic.\r\n5. Calculate and report the skewness and kurtosis for after. Based on these values, how would\r\nyou describe the shape of after?\r\nThe values of these descriptive measures are shown below.\r\n> skewness(after)\r\n[1] 0.3235134\r\n> kurtosis(after)\r\n[1] 1.452301\r\nWe should do for this one just like we did previously. We would again compare the sample\r\nskewness and kurtosis values (in absolute value) to 0.378 and 0.756, respectively.\r\n6. Plot histograms of before and after and compare them to your answers to (4) and (5).\r\nThe graphs are shown below.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/04f304a0-87fb-4759-b3b2-ca5321091574.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f45e8588b135c419f07dd4c3fb7c0eb817b8e9e43b9d719570d30a12d6fc9780",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "ddf7da7e-c795-4513-98b8-c37266389529",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 81,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "3.6. COMPARING POPULATIONS 65\r\nHistogram of before\r\nbefore\r\nFrequency\r\n50 60 70 80 90\r\n0 10 20 30 40 50",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ddf7da7e-c795-4513-98b8-c37266389529.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=77ccf124360ed76cfdc029d424ac487d7a7fd62f9996b23d1d930459b1fbd1eb",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "55cd7c72-f6f3-425f-8770-817f03f45511",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 82,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "66 CHAPTER 3. DATA DESCRIPTION\r\nHistogram of after\r\nafter\r\nFrequency\r\n50 60 70 80 90\r\n0 10 20 30 40 50 60\r\nAnswers will vary. We are looking for visual consistency in the histograms to our statements\r\nabove.\r\nExercise 3.6. Describe the following data sets just as if you were communicating with an alien,\r\nbut one who has had a statistics class. Mention the salient features (data type, important proper\u0002ties, anything special). Support your answers with the appropriate visual displays and descriptive\r\nstatistics.\r\n1. Conversion rates of Euro currencies stored in euro.\r\n2. State abbreviations stored in state.abb.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/55cd7c72-f6f3-425f-8770-817f03f45511.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=978b6620110cacbcd53f3db5843c8919022029779ab5d0d38a772625a2dd2685",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 382
      },
      {
        "segments": [
          {
            "segment_id": "1f462ccd-1a51-4875-bfd7-097d2e86ea90",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 83,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 4\r\nProbability\r\nIn this chapter we define the basic terminology associated with probability and derive some of its\r\nproperties. We discuss three interpretations of probability. We discuss conditional probability and\r\nindependent events, along with Bayes’ Theorem. We finish the chapter with an introduction to\r\nrandom variables, which paves the way for the next two chapters.\r\nIn this book we distinguish between two types of experiments: deterministic and random. A\r\ndeterministic experiment is one whose outcome may be predicted with certainty beforehand, such\r\nas combining Hydrogen and Oxygen, or adding two numbers such as 2 + 3. A random experiment\r\nis one whose outcome is determined by chance. We posit that the outcome of a random experiment\r\nmay not be predicted with certainty beforehand, even in principle. Examples of random experi\u0002ments include tossing a coin, rolling a die, and throwing a dart on a board, how many red lights\r\nyou encounter on the drive home, how many ants traverse a certain patch of sidewalk over a short\r\nperiod, etc.\r\nWhat do I want them to know?\r\n• that there are multiple interpretations of probability, and the methods used depend somewhat\r\non the philosophy chosen\r\n• nuts and bolts of basic probability jargon: sample spaces, events, probability functions, etc.\r\n• how to count\r\n• conditional probability and its relationship with independence\r\n• Bayes’ Rule and how it relates to the subjective view of probability\r\n• what we mean by ’random variables’, and where they come from\r\n4.1 Sample Spaces\r\nFor a random experiment E, the set of all possible outcomes of E is called the sample space and\r\nis denoted by the letter S . For a coin-toss experiment, S would be the results “Head” and “Tail”,\r\n67",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1f462ccd-1a51-4875-bfd7-097d2e86ea90.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=95db370e574738f469202111184deb614af9a54a82404882424a9adc5ae466d8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 287
      },
      {
        "segments": [
          {
            "segment_id": "9763a645-cc13-4683-8a95-2e07339a9fc1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 84,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "68 CHAPTER 4. PROBABILITY\r\nDETERMINISTIC\r\n2H2 + O2 → H2O\r\n3 + 4 = 7\r\nExperiments\r\nRANDOM\r\ntoss coin, roll die\r\ncount ants on sidewalk\r\nmeasure rainfall\r\nFigure 4.0.1: Two types of experiments\r\nwhich we may represent by S = {H, T}. Formally, the performance of a random experiment is the\r\nunpredictable selection of an outcome in S .\r\n4.1.1 How to do it with R\r\nMost of the probability work in this book is done with the prob package [52]. A sample space\r\nis (usually) represented by a data frame, that is, a rectangular collection of variables (see Section\r\n3.5.2). Each row of the data frame corresponds to an outcome of the experiment. The data frame\r\nchoice is convenient both for its simplicity and its compatibility with the R Commander. Data\r\nframes alone are, however, not sufficient to describe some of the more interesting probabilistic\r\napplications we will study later; to handle those we will need to consider a more general list data\r\nstructure. See Section 4.6.3 for details.\r\nExample 4.1. Consider the random experiment of dropping a Styrofoam cup onto the floor from\r\na height of four feet. The cup hits the ground and eventually comes to rest. It could land upside\r\ndown, right side up, or it could land on its side. We represent these possible outcomes of the random\r\nexperiment by the following.\r\n> S <- data.frame(lands = c(\"down\", \"up\", \"side\"))\r\n> S\r\nlands\r\n1 down\r\n2 up\r\n3 side\r\nThe sample space S contains the column lands which stores the outcomes \"down\", \"up\", and\r\n\"side\".",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9763a645-cc13-4683-8a95-2e07339a9fc1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d936dd5729c7347e27d85d9468c9247318bb073077aa5d27efe3361fcac02995",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 260
      },
      {
        "segments": [
          {
            "segment_id": "7e3eb4e1-e106-4b73-9611-88348b13d1c8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 85,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.1. SAMPLE SPACES 69\r\nSome sample spaces are so common that convenience wrappers were written to set them up with\r\nminimal effort. The underlying machinery that does the work includes the expand.grid function\r\nin the base package, combn in the combinat package [14], and permsn in the prob package1.\r\nConsider the random experiment of tossing a coin. The outcomes are H and T. We can set up\r\nthe sample space quickly with the tosscoin function:\r\n> library(prob)\r\n> tosscoin(1)\r\ntoss1\r\n1 H\r\n2 T\r\nThe number 1 tells tosscoin that we only want to toss the coin once. We could toss it three\r\ntimes:\r\n> tosscoin(3)\r\ntoss1 toss2 toss3\r\n1 H H H\r\n2 T H H\r\n3 H T H\r\n4 T T H\r\n5 H H T\r\n6 T H T\r\n7 H T T\r\n8 T T T\r\nAlternatively we could roll a fair die:\r\n> rolldie(1)\r\nX1\r\n1 1\r\n2 2\r\n3 3\r\n4 4\r\n5 5\r\n6 6\r\nThe rolldie function defaults to a 6-sided die, but we can specify others with the nsides\r\nargument. The command rolldie(3, nsides = 4) would be used to roll a 4-sided die three\r\ntimes.\r\nPerhaps we would like to draw one card from a standard set of playing cards (it is a long data\r\nframe):\r\n> head(cards())\r\n1The seasoned R user can get the job done without the convenience wrappers. I encourage the beginner to use them\r\nto get started, but I also recommend that introductory students wean themselves as soon as possible. The wrappers were\r\ndesigned for ease and intuitive use, not for speed or efficiency.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7e3eb4e1-e106-4b73-9611-88348b13d1c8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f32200a8874fdd2f2cf192eee4100df49f6df36604314cf855424de29da6b1ef",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 269
      },
      {
        "segments": [
          {
            "segment_id": "23debbe2-de8e-4360-80dd-ca8851713168",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 86,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "70 CHAPTER 4. PROBABILITY\r\nrank suit\r\n1 2 Club\r\n2 3 Club\r\n3 4 Club\r\n4 5 Club\r\n5 6 Club\r\n6 7 Club\r\nThe cards function that we just used has optional arguments jokers (if you would like Jokers\r\nto be in the deck) and makespace which we will discuss later. There is also a roulette function\r\nwhich returns the sample space associated with one spin on a roulette wheel. There are EU and\r\nUSA versions available. Interested readers may contribute any other game or sample spaces that\r\nmay be of general interest.\r\n4.1.2 Sampling from Urns\r\nThis is perhaps the most fundamental type of random experiment. We have an urn that contains a\r\nbunch of distinguishable objects (balls) inside. We shake up the urn, reach inside, grab a ball, and\r\ntake a look. That’s all.\r\nBut there are all sorts of variations on this theme. Maybe we would like to grab more than one\r\nball – say, two balls. What are all of the possible outcomes of the experiment now? It depends on\r\nhow we sample. We could select a ball, take a look, put it back, and sample again. Another way\r\nwould be to select a ball, take a look – but do not put it back – and sample again (equivalently, just\r\nreach in and grab two balls). There are certainly more possible outcomes of the experiment in the\r\nformer case than in the latter. In the first (second) case we say that sampling is done with (without)\r\nreplacement.\r\nThere is more. Suppose we do not actually keep track of which ball came first. All we observe\r\nare the two balls, and we have no idea about the order in which they were selected. We call this\r\nunordered sampling (in contrast to ordered) because the order of the selections does not matter\r\nwith respect to what we observe. We might as well have selected the balls and put them in a bag\r\nbefore looking.\r\nNote that this one general class of random experiments contains as a special case all of the\r\ncommon elementary random experiments. Tossing a coin twice is equivalent to selecting two balls\r\nlabeled H and T from an urn, with replacement. The die-roll experiment is equivalent to selecting\r\na ball from an urn with six elements, labeled 1 through 6.\r\n4.1.3 How to do it with R\r\nThe prob package accomplishes sampling from urns with the urnsamples function, which has\r\narguments x, size, replace, and ordered. The argument x represents the urn from which sam\u0002pling is to be done. The size argument tells how large the sample will be. The ordered and\r\nreplace arguments are logical and specify how sampling will be performed. We will discuss each\r\nin turn.\r\nExample 4.2. Let our urn simply contain three balls, labeled 1, 2, and 3, respectively. We are\r\ngoing to take a sample of size 2 from the urn.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/23debbe2-de8e-4360-80dd-ca8851713168.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f68e0ed861174914b9fd9b5347ca8870f13919fece25b5b2164ee92d83f575e5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 484
      },
      {
        "segments": [
          {
            "segment_id": "27e4f011-8618-4414-a617-5cccb722eed2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 87,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.1. SAMPLE SPACES 71\r\nOrdered, With Replacement\r\nIf sampling is with replacement, then we can get any outcome 1, 2, or 3 on any draw. Further, by\r\n“ordered” we mean that we shall keep track of the order of the draws that we observe. We can\r\naccomplish this in R with\r\n> urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)\r\nX1 X2\r\n1 1 1\r\n2 2 1\r\n3 3 1\r\n4 1 2\r\n5 2 2\r\n6 3 2\r\n7 1 3\r\n8 2 3\r\n9 3 3\r\nNotice that rows 2 and 4 are identical, save for the order in which the numbers are shown.\r\nFurther, note that every possible pair of the numbers 1 through 3 are listed. This experiment is\r\nequivalent to rolling a 3-sided die twice, which we could have accomplished with rolldie(2,\r\nnsides = 3).\r\nOrdered, Without Replacement\r\nHere sampling is without replacement, so we may not observe the same number twice in any row.\r\nOrder is still important, however, so we expect to see the outcomes 1,2 and 2,1 somewhere in our\r\ndata frame.\r\n> urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)\r\nX1 X2\r\n1 1 2\r\n2 2 1\r\n3 1 3\r\n4 3 1\r\n5 2 3\r\n6 3 2\r\nThis is just as we expected. Notice that there are less rows in this answer due to the more\r\nrestrictive sampling procedure. If the numbers 1, 2, and 3 represented “Fred”, “Mary”, and “Sue”,\r\nrespectively, then this experiment would be equivalent to selecting two people of the three to serve\r\nas president and vice-president of a company, respectively, and the sample space shown above lists\r\nall possible ways that this could be done.\r\nUnordered, Without Replacement\r\nAgain, we may not observe the same outcome twice, but in this case, we will only retain those\r\noutcomes which (when jumbled) would not duplicate earlier ones.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/27e4f011-8618-4414-a617-5cccb722eed2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=00a3c308017a072ae086f028c06221cf2d87899db363623f2ef6277894b1f499",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 317
      },
      {
        "segments": [
          {
            "segment_id": "5bce1fff-aa04-4c42-98d7-f7b806e426c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 88,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "72 CHAPTER 4. PROBABILITY\r\n> urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE)\r\nX1 X2\r\n1 1 2\r\n2 1 3\r\n3 2 3\r\nThis experiment is equivalent to reaching in the urn, picking a pair, and looking to see what\r\nthey are. This is the default setting of urnsamples, so we would have received the same output by\r\nsimply typing urnsamples(1:3, 2).\r\nUnordered, With Replacement\r\nThe last possibility is perhaps the most interesting. We replace the balls after every draw, but we\r\ndo not remember the order in which the draws came.\r\n> urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE)\r\nX1 X2\r\n1 1 1\r\n2 1 2\r\n3 1 3\r\n4 2 2\r\n5 2 3\r\n6 3 3\r\nWe may interpret this experiment in a number of alternative ways. One way is to consider this\r\nas simply putting two 3-sided dice in a cup, shaking the cup, and looking inside – as in a game\r\nof Liar’s Dice, for instance. Each row of the sample space is a potential pair we could observe.\r\nAnother way is to view each outcome as a separate method to distribute two identical golf balls into\r\nthree boxes labeled 1, 2, and 3. Regardless of the interpretation, urnsamples lists every possible\r\nway that the experiment can conclude.\r\nNote that the urn does not need to contain numbers; we could have just as easily taken our\r\nurn to be x = c(\"Red\",\"Blue\",\"Green\"). But, there is an important point to mention before\r\nproceeding. Astute readers will notice that in our example, the balls in the urn were distinguishable\r\nin the sense that each had a unique label to distinguish it from the others in the urn. A natural\r\nquestion would be, “What happens if your urn has indistinguishable elements, for example, what\r\nif x = c(\"Red\",\"Red\",\"Blue\")?” The answer is that urnsamples behaves as if each ball in\r\nthe urn is distinguishable, regardless of its actual contents. We may thus imagine that while there\r\nare two red balls in the urn, the balls are such that we can tell them apart (in principle) by looking\r\nclosely enough at the imperfections on their surface.\r\nIn this way, when the x argument of urnsamples has repeated elements, the resulting sam\u0002ple space may appear to be ordered = TRUE even when, in fact, the call to the function was\r\nurnsamples(..., ordered = FALSE). Similar remarks apply for the replace argument.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5bce1fff-aa04-4c42-98d7-f7b806e426c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=078dae1957a9767cbd21475a46f11fdbfc11327af57151d33275a2090125c42f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 407
      },
      {
        "segments": [
          {
            "segment_id": "4880f45e-5eee-4c8c-b7af-f4678e14c5f9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 89,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.2. EVENTS 73\r\n4.2 Events\r\nAn event A is merely a collection of outcomes, or in other words, a subset of the sample space2.\r\nAfter the performance of a random experiment E we say that the event A occurred if the experi\u0002ment’s outcome belongs to A. We say that a bunch of events A1, A2, A3, . . . are mutually exclusive\r\nor disjoint if Ai ∩ Aj = ∅ for any distinct pair Ai , Aj. For instance, in the coin-toss experiment the\r\nevents A = {Heads} and B = {Tails} would be mutually exclusive. Now would be a good time to\r\nreview the algebra of sets in Appendix E.1.\r\n4.2.1 How to do it with R\r\nGiven a data frame sample/probability space S, we may extract rows using the [] operator:\r\n> S <- tosscoin(2, makespace = TRUE)\r\ntoss1 toss2 probs\r\n1 H H 0.25\r\n2 T H 0.25\r\n3 H T 0.25\r\n4 T T 0.25\r\n> S[1:3, ]\r\ntoss1 toss2 probs\r\n1 H H 0.25\r\n2 T H 0.25\r\n3 H T 0.25\r\n> S[c(2, 4), ]\r\ntoss1 toss2 probs\r\n2 T H 0.25\r\n4 T T 0.25\r\nand so forth. We may also extract rows that satisfy a logical expression using the subset\r\nfunction, for instance\r\n> S <- cards()\r\n> subset(S, suit == \"Heart\")\r\nrank suit\r\n27 2 Heart\r\n28 3 Heart\r\n29 4 Heart\r\n30 5 Heart\r\n31 6 Heart\r\n2This naive definition works for finite or countably infinite sample spaces, but is inadequate for sample spaces in general.\r\nIn this book, we will not address the subtleties that arise, but will refer the interested reader to any text on advanced\r\nprobability or measure theory.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4880f45e-5eee-4c8c-b7af-f4678e14c5f9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4886db6e1257aa88b02627d53ffa1645327d6bc8a2dbd997b3fa452d0a40ca28",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "f49c1bde-f953-476b-8536-a773b5bf06fb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 90,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "74 CHAPTER 4. PROBABILITY\r\n32 7 Heart\r\n33 8 Heart\r\n34 9 Heart\r\n35 10 Heart\r\n36 J Heart\r\n37 Q Heart\r\n38 K Heart\r\n39 A Heart\r\n> subset(S, rank %in% 7:9)\r\nrank suit\r\n6 7 Club\r\n7 8 Club\r\n8 9 Club\r\n19 7 Diamond\r\n20 8 Diamond\r\n21 9 Diamond\r\n32 7 Heart\r\n33 8 Heart\r\n34 9 Heart\r\n45 7 Spade\r\n46 8 Spade\r\n47 9 Spade\r\nWe could continue indefinitely. Also note that mathematical expressions are allowed:\r\n> subset(rolldie(3), X1 + X2 + X3 > 16)\r\nX1 X2 X3\r\n180 6 6 5\r\n210 6 5 6\r\n215 5 6 6\r\n216 6 6 6\r\n4.2.2 Functions for Finding Subsets\r\nIt does not take long before the subsets of interest become complicated to specify. Yet the main\r\nidea remains: we have a particular logical condition to apply to each row. If the row satisfies the\r\ncondition, then it should be in the subset. It should not be in the subset otherwise. The ease with\r\nwhich the condition may be coded depends of course on the question being asked. Here are a few\r\nfunctions to get started.\r\nThe %in% function\r\nThe function %in% helps to learn whether each value of one vector lies somewhere inside another\r\nvector.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f49c1bde-f953-476b-8536-a773b5bf06fb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=af3781dd1b50cfe70a6c8c1f4706a3ef309905018881bc279a90d110fe485737",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 493
      },
      {
        "segments": [
          {
            "segment_id": "ef7d2c57-200d-48ee-899f-a02f811bc131",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 91,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.2. EVENTS 75\r\n> x <- 1:10\r\n> y <- 8:12\r\n> y %in% x\r\n[1] TRUE TRUE TRUE FALSE FALSE\r\nNotice that the returned value is a vector of length 5 which tests whether each element of y is\r\nin x, in turn.\r\nThe isin function\r\nIt is more common to want to know whether the whole vector y is in x. We can do this with the\r\nisin function.\r\n> isin(x, y)\r\n[1] FALSE\r\nOf course, one may ask why we did not try something like all(y %in% x), which would give\r\na single result, TRUE. The reason is that the answers are different in the case that y has repeated\r\nvalues. Compare:\r\n> x <- 1:10\r\n> y <- c(3, 3, 7)\r\n> all(y %in% x)\r\n[1] TRUE\r\n> isin(x, y)\r\n[1] FALSE\r\nThe reason for the above is of course that x contains the value 3, but x does not have two\r\n3’s. The difference is important when rolling multiple dice, playing cards, etc. Note that there is\r\nan optional argument ordered which tests whether the elements of y appear in x in the order in\r\nwhich they are appear in y. The consequences are\r\n> isin(x, c(3, 4, 5), ordered = TRUE)\r\n[1] TRUE\r\n> isin(x, c(3, 5, 4), ordered = TRUE)\r\n[1] FALSE\r\nThe connection to probability is that have a data frame sample space and we would like to find a\r\nsubset of that space. A data.frame method was written for isin that simply applies the function\r\nto each row of the data frame. We can see the method in action with the following:\r\n> S <- rolldie(4)\r\n> subset(S, isin(S, c(2, 2, 6), ordered = TRUE))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ef7d2c57-200d-48ee-899f-a02f811bc131.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d18f179713728ec4fbed67174f532f440fb029b6c55860d8a9d089271bbe5cbe",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 284
      },
      {
        "segments": [
          {
            "segment_id": "08d5810d-57cc-4c59-8622-73f5bb3d4441",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 92,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "76 CHAPTER 4. PROBABILITY\r\nX1 X2 X3 X4\r\n188 2 2 6 1\r\n404 2 2 6 2\r\n620 2 2 6 3\r\n836 2 2 6 4\r\n1052 2 2 6 5\r\n1088 2 2 1 6\r\n1118 2 1 2 6\r\n1123 1 2 2 6\r\n1124 2 2 2 6\r\n1125 3 2 2 6\r\n1126 4 2 2 6\r\n1127 5 2 2 6\r\n1128 6 2 2 6\r\n1130 2 3 2 6\r\n1136 2 4 2 6\r\n1142 2 5 2 6\r\n1148 2 6 2 6\r\n1160 2 2 3 6\r\n1196 2 2 4 6\r\n1232 2 2 5 6\r\n1268 2 2 6 6\r\nThere are a few other functions written to find useful subsets, namely, countrep and isrep.\r\nEssentially these were written to test for (or count) a specific number of designated values in out\u0002comes. See the documentation for details.\r\n4.2.3 Set Union, Intersection, and Difference\r\nGiven subsets A and B, it is often useful to manipulate them in an algebraic fashion. To this end,\r\nwe have three set operations at our disposal: union, intersection, and difference. Below is a table\r\nthat summarizes the pertinent information about these operations.\r\nName Denoted Defined by elements Code\r\nUnion A ∪ B in A or B or both union(A,B)\r\nIntersection A ∩ B in both A and B intersect(A,B)\r\nDifference A\\B in A but not in B setdiff(A,B)\r\nSome examples follow.\r\n> S = cards()\r\n> A = subset(S, suit == \"Heart\")\r\n> B = subset(S, rank %in% 7:9)\r\nWe can now do some set algebra:\r\n> union(A, B)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/08d5810d-57cc-4c59-8622-73f5bb3d4441.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=db3667f67431273c986db9f1d4bdd50fdf683f065b14b6d49736bbd1a40bdbe6",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "3f5c4aab-2f8c-488c-88a3-ea4d1c646b06",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 93,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.2. EVENTS 77\r\nrank suit\r\n6 7 Club\r\n7 8 Club\r\n8 9 Club\r\n19 7 Diamond\r\n20 8 Diamond\r\n21 9 Diamond\r\n27 2 Heart\r\n28 3 Heart\r\n29 4 Heart\r\n30 5 Heart\r\n31 6 Heart\r\n32 7 Heart\r\n33 8 Heart\r\n34 9 Heart\r\n35 10 Heart\r\n36 J Heart\r\n37 Q Heart\r\n38 K Heart\r\n39 A Heart\r\n45 7 Spade\r\n46 8 Spade\r\n47 9 Spade\r\n> intersect(A, B)\r\nrank suit\r\n32 7 Heart\r\n33 8 Heart\r\n34 9 Heart\r\n> setdiff(A, B)\r\nrank suit\r\n27 2 Heart\r\n28 3 Heart\r\n29 4 Heart\r\n30 5 Heart\r\n31 6 Heart\r\n35 10 Heart\r\n36 J Heart\r\n37 Q Heart\r\n38 K Heart\r\n39 A Heart\r\n> setdiff(B, A)\r\nrank suit\r\n6 7 Club\r\n7 8 Club",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3f5c4aab-2f8c-488c-88a3-ea4d1c646b06.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63fbd8a139b3d55870e5119255c5a28bc4433fc9d188a136b00edce973617946",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 397
      },
      {
        "segments": [
          {
            "segment_id": "342ca14a-0967-46b8-a42f-ce1743a95eb9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 94,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "78 CHAPTER 4. PROBABILITY\r\n8 9 Club\r\n19 7 Diamond\r\n20 8 Diamond\r\n21 9 Diamond\r\n45 7 Spade\r\n46 8 Spade\r\n47 9 Spade\r\nNotice that setdiff is not symmetric. Further, note that we can calculate the complement of a\r\nset A, denoted A\r\nc\r\nand defined to be the elements of S that are not in A simply with setdiff(S,A).\r\nThere have been methods written for intersect, setdiff, subset, and union in the case\r\nthat the input objects are of class ps. See Section 4.6.3.\r\nNote 4.3. When the prob package loads you will notice a message: “The following object(s)\r\nare masked from package:base : intersect, setdiff, union”. The reason for this mes\u0002sage is that there already exist methods for the functions intersect, setdiff, subset, and\r\nunion in the base package which ships with R. However, these methods were designed for when\r\nthe arguments are vectors of the same mode. Since we are manipulating sample spaces which are\r\ndata frames and lists, it was necessary to write methods to handle those cases as well. When the\r\nprob package is loaded, R recognizes that there are multiple versions of the same function in the\r\nsearch path and acts to shield the new definitions from the existing ones. But there is no cause for\r\nalarm, thankfully, because the prob functions have been carefully defined to match the usual base\r\npackage definition in the case that the arguments are vectors.\r\n4.3 Model Assignment\r\nLet us take a look at the coin-toss experiment more closely. What do we mean when we say “the\r\nprobability of Heads” or write IP(Heads)? Given a coin and an itchy thumb, how do we go about\r\nfinding what IP(Heads) should be?\r\n4.3.1 The Measure Theory Approach\r\nThis approach states that the way to handle IP(Heads) is to define a mathematical function, called\r\na probability measure, on the sample space. Probability measures satisfy certain axioms (to be\r\nintroduced later) and have special mathematical properties, so not just any mathematical function\r\nwill do. But in any given physical circumstance there are typically all sorts of probability measures\r\nfrom which to choose, and it is left to the experimenter to make a reasonable choice – usually based\r\non considerations of objectivity. For the tossing coin example, a valid probability measure assigns\r\nprobability p to the event {Heads}, where p is some number 0 ≤ p ≤ 1. An experimenter that\r\nwishes to incorporate the symmetry of the coin would choose p = 1/2 to balance the likelihood of\r\n{Heads} and {Tails}.\r\nOnce the probability measure is chosen (or determined), there is not much left to do. All\r\nassignments of probability are made by the probability function, and the experimenter needs only to\r\nplug the event {Heads} into to the probability function to find IP(Heads). In this way, the probability\r\nof an event is simply a calculated value, nothing more, nothing less. Of course this is not the whole\r\nstory; there are many theorems and consequences associated with this approach that will keep",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/342ca14a-0967-46b8-a42f-ce1743a95eb9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8f684869f86373f66bf8a4f819d8e78dc6be03699b3acaa7d662b4e98276de8c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 502
      },
      {
        "segments": [
          {
            "segment_id": "e853ee52-bb93-4f12-9390-a659b1c25f0e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 95,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.3. MODEL ASSIGNMENT 79\r\nus occupied for the remainder of this book. The approach is called measure theory because the\r\nmeasure (probability) of a set (event) is associated with how big it is (how likely it is to occur).\r\nThe measure theory approach is well suited for situations where there is symmetry to the exper\u0002iment, such as flipping a balanced coin or spinning an arrow around a circle with well-defined pie\r\nslices. It is also handy because of its mathematical simplicity, elegance, and flexibility. There are\r\nliterally volumes of information that one can prove about probability measures, and the cold rules\r\nof mathematics allow us to analyze intricate probabilistic problems with vigor.\r\nThe large degree of flexibility is also a disadvantage, however. When symmetry fails it is\r\nnot always obvious what an “objective” choice of probability measure should be; for instance,\r\nwhat probability should we assign to {Heads} if we spin the coin rather than flip it? (It is not\r\n1/2.) Furthermore, the mathematical rules are restrictive when we wish to incorporate subjective\r\nknowledge into the model, knowledge which changes over time and depends on the experimenter,\r\nsuch as personal knowledge about the properties of the specific coin being flipped, or of the person\r\ndoing the flipping.\r\nThe mathematician who revolutionized this way to do probability theory was Andrey Kol\u0002mogorov, who published a landmark monograph in 1933. See\r\nhttp://www-history.mcs.st-andrews.ac.uk/Mathematicians/Kolmogorov.html\r\nfor more information.\r\n4.3.2 Relative Frequency Approach\r\nThis approach states that the way to determine IP(Heads) is to flip the coin repeatedly, in exactly\r\nthe same way each time. Keep a tally of the number of flips and the number of Heads observed.\r\nThen a good approximation to IP(Heads) will be\r\nIP(Heads) ≈\r\nnumber of observed Heads\r\ntotal number of flips\r\n. (4.3.1)\r\nThe mathematical underpinning of this approach is the celebrated Law of Large Numbers,\r\nwhich may be loosely described as follows. Let E be a random experiment in which the event A\r\neither does or does not occur. Perform the experiment repeatedly, in an identical manner, in such\r\na way that the successive experiments do not influence each other. After each experiment, keep\r\na running tally of whether or not the event A occurred. Let S n count the number of times that A\r\noccurred in the n experiments. Then the law of large numbers says that\r\nS n\r\nn\r\n→ IP(A) as n → ∞. (4.3.2)\r\nAs the reasoning goes, to learn about the probability of an event A we need only repeat the\r\nrandom experiment to get a reasonable estimate of the probability’s value, and if we are not satisfied\r\nwith our estimate then we may simply repeat the experiment more times all the while confident that\r\nwith more and more experiments our estimate will stabilize to the true value.\r\nThe frequentist approach is good because it is relatively light on assumptions and does not\r\nworry about symmetry or claims of objectivity like the measure-theoretic approach does. It is\r\nperfect for the spinning coin experiment. One drawback to the method is that one can never know\r\nthe exact value of a probability, only a long-run approximation. It also does not work well with",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e853ee52-bb93-4f12-9390-a659b1c25f0e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e0d40417b493fb624005a4496b0d60f072325981ff2ce75608cdc5618e763721",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "e853ee52-bb93-4f12-9390-a659b1c25f0e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 95,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.3. MODEL ASSIGNMENT 79\r\nus occupied for the remainder of this book. The approach is called measure theory because the\r\nmeasure (probability) of a set (event) is associated with how big it is (how likely it is to occur).\r\nThe measure theory approach is well suited for situations where there is symmetry to the exper\u0002iment, such as flipping a balanced coin or spinning an arrow around a circle with well-defined pie\r\nslices. It is also handy because of its mathematical simplicity, elegance, and flexibility. There are\r\nliterally volumes of information that one can prove about probability measures, and the cold rules\r\nof mathematics allow us to analyze intricate probabilistic problems with vigor.\r\nThe large degree of flexibility is also a disadvantage, however. When symmetry fails it is\r\nnot always obvious what an “objective” choice of probability measure should be; for instance,\r\nwhat probability should we assign to {Heads} if we spin the coin rather than flip it? (It is not\r\n1/2.) Furthermore, the mathematical rules are restrictive when we wish to incorporate subjective\r\nknowledge into the model, knowledge which changes over time and depends on the experimenter,\r\nsuch as personal knowledge about the properties of the specific coin being flipped, or of the person\r\ndoing the flipping.\r\nThe mathematician who revolutionized this way to do probability theory was Andrey Kol\u0002mogorov, who published a landmark monograph in 1933. See\r\nhttp://www-history.mcs.st-andrews.ac.uk/Mathematicians/Kolmogorov.html\r\nfor more information.\r\n4.3.2 Relative Frequency Approach\r\nThis approach states that the way to determine IP(Heads) is to flip the coin repeatedly, in exactly\r\nthe same way each time. Keep a tally of the number of flips and the number of Heads observed.\r\nThen a good approximation to IP(Heads) will be\r\nIP(Heads) ≈\r\nnumber of observed Heads\r\ntotal number of flips\r\n. (4.3.1)\r\nThe mathematical underpinning of this approach is the celebrated Law of Large Numbers,\r\nwhich may be loosely described as follows. Let E be a random experiment in which the event A\r\neither does or does not occur. Perform the experiment repeatedly, in an identical manner, in such\r\na way that the successive experiments do not influence each other. After each experiment, keep\r\na running tally of whether or not the event A occurred. Let S n count the number of times that A\r\noccurred in the n experiments. Then the law of large numbers says that\r\nS n\r\nn\r\n→ IP(A) as n → ∞. (4.3.2)\r\nAs the reasoning goes, to learn about the probability of an event A we need only repeat the\r\nrandom experiment to get a reasonable estimate of the probability’s value, and if we are not satisfied\r\nwith our estimate then we may simply repeat the experiment more times all the while confident that\r\nwith more and more experiments our estimate will stabilize to the true value.\r\nThe frequentist approach is good because it is relatively light on assumptions and does not\r\nworry about symmetry or claims of objectivity like the measure-theoretic approach does. It is\r\nperfect for the spinning coin experiment. One drawback to the method is that one can never know\r\nthe exact value of a probability, only a long-run approximation. It also does not work well with",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e853ee52-bb93-4f12-9390-a659b1c25f0e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e0d40417b493fb624005a4496b0d60f072325981ff2ce75608cdc5618e763721",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "b44720b1-31ed-41f4-9222-13d137d76072",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 96,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "80 CHAPTER 4. PROBABILITY\r\nexperiments that can not be repeated indefinitely, say, the probability that it will rain today, the\r\nchances that you get will get an A in your Statistics class, or the probability that the world is\r\ndestroyed by nuclear war.\r\nThis approach was espoused by Richard von Mises in the early twentieth century, and some of\r\nhis main ideas were incorporated into the measure theory approach. See\r\nhttp://www-history.mcs.st-andrews.ac.uk/Biographies/Mises.html\r\nfor more.\r\n4.3.3 The Subjective Approach\r\nThe subjective approach interprets probability as the experimenter’s degree of belief that the event\r\nwill occur. The estimate of the probability of an event is based on the totality of the individual’s\r\nknowledge at the time. As new information becomes available, the estimate is modified accordingly\r\nto best reflect his/her current knowledge. The method by which the probabilities are updated is\r\ncommonly done with Bayes’ Rule, discussed in Section 4.8.\r\nSo for the coin toss example, a person may have IP(Heads) = 1/2 in the absence of additional\r\ninformation. But perhaps the observer knows additional information about the coin or the thrower\r\nthat would shift the probability in a certain direction. For instance, parlor magicians may be trained\r\nto be quite skilled at tossing coins, and some are so skilled that they may toss a fair coin and get\r\nnothing but Heads, indefinitely. I have seen this. It was similarly claimed in Bringing Down the\r\nHouse [65] that MIT students were accomplished enough with cards to be able to cut a deck to the\r\nsame location, every single time. In such cases, one clearly should use the additional information\r\nto assign IP(Heads) away from the symmetry value of 1/2.\r\nThis approach works well in situations that cannot be repeated indefinitely, for example, to\r\nassign your probability that you will get an A in this class, the chances of a devastating nuclear\r\nwar, or the likelihood that a cure for the common cold will be discovered.\r\nThe roots of subjective probability reach back a long time. See\r\nhttp://en.wikipedia.org/wiki/Subjective_probability\r\nfor a short discussion and links to references about the subjective approach.\r\n4.3.4 Equally Likely Model (ELM)\r\nWe have seen several approaches to the assignment of a probability model to a given random exper\u0002iment and they are very different in their underlying interpretation. But they all cross paths when\r\nit comes to the equally likely model which assigns equal probability to all elementary outcomes of\r\nthe experiment.\r\nThe ELM appears in the measure theory approach when the experiment boasts symmetry of\r\nsome kind. If symmetry guarantees that all outcomes have equal “size”, and if outcomes with\r\nequal “size” should get the same probability, then the ELM is a logical objective choice for the\r\nexperimenter. Consider the balanced 6-sided die, the fair coin, or the dart board with equal-sized\r\nwedges.\r\nThe ELM appears in the subjective approach when the experimenter resorts to indifference or\r\nignorance with respect to his/her knowledge of the outcome of the experiment. If the experimenter",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b44720b1-31ed-41f4-9222-13d137d76072.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=df1e1597a138fa3bf81ca1474f62c790df3782938f75eb585ca32da3277b63f0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 492
      },
      {
        "segments": [
          {
            "segment_id": "898da27f-d8c2-489b-8185-e178e4c6e44b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 97,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.3. MODEL ASSIGNMENT 81\r\nhas no prior knowledge to suggest that (s)he prefer Heads over Tails, then it is reasonable for the\r\nhim/her to assign equal subjective probability to both possible outcomes.\r\nThe ELM appears in the relative frequency approach as a fascinating fact of Nature: when we\r\nflip balanced coins over and over again, we observe that the proportion of times that the coin comes\r\nup Heads tends to 1/2. Of course if we assume that the measure theory applies then we can prove\r\nthat the sample proportion must tend to 1/2 as expected, but that is putting the cart before the horse,\r\nin a manner of speaking.\r\nThe ELM is only available when there are finitely many elements in the sample space.\r\n4.3.5 How to do it with R\r\nIn the prob package, a probability space is an object of outcomes S and a vector of probabilities\r\n(called “probs”) with entries that correspond to each outcome in S. When S is a data frame, we may\r\nsimply add a column called probs to S and we will be finished; the probability space will simply\r\nbe a data frame which we may call S. In the case that S is a list, we may combine the outcomes\r\nand probs into a larger list, space; it will have two components: outcomes and probs. The only\r\nrequirements we need are for the entries of probs to be nonnegative and sum(probs) to be one.\r\nTo accomplish this in R, we may use the probspace function. The general syntax is probspace(x,\r\nprobs), where x is a sample space of outcomes and probs is a vector (of the same length as the\r\nnumber of outcomes in x). The specific choice of probs depends on the context of the problem,\r\nand some examples follow to demonstrate some of the more common choices.\r\nExample 4.4. The Equally Likely Model asserts that every outcome of the sample space has the\r\nsame probability, thus, if a sample space has n outcomes, then probs would be a vector of length\r\nn with identical entries 1/n. The quickest way to generate probs is with the rep function. We\r\nwill start with the experiment of rolling a die, so that n = 6. We will construct the sample space,\r\ngenerate the probs vector, and put them together with probspace.\r\n> outcomes <- rolldie(1)\r\nX1\r\n1 1\r\n2 2\r\n3 3\r\n4 4\r\n5 5\r\n6 6\r\n> p <- rep(1/6, times = 6)\r\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\r\n> probspace(outcomes, probs = p)\r\nX1 probs\r\n1 1 0.1666667\r\n2 2 0.1666667\r\n3 3 0.1666667\r\n4 4 0.1666667\r\n5 5 0.1666667\r\n6 6 0.1666667",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/898da27f-d8c2-489b-8185-e178e4c6e44b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ee58dcea0453720d0cd6f744608a3cbe5a4470194e36b74eae731c171a2bbc29",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 444
      },
      {
        "segments": [
          {
            "segment_id": "be017ef8-0f9f-41fe-ad6b-f3732d1ea250",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 98,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "82 CHAPTER 4. PROBABILITY\r\nThe probspace function is designed to save us some time in many of the most common situa\u0002tions. For example, due to the especial simplicity of the sample space in this case, we could have\r\nachieved the same result with only (note the name change for the first column)\r\n> probspace(1:6, probs = p)\r\nx probs\r\n1 1 0.1666667\r\n2 2 0.1666667\r\n3 3 0.1666667\r\n4 4 0.1666667\r\n5 5 0.1666667\r\n6 6 0.1666667\r\nFurther, since the equally likely model plays such a fundamental role in the study of probability\r\nthe probspace function will assume that the equally model is desired if no probs are specified.\r\nThus, we get the same answer with only\r\n> probspace(1:6)\r\nx probs\r\n1 1 0.1666667\r\n2 2 0.1666667\r\n3 3 0.1666667\r\n4 4 0.1666667\r\n5 5 0.1666667\r\n6 6 0.1666667\r\nAnd finally, since rolling dice is such a common experiment in probability classes, the rolldie\r\nfunction has an additional logical argument makespace that will add a column of equally likely\r\nprobs to the generated sample space:\r\n> rolldie(1, makespace = TRUE)\r\nX1 probs\r\n1 1 0.1666667\r\n2 2 0.1666667\r\n3 3 0.1666667\r\n4 4 0.1666667\r\n5 5 0.1666667\r\n6 6 0.1666667\r\nor just rolldie(1, TRUE). Many of the other sample space functions (tosscoin, cards, roulette,\r\netc.) have similar makespace arguments. Check the documentation for details.\r\nOne sample space function that does NOT have a makespace option is the urnsamples func\u0002tion. This was intentional. The reason is that under the varied sampling assumptions the outcomes\r\nin the respective sample spaces are NOT, in general, equally likely. It is important for the user to\r\ncarefully consider the experiment to decide whether or not the outcomes are equally likely and then\r\nuse probspace to assign the model.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/be017ef8-0f9f-41fe-ad6b-f3732d1ea250.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=77cf0ec53c5dbb444c68843301040e1d49a83d8ca5f37e38878d1e553ad21048",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 293
      },
      {
        "segments": [
          {
            "segment_id": "bb12dd9b-f2e5-4804-97ee-63be0b759ac3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 99,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.4. PROPERTIES OF PROBABILITY 83\r\nExample 4.5. An unbalanced coin. While the makespace argument to tosscoin is useful to\r\nrepresent the tossing of a fair coin, it is not always appropriate. For example, suppose our coin is\r\nnot perfectly balanced, for instance, maybe the “H” side is somewhat heavier such that the chances\r\nof a H appearing in a single toss is 0.70 instead of 0.5. We may set up the probability space with\r\n> probspace(tosscoin(1), probs = c(0.7, 0.3))\r\ntoss1 probs\r\n1 H 0.7\r\n2 T 0.3\r\nThe same procedure can be used to represent an unbalanced die, roulette wheel, etc.\r\n4.3.6 Words of Warning\r\nIt should be mentioned that while the splendour of R is uncontested, it, like everything else, has\r\nlimits both with respect to the sample/probability spaces it can manage and with respect to the finite\r\naccuracy of the representation of most numbers (see the R FAQ 7.31). When playing around with\r\nprobability, one may be tempted to set up a probability space for tossing 100 coins or rolling 50\r\ndice in an attempt to answer some scintillating question. (Bear in mind: rolling a die just 9 times\r\nhas a sample space with over 10 million outcomes.)\r\nAlas, even if there were enough RAM to barely hold the sample space (and there were enough\r\ntime to wait for it to be generated), the infinitesimal probabilities that are associated with so many\r\noutcomes make it difficult for the underlying machinery to handle reliably. In some cases, special\r\nalgorithms need to be called just to give something that holds asymptotically. User beware.\r\n4.4 Properties of Probability\r\n4.4.1 Probability Functions\r\nA probability function is a rule that associates with each event A of the sample space a unique\r\nnumber IP(A) = p, called the probability of A. Any probability function IP satisfies the following\r\nthree Kolmogorov Axioms:\r\nAxiom 4.6. IP(A) ≥ 0 for any event A ⊂ S .\r\nAxiom 4.7. IP(S ) = 1.\r\nAxiom 4.8. If the events A1, A2, A3. . . are disjoint then\r\nIP\r\n\r\n\r\n[n\r\ni=1\r\nAi\r\n\r\n =\r\nXn\r\ni=1\r\nIP(Ai) for every n, (4.4.1)\r\nand furthermore,\r\nIP\r\n\r\n\r\n[∞\r\ni=1\r\nAi\r\n\r\n =\r\nX∞\r\ni=1\r\nIP(Ai). (4.4.2)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/bb12dd9b-f2e5-4804-97ee-63be0b759ac3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=579a4976948960252e723354ccfa1f57693feb70e758a0a86e494958ae043b60",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 370
      },
      {
        "segments": [
          {
            "segment_id": "90a071b7-bcde-4450-adf2-cb08029f05bf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 100,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "84 CHAPTER 4. PROBABILITY\r\nThe intuition behind the axioms: first, the probability of an event should never be negative.\r\nAnd since the sample space contains all possible outcomes, its probability should be one, or 100%.\r\nThe final axiom may look intimidating, but it simply means that for a sequence of disjoint events\r\n(in other words, sets that do not overlap), their total probability (measure) should equal the sum of\r\nits parts. For example, the chance of rolling a 1 or a 2 on a die is the chance of rolling a 1 plus the\r\nchance of rolling a 2. The connection to measure theory could not be more clear.\r\n4.4.2 Properties\r\nFor any events A and B,\r\n1. IP(A\r\nc\r\n) = 1 − IP(A).\r\nProof. Since A ∪ A\r\nc = S and A ∩ Ac = ∅, we have\r\n1 = IP(S ) = IP(A ∪ A\r\nc\r\n) = IP(A) + IP(A\r\nc\r\n).\r\n\u0003\r\n2. IP(∅) = 0.\r\nProof. Note that ∅ = S\r\nc\r\n, and use Property 1. \u0003\r\n3. If A ⊂ B , then IP(A) ≤ IP(B).\r\nProof. Write B = A ∪ (B ∩ A\r\nc\r\n), and notice that A ∩ (B ∩ A\r\nc\r\n) = ∅; thus\r\nIP(B) = IP(A ∪ (B ∩ A\r\nc\r\n)) = IP(A) + IP (B ∩ A\r\nc\r\n) ≥ IP(A),\r\nsince IP (B ∩ A\r\nc\r\n) ≥ 0. \u0003\r\n4. 0 ≤ IP(A) ≤ 1.\r\nProof. The left inequality is immediate from Axiom 4.6, and the second inequality follows\r\nfrom Property 3 since A ⊂ S . \u0003\r\n5. The General Addition Rule.\r\nIP(A ∪ B) = IP(A) + IP(B) − IP(A ∩ B). (4.4.3)\r\nMore generally, for events A1, A2, A3,. . . , An,\r\nIP\r\n\r\n\r\n[n\r\ni=1\r\nAi\r\n\r\n =\r\nXn\r\ni=1\r\nIP(Ai) −\r\nXn−1\r\ni=1\r\nXn\r\nj=i+1\r\nIP(Ai ∩ Aj) + · · · + (−1)n−1IP\r\n\r\n\r\n\\n\r\ni=1\r\nAi\r\n\r\n\r\n(4.4.4)\r\n6. The Theorem of Total Probability. Let B1, B2, . . . , Bn be mutually exclusive and exhaustive.\r\nThen\r\nIP(A) = IP(A ∩ B1) + IP(A ∩ B2) + · · · + IP(A ∩ Bn). (4.4.5)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/90a071b7-bcde-4450-adf2-cb08029f05bf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ba342f03d96295d7bedbb5a267dc9c9be470cf6e981353fe7804f8c7fc69bf63",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 368
      },
      {
        "segments": [
          {
            "segment_id": "690ea7a5-2020-418d-b2e2-7baaaa02dcac",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 101,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.4. PROPERTIES OF PROBABILITY 85\r\n4.4.3 Assigning Probabilities\r\nA model of particular interest is the equally likely model. The idea is to divide the sample space S\r\ninto a finite collection of elementary events {a1, a2, . . . , aN} that are equally likely in the sense that\r\neach ai has equal chances of occurring. The probability function associated with this model must\r\nsatisfy IP(S ) = 1, by Axiom 2. On the other hand, it must also satisfy\r\nIP(S ) = IP({a1, a2, . . . , aN}) = IP(a1 ∪ a2 ∪ · · · ∪ aN) =\r\nX\r\nN\r\ni=1\r\nIP(ai),\r\nby Axiom 3. Since IP(ai) is the same for all i, each one necessarily equals 1/N.\r\nFor an event A ⊂ S , we write it as a collection of elementary outcomes: if A =\r\n\b\r\nai1\r\n, ai2\r\n, . . . , aik\r\n\t\r\nthen A has k elements and\r\nIP(A) = IP(ai1) + IP(ai2) + · · · + IP(aik),\r\n=\r\n1\r\nN\r\n+\r\n1\r\nN\r\n+ · · · +\r\n1\r\nN\r\n,\r\n=\r\nk\r\nN\r\n=\r\n#(A)\r\n#(S )\r\n.\r\nIn other words, under the equally likely model, the probability of an event A is determined by the\r\nnumber of elementary events that A contains.\r\nExample 4.9. Consider the random experiment E of tossing a coin. Then the sample space is\r\nS = {H, T}, and under the equally likely model, these two outcomes have IP(H) = IP(T) = 1/2.\r\nThis model is taken when it is reasonable to assume that the coin is fair.\r\nExample 4.10. Suppose the experiment E consists of tossing a fair coin twice. The sample space\r\nmay be represented by S = {HH, HT, T H, T T}. Given that the coin is fair and that the coin is\r\ntossed in an independent and identical manner, it is reasonable to apply the equally likely model.\r\nWhat is IP(at least 1 Head)? Looking at the sample space we see the elements HH, HT, and\r\nT H have at least one Head; thus, IP(at least 1 Head) = 3/4.\r\nWhat is IP(no Heads)? Notice that the event {no Heads} = {at least one Head}\r\nc\r\n, which by Prop\u0002erty 1 means IP(no Heads) = 1 − IP(at least one Head) = 1 − 3/4 = 1/4. It is obvious in this simple\r\nexample that the only outcome with no Heads is T T, however, this complementation trick is useful\r\nin more complicated circumstances.\r\nExample 4.11. Imagine a three child family, each child being either Boy (B) or Girl (G). An\r\nexample sequence of siblings would be BGB. The sample space may be written\r\nS =\r\n(\r\nBBB, BGB, GBB, GGB,\r\nBBG, BGG, GBG, GGG )\r\n.\r\nNote that for many reasons (for instance, it turns out that girls are slightly more likely to be born\r\nthan boys), this sample space is not equally likely. For the sake of argument, however, we will\r\nassume that the elementary outcomes each have probability 1/8.\r\nWhat is IP(exactly 2 Boys)? Inspecting the sample space reveals three outcomes with exactly\r\ntwo boys: {BBG, BGB, GBB}. Therefore IP(exactly 2 Boys) = 3/8.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/690ea7a5-2020-418d-b2e2-7baaaa02dcac.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=60ad7f34a2fb63a552c81bfab0a0d3cc4b8bee6508eb14d291ec22e32ab2f1cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 528
      },
      {
        "segments": [
          {
            "segment_id": "690ea7a5-2020-418d-b2e2-7baaaa02dcac",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 101,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.4. PROPERTIES OF PROBABILITY 85\r\n4.4.3 Assigning Probabilities\r\nA model of particular interest is the equally likely model. The idea is to divide the sample space S\r\ninto a finite collection of elementary events {a1, a2, . . . , aN} that are equally likely in the sense that\r\neach ai has equal chances of occurring. The probability function associated with this model must\r\nsatisfy IP(S ) = 1, by Axiom 2. On the other hand, it must also satisfy\r\nIP(S ) = IP({a1, a2, . . . , aN}) = IP(a1 ∪ a2 ∪ · · · ∪ aN) =\r\nX\r\nN\r\ni=1\r\nIP(ai),\r\nby Axiom 3. Since IP(ai) is the same for all i, each one necessarily equals 1/N.\r\nFor an event A ⊂ S , we write it as a collection of elementary outcomes: if A =\r\n\b\r\nai1\r\n, ai2\r\n, . . . , aik\r\n\t\r\nthen A has k elements and\r\nIP(A) = IP(ai1) + IP(ai2) + · · · + IP(aik),\r\n=\r\n1\r\nN\r\n+\r\n1\r\nN\r\n+ · · · +\r\n1\r\nN\r\n,\r\n=\r\nk\r\nN\r\n=\r\n#(A)\r\n#(S )\r\n.\r\nIn other words, under the equally likely model, the probability of an event A is determined by the\r\nnumber of elementary events that A contains.\r\nExample 4.9. Consider the random experiment E of tossing a coin. Then the sample space is\r\nS = {H, T}, and under the equally likely model, these two outcomes have IP(H) = IP(T) = 1/2.\r\nThis model is taken when it is reasonable to assume that the coin is fair.\r\nExample 4.10. Suppose the experiment E consists of tossing a fair coin twice. The sample space\r\nmay be represented by S = {HH, HT, T H, T T}. Given that the coin is fair and that the coin is\r\ntossed in an independent and identical manner, it is reasonable to apply the equally likely model.\r\nWhat is IP(at least 1 Head)? Looking at the sample space we see the elements HH, HT, and\r\nT H have at least one Head; thus, IP(at least 1 Head) = 3/4.\r\nWhat is IP(no Heads)? Notice that the event {no Heads} = {at least one Head}\r\nc\r\n, which by Prop\u0002erty 1 means IP(no Heads) = 1 − IP(at least one Head) = 1 − 3/4 = 1/4. It is obvious in this simple\r\nexample that the only outcome with no Heads is T T, however, this complementation trick is useful\r\nin more complicated circumstances.\r\nExample 4.11. Imagine a three child family, each child being either Boy (B) or Girl (G). An\r\nexample sequence of siblings would be BGB. The sample space may be written\r\nS =\r\n(\r\nBBB, BGB, GBB, GGB,\r\nBBG, BGG, GBG, GGG )\r\n.\r\nNote that for many reasons (for instance, it turns out that girls are slightly more likely to be born\r\nthan boys), this sample space is not equally likely. For the sake of argument, however, we will\r\nassume that the elementary outcomes each have probability 1/8.\r\nWhat is IP(exactly 2 Boys)? Inspecting the sample space reveals three outcomes with exactly\r\ntwo boys: {BBG, BGB, GBB}. Therefore IP(exactly 2 Boys) = 3/8.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/690ea7a5-2020-418d-b2e2-7baaaa02dcac.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=60ad7f34a2fb63a552c81bfab0a0d3cc4b8bee6508eb14d291ec22e32ab2f1cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 528
      },
      {
        "segments": [
          {
            "segment_id": "72fbae79-2a78-4ee9-a5b4-3372b4b753f6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 102,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "86 CHAPTER 4. PROBABILITY\r\nWhat is IP(at most 2 Boys)? One way to solve the problem would be to count the outcomes\r\nthat have 2 or less Boys, but a quicker way would be to recognize that the only way that the event\r\n\b\r\nat most 2 Boys\tdoes not occur is the event {all Girls}. Thus\r\nIP(at most 2 Boys) = 1 − IP(GGG) = 1 − 1/8 = 7/8.\r\nExample 4.12. Consider the experiment of rolling a six-sided die, and let the outcome be the face\r\nshowing up when the die comes to rest. Then S = {1, 2, 3, 4, 5, 6}. It is usually reasonable to\r\nsuppose that the die is fair, so that the six outcomes are equally likely.\r\nExample 4.13. Consider a standard deck of 52 cards. These are usually labeled with the four suits:\r\nClubs, Diamonds, Hearts, and Spades, and the 13 ranks: 2, 3, 4, . . . , 10, Jack (J), Queen (Q), King\r\n(K), and Ace (A). Depending on the game played, the Ace may be ranked below 2 or above King.\r\nLet the random experiment E consist of drawing exactly one card from a well-shuffled deck, and\r\nlet the outcome be the face of the card. Define the events A = {draw an Ace} and B = {draw a Club}.\r\nBear in mind: we are only drawing one card.\r\nImmediately we have IP(A) = 4/52 since there are four Aces in the deck; similarly, there are 13\r\nClubs which implies IP(B) = 13/52.\r\nWhat is IP(A ∩ B)? We realize that there is only one card of the 52 which is an Ace and a Club\r\nat the same time, namely, the Ace of Clubs. Therefore IP(A ∩ B) = 1/52.\r\nTo find IP(A ∪ B) we may use the above with the General Addition Rule to get\r\nIP(A ∪ B) = IP(A) + IP(B) − IP(A ∩ B),\r\n= 4/52 + 13/52 − 1/52,\r\n= 16/52.\r\nExample 4.14. Staying with the deck of cards, let another random experiment be the selec\u0002tion of a five card stud poker hand, where “five card stud” means that we draw exactly five\r\ncards from the deck without replacement, no more, and no less. It turns out that the sample\r\nspace S is so large and complicated that we will be obliged to settle for the trivial description\r\nS =\r\n\b\r\nall possible 5 card hands\tfor the time being. We will have a more precise description later.\r\nWhat is IP(Royal Flush), or in other words, IP(A, K, Q, J, 10 all in the same suit)?\r\nIt should be clear that there are only four possible royal flushes. Thus, if we could only count\r\nthe number of outcomes in S then we could simply divide four by that number and we would have\r\nour answer under the equally likely model. This is the subject of Section 4.5.\r\n4.4.4 How to do it with R\r\nProbabilities are calculated in the prob package with the prob function.\r\nConsider the experiment of drawing a card from a standard deck of playing cards. Let’s denote\r\nthe probability space associated with the experiment as S, and let the subsets A and B be defined by\r\nthe following:\r\n> S <- cards(makespace = TRUE)\r\n> A <- subset(S, suit == \"Heart\")\r\n> B <- subset(S, rank %in% 7:9)\r\nNow it is easy to calculate",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/72fbae79-2a78-4ee9-a5b4-3372b4b753f6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8e3d468edd9cea128de9f4e65e884d992f7067a825ebfba0238b6e06d6c5dc59",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 565
      },
      {
        "segments": [
          {
            "segment_id": "72fbae79-2a78-4ee9-a5b4-3372b4b753f6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 102,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "86 CHAPTER 4. PROBABILITY\r\nWhat is IP(at most 2 Boys)? One way to solve the problem would be to count the outcomes\r\nthat have 2 or less Boys, but a quicker way would be to recognize that the only way that the event\r\n\b\r\nat most 2 Boys\tdoes not occur is the event {all Girls}. Thus\r\nIP(at most 2 Boys) = 1 − IP(GGG) = 1 − 1/8 = 7/8.\r\nExample 4.12. Consider the experiment of rolling a six-sided die, and let the outcome be the face\r\nshowing up when the die comes to rest. Then S = {1, 2, 3, 4, 5, 6}. It is usually reasonable to\r\nsuppose that the die is fair, so that the six outcomes are equally likely.\r\nExample 4.13. Consider a standard deck of 52 cards. These are usually labeled with the four suits:\r\nClubs, Diamonds, Hearts, and Spades, and the 13 ranks: 2, 3, 4, . . . , 10, Jack (J), Queen (Q), King\r\n(K), and Ace (A). Depending on the game played, the Ace may be ranked below 2 or above King.\r\nLet the random experiment E consist of drawing exactly one card from a well-shuffled deck, and\r\nlet the outcome be the face of the card. Define the events A = {draw an Ace} and B = {draw a Club}.\r\nBear in mind: we are only drawing one card.\r\nImmediately we have IP(A) = 4/52 since there are four Aces in the deck; similarly, there are 13\r\nClubs which implies IP(B) = 13/52.\r\nWhat is IP(A ∩ B)? We realize that there is only one card of the 52 which is an Ace and a Club\r\nat the same time, namely, the Ace of Clubs. Therefore IP(A ∩ B) = 1/52.\r\nTo find IP(A ∪ B) we may use the above with the General Addition Rule to get\r\nIP(A ∪ B) = IP(A) + IP(B) − IP(A ∩ B),\r\n= 4/52 + 13/52 − 1/52,\r\n= 16/52.\r\nExample 4.14. Staying with the deck of cards, let another random experiment be the selec\u0002tion of a five card stud poker hand, where “five card stud” means that we draw exactly five\r\ncards from the deck without replacement, no more, and no less. It turns out that the sample\r\nspace S is so large and complicated that we will be obliged to settle for the trivial description\r\nS =\r\n\b\r\nall possible 5 card hands\tfor the time being. We will have a more precise description later.\r\nWhat is IP(Royal Flush), or in other words, IP(A, K, Q, J, 10 all in the same suit)?\r\nIt should be clear that there are only four possible royal flushes. Thus, if we could only count\r\nthe number of outcomes in S then we could simply divide four by that number and we would have\r\nour answer under the equally likely model. This is the subject of Section 4.5.\r\n4.4.4 How to do it with R\r\nProbabilities are calculated in the prob package with the prob function.\r\nConsider the experiment of drawing a card from a standard deck of playing cards. Let’s denote\r\nthe probability space associated with the experiment as S, and let the subsets A and B be defined by\r\nthe following:\r\n> S <- cards(makespace = TRUE)\r\n> A <- subset(S, suit == \"Heart\")\r\n> B <- subset(S, rank %in% 7:9)\r\nNow it is easy to calculate",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/72fbae79-2a78-4ee9-a5b4-3372b4b753f6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8e3d468edd9cea128de9f4e65e884d992f7067a825ebfba0238b6e06d6c5dc59",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 565
      },
      {
        "segments": [
          {
            "segment_id": "e8869429-6d0e-4f20-af9f-5c39f4037b01",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 103,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.5. COUNTING METHODS 87\r\n> prob(A)\r\n[1] 0.25\r\nNote that we can get the same answer with\r\n> prob(S, suit == \"Heart\")\r\n[1] 0.25\r\nWe also find prob(B)= 0.23 (listed here approximately, but 12/52 actually) and prob(S)= 1.\r\nInternally, the prob function operates by summing the probs column of its argument. It will find\r\nsubsets on-the-fly if desired.\r\nWe have as yet glossed over the details. More specifically, prob has three arguments: x, which\r\nis a probability space (or a subset of one), event, which is a logical expression used to define a\r\nsubset, and given, which is described in Section 4.6.\r\nWARNING. The event argument is used to define a subset of x, that is, the only outcomes used\r\nin the probability calculation will be those that are elements of x and satisfy event simultaneously.\r\nIn other words, prob(x, event) calculates\r\nprob( intersect (x, subset(x, event)))\r\nConsequently, x should be the entire probability space in the case that event is non-null.\r\n4.5 Counting Methods\r\nThe equally-likely model is a convenient and popular way to analyze random experiments. And\r\nwhen the equally likely model applies, finding the probability of an event A amounts to nothing\r\nmore than counting the number of outcomes that A contains (together with the number of events in\r\nS ). Hence, to be a master of probability one must be skilled at counting outcomes in events of all\r\nkinds.\r\nProposition 4.15. The Multiplication Principle. Suppose that an experiment is composed of two\r\nsuccessive steps. Further suppose that the first step may be performed in n1 distinct ways while the\r\nsecond step may be performed in n2 distinct ways. Then the experiment may be performed in n1n2\r\ndistinct ways.\r\nMore generally, if the experiment is composed of k successive steps which may be performed\r\nin n1, n2, . . . , nk distinct ways, respectively, then the experiment may be performed in n1n2 · · · nk\r\ndistinct ways.\r\nExample 4.16. We would like to order a pizza. It will be sure to have cheese (and marinara sauce),\r\nbut we may elect to add one or more of the following five (5) available toppings:\r\npepperoni, sausage, anchovies, olives, and green peppers.\r\nHow many distinct pizzas are possible?\r\nThere are many ways to approach the problem, but the quickest avenue employs the Multipli\u0002cation Principle directly. We will separate the action of ordering the pizza into a series of stages.\r\nAt the first stage, we will decide whether or not to include pepperoni on the pizza (two possibili\u0002ties). At the next stage, we will decide whether or not to include sausage on the pizza (again, two",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e8869429-6d0e-4f20-af9f-5c39f4037b01.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ac1b5a6f7bd9c61ef9c65d4b11ca5b191750df725e421f338256963c8220276",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 437
      },
      {
        "segments": [
          {
            "segment_id": "cd323414-fa7b-4205-9a33-c706538cbe4f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 104,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "88 CHAPTER 4. PROBABILITY\r\npossibilities). We will continue in this fashion until at last we will decide whether or not to include\r\ngreen peppers on the pizza.\r\nAt each stage we will have had two options, or ways, to select a pizza to be made. The Multi\u0002plication Principle says that we should multiply the 2’s to find the total number of possible pizzas:\r\n2 · 2 · 2 · 2 · 2 = 2\r\n5 = 32.\r\nExample 4.17. We would like to buy a desktop computer to study statistics. We go to a website\r\nto build our computer our way. Given a line of products we have many options to customize our\r\ncomputer. In particular, there are 2 choices for a processor, 3 different operating systems, 4 levels\r\nof memory, 4 hard drives of differing sizes, and 10 choices for a monitor. How many possible types\r\nof computer must Gell be prepared to build? Answer: 2 · 3 · 4 · 4 · 10 = 960.\r\n4.5.1 Ordered Samples\r\nImagine a bag with n distinguishable balls inside. Now shake up the bag and select k balls at\r\nrandom. How many possible sequences might we observe?\r\nProposition 4.18. The number of ways in which one may select an ordered sample of k subjects\r\nfrom a population that has n distinguishable members is\r\n• n\r\nk\r\nif sampling is done with replacement,\r\n• n(n − 1)(n − 2) · · · (n − k + 1) if sampling is done without replacement.\r\nRecall from calculus the notation for factorials:\r\n1! = 1,\r\n2! = 2 · 1 = 2,\r\n3! = 3 · 2 · 1 = 6,\r\n.\r\n.\r\n.\r\nn! = n(n − 1)(n − 2) · · · 3 · 2 · 1.\r\nFact 4.19. The number of permutations of n elements is n!.\r\nExample 4.20. Take a coin and flip it 7 times. How many sequences of Heads and Tails are\r\npossible? Answer: 2\r\n7 = 128.\r\nExample 4.21. In a class of 20 students, we randomly select a class president, a class vice\u0002president, and a treasurer. How many ways can this be done? Answer: 20 · 19 · 18 = 6840.\r\nExample 4.22. We rent five movies to watch over the span of two nights. We wish to watch 3\r\nmovies on the first night. How many distinct sequences of 3 movies could we possibly watch?\r\nAnswer: 5 · 4 · 3 = 60.\r\n4.5.2 Unordered Samples\r\nThe number of ways in which one may select an unordered sample of k subjects from a population\r\nthat has n distinguishable members is",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/cd323414-fa7b-4205-9a33-c706538cbe4f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0a39d4ebec3c0f46b7124c7b898b0b4d24a962638d144e808f52862da8e24852",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 435
      },
      {
        "segments": [
          {
            "segment_id": "67cee9fd-012a-4017-b652-21106fd28dda",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 105,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.5. COUNTING METHODS 89\r\nordered = TRUE ordered = FALSE\r\nreplace = TRUE n\r\nk (n−1+k)!\r\n(n−1)!k!\r\nreplace = FALSE n!\r\n(n−k)! \u0010\r\nn\r\nk\r\n\u0011\r\nTable 4.1: Sampling k from n objects with urnsamples\r\n• (n − 1 + k)!/[(n − 1)!k!] if sampling is done with replacement,\r\n• n!/[k!(n − k)!] if sampling is done without replacement.\r\nThe quantity n!/[k!(n−k)!] is called a binomial coefficient and plays a special role in mathematics;\r\nit is denoted\r\n \r\nn\r\nk\r\n!\r\n=\r\nn!\r\nk!(n − k)!\r\n(4.5.1)\r\nand is read “n choose k”.\r\nExample 4.23. You rent five movies to watch over the span of two nights, but only wish to watch\r\n3 movies the first night. Your friend, Fred, wishes to borrow some movies to watch at his house\r\non the first night. You owe Fred a favor, and allow him to select 2 movies from the set of 5. How\r\nmany choices does Fred have? Answer: \u0010\r\n5\r\n2\r\n\u0011\r\n= 10.\r\nExample 4.24. Place 3 six-sided dice into a cup. Next, shake the cup well and pour out the dice.\r\nHow many distinct rolls are possible? Answer: (6 − 1 + 3)!/[(6 − 1)!3!] =\r\n\u0010\r\n8\r\n5\r\n\u0011\r\n= 56.\r\n4.5.3 How to do it with R\r\nThe factorial n! is computed with the command factorial(n) and the binomial coefficient \u0010\r\nn\r\nk\r\n\u0011\r\nwith the command choose(n,k).\r\nThe sample spaces we have computed so far have been relatively small, and we can visually\r\nstudy them without much trouble. However, it is very easy to generate sample spaces that are\r\nprohibitively large. And while R is wonderful and powerful and does almost everything except\r\nwash windows, even R has limits of which we should be mindful.\r\nBut we often do not need to actually generate the sample space; it suffices to count the number\r\nof outcomes. The nsamp function will calculate the number of rows in a sample space made by\r\nurnsamples without actually devoting the memory resources necessary to generate the space.\r\nThe arguments are n, the number of (distinguishable) objects in the urn, k, the sample size, and\r\nreplace, ordered, as above.\r\nExample 4.25. We will compute the number of outcomes for each of the four urnsamples exam\u0002ples that we saw in Example 4.2. Recall that we took a sample of size two from an urn with three\r\ndistinguishable elements.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/67cee9fd-012a-4017-b652-21106fd28dda.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1cec33698fe411e9454dbd6a86b209333d90d34485836fd80c7e4656a46a9bf8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 395
      },
      {
        "segments": [
          {
            "segment_id": "aea768a7-6d6d-4c89-994c-a8208abe37ca",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 106,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "90 CHAPTER 4. PROBABILITY\r\n> nsamp(n = 3, k = 2, replace = TRUE, ordered = TRUE)\r\n[1] 9\r\n> nsamp(n = 3, k = 2, replace = FALSE, ordered = TRUE)\r\n[1] 6\r\n> nsamp(n = 3, k = 2, replace = FALSE, ordered = FALSE)\r\n[1] 3\r\n> nsamp(n = 3, k = 2, replace = TRUE, ordered = FALSE)\r\n[1] 6\r\nCompare these answers with the length of the data frames generated above.\r\nThe Multiplication Principle\r\nA benefit of nsamp is that it is vectorized so that entering vectors instead of numbers for n, k,\r\nreplace, and ordered results in a vector of corresponding answers. This becomes particularly\r\nconvenient for combinatorics problems.\r\nExample 4.26. There are 11 artists who each submit a portfolio containing 7 paintings for compe\u0002tition in an art exhibition. Unfortunately, the gallery director only has space in the winners’ section\r\nto accommodate 12 paintings in a row equally spread over three consecutive walls. The director\r\ndecides to give the first, second, and third place winners each a wall to display the work of their\r\nchoice. The walls boast 31 separate lighting options apiece. How many displays are possible?\r\nAnswer: The judges will pick 3 (ranked) winners out of 11 (with rep = FALSE, ord = TRUE).\r\nEach artist will select 4 of his/her paintings from 7 for display in a row (rep = FALSE, ord =\r\nTRUE), and lastly, each of the 3 walls has 31 lighting possibilities (rep = TRUE, ord = TRUE).\r\nThese three numbers can be calculated quickly with\r\n> n <- c(11, 7, 31)\r\n> k <- c(3, 4, 3)\r\n> r <- c(FALSE, FALSE, TRUE)\r\n> x <- nsamp(n, k, rep = r, ord = TRUE)\r\n[1] 990 840 29791\r\n(Notice that ordered is always TRUE; nsamp will recycle ordered and replace to the appro\u0002priate length.) By the Multiplication Principle, the number of ways to complete the experiment is\r\nthe product of the entries of x:\r\n> prod(x)\r\n[1] 24774195600\r\nCompare this with the some other ways to compute the same thing:\r\n> (11 * 10 * 9) * (7 * 6 * 5 * 4) * 313",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/aea768a7-6d6d-4c89-994c-a8208abe37ca.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bec8b2524855cb1b2f851e661858b05479840f03ef65e56851c4b9d05c67c3cf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 359
      },
      {
        "segments": [
          {
            "segment_id": "b81170df-3e75-4dfd-830a-d01d61e4597d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 107,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.5. COUNTING METHODS 91\r\n[1] 260290800\r\nor alternatively\r\n> prod(9:11) * prod(4:7) * 313\r\n[1] 260290800\r\nor even\r\n> prod(factorial(c(11, 7))/factorial(c(8, 3))) * 313\r\n[1] 260290800\r\nAs one can guess, in many of the standard counting problems there aren’t substantial savings\r\nin the amount of typing; it is about the same using nsamp versus factorial and choose. But the\r\nvirtue of nsamp lies in its collecting the relevant counting formulas in a one-stop shop. Ultimately,\r\nit is up to the user to choose the method that works best for him/herself.\r\nExample 4.27. The Birthday Problem. Suppose that there are n people together in a room. Each\r\nperson announces the date of his/her birthday in turn. The question is: what is the probability of\r\nat least one match? If we let the event A represent {there is at least one match}, then would like to\r\nknow IP(A), but as we will see, it is more convenient to calculate IP(A\r\nc\r\n).\r\nFor starters we will ignore leap years and assume that there are only 365 days in a year. Second,\r\nwe will assume that births are equally distributed over the course of a year (which is not true due to\r\nall sorts of complications such as hospital delivery schedules). See http://en.wikipedia.org/\r\nwiki/Birthday_problem for more.\r\nLet us next think about the sample space. There are 365 possibilities for the first person’s\r\nbirthday, 365 possibilities for the second, and so forth. The total number of possible birthday\r\nsequences is therefore #(S ) = 365n.\r\nNow we will use the complementation trick we saw in Example 4.11. We realize that the only\r\nsituation in which A does not occur is if there are no matches among all people in the room, that is,\r\nonly when everybody’s birthday is different, so\r\nIP(A) = 1 − IP(A\r\nc\r\n) = 1 −\r\n#(A\r\nc\r\n)\r\n#(S )\r\n,\r\nsince the outcomes are equally likely. Let us then suppose that there are no matches. The first\r\nperson has one of 365 possible birthdays. The second person must not match the first, thus, the\r\nsecond person has only 364 available birthdays from which to choose. Similarly, the third person\r\nhas only 363 possible birthdays, and so forth, until we reach the n\r\nth person, who has only 365−n+1\r\nremaining possible days for a birthday. By the Multiplication Principle, we have #(A\r\nc\r\n) = 365 ·\r\n364 · · · (365 − n + 1), and\r\nIP(A) = 1 −\r\n365 · 364 · · · (365 − n + 1)\r\n365n\r\n= 1 −\r\n364\r\n365 ·\r\n363\r\n365 · · ·\r\n(365 − n + 1)\r\n365 . (4.5.2)\r\nAs a surprising consequence, consider this: how many people does it take to be in the room so\r\nthat the probability of at least one match is at least 0.50? Clearly, if there is only n = 1 person in\r\nthe room then the probability of a match is zero, and when there are n = 366 people in the room",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b81170df-3e75-4dfd-830a-d01d61e4597d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=464f56ea08cd26e96eeff77c38cc7ce22e05479eff127df8b7571d634b18e03c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 504
      },
      {
        "segments": [
          {
            "segment_id": "6c7c104d-7d28-480e-95dc-b4ad2cc45476",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 108,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "92 CHAPTER 4. PROBABILITY\r\n●●●●●●●●●●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●●●●●●●●●●●●●●●●●●●●\r\n0 10 20 30 40 50\r\n0.0 0.2 0.4 0.6 0.8 1.0\r\nNumber of people in room\r\nProb(at least one match)\r\nFigure 4.5.1: The birthday problem\r\nThe horizontal line is at p = 0.50 and the vertical line is at n = 23.\r\nthere is a 100% chance of a match (recall that we are ignoring leap years). So how many people\r\ndoes it take so that there is an equal chance of a match and no match?\r\nWhen I have asked this question to students, the usual response is “somewhere around n = 180\r\npeople” in the room. The reasoning seems to be that in order to get a 50% chance of a match, there\r\nshould be 50% of the available days to be occupied. The number of students in a typical classroom\r\nis 25, so as a companion question I ask students to estimate the probability of a match when there\r\nare n = 25 students in the room. Common estimates are a 1%, or 0.5%, or even 0.1% chance of a\r\nmatch. After they have given their estimates, we go around the room and each student announces\r\ntheir birthday. More often than not, we observe a match in the class, to the students’ disbelief.\r\nStudents are usually surprised to hear that, using the formula above, one needs only n = 23\r\nstudents to have a greater than 50% chance of at least one match. Figure 4.5.1 shows a graph of the\r\nbirthday probabilities:\r\n4.5.4 How to do it with R\r\nWe can make the plot in Figure 4.5.1 with the following sequence of commands.\r\ng <- Vectorize ( pbirthday .ipsur)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6c7c104d-7d28-480e-95dc-b4ad2cc45476.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b905be6461628d3f4b3c8122492cf04307474c95f107e3adbf0a81fe485449d2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 297
      },
      {
        "segments": [
          {
            "segment_id": "4745d101-3a13-4d55-a92b-cf52129f1c29",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 109,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.6. CONDITIONAL PROBABILITY 93\r\nplot (1:50 , g(1:50) ,\r\nxlab = \"Number of people in room\",\r\nylab = \"Prob(at least one match)\",\r\nmain = \"The Birthday Problem\")\r\nabline(h = 0.5)\r\nabline(v = 23, lty = 2) # dashed line\r\nThere is a Birthday problem item in the Probability menu of RcmdrPlugin.IPSUR.\r\nIn the base R version, one can compute approximate probabilities for the more general case of\r\nprobabilities other than 1/2, for differing total number of days in the year, and even for more than\r\ntwo matches.\r\n4.6 Conditional Probability\r\nConsider a full deck of 52 standard playing cards. Now select two cards from the deck, in succes\u0002sion. Let A = {first card drawn is an Ace} and B = {second card drawn is an Ace}. Since there are\r\nfour Aces in the deck, it is natural to assign IP(A) = 4/52. Suppose we look at the first card. What\r\nnow is the probability of B? Of course, the answer depends on the value of the first card. If the first\r\ncard is an Ace, then the probability that the second also is an Ace should be 3/51, but if the first\r\ncard is not an Ace, then the probability that the second is an Ace should be 4/51. As notation for\r\nthese two situations we write\r\nIP(B|A) = 3/51, IP(B|A\r\nc\r\n) = 4/51.\r\nDefinition 4.28. The conditional probability of B given A, denoted IP(B|A), is defined by\r\nIP(B|A) =\r\nIP(A ∩ B)\r\nIP(A)\r\n, if IP(A) > 0. (4.6.1)\r\nWe will not be discussing a conditional probability of B given A when IP(A) = 0, even though this\r\ntheory exists, is well developed, and forms the foundation for the study of stochastic processes3.\r\nExample 4.29. Toss a coin twice. The sample space is given by S = {HH, HT, T H, T T}. Let\r\nA = {a head occurs} and B = {a head and tail occur}. It should be clear that IP(A) = 3/4, IP(B) =\r\n2/4, and IP(A ∩ B) = 2/4. What now are the probabilities IP(A|B) and IP(B|A)?\r\nIP(A|B) =\r\nIP(A ∩ B)\r\nIP(B)\r\n=\r\n2/4\r\n2/4\r\n= 1,\r\nin other words, once we know that a Head and Tail occur, we may be certain that a Head occurs.\r\nNext\r\nIP(B|A) =\r\nIP(A ∩ B)\r\nIP(A)\r\n=\r\n2/4\r\n3/4\r\n=\r\n2\r\n3\r\n,\r\nwhich means that given the information that a Head has occurred, we no longer need to account for\r\nthe outcome T T, and the remaining three outcomes are equally likely with exactly two outcomes\r\nlying in the set B.\r\n3Conditional probability in this case is defined by means of conditional expectation, a topic that is well beyond the scope\r\nof this text. The interested reader should consult an advanced text on probability theory, such as Billingsley, Resnick, or\r\nAsh Dooleans-Dade.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4745d101-3a13-4d55-a92b-cf52129f1c29.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c892e3e7aa83c66f8d34613e34ef5ade76d0a938e5c34586a5c463f80a9d6251",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 469
      },
      {
        "segments": [
          {
            "segment_id": "7af21fab-c257-4591-8562-400f3dcabadb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 110,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "94 CHAPTER 4. PROBABILITY First Roll\r\nSecond Roll\r\n1 2 3 4 5 6\r\n1\r\n\u0010\r\n2\r\n\u0010 \r\r\n3\r\n\u0010 \r \r\r\n4 ⊗ \r \r\r\n5 \r \r ⊗ \r\r\n6 \r \r \r \r ⊗\r\nTable 4.2: Rolling two dice\r\nExample 4.30. Toss a six-sided die twice. The sample space consists of all ordered pairs (i, j)\r\nof the numbers 1, 2, . . . , 6, that is, S = {(1, 1), (1, 2), . . . ,(6, 6)}. We know from Section 4.5 that\r\n#(S ) = 6\r\n2 = 36. Let A = {outcomes match} and B = {sum of outcomes at least 8}. The sample\r\nspace may be represented by a matrix:\r\nThe outcomes lying in the event A are marked with the symbol “\u0010”, the outcomes falling in B\r\nare marked with “\r”, and those in both A and B are marked “⊗”. Now it is clear that IP(A) = 6/36,\r\nIP(B) = 15/36, and IP(A ∩ B) = 3/36. Finally,\r\nIP(A|B) =\r\n3/36\r\n15/36\r\n=\r\n1\r\n5\r\n, IP(B|A) =\r\n3/36\r\n6/36\r\n=\r\n1\r\n2\r\n.\r\nAgain, we see that given the knowledge that B occurred (the 15 outcomes in the lower right tri\u0002angle), there are 3 of the 15 that fall into the set A, thus the probability is 3/15. Similarly, given\r\nthat A occurred (we are on the diagonal), there are 3 out of 6 outcomes that also fall in B, thus, the\r\nprobability of B given A is 1/2.\r\n4.6.1 How to do it with R\r\nContinuing with Example 4.30, the first thing to do is set up the probability space with the rolldie\r\nfunction.\r\n> library(prob)\r\n> S <- rolldie(2, makespace = TRUE) # assumes ELM\r\n> head(S) # first few rows\r\nX1 X2 probs\r\n1 1 1 0.02777778\r\n2 2 1 0.02777778\r\n3 3 1 0.02777778\r\n4 4 1 0.02777778\r\n5 5 1 0.02777778\r\n6 6 1 0.02777778\r\nNext we define the events",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7af21fab-c257-4591-8562-400f3dcabadb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c6c6d0cbf32199ad332128d61e99d6f8dc248c5a1cfcd4cc674e3c4c98c199dd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 316
      },
      {
        "segments": [
          {
            "segment_id": "7717e371-9bb4-4769-bbb0-47fb85d7c387",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 111,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.6. CONDITIONAL PROBABILITY 95\r\n> A <- subset(S, X1 == X2)\r\n> B <- subset(S, X1 + X2 >= 8)\r\nAnd now we are ready to calculate probabilities. To do conditional probability, we use the\r\ngiven argument of the prob function:\r\n> prob(A, given = B)\r\n[1] 0.2\r\n> prob(B, given = A)\r\n[1] 0.5\r\nNote that we do not actually need to define the events A and B separately as long as we reference\r\nthe original probability space S as the first argument of the prob calculation:\r\n> prob(S, X1==X2, given = (X1 + X2 >= 8) )\r\n[1] 0.2\r\n> prob(S, X1+X2 >= 8, given = (X1==X2) )\r\n[1] 0.5\r\n4.6.2 Properties and Rules\r\nThe following theorem establishes that conditional probabilities behave just like regular probabili\u0002ties when the conditioned event is fixed.\r\nTheorem 4.31. For any fixed event A with IP(A) > 0,\r\n1. IP(B|A) ≥ 0, for all events B ⊂ S ,\r\n2. IP(S |A) = 1, and\r\n3. If B1, B2, B3,. . . are disjoint events, then\r\nIP\r\n\r\n\r\n[∞\r\nk=1\r\nBk\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\nA\r\n\r\n =\r\nX∞\r\nk=1\r\nIP(Bk|A). (4.6.2)\r\nIn other words, IP(·|A) is a legitimate probability function. With this fact in mind, the following\r\nproperties are immediate:\r\nProposition 4.32. For any events A, B, and C with IP(A) > 0,\r\n1. IP(B\r\nc\r\n|A) = 1 − IP(B|A).\r\n2. If B ⊂ C then IP(B|A) ≤ IP(C|A).\r\n3. IP[(B ∪ C)|A] = IP(B|A) + IP(C|A) − IP[(B ∩ C|A)].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7717e371-9bb4-4769-bbb0-47fb85d7c387.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e9f4a59b326fac038083b67406536e4eceb8162fa5f28f7a59ec07e23e31d7db",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 248
      },
      {
        "segments": [
          {
            "segment_id": "ad906231-7ee7-404b-b7e4-1ef2adfd8ffa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 112,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "96 CHAPTER 4. PROBABILITY\r\n4. The Multiplication Rule. For any two events A and B,\r\nIP(A ∩ B) = IP(A) IP(B|A). (4.6.3)\r\nAnd more generally, for events A1, A2, A3,. . . , An,\r\nIP(A1 ∩ A2 ∩ · · · ∩ An) = IP(A1) IP(A2|A1) · · · IP(An|A1 ∩ A2 ∩ · · · ∩ An−1). (4.6.4)\r\nThe Multiplication Rule is very important because it allows us to find probabilities in random\r\nexperiments that have a sequential structure, as the next example shows.\r\nExample 4.33. At the beginning of the section we drew two cards from a standard playing deck.\r\nNow we may answer our original question, what is IP(both Aces)?\r\nIP(both Aces) = IP(A ∩ B) = IP(A) IP(B|A) =\r\n4\r\n52 ·\r\n3\r\n51 ≈ 0.00452.\r\n4.6.3 How to do it with R\r\nContinuing Example 4.33, we set up the probability space by way of a three step process. First we\r\nemploy the cards function to get a data frame L with two columns: rank and suit. Both columns\r\nare stored internally as factors with 13 and 4 levels, respectively.\r\nNext we sample two cards randomly from the L data frame by way of the urnsamples function.\r\nIt returns a list M which contains all possible pairs of rows from L (there are choose(52,2) of\r\nthem). The sample space for this experiment is exactly the list M.\r\nAt long last we associate a probability model with the sample space. This is right down the\r\nprobspace function’s alley. It assumes the equally likely model by default. We call this result N\r\nwhich is an object of class ps – short for “probability space”.\r\nBut do not be intimidated. The object N is nothing more than a list with two elements: outcomes\r\nand probs. The outcomes element is itself just another list, with choose(52,2) entries, each one\r\na data frame with two rows which correspond to the pair of cards chosen. The probs element is\r\njust a vector with choose(52,2) entries all the same: 1/choose(52,2).\r\nPutting all of this together we do\r\n> library(prob)\r\n> L <- cards()\r\n> M <- urnsamples(L, size = 2)\r\n> N <- probspace(M)\r\nNow that we have the probability space N we are ready to do some probability. We use the prob\r\nfunction, just like before. The only trick is to specify the event of interest correctly, and recall that\r\nwe were interested in IP(both Aces). But if the cards are both Aces then the rank of both cards\r\nshould be \"A\", which sounds like a job for the all function:\r\n> prob(N, all(rank == \"A\"))\r\n[1] 0.004524887\r\nNote that this value matches what we found in Example 4.33, above. We could calculate all\r\nsorts of probabilities at this point; we are limited only by the complexity of the event’s computer\r\nrepresentation.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ad906231-7ee7-404b-b7e4-1ef2adfd8ffa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f157b544385576f4962c0a77372f5b882bf2ed0618c5ecd1101c33d9bf0a36e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 470
      },
      {
        "segments": [
          {
            "segment_id": "72c3f6cb-3dde-47e6-99d8-dbb46e49782f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 113,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.6. CONDITIONAL PROBABILITY 97\r\nExample 4.34. Consider an urn with 10 balls inside, 7 of which are red and 3 of which are green.\r\nSelect 3 balls successively from the urn. Let A =\r\n\b\r\n1\r\nst ball is red\t\r\n, B =\r\nn\r\n2\r\nnd ball is redo\r\n, and C =\r\nn\r\n3\r\nrd ball is redo\r\n. Then\r\nIP(all 3 balls are red) = IP(A ∩ B ∩ C) =\r\n7\r\n10\r\n·\r\n6\r\n9\r\n·\r\n5\r\n8\r\n≈ 0.2917.\r\n4.6.4 How to do it with R\r\nExample 4.34 is similar to Example 4.33, but it is even easier. We need to set up an urn (vector\r\nL) to hold the balls, we sample from L to get the sample space (data frame M), and we associate a\r\nprobability vector (column probs) with the outcomes (rows of M) of the sample space. The final\r\nresult is a probability space (an ordinary data frame N).\r\nIt is easier for us this time because our urn is a vector instead of a cards() data frame. Be\u0002fore there were two dimensions of information associated with the outcomes (rank and suit) but\r\npresently we have only one dimension (color).\r\n> library(prob)\r\n> L <- rep(c(\"red\", \"green\"), times = c(7, 3))\r\n> M <- urnsamples(L, size = 3, replace = FALSE, ordered = TRUE)\r\n> N <- probspace(M)\r\nNow let us think about how to set up the event {all 3 balls are red}. Rows of N that satisfy this\r\ncondition have X1==\"red\"& X2==\"red\"& X3==\"red\", but there must be an easier way. Indeed,\r\nthere is. The isrep function (short for “is repeated”) in the prob package was written for this\r\npurpose. The command isrep(N,\"red\",3) will test each row of N to see whether the value\r\n\"red\" appears 3 times. The result is exactly what we need to define an event with the prob\r\nfunction. Observe\r\n> prob(N, isrep(N, \"red\", 3))\r\n[1] 0.2916667\r\nNote that this answer matches what we found in Example 4.34. Now let us try some other\r\nprobability questions. What is the probability of getting two \"red\"s?\r\n> prob(N, isrep(N, \"red\", 2))\r\n[1] 0.525\r\nNote that the exact value is 21/40; we will learn a quick way to compute this in Section 5.6.\r\nWhat is the probability of observing \"red\", then \"green\", then \"red\"?\r\n> prob(N, isin(N, c(\"red\", \"green\", \"red\"), ordered = TRUE))\r\n[1] 0.175\r\nNote that the exact value is 7/20 (do it with the Multiplication Rule). What is the probability\r\nof observing \"red\", \"green\", and \"red\", in no particular order?",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/72c3f6cb-3dde-47e6-99d8-dbb46e49782f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0b9a318e101911dd1f6efb74d141d1ed0c6f47dda3631c5eee9c4d581ebdc9ab",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 422
      },
      {
        "segments": [
          {
            "segment_id": "67b6147e-a552-45eb-80c0-3bdf8b23d05f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 114,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "98 CHAPTER 4. PROBABILITY\r\n> prob(N, isin(N, c(\"red\", \"green\", \"red\")))\r\n[1] 0.525\r\nWe already knew this. It is the probability of observing two \"red\"s, above.\r\nExample 4.35. Consider two urns, the first with 5 red balls and 3 green balls, and the second with\r\n2 red balls and 6 green balls. Your friend randomly selects one ball from the first urn and transfers\r\nit to the second urn, without disclosing the color of the ball. You select one ball from the second\r\nurn. What is the probability that the selected ball is red? Let A = {transferred ball is red} and\r\nB = {selected ball is red}. Write\r\nB = S ∩ B\r\n= (A ∪ A\r\nc\r\n) ∩ B\r\n= (A ∩ B) ∪ (A\r\nc ∩ B)\r\nand notice that A ∩ B and A\r\nc ∩ B are disjoint. Therefore\r\nIP(B) = IP(A ∩ B) + IP(A\r\nc ∩ B)\r\n= IP(A) IP(B|A) + IP(A\r\nc\r\n) IP(B|A\r\nc\r\n)\r\n=\r\n5\r\n8\r\n·\r\n3\r\n9\r\n+\r\n3\r\n8\r\n·\r\n2\r\n9\r\n=\r\n21\r\n72\r\n(which is 7/24 in lowest terms).\r\nExample 4.36. We saw the RcmdrTestDrive data set in Chapter 2 in which a two-way table of\r\nthe smoking status versus the gender was\r\ngender\r\nsmoke Female Male Sum\r\nNo 80 54 134\r\nYes 15 19 34\r\nSum 95 73 168\r\nIf one person were selected at random from the data set, then we see from the two-way table\r\nthat IP(Female) = 70/168 and IP(Smoker) = 32/168. Now suppose that one of the subjects quits\r\nsmoking, but we do not know the person’s gender. If we select one subject at random, what now is\r\nIP(Female)? Let A =\r\n\b\r\nthe quitter is a female\tand B =\r\n\b\r\nselected person is a female\t. Write\r\nB = S ∩ B\r\n= (A ∪ A\r\nc\r\n) ∩ B\r\n= (A ∩ B) ∪ (A\r\nc ∩ B)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/67b6147e-a552-45eb-80c0-3bdf8b23d05f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f288648e2f1e16abeedefb80e32f19d9242cddd2bd32a991ca8c4f7b2dcc7ba",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 322
      },
      {
        "segments": [
          {
            "segment_id": "2576210d-c495-4cb6-ba6e-0d765b299ece",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 115,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.7. INDEPENDENT EVENTS 99\r\nand notice that A ∩ B and A\r\nc ∩ B are disjoint. Therefore\r\nIP(B) = IP(A ∩ B) + IP(A\r\nc ∩ B),\r\n= IP(A) IP(B|A) + IP(A\r\nc\r\n) IP(B|A\r\nc\r\n),\r\n=\r\n5\r\n8\r\n·\r\n3\r\n9\r\n+\r\n3\r\n8\r\n·\r\n2\r\n9\r\n,\r\n=\r\n21\r\n72\r\n,\r\n(which is 7/24 in lowest terms).\r\nUsing the same reasoning, we can return to the example from the beginning of the section and\r\nshow that\r\nIP({second card is an Ace}) = 4/52.\r\n.\r\n4.7 Independent Events\r\nToss a coin twice. The sample space is S = {HH, HT, T H, T T}. We know that IP(1st toss is H) =\r\n2/4, IP(2nd toss is H) = 2/4, and IP(both H) = 1/4. Then\r\nIP(2nd toss is H | 1\r\nst toss is H) =\r\nIP(both H)\r\nIP(1st toss is H)\r\n,\r\n=\r\n1/4\r\n2/4\r\n,\r\n= IP(2nd toss is H).\r\nIntuitively, this means that the information that the first toss is H has no bearing on the probability\r\nthat the second toss is H. The coin does not remember the result of the first toss.\r\nDefinition 4.37. Events A and B are said to be independent if\r\nIP(A ∩ B) = IP(A) IP(B). (4.7.1)\r\nOtherwise, the events are said to be dependent.\r\nThe connection with the above example stems from the following. We know from Section 4.6\r\nthat when IP(B) > 0 we may write\r\nIP(A|B) =\r\nIP(A ∩ B)\r\nIP(B)\r\n. (4.7.2)\r\nIn the case that A and B are independent, the numerator of the fraction factors so that IP(B) cancels\r\nwith the result:\r\nIP(A|B) = IP(A) when A, B are independent. (4.7.3)\r\nThe interpretation in the case of independence is that the information that the event B occurred\r\ndoes not influence the probability of the event A occurring. Similarly, IP(B|A) = IP(B), and so",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/2576210d-c495-4cb6-ba6e-0d765b299ece.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0316769e71ce726c9a321801ed6bfca2b49641683ad5ef537d6ce941a418624d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 311
      },
      {
        "segments": [
          {
            "segment_id": "497f573e-1c59-48a5-aeb4-03652919fc58",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 116,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "100 CHAPTER 4. PROBABILITY\r\nthe occurrence of the event A likewise does not affect the probability of event B. It may seem\r\nmore natural to define A and B to be independent when IP(A|B) = IP(A); however, the conditional\r\nprobability IP(A|B) is only defined when IP(B) > 0. Our definition is not limited by this restriction.\r\nIt can be shown that when IP(A), IP(B) > 0 the two notions of independence are equivalent.\r\nProposition 4.38. If the events A and B are independent then\r\n• A and Bc are independent,\r\n• A\r\nc and B are independent,\r\n• A\r\nc and Bc are independent.\r\nProof. Suppose that A and B are independent. We will show the second one; the others are similar.\r\nWe need to show that\r\nIP(A\r\nc ∩ B) = IP(Ac\r\n) IP(B).\r\nTo this end, note that the Multiplication Rule, Equation 4.6.3 implies\r\nIP(A\r\nc ∩ B) = IP(B) IP(Ac\r\n|B),\r\n= IP(B)[1 − IP(A|B)],\r\n= IP(B) IP(A\r\nc\r\n).\r\n\u0003\r\nDefinition 4.39. The events A, B, and C are mutually independent if the following four conditions\r\nare met:\r\nIP(A ∩ B) = IP(A) IP(B),\r\nIP(A ∩ C) = IP(A) IP(C),\r\nIP(B ∩ C) = IP(B) IP(C),\r\nand\r\nIP(A ∩ B ∩ C) = IP(A) IP(B) IP(C).\r\nIf only the first three conditions hold then A, B, and C are said to be independent pairwise. Note\r\nthat pairwise independence is not the same as mutual independence when the number of events is\r\nlarger than two.\r\nWe can now deduce the pattern for n events, n > 3. The events will be mutually independent\r\nonly if they satisfy the product equality pairwise, then in groups of three, in groups of four, and\r\nso forth, up to all n events at once. For n events, there will be 2n − n − 1 equations that must\r\nbe satisfied (see Exercise 4.1). Although these requirements for a set of events to be mutually\r\nindependent may seem stringent, the good news is that for most of the situations considered in this\r\nbook the conditions will all be met (or at least we will suppose that they are).\r\nExample 4.40. Toss ten coins. What is the probability of observing at least one Head? Answer:\r\nLet Ai =\r\nn\r\nthe i\r\nth coin shows H\r\no\r\n, i = 1, 2, . . . , 10. Supposing that we toss the coins in such a way\r\nthat they do not interfere with each other, this is one of the situations where all of the Ai may be",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/497f573e-1c59-48a5-aeb4-03652919fc58.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f74efad64c1228ed449b75303f73cbae647b671aec7493cc343a5415eac7b664",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 424
      },
      {
        "segments": [
          {
            "segment_id": "f6f7c73b-e7d1-421b-b4f0-04e21ae4df63",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 117,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.7. INDEPENDENT EVENTS 101\r\nconsidered mutually independent due to the nature of the tossing. Of course, the only way that\r\nthere will not be at least one Head showing is if all tosses are Tails. Therefore,\r\nIP(at least one H) = 1 − IP(all T),\r\n= 1 − IP(A\r\nc\r\n1 ∩ A\r\nc\r\n2 ∩ · · · ∩ A\r\nc\r\n10),\r\n= 1 − IP(A\r\nc\r\n1\r\n) IP(A\r\nc\r\n2\r\n) · · · IP(A\r\nc\r\n10),\r\n= 1 −\r\n \r\n1\r\n2\r\n!10\r\n,\r\nwhich is approximately 0.9990234.\r\n4.7.1 How to do it with R\r\nExample 4.41. Toss ten coins. What is the probability of observing at least one Head?\r\n> S <- tosscoin(10, makespace = TRUE)\r\n> A <- subset(S, isrep(S, vals = \"T\", nrep = 10))\r\n> 1 - prob(A)\r\n[1] 0.9990234\r\nCompare this answer to what we got in Example 4.40.\r\nIndependent, Repeated Experiments\r\nGeneralizing from above it is common to repeat a certain experiment multiple times under identical\r\nconditions and in an independent manner. We have seen many examples of this already: tossing a\r\ncoin repeatedly, rolling a die or dice, etc.\r\nThe iidspace function was designed specifically for this situation. It has three arguments: x,\r\nwhich is a vector of outcomes, ntrials, which is an integer telling how many times to repeat the\r\nexperiment, and probs to specify the probabilities of the outcomes of x in a single trial.\r\nExample 4.42. An unbalanced coin (continued, see Example 4.5). It was easy enough to set up\r\nthe probability space for one unbalanced toss, however, the situation becomes more complicated\r\nwhen there are many tosses involved. Clearly, the outcome HHH should not have the same proba\u0002bility as T T T, which should again not have the same probability as HT H. At the same time, there\r\nis symmetry in the experiment in that the coin does not remember the face it shows from toss to\r\ntoss, and it is easy enough to toss the coin in a similar way repeatedly.\r\nWe may represent tossing our unbalanced coin three times with the following:\r\n> iidspace(c(\"H\",\"T\"), ntrials = 3, probs = c(0.7, 0.3))\r\nX1 X2 X3 probs\r\n1 H H H 0.343\r\n2 T H H 0.147\r\n3 H T H 0.147\r\n4 T T H 0.063\r\n5 H H T 0.147",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f6f7c73b-e7d1-421b-b4f0-04e21ae4df63.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b8f542b5d9110423463469b3c2389d335bf7718cd32dc3a82598ed154991e7d2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 386
      },
      {
        "segments": [
          {
            "segment_id": "31018f64-38aa-4756-9590-2559dc21a176",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 118,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "102 CHAPTER 4. PROBABILITY\r\n6 T H T 0.063\r\n7 H T T 0.063\r\n8 T T T 0.027\r\nAs expected, the outcome HHH has the largest probability, while T T T has the smallest. (Since\r\nthe trials are independent, IP(HHH) = 0.7\r\n3\r\nand IP(T T T) = 0.3\r\n3\r\n, etc.) Note that the result of the\r\nfunction call is a probability space, not a sample space (which we could construct already with the\r\ntosscoin or urnsamples functions). The same procedure could be used to model an unbalanced\r\ndie or any other experiment that may be represented with a vector of possible outcomes.\r\nNote that iidspace will assume x has equally likely outcomes if no probs argument is speci\u0002fied. Also note that the argument x is a vector, not a data frame. Something like iidspace(tosscoin(1),...)\r\nwould give an error.\r\n4.8 Bayes’ Rule\r\nWe mentioned the subjective view of probability in Section 4.3. In this section we introduce a rule\r\nthat allows us to update our probabilities when new information becomes available.\r\nTheorem 4.43. (Bayes’ Rule). Let B1, B2, . . . , Bn be mutually exclusive and exhaustive and let A\r\nbe an event with IP(A) > 0. Then\r\nIP(Bk|A) =\r\nIP(Bk) IP(A|Bk)\r\nPn\r\ni=1\r\nIP(Bi) IP(A|Bi)\r\n, k = 1, 2, . . . , n. (4.8.1)\r\nProof. The proof follows from looking at IP(Bk ∩ A) in two different ways. For simplicity, suppose\r\nthat P(Bk) > 0 for all k. Then\r\nIP(A) IP(Bk|A) = IP(Bk ∩ A) = IP(Bk) IP(A|Bk).\r\nSince IP(A) > 0 we may divide through to obtain\r\nIP(Bk|A) =\r\nIP(Bk) IP(A|Bk)\r\nIP(A)\r\n.\r\nNow remembering that {Bk} is a partition, the Theorem of Total Probability (Equation 4.4.5) gives\r\nthe denominator of the last expression to be\r\nIP(A) =\r\nXn\r\nk=1\r\nIP(Bk ∩ A) =\r\nXn\r\nk=1\r\nIP(Bk) IP(A|Bk).\r\n\u0003\r\nWhat does it mean? Usually in applications we are given (or know) a priori probabilities IP(Bk).\r\nWe go out and collect some data, which we represent by the event A. We want to know: how do\r\nwe update IP(Bk) to IP(Bk|A)? The answer: Bayes’ Rule.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/31018f64-38aa-4756-9590-2559dc21a176.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5f6ee515886cb654d412a572e6f7fa8496b812ecef4830f919e8f9b36a9b0420",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 351
      },
      {
        "segments": [
          {
            "segment_id": "515a0f31-0f45-42c4-85c3-e0c11607ee4b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 119,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.8. BAYES’ RULE 103\r\nExample 4.44. Misfiling Assistants. In this problem, there are three assistants working at a com\u0002pany: Moe, Larry, and Curly. Their primary job duty is to file paperwork in the filing cabinet when\r\npapers become available. The three assistants have different work schedules:\r\nMoe Larry Curly\r\nWorkload 60% 30% 10%\r\nThat is, Moe works 60% of the time, Larry works 30% of the time, and Curly does the remaining\r\n10%, and they file documents at approximately the same speed. Suppose a person were to select\r\none of the documents from the cabinet at random. Let M be the event\r\nM = {Moe filed the document}\r\nand let L and C be the events that Larry and Curly, respectively, filed the document. What are\r\nthese events’ respective probabilities? In the absence of additional information, reasonable prior\r\nprobabilities would just be\r\nMoe Larry Curly\r\nPrior Probability IP(M) = 0.60 IP(L) = 0.30 IP(C) = 0.10\r\nNow, the boss comes in one day, opens up the file cabinet, and selects a file at random. The boss\r\ndiscovers that the file has been misplaced. The boss is so angry at the mistake that (s)he threatens\r\nto fire the one who erred. The question is: who misplaced the file?\r\nThe boss decides to use probability to decide, and walks straight to the workload schedule.\r\n(S)he reasons that, since the three employees work at the same speed, the probability that a ran\u0002domly selected file would have been filed by each one would be proportional to his workload. The\r\nboss notifies Moe that he has until the end of the day to empty his desk.\r\nBut Moe argues in his defense that the boss has ignored additional information. Moe’s like\u0002lihood of having misfiled a document is smaller than Larry’s and Curly’s, since he is a diligent\r\nworker who pays close attention to his work. Moe admits that he works longer than the others,\r\nbut he doesn’t make as many mistakes as they do. Thus, Moe recommends that – before making a\r\ndecision – the boss should update the probability (initially based on workload alone) to incorporate\r\nthe likelihood of having observed a misfiled document.\r\nAnd, as it turns out, the boss has information about Moe, Larry, and Curly’s filing accuracy in\r\nthe past (due to historical performance evaluations). The performance information may be repre\u0002sented by the following table:\r\nMoe Larry Curly\r\nMisfile Rate 0.003 0.007 0.010\r\nIn other words, on the average, Moe misfiles 0.3% of the documents he is supposed to file.\r\nNotice that Moe was correct: he is the most accurate filer, followed by Larry, and lastly Curly. If\r\nthe boss were to make a decision based only on the worker’s overall accuracy, then Curly should\r\nget the axe. But Curly hears this and interjects that he only works a short period during the day,\r\nand consequently makes mistakes only very rarely; there is only the tiniest chance that he misfiled\r\nthis particular document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/515a0f31-0f45-42c4-85c3-e0c11607ee4b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f70f6a2791d46bf8e8fa6786ded1e48f9809e851bf4c130c3f466152c3a0d0c6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 492
      },
      {
        "segments": [
          {
            "segment_id": "3efef37e-e496-4605-a37a-dee2bd602c40",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 120,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "104 CHAPTER 4. PROBABILITY\r\nThe boss would like to use this updated information to update the probabilities for the three\r\nassistants, that is, (s)he wants to use the additional likelihood that the document was misfiled to\r\nupdate his/her beliefs about the likely culprit. Let A be the event that a document is misfiled. What\r\nthe boss would like to know are the three probabilities\r\nIP(M|A), IP(L|A), and IP(C|A).\r\nWe will show the calculation for IP(M|A), the other two cases being similar. We use Bayes’ Rule\r\nin the form\r\nIP(M|A) =\r\nIP(M ∩ A)\r\nIP(A)\r\n.\r\nLet’s try to find IP(M ∩ A), which is just IP(M) · IP(A|M) by the Multiplication Rule. We already\r\nknow IP(M) = 0.6 and IP(A|M) is nothing more than Moe’s misfile rate, given above to be IP(A|M) =\r\n0.003. Thus, we compute\r\nIP(M ∩ A) = (0.6)(0.003) = 0.0018.\r\nUsing the same procedure we may calculate\r\nIP(L|A) = 0.0021 and IP(C|A) = 0.0010.\r\nNow let’s find the denominator, IP(A). The key here is the notion that if a file is misplaced, then\r\neither Moe or Larry or Curly must have filed it; there is no one else around to do the misfiling.\r\nFurther, these possibilities are mutually exclusive. We may use the Theorem of Total Probability\r\n4.4.5 to write\r\nIP(A) = IP(A ∩ M) + IP(A ∩ L) + IP(A ∩ C).\r\nLuckily, we have computed these above. Thus\r\nIP(A) = 0.0018 + 0.0021 + 0.0010 = 0.0049.\r\nTherefore, Bayes’ Rule yields\r\nIP(M|A) =\r\n0.0018\r\n0.0049\r\n≈ 0.37.\r\nThis last quantity is called the posterior probability that Moe misfiled the document, since it incor\u0002porates the observed data that a randomly selected file was misplaced (which is governed by the\r\nmisfile rate). We can use the same argument to calculate\r\nMoe Larry Curly\r\nPosterior Probability IP(M|A) ≈ 0.37 IP(L|A) ≈ 0.43 IP(C|A) ≈ 0.20\r\nThe conclusion: Larry gets the axe. What is happening is an intricate interplay between the\r\ntime on the job and the misfile rate. It is not obvious who the winner (or in this case, loser) will be,\r\nand the statistician needs to consult Bayes’ Rule to determine the best course of action.\r\nExample 4.45. Suppose the boss gets a change of heart and does not fire anybody. But the next\r\nday (s)he randomly selects another file and again finds it to be misplaced. To decide whom to\r\nfire now, the boss would use the same procedure, with one small change. (S)he would not use\r\nthe prior probabilities 60%, 30%, and 10%; those are old news. Instead, she would replace the\r\nprior probabilities with the posterior probabilities just calculated. After the math she will have new\r\nposterior probabilities, updated even more from the day before.\r\nIn this way, probabilities found by Bayes’ rule are always on the cutting edge, always updated\r\nwith respect to the best information available at the time.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3efef37e-e496-4605-a37a-dee2bd602c40.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f24af5e6b38786c8fb9a0dd933b7f0087f327523182678e945902c891fb95a7f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 478
      },
      {
        "segments": [
          {
            "segment_id": "c1b987dd-1e41-4e34-8f05-eecbb2be88fe",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 121,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.8. BAYES’ RULE 105\r\n4.8.1 How to do it with R\r\nThere are not any special functions for Bayes’ Rule in the prob package, but problems like the\r\nones above are easy enough to do by hand.\r\nExample 4.46. Misfiling assistants (continued from Example 4.44). We store the prior probabili\u0002ties and the likelihoods in vectors and go to town.\r\n> prior <- c(0.6, 0.3, 0.1)\r\n> like <- c(0.003, 0.007, 0.01)\r\n> post <- prior * like\r\n> post/sum(post)\r\n[1] 0.3673469 0.4285714 0.2040816\r\nCompare these answers with what we got in Example 4.44. We would replace prior with\r\npost in a future calculation. We could raise like to a power to see how the posterior is affected\r\nby future document mistakes. (Do you see why? Think back to Section 4.7.)\r\nExample 4.47. Let us incorporate the posterior probability (post) information from the last exam\u0002ple and suppose that the assistants misfile seven more documents. Using Bayes’ Rule, what would\r\nthe new posterior probabilities be?\r\n> newprior <- post\r\n> post <- newprior * like^7\r\n> post/sum(post)\r\n[1] 0.0003355044 0.1473949328 0.8522695627\r\nWe see that the individual with the highest probability of having misfiled all eight documents\r\ngiven the observed data is no longer Larry, but Curly.\r\nThere are two important points. First, we did not divide post by the sum of its entries until\r\nthe very last step; we do not need to calculate it, and it will save us computing time to postpone\r\nnormalization until absolutely necessary, namely, until we finally want to interpret them as proba\u0002bilities.\r\nSecond, the reader might be wondering what the boss would get if (s)he skipped the intermedi\u0002ate step of calculating the posterior after only one misfiled document. What if she started from the\r\noriginal prior, then observed eight misfiled documents, and calculated the posterior? What would\r\nshe get? It must be the same answer, of course.\r\n> fastpost <- prior * like^8\r\n> fastpost/sum(fastpost)\r\n[1] 0.0003355044 0.1473949328 0.8522695627\r\nCompare this to what we got in Example 4.45.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c1b987dd-1e41-4e34-8f05-eecbb2be88fe.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=57254552e9bbbb838deb7ab29e171d7efc6f0b0abab07cf9354a2c67b9d8e71b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 332
      },
      {
        "segments": [
          {
            "segment_id": "4e4586b7-012b-4105-b6a0-241f1ea4d746",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 122,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "106 CHAPTER 4. PROBABILITY\r\n4.9 Random Variables\r\nWe already know about experiments, sample spaces, and events. In this section, we are interested\r\nin a number that is associated with the experiment. We conduct a random experiment E and after\r\nlearning the outcome ω in S we calculate a number X. That is, to each outcome ω in the sample\r\nspace we associate a number X(ω) = x.\r\nDefinition 4.48. A random variable X is a function X : S → R that associates to each outcome\r\nω ∈ S exactly one number X(ω) = x.\r\nWe usually denote random variables by uppercase letters such as X, Y, and Z, and we denote\r\ntheir observed values by lowercase letters x, y, and z. Just as S is the set of all possible outcomes\r\nof E, we call the set of all possible values of X the support of X and denote it by S X.\r\nExample 4.49. Let E be the experiment of flipping a coin twice. We have seen that the sample\r\nspace is S = {HH, HT, T H, T T}. Now define the random variable X = the number of heads.\r\nThat is, for example, X(HH) = 2, while X(HT) = 1. We may make a table of the possibilities:\r\nω ∈ S HH HT T H T T\r\nX(ω) = x 2 1 1 0\r\nTaking a look at the second row of the table, we see that the support of X – the set of all numbers\r\nthat X assumes – would be S X = {0, 1, 2}.\r\nExample 4.50. Let E be the experiment of flipping a coin repeatedly until observing a Head.\r\nThe sample space would be S = {H, T H, T T H, T T T H, . . .}. Now define the random variable\r\nY = the number of Tails before the first head. Then the support of Y would be S Y = {0, 1, 2, . . .}.\r\nExample 4.51. Let E be the experiment of tossing a coin in the air, and define the random variable\r\nZ = the time (in seconds) until the coin hits the ground. In this case, the sample space is inconve\u0002nient to describe. Yet the support of Z would be (0, ∞). Of course, it is reasonable to suppose that\r\nthe coin will return to Earth in a short amount of time; in practice, the set (0, ∞) is admittedly too\r\nlarge. However, we will find that in many circumstances it is mathematically convenient to study\r\nthe extended set rather than a restricted one.\r\nThere are important differences between the supports of X, Y, and Z. The support of X is a\r\nfinite collection of elements that can be inspected all at once. And while the support of Y cannot be\r\nexhaustively written down, its elements can nevertheless be listed in a naturally ordered sequence.\r\nRandom variables with supports similar to those of X and Y are called discrete random variables.\r\nWe study these in Chapter 5.\r\nIn contrast, the support of Z is a continuous interval, containing all rational and irrational pos\u0002itive real numbers. For this reason4\r\n, random variables with supports like Z are called continuous\r\nrandom variables, to be studied in Chapter 6.\r\n4.9.1 How to do it with R\r\nThe primary vessel for this task is the addrv function. There are two ways to use it, and we will\r\ndescribe both.\r\n4This isn’t really the reason, but it serves as an effective litmus test at the introductory level. See Billingsley or Resnick.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4e4586b7-012b-4105-b6a0-241f1ea4d746.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7bb4f503c0717715f8c65ea2b3fd097dd3a98b6458e25cb38dafe4e7f3f088ad",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 594
      },
      {
        "segments": [
          {
            "segment_id": "4e4586b7-012b-4105-b6a0-241f1ea4d746",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 122,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "106 CHAPTER 4. PROBABILITY\r\n4.9 Random Variables\r\nWe already know about experiments, sample spaces, and events. In this section, we are interested\r\nin a number that is associated with the experiment. We conduct a random experiment E and after\r\nlearning the outcome ω in S we calculate a number X. That is, to each outcome ω in the sample\r\nspace we associate a number X(ω) = x.\r\nDefinition 4.48. A random variable X is a function X : S → R that associates to each outcome\r\nω ∈ S exactly one number X(ω) = x.\r\nWe usually denote random variables by uppercase letters such as X, Y, and Z, and we denote\r\ntheir observed values by lowercase letters x, y, and z. Just as S is the set of all possible outcomes\r\nof E, we call the set of all possible values of X the support of X and denote it by S X.\r\nExample 4.49. Let E be the experiment of flipping a coin twice. We have seen that the sample\r\nspace is S = {HH, HT, T H, T T}. Now define the random variable X = the number of heads.\r\nThat is, for example, X(HH) = 2, while X(HT) = 1. We may make a table of the possibilities:\r\nω ∈ S HH HT T H T T\r\nX(ω) = x 2 1 1 0\r\nTaking a look at the second row of the table, we see that the support of X – the set of all numbers\r\nthat X assumes – would be S X = {0, 1, 2}.\r\nExample 4.50. Let E be the experiment of flipping a coin repeatedly until observing a Head.\r\nThe sample space would be S = {H, T H, T T H, T T T H, . . .}. Now define the random variable\r\nY = the number of Tails before the first head. Then the support of Y would be S Y = {0, 1, 2, . . .}.\r\nExample 4.51. Let E be the experiment of tossing a coin in the air, and define the random variable\r\nZ = the time (in seconds) until the coin hits the ground. In this case, the sample space is inconve\u0002nient to describe. Yet the support of Z would be (0, ∞). Of course, it is reasonable to suppose that\r\nthe coin will return to Earth in a short amount of time; in practice, the set (0, ∞) is admittedly too\r\nlarge. However, we will find that in many circumstances it is mathematically convenient to study\r\nthe extended set rather than a restricted one.\r\nThere are important differences between the supports of X, Y, and Z. The support of X is a\r\nfinite collection of elements that can be inspected all at once. And while the support of Y cannot be\r\nexhaustively written down, its elements can nevertheless be listed in a naturally ordered sequence.\r\nRandom variables with supports similar to those of X and Y are called discrete random variables.\r\nWe study these in Chapter 5.\r\nIn contrast, the support of Z is a continuous interval, containing all rational and irrational pos\u0002itive real numbers. For this reason4\r\n, random variables with supports like Z are called continuous\r\nrandom variables, to be studied in Chapter 6.\r\n4.9.1 How to do it with R\r\nThe primary vessel for this task is the addrv function. There are two ways to use it, and we will\r\ndescribe both.\r\n4This isn’t really the reason, but it serves as an effective litmus test at the introductory level. See Billingsley or Resnick.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4e4586b7-012b-4105-b6a0-241f1ea4d746.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7bb4f503c0717715f8c65ea2b3fd097dd3a98b6458e25cb38dafe4e7f3f088ad",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 594
      },
      {
        "segments": [
          {
            "segment_id": "0467d291-b132-497d-ada1-57ff356d993a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 123,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.9. RANDOM VARIABLES 107\r\nSupply a Defining Formula\r\nThe first method is based on the transform function. See ?transform. The idea is to write\r\na formula defining the random variable inside the function, and it will be added as a column to\r\nthe data frame. As an example, let us roll a 4-sided die three times, and let us define the random\r\nvariable U = X1 − X2 + X3.\r\n> S <- rolldie(3, nsides = 4, makespace = TRUE)\r\n> S <- addrv(S, U = X1 - X2 + X3)\r\nNow let’s take a look at the values of U. In the interest of space, we will only reproduce the\r\nfirst few rows of S (there are 43 = 64 rows in total).\r\n> head(S)\r\nX1 X2 X3 U probs\r\n1 1 1 1 1 0.015625\r\n2 2 1 1 2 0.015625\r\n3 3 1 1 3 0.015625\r\n4 4 1 1 4 0.015625\r\n5 1 2 1 0 0.015625\r\n6 2 2 1 1 0.015625\r\nWe see from the U column it is operating just like it should. We can now answer questions like\r\n> prob(S, U > 6)\r\n[1] 0.015625\r\nSupply a Function\r\nSometimes we have a function laying around that we would like to apply to some of the outcome\r\nvariables, but it is unfortunately tedious to write out the formula defining what the new variable\r\nwould be. The addrv function has an argument FUN specifically for this case. Its value should be a\r\nlegitimate function from R, such as sum, mean, median, etc. Or, you can define your own function.\r\nContinuing the previous example, let’s define V = max(X1, X2, X3) and W = X1 + X2 + X3.\r\n> S <- addrv(S, FUN = max, invars = c(\"X1\", \"X2\", \"X3\"), name = \"V\")\r\n> S <- addrv(S, FUN = sum, invars = c(\"X1\", \"X2\", \"X3\"), name = \"W\")\r\n> head(S)\r\nX1 X2 X3 U V W probs\r\n1 1 1 1 1 1 3 0.015625\r\n2 2 1 1 2 2 4 0.015625\r\n3 3 1 1 3 3 5 0.015625\r\n4 4 1 1 4 4 6 0.015625\r\n5 1 2 1 0 2 4 0.015625\r\n6 2 2 1 1 2 5 0.015625",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0467d291-b132-497d-ada1-57ff356d993a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=094547123b8ea1574fb4619a6a16b00647978a5faf422972a83f415cd93ff072",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 371
      },
      {
        "segments": [
          {
            "segment_id": "adc1abb3-4bf2-42cc-9662-e8756f95bbfb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 124,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "108 CHAPTER 4. PROBABILITY\r\nNotice that addrv has an invars argument to specify exactly to which columns one would\r\nlike to apply the function FUN. If no input variables are specified, then addrv will apply FUN to\r\nall non-probs columns. Further, addrv has an optional argument name to give the new variable;\r\nthis can be useful when adding several random variables to a probability space (as above). If not\r\nspecified, the default name is “X”.\r\nMarginal Distributions\r\nAs we can see above, often after adding a random variable V to a probability space one will find\r\nthat V has values that are repeated, so that it becomes difficult to understand what the ultimate\r\nbehavior of V actually is. We can use the marginal function to aggregate the rows of the sample\r\nspace by values of V, all the while accumulating the probability associated with V’s distinct values.\r\nContinuing our example from above, suppose we would like to focus entirely on the values and\r\nprobabilities of V = max(X1, X2, X3).\r\n> marginal(S, vars = \"V\")\r\nV probs\r\n1 1 0.015625\r\n2 2 0.109375\r\n3 3 0.296875\r\n4 4 0.578125\r\nWe could save the probability space of V in a data frame and study it further, if we wish. As\r\na final remark, we can calculate the marginal distributions of multiple variables desired using the\r\nvars argument. For example, suppose we would like to examine the joint distribution of V and W.\r\n> marginal(S, vars = c(\"V\", \"W\"))\r\nV W probs\r\n1 1 3 0.015625\r\n2 2 4 0.046875\r\n3 2 5 0.046875\r\n4 3 5 0.046875\r\n5 2 6 0.015625\r\n6 3 6 0.093750\r\n7 4 6 0.046875\r\n8 3 7 0.093750\r\n9 4 7 0.093750\r\n10 3 8 0.046875\r\n11 4 8 0.140625\r\n12 3 9 0.015625\r\n13 4 9 0.140625\r\n14 4 10 0.093750\r\n15 4 11 0.046875\r\n16 4 12 0.015625\r\nNote that the default value of vars is the names of all columns except probs. This can be\r\nuseful if there are duplicated rows in the probability space.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/adc1abb3-4bf2-42cc-9662-e8756f95bbfb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f56e8bc95635ceb57e43f398ab63974ed3c117a68e82a5936d759635722786aa",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "65401d3e-aa6d-4c48-bac4-8a1e978363c6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 125,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "4.9. RANDOM VARIABLES 109\r\nChapter Exercises\r\nExercise 4.1. Prove the assertion given in the text: the number of conditions that the events A1, A2,\r\n. . . , An must satisfy in order to be mutually independent is 2n − n − 1. (Hint: think about Pascal’s\r\ntriangle.)\r\nAnswer: The events must satisfy the product equalities two at a time, of which there are \u0010\r\nn\r\n2\r\n\u0011\r\n, then\r\nthey must satisfy an additional \u0010\r\nn\r\n3\r\n\u0011\r\nconditions three at a time, and so on, until they satisfy the \u0010\r\nn\r\nn\r\n\u0011\r\n= 1\r\ncondition including all n events. In total, there are\r\n \r\nn\r\n2\r\n!\r\n+\r\n \r\nn\r\n3\r\n!\r\n+ · · · +\r\n \r\nn\r\nn\r\n!\r\n=\r\nXn\r\nk=0\r\n \r\nn\r\nk\r\n!\r\n−\r\n\" n\r\n0\r\n!\r\n+\r\n \r\nn\r\n1\r\n!#\r\nconditions to be satisfied, but the binomial series in the expression on the right is the sum of the\r\nentries of the n\r\nth row of Pascal’s triangle, which is 2n\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/65401d3e-aa6d-4c48-bac4-8a1e978363c6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5e81b1a02b9f7fbdac477ff4d01a09c5b0d759288dffa4440234a578629d64cf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 510
      },
      {
        "segments": [
          {
            "segment_id": "ca31b2b5-f860-4e9b-9175-350106b76166",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 126,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "110 CHAPTER 4. PROBABILITY",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ca31b2b5-f860-4e9b-9175-350106b76166.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1fe0b5a403e3867467c6b56685929b30f5110073232d72be370ffba0a2a7060c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "0a6297f5-902a-4ac5-9fbf-f9ebc723c513",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 127,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 5\r\nDiscrete Distributions\r\nIn this chapter we introduce discrete random variables, those who take values in a finite or countably\r\ninfinite support set. We discuss probability mass functions and some special expectations, namely,\r\nthe mean, variance and standard deviation. Some of the more important discrete distributions are\r\nexplored in detail, and the more general concept of expectation is defined, which paves the way for\r\nmoment generating functions.\r\nWe give special attention to the empirical distribution since it plays such a fundamental role\r\nwith respect to re sampling and Chapter 13; it will also be needed in Section 10.5.1 where we\r\ndiscuss the Kolmogorov-Smirnov test. Following this is a section in which we introduce a catalogue\r\nof discrete random variables that can be used to model experiments.\r\nThere are some comments on simulation, and we mention transformations of random variables\r\nin the discrete case. The interested reader who would like to learn more about any of the assorted\r\ndiscrete distributions mentioned here should take a look at Univariate Discrete Distributions by\r\nJohnson et al [50].\r\nWhat do I want them to know?\r\n• how to choose a reasonable discrete model under a variety of physical circumstances\r\n• the notion of mathematical expectation, how to calculate it, and basic properties\r\n• moment generating functions (yes, I want them to hear about those)\r\n• the general tools of the trade for manipulation of continuous random variables, integration,\r\netc.\r\n• some details on a couple of discrete models, and exposure to a bunch of other ones\r\n• how to make new discrete random variables from old ones\r\n111",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0a6297f5-902a-4ac5-9fbf-f9ebc723c513.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9da24b62594f81a021e851fb04025204517f75bffc19ca4b660fd6bf51ffdbd3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 269
      },
      {
        "segments": [
          {
            "segment_id": "e6f4bb80-3c19-4753-9f69-e8363895484c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 128,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "112 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\n5.1 Discrete Random Variables\r\n5.1.1 Probability Mass Functions\r\nDiscrete random variables are characterized by their supports which take the form\r\nS X = {u1, u2, . . . , uk} or S X = {u1, u2, u3 . . .}. (5.1.1)\r\nEvery discrete random variable X has associated with it a probability mass function (PMF) fX :\r\nS X → [0, 1] defined by\r\nfX(x) = IP(X = x), x ∈ S X. (5.1.2)\r\nSince values of the PMF represent probabilities, we know from Chapter 4 that PMFs enjoy certain\r\nproperties. In particular, all PMFs satisfy\r\n1. fX(x) > 0 for x ∈ S ,\r\n2. P\r\nx∈S\r\nfX(x) = 1, and\r\n3. IP(X ∈ A) =\r\nP\r\nx∈A fX(x), for any event A ⊂ S .\r\nExample 5.1. Toss a coin 3 times. The sample space would be\r\nS = {HHH, HT H, T HH, T T H, HHT, HT T, T HT, T T T} .\r\nNow let X be the number of Heads observed. Then X has support S X = {0, 1, 2, 3}. Assuming that\r\nthe coin is fair and was tossed in exactly the same way each time, it is not unreasonable to suppose\r\nthat the outcomes in the sample space are all equally likely. What is the PMF of X? Notice that\r\nX is zero exactly when the outcome T T T occurs, and this event has probability 1/8. Therefore,\r\nfX(0) = 1/8, and the same reasoning shows that fX(3) = 1/8. Exactly three outcomes result in\r\nX = 1, thus, fX(1) = 3/8 and fX(3) holds the remaining 3/8 probability (the total is 1). We can\r\nrepresent the PMF with a table:\r\nx ∈ S X 0 1 2 3 Total\r\nfX(x) = IP(X = x) 1/8 3/8 3/8 1/8 1\r\n5.1.2 Mean, Variance, and Standard Deviation\r\nThere are numbers associated with PMFs. One important example is the mean µ, also known as\r\nIE X (which we will discuss later):\r\nµ = IE X =\r\nX\r\nx∈S\r\nx fX(x), (5.1.3)\r\nprovided the (potentially infinite) series P|x| fX(x) is convergent. Another important number is the\r\nvariance:\r\nσ\r\n2 =\r\nX\r\nx∈S\r\n(x − µ)\r\n2\r\nfX(x), (5.1.4)\r\nwhich can be computed (see Exercise 5.3) with the alternate formula σ\r\n2 =\r\nP\r\nx\r\n2\r\nfX(x)−µ\r\n2\r\n. Directly\r\ndefined from the variance is the standard deviation σ =\r\n√\r\nσ2.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e6f4bb80-3c19-4753-9f69-e8363895484c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a878514f1b8d98585f21bad345e2a82a5ac8294874c86dbe5b9f0288b2758c5b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 402
      },
      {
        "segments": [
          {
            "segment_id": "74a81425-069b-4ab8-afba-8582c803e2e2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 129,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.1. DISCRETE RANDOM VARIABLES 113\r\nExample 5.2. We will calculate the mean of X in Example 5.1.\r\nµ =\r\nX\r\n3\r\nx=0\r\nx fX(x) = 0 ·\r\n1\r\n8\r\n+ 1 ·\r\n3\r\n8\r\n+ 2 ·\r\n3\r\n8\r\n+ 3 ·\r\n1\r\n8\r\n= 3.5.\r\nWe interpret µ = 3.5 by reasoning that if we were to repeat the random experiment many times,\r\nindependently each time, observe many corresponding outcomes of the random variable X, and\r\ntake the sample mean of the observations, then the calculated value would fall close to 3.5. The\r\napproximation would get better as we observe more and more values of X (another form of the Law\r\nof Large Numbers; see Section 4.3). Another way it is commonly stated is that X is 3.5 “on the\r\naverage” or “in the long run”.\r\nRemark 5.3. Note that although we say X is 3.5 on the average, we must keep in mind that our X\r\nnever actually equals 3.5 (in fact, it is impossible for X to equal 3.5).\r\nRelated to the probability mass function fX(x) = IP(X = x) is another important function called\r\nthe cumulative distribution function (CDF), FX. It is defined by the formula\r\nFX(t) = IP(X ≤ t), −∞ < t < ∞. (5.1.5)\r\nWe know that all PMFs satisfy certain properties, and a similar statement may be made for\r\nCDFs. In particular, any CDF FX satisfies\r\n• FX is nondecreasing (t1 ≤ t2 implies FX(t1) ≤ FX(t2)).\r\n• FX is right-continuous (limt→a\r\n+ FX(t) = FX(a) for all a ∈ R).\r\n• limt→−∞ FX(t) = 0 and limt→∞ FX(t) = 1.\r\nWe say that X has the distribution FX and we write X ∼ FX. In an abuse of notation we will also\r\nwrite X ∼ fX and for the named distributions the PMF or CDF will be identified by the family name\r\ninstead of the defining formula.\r\n5.1.3 How to do it with R\r\nThe mean and variance of a discrete random variable is easy to compute at the console. Let’s return\r\nto Example 5.2. We will start by defining a vector x containing the support of X, and a vector f to\r\ncontain the values of fX at the respective outcomes in x:\r\n> x <- c(0,1,2,3)\r\n> f <- c(1/8, 3/8, 3/8, 1/8)\r\nTo calculate the mean µ, we need to multiply the corresponding values of x and f and add\r\nthem. This is easily accomplished in R since operations on vectors are performed element-wise\r\n(see Section 2.3.4):\r\n> mu <- sum(x * f)\r\n> mu\r\n[1] 1.5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/74a81425-069b-4ab8-afba-8582c803e2e2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3e2c724b49c6f570868521dc68ae94ad77ea7ffea2d93e6e6902fb9a0305f4f6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 429
      },
      {
        "segments": [
          {
            "segment_id": "0f890012-a739-4057-a769-ef1433f25cf9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 130,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "114 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nTo compute the variance σ\r\n2\r\n, we subtract the value of mu from each entry in x, square the\r\nanswers, multiply by f, and sum. The standard deviation σ is simply the square root of σ\r\n2\r\n.\r\n> sigma2 <- sum((x-mu)^2 * f)\r\n> sigma2\r\n[1] 0.75\r\n> sigma <- sqrt(sigma2)\r\n> sigma\r\n[1] 0.8660254\r\nFinally, we may find the values of the CDF FX on the support by accumulating the probabilities\r\nin fX with the cumsum function.\r\n> F = cumsum(f)\r\n> F\r\n[1] 0.125 0.500 0.875 1.000\r\nAs easy as this is, it is even easier to do with the distrEx package [74]. We define a random\r\nvariable X as an object, then compute things from the object such as mean, variance, and standard\r\ndeviation with the functions E, var, and sd:\r\n> library(distrEx)\r\n> X <- DiscreteDistribution(supp = 0:3, prob = c(1,3,3,1)/8)\r\n> E(X); var(X); sd(X)\r\n[1] 1.5\r\n[1] 0.75\r\n[1] 0.8660254\r\n5.2 The Discrete Uniform Distribution\r\nWe have seen the basic building blocks of discrete distributions and we now study particular models\r\nthat statisticians often encounter in the field. Perhaps the most fundamental of all is the discrete\r\nuniform distribution.\r\nA random variable X with the discrete uniform distribution on the integers 1, 2, . . . , m has PMF\r\nfX(x) =\r\n1\r\nm\r\n, x = 1, 2, . . . , m. (5.2.1)\r\nWe write X ∼ disunif(m). A random experiment where this distribution occurs is the choice of\r\nan integer at random between 1 and 100, inclusive. Let X be the number chosen. Then X ∼\r\ndisunif(m = 100) and\r\nIP(X = x) =\r\n1\r\n100\r\n, x = 1, . . . , 100.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0f890012-a739-4057-a769-ef1433f25cf9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8b42aab933b778fa254b4ac6cc8fa67fd317df1de700f29fca28125a76618930",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 290
      },
      {
        "segments": [
          {
            "segment_id": "da10ab2b-5263-41a0-9ddf-405b715b578f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 131,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.2. THE DISCRETE UNIFORM DISTRIBUTION 115\r\nWe find a direct formula for the mean of X ∼ disunif(m):\r\nµ =\r\nXm\r\nx=1\r\nx fX(x) =\r\nXm\r\nx=1\r\nx ·\r\n1\r\nm\r\n=\r\n1\r\nm\r\n(1 + 2 + · · · + m) =\r\nm + 1\r\n2\r\n, (5.2.2)\r\nwhere we have used the famous identity 1 + 2 + · · · + m = m(m + 1)/2. That is, if we repeatedly\r\nchoose integers at random from 1 to m then, on the average, we expect to get (m + 1)/2. To get the\r\nvariance we first calculate\r\nXm\r\nx=1\r\nx\r\n2\r\nfX(x) =\r\n1\r\nm\r\nXm\r\nx=1\r\nx\r\n2 =\r\n1\r\nm\r\nm(m + 1)(2m + 1)\r\n6\r\n=\r\n(m + 1)(2m + 1)\r\n6\r\n,\r\nand finally,\r\nσ\r\n2 =\r\nXm\r\nx=1\r\nx\r\n2\r\nfX(x) − µ\r\n2 =\r\n(m + 1)(2m + 1)\r\n6\r\n−\r\n \r\nm + 1\r\n2\r\n!2\r\n= · · · =\r\nm\r\n2 − 1\r\n12\r\n. (5.2.3)\r\nExample 5.4. Roll a die and let X be the upward face showing. Then m = 6, µ = 7/2 = 3.5, and\r\nσ\r\n2 = (62 − 1)/12 = 35/12.\r\n5.2.1 How to do it with R\r\nFrom the console: One can choose an integer at random with the sample function. The general\r\nsyntax to simulate a discrete uniform random variable is sample(x, size, replace = TRUE).\r\nThe argument x identifies the numbers from which to randomly sample. If x is a number, then\r\nsampling is done from 1 to x. The argument size tells how big the sample size should be, and\r\nreplace tells whether or not numbers should be replaced in the urn after having been sampled.\r\nThe default option is replace = FALSE but for discrete uniforms the sampled values should be\r\nreplaced. Some examples follow.\r\n5.2.2 Examples\r\n• To roll a fair die 3000 times, do sample(6, size = 3000, replace = TRUE).\r\n• To choose 27 random numbers from 30 to 70, do sample(30:70, size = 27, replace\r\n= TRUE).\r\n• To flip a fair coin 1000 times, do sample(c(\"H\",\"T\"), size = 1000, replace =\r\nTRUE).\r\nWith the R Commander: Follow the sequence Probability . Discrete Distributions . Discrete\r\nUniform distribution . Simulate Discrete uniform variates. . . .\r\nSuppose we would like to roll a fair die 3000 times. In the Number of samples field we enter\r\n1. Next, we describe what interval of integers to be sampled. Since there are six faces numbered 1\r\nthrough 6, we set from = 1, we set to = 6, and set by = 1 (to indicate that we travel from 1 to\r\n6 in increments of 1 unit). We will generate a list of 3000 numbers selected from among 1, 2, . . . ,\r\n6, and we store the results of the simulation. For the time being, we select New Data set. Click\r\nOK.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/da10ab2b-5263-41a0-9ddf-405b715b578f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2ecf55cf2f68779b17c2c8031de016c455950258d25d415b444df6fadb4efc77",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 488
      },
      {
        "segments": [
          {
            "segment_id": "691548e1-265f-423d-9b3a-56a9e12b2d77",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 132,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "116 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nSince we are defining a new data set, the R Commander requests a name for the data set. The\r\ndefault name is Simset1, although in principle you could name it whatever you like (according to\r\nR’s rules for object names). We wish to have a list that is 3000 long, so we set Sample Size =\r\n3000 and click OK.\r\nIn the R Console window, the R Commander should tell you that Simset1 has been initialized,\r\nand it should also alert you that There was 1 discrete uniform variate sample stored\r\nin Simset 1.. To take a look at the rolls of the die, we click View data set and a window opens.\r\nThe default name for the variable is disunif.sim1.\r\n5.3 The Binomial Distribution\r\nThe binomial distribution is based on a Bernoulli trial, which is a random experiment in which\r\nthere are only two possible outcomes: success (S ) and failure (F). We conduct the Bernoulli trial\r\nand let\r\nX =\r\n\r\n\r\n\r\n1 if the outcome is S ,\r\n0 if the outcome is F.\r\n(5.3.1)\r\nIf the probability of success is p then the probability of failure must be 1 − p = q and the PMF of\r\nX is\r\nfX(x) = p\r\nx\r\n(1 − p)\r\n1−x\r\n, x = 0, 1. (5.3.2)\r\nIt is easy to calculate µ = IE X = p and IE X\r\n2 = p so that σ2 = p − p2 = p(1 − p).\r\n5.3.1 The Binomial Model\r\nThe Binomial model has three defining properties:\r\n• Bernoulli trials are conducted n times,\r\n• the trials are independent,\r\n• the probability of success p does not change between trials.\r\nIf X counts the number of successes in the n independent trials, then the PMF of X is\r\nfX(x) =\r\n \r\nn\r\nx\r\n!\r\np\r\nx\r\n(1 − p)\r\nn−x\r\n, x = 0, 1, 2, . . . , n. (5.3.3)\r\nWe say that X has a binomial distribution and we write X ∼ binom(size = n, prob = p). It is\r\nclear that fX(x) ≥ 0 for all x in the support because the value is the product of nonnegative numbers.\r\nWe next check that Pf(x) = 1:\r\nXn\r\nx=0\r\n \r\nn\r\nx\r\n!\r\np\r\nx\r\n(1 − p)\r\nn−x = [p + (1 − p)]n = 1n = 1.\r\nWe next find the mean:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/691548e1-265f-423d-9b3a-56a9e12b2d77.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=117928b9bad02e5b520612fe209b4156cb2268d7a67251890739f40001d83d4f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 400
      },
      {
        "segments": [
          {
            "segment_id": "60467892-8b3c-4446-a65b-940738dc2677",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 133,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.3. THE BINOMIAL DISTRIBUTION 117\r\nµ =\r\nXn\r\nx=0\r\nx\r\n \r\nn\r\nx\r\n!\r\np\r\nx\r\n(1 − p)\r\nn−x\r\n,\r\n=\r\nXn\r\nx=1\r\nx\r\nn!\r\nx!(n − x)!\r\np\r\nx\r\nq\r\nn−x\r\n,\r\n=n · p\r\nXn\r\nx=1\r\n(n − 1)!\r\n(x − 1)!(n − x)!\r\np\r\nx−1\r\nq\r\nn−x\r\n,\r\n=np Xn−1\r\nx−1=0\r\n \r\nn − 1\r\nx − 1\r\n!\r\np\r\n(x−1)(1 − p)(n−1)−(x−1)\r\n,\r\n=np.\r\nA similar argument shows that IE X(X − 1) = n(n − 1)p\r\n2\r\n(see Exercise 5.4). Therefore\r\nσ\r\n2 = IE X(X − 1) + IE X − [IE X]2\r\n,\r\n=n(n − 1)p\r\n2 + np − (np)2\r\n,\r\n=n\r\n2\r\np\r\n2 − np2 + np − n2\r\np\r\n2\r\n,\r\n=np − np2 = np(1 − p).\r\nExample 5.5. A four-child family. Each child may be either a boy (B) or a girl (G). For simplicity\r\nwe suppose that IP(B) = IP(G) = 1/2 and that the genders of the children are determined indepen\u0002dently. If we let X count the number of B’s, then X ∼ binom(size = 4, prob = 1/2). Further,\r\nIP(X = 2) is\r\nfX(2) =\r\n \r\n4\r\n2\r\n!\r\n(1/2)2(1/2)2 =\r\n6\r\n2\r\n4\r\n.\r\nThe mean number of boys is 4(1/2) = 2 and the variance of X is 4(1/2)(1/2) = 1.\r\n5.3.2 How to do it with R\r\nThe corresponding R function for the PMF and CDF are dbinom and pbinom, respectively. We\r\ndemonstrate their use in the following examples.\r\nExample 5.6. We can calculate it in R Commander under the Binomial Distribution menu with the\r\nBinomial probabilities menu item.\r\nPr\r\n0 0.0625\r\n1 0.2500\r\n2 0.3750\r\n3 0.2500\r\n4 0.0625\r\nWe know that the binom(size = 4, prob = 1/2) distribution is supported on the integers 0, 1,\r\n2, 3, and 4; thus the table is complete. We can read off the answer to be IP(X = 2) = 0.3750.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/60467892-8b3c-4446-a65b-940738dc2677.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=889add26fbcec0f937f0a877ef5a04f11555b95c2fa6b73cb0eaaaf6b3cee6be",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 317
      },
      {
        "segments": [
          {
            "segment_id": "06f42a31-7ace-4574-8917-ad05cddff870",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 134,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "118 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nExample 5.7. Roll 12 dice simultaneously, and let X denote the number of 6’s that appear. We\r\nwish to find the probability of getting seven, eight, or nine 6’s. If we let S =\r\n\b\r\nget a 6 on one roll\t,\r\nthen IP(S ) = 1/6 and the rolls constitute Bernoulli trials; thus X ∼ binom(size =12, prob =1/6)\r\nand our task is to find IP(7 ≤ X ≤ 9). This is just\r\nIP(7 ≤ X ≤ 9) =\r\nX\r\n9\r\nx=7\r\n \r\n12\r\nx\r\n!\r\n(1/6)x(5/6)12−x.\r\nAgain, one method to solve this problem would be to generate a probability mass table and add\r\nup the relevant rows. However, an alternative method is to notice that IP(7 ≤ X ≤ 9) = IP(X ≤\r\n9) − IP(X ≤ 6) = FX(9) − FX(6), so we could get the same answer by using the Binomial tail\r\nprobabilities. . . menu in the R Commander or the following from the command line:\r\n> pbinom(9, size=12, prob=1/6) - pbinom(6, size=12, prob=1/6)\r\n[1] 0.001291758\r\n> diff(pbinom(c(6,9), size = 12, prob = 1/6)) # same thing\r\n[1] 0.001291758\r\nExample 5.8. Toss a coin three times and let X be the number of Heads observed. We know from\r\nbefore that X ∼ binom(size = 3, prob = 1/2) which implies the following PMF:\r\nx = #of Heads 0 1 2 3\r\nf(x) = IP(X = x) 1/8 3/8 3/8 1/8\r\nOur next goal is to write down the CDF of X explicitly. The first case is easy: it is impossible\r\nfor X to be negative, so if x < 0 then we should have IP(X ≤ x) = 0. Now choose a value x\r\nsatisfying 0 ≤ x < 1, say, x = 0.3. The only way that X ≤ x could happen would be if X = 0,\r\ntherefore, IP(X ≤ x) should equal IP(X = 0), and the same is true for any 0 ≤ x < 1. Similarly, for\r\nany 1 ≤ x < 2, say, x = 1.73, the event {X ≤ x} is exactly the event {X = 0 or X = 1}. Consequently,\r\nIP(X ≤ x) should equal IP(X = 0 or X = 1) = IP(X = 0) + IP(X = 1). Continuing in this fashion,\r\nwe may figure out the values of FX(x) for all possible inputs −∞ < x < ∞, and we may summarize\r\nour observations with the following piecewise defined function:\r\nFX(x) = IP(X ≤ x) =\r\n\r\n\r\n\r\n0, x < 0,\r\n1\r\n8\r\n, 0 ≤ x < 1,\r\n1\r\n8\r\n+\r\n3\r\n8\r\n=\r\n4\r\n8\r\n, 1 ≤ x < 2,\r\n4\r\n8\r\n+\r\n3\r\n8\r\n=\r\n7\r\n8\r\n, 2 ≤ x < 3,\r\n1, x ≥ 3.\r\nIn particular, the CDF of X is defined for the entire real line, R. The CDF is right continuous and\r\nnondecreasing. A graph of the binom(size = 3, prob = 1/2) CDF is shown in Figure 5.3.1.\r\nExample 5.9. Another way to do Example 5.8 is with the distr family of packages [74]. They\r\nuse an object oriented approach to random variables, that is, a random variable is stored in an object",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/06f42a31-7ace-4574-8917-ad05cddff870.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d115edc1e4f1fce1bd03fa4a12f63b36f47829e7319605498ac187304717b641",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "06f42a31-7ace-4574-8917-ad05cddff870",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 134,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "118 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nExample 5.7. Roll 12 dice simultaneously, and let X denote the number of 6’s that appear. We\r\nwish to find the probability of getting seven, eight, or nine 6’s. If we let S =\r\n\b\r\nget a 6 on one roll\t,\r\nthen IP(S ) = 1/6 and the rolls constitute Bernoulli trials; thus X ∼ binom(size =12, prob =1/6)\r\nand our task is to find IP(7 ≤ X ≤ 9). This is just\r\nIP(7 ≤ X ≤ 9) =\r\nX\r\n9\r\nx=7\r\n \r\n12\r\nx\r\n!\r\n(1/6)x(5/6)12−x.\r\nAgain, one method to solve this problem would be to generate a probability mass table and add\r\nup the relevant rows. However, an alternative method is to notice that IP(7 ≤ X ≤ 9) = IP(X ≤\r\n9) − IP(X ≤ 6) = FX(9) − FX(6), so we could get the same answer by using the Binomial tail\r\nprobabilities. . . menu in the R Commander or the following from the command line:\r\n> pbinom(9, size=12, prob=1/6) - pbinom(6, size=12, prob=1/6)\r\n[1] 0.001291758\r\n> diff(pbinom(c(6,9), size = 12, prob = 1/6)) # same thing\r\n[1] 0.001291758\r\nExample 5.8. Toss a coin three times and let X be the number of Heads observed. We know from\r\nbefore that X ∼ binom(size = 3, prob = 1/2) which implies the following PMF:\r\nx = #of Heads 0 1 2 3\r\nf(x) = IP(X = x) 1/8 3/8 3/8 1/8\r\nOur next goal is to write down the CDF of X explicitly. The first case is easy: it is impossible\r\nfor X to be negative, so if x < 0 then we should have IP(X ≤ x) = 0. Now choose a value x\r\nsatisfying 0 ≤ x < 1, say, x = 0.3. The only way that X ≤ x could happen would be if X = 0,\r\ntherefore, IP(X ≤ x) should equal IP(X = 0), and the same is true for any 0 ≤ x < 1. Similarly, for\r\nany 1 ≤ x < 2, say, x = 1.73, the event {X ≤ x} is exactly the event {X = 0 or X = 1}. Consequently,\r\nIP(X ≤ x) should equal IP(X = 0 or X = 1) = IP(X = 0) + IP(X = 1). Continuing in this fashion,\r\nwe may figure out the values of FX(x) for all possible inputs −∞ < x < ∞, and we may summarize\r\nour observations with the following piecewise defined function:\r\nFX(x) = IP(X ≤ x) =\r\n\r\n\r\n\r\n0, x < 0,\r\n1\r\n8\r\n, 0 ≤ x < 1,\r\n1\r\n8\r\n+\r\n3\r\n8\r\n=\r\n4\r\n8\r\n, 1 ≤ x < 2,\r\n4\r\n8\r\n+\r\n3\r\n8\r\n=\r\n7\r\n8\r\n, 2 ≤ x < 3,\r\n1, x ≥ 3.\r\nIn particular, the CDF of X is defined for the entire real line, R. The CDF is right continuous and\r\nnondecreasing. A graph of the binom(size = 3, prob = 1/2) CDF is shown in Figure 5.3.1.\r\nExample 5.9. Another way to do Example 5.8 is with the distr family of packages [74]. They\r\nuse an object oriented approach to random variables, that is, a random variable is stored in an object",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/06f42a31-7ace-4574-8917-ad05cddff870.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d115edc1e4f1fce1bd03fa4a12f63b36f47829e7319605498ac187304717b641",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "784e29fd-cf3d-4f14-8481-2ba4bd328db4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 135,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.3. THE BINOMIAL DISTRIBUTION 119\r\n−1 0 1 2 3 4\r\n0.0 0.2 0.4 0.6 0.8 1.0\r\nnumber of successes\r\ncumulative probability\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\nFigure 5.3.1: Graph of the binom(size = 3, prob = 1/2) CDF",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/784e29fd-cf3d-4f14-8481-2ba4bd328db4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3b8ca9922edd20ae1cf5e9679439357c946effff206275e43e3dce53e8112c02",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "d550092a-28df-44fa-a986-300625d74fe4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 136,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "120 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nX, and then questions about the random variable translate to functions on and involving X. Random\r\nvariables with distributions from the base package are specified by capitalizing the name of the\r\ndistribution.\r\n> library(distr)\r\n> X <- Binom(size = 3, prob = 1/2)\r\n> X\r\nDistribution Object of Class: Binom\r\nsize: 3\r\nprob: 0.5\r\nThe analogue of the dbinom function for X is the d(X) function, and the analogue of the pbinom\r\nfunction is the p(X) function. Compare the following:\r\n> d(X)(1) # pmf of X evaluated at x = 1\r\n[1] 0.375\r\n> p(X)(2) # cdf of X evaluated at x = 2\r\n[1] 0.875\r\nRandom variables defined via the distr package may be plotted, which will return graphs of\r\nthe PMF, CDF, and quantile function (introduced in Section 6.3.1). See Figure 5.3.2 for an example.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d550092a-28df-44fa-a986-300625d74fe4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=07bcc63ef9d1583ae36c4608ebb0e3b174c017dffadcac56eced92d4d773282d",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "7f0242af-b25e-4dad-b1ab-60ffb61d9a16",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 137,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.3. THE BINOMIAL DISTRIBUTION 121\r\nGiven X ∼ dbinom(size = n, prob = p).\r\nHow to do: with stats (default) with distr\r\nPMF: IP(X = x) dbinom(x, size = n, prob = p) d(X)(x)\r\nCDF: IP(X ≤ x) pbinom(x, size = n, prob = p) p(X)(x)\r\nSimulate k variates rbinom(k, size = n, prob = p) r(X)(k)\r\nFor distr need X <- Binom(size =n, prob =p)\r\nTable 5.1: Correspondence between stats and distr\r\n0.0 1.0 2.0 3.0\r\n0.0 0.1 0.2 0.3\r\nx\r\nd(x)\r\nProbability function of Binom(3, 0.5)\r\n●\r\n● ●\r\n●\r\n−1 0 1 2 3 4\r\n0.0 0.2 0.4 0.6 0.8 1.0\r\nq\r\np(q)\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\nCDF of Binom(3, 0.5)\r\n●\r\n●\r\n●\r\n●\r\n0.0 0.4 0.8\r\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\r\np\r\nq(p)\r\n●\r\n●\r\n●\r\n●\r\nQuantile function of Binom(3, 0.5)\r\n●\r\n●\r\n●\r\n●\r\nFigure 5.3.2: The binom(size = 3, prob = 0.5) distribution from the distr package",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7f0242af-b25e-4dad-b1ab-60ffb61d9a16.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5c48dd6b9fd1b805b207fb355333c52c21985aa70ee8d7279ab542c82c723d7d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 345
      },
      {
        "segments": [
          {
            "segment_id": "a6eba6ed-f125-4a78-83d0-17f9611c8aa5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 138,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "122 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\n5.4 Expectation and Moment Generating Functions\r\n5.4.1 The Expectation Operator\r\nWe next generalize some of the concepts from Section 5.1.2. There we saw that every1 PMF has\r\ntwo important numbers associated with it:\r\nµ =\r\nX\r\nx∈S\r\nx fX(x), σ2 =\r\nX\r\nx∈S\r\n(x − µ)\r\n2\r\nfX(x). (5.4.1)\r\nIntuitively, for repeated observations of X we would expect the sample mean to closely approximate\r\nµ as the sample size increases without bound. For this reason we call µ the expected value of X and\r\nwe write µ = IE X, where IE is an expectation operator.\r\nDefinition 5.10. More generally, given a function g we define the expected value of g(X) by\r\nIE g(X) =\r\nX\r\nx∈S\r\ng(x)fX(x), (5.4.2)\r\nprovided the (potentially infinite) series P\r\nx\r\n|g(x)| f(x) is convergent. We say that IE g(X) exists.\r\nIn this notation the variance is σ\r\n2 = IE(X − µ)2\r\nand we prove the identity\r\nIE(X − µ)\r\n2 = IE X2 − (IE X)2\r\n(5.4.3)\r\nin Exercise 5.3. Intuitively, for repeated observations of X we would expect the sample mean of\r\nthe g(X) values to closely approximate IE g(X) as the sample size increases without bound.\r\nLet us take the analogy further. If we expect g(X) to be close to IE g(X) on the average, where\r\nwould we expect 3g(X) to be on the average? It could only be 3 IE g(X). The following theorem\r\nmakes this idea precise.\r\nProposition 5.11. For any functions g and h, any random variable X, and any constant c:\r\n1. IE c = c,\r\n2. IE[c · g(X)] = c IE g(X)\r\n3. IE[g(X) + h(X)] = IE g(X) + IE h(X),\r\nprovided IE g(X) and IE h(X) exist.\r\nProof. Go directly from the definition. For example,\r\nIE[c · g(X)] =\r\nX\r\nx∈S\r\nc · g(x)fX(x) = c ·\r\nX\r\nx∈S\r\ng(x)fX(x) = c IE g(X).\r\n\u0003\r\n1Not every, only those PMFs for which the (potentially infinite) series converges.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a6eba6ed-f125-4a78-83d0-17f9611c8aa5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=10462403f0f27c2c26b9925a7366c8e6872f4e70e8208a2451eb84a6678f45fd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 329
      },
      {
        "segments": [
          {
            "segment_id": "5d65c5d6-56b0-4d71-8c02-c5f755d21608",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 139,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.4. EXPECTATION AND MOMENT GENERATING FUNCTIONS 123\r\n5.4.2 Moment Generating Functions\r\nDefinition 5.12. Given a random variable X, its moment generating function (abbreviated MGF) is\r\ndefined by the formula\r\nMX(t) = IE etX =\r\nX\r\nx∈S\r\ne\r\ntx fX(x), (5.4.4)\r\nprovided the (potentially infinite) series is convergent for all t in a neighborhood of zero (that is,\r\nfor all −\u000f < t < \u000f, for some \u000f > 0).\r\nNote that for any MGF MX,\r\nMX(0) = IE e0·X = IE 1 = 1. (5.4.5)\r\nWe will calculate the MGF for the two distributions introduced above.\r\nExample 5.13. Find the MGF for X ∼ disunif(m).\r\nSince f(x) = 1/m, the MGF takes the form\r\nM(t) =\r\nXm\r\nx=1\r\ne\r\ntx 1\r\nm\r\n=\r\n1\r\nm\r\n(et + e\r\n2t + · · · + emt), for any t.\r\nExample 5.14. Find the MGF for X ∼ binom(size = n, prob = p).\r\nMX(t) =\r\nXn\r\nx=0\r\ne\r\ntx \r\nn\r\nx\r\n!\r\np\r\nx\r\n(1 − p)\r\nn−x\r\n,\r\n=\r\nXn\r\nx=0\r\n \r\nn\r\nx\r\n!\r\n(pe\r\nt\r\n)\r\nx\r\nq\r\nn−x\r\n,\r\n=(pe\r\nt + q)n\r\n, for any t.\r\nApplications\r\nWe will discuss three applications of moment generating functions in this book. The first is the fact\r\nthat an MGF may be used to accurately identify the probability distribution that generated it, which\r\nrests on the following:\r\nTheorem 5.15. The moment generating function, if it exists in a neighborhood of zero, determines\r\na probability distribution uniquely.\r\nProof. Unfortunately, the proof of such a theorem is beyond the scope of a text like this one.\r\nInterested readers could consult Billingsley [8]. \u0003\r\nWe will see an example of Theorem 5.15 in action.\r\nExample 5.16. Suppose we encounter a random variable which has MGF\r\nMX(t) = (0.3 + 0.7et)\r\n13\r\n.\r\nThen X ∼ binom(size = 13, prob = 0.7).\r\nAn MGF is also known as a “Laplace Transform” and is manipulated in that context in many\r\nbranches of science and engineering.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5d65c5d6-56b0-4d71-8c02-c5f755d21608.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bf4a9729f0c1cc5085d8edf7f7d2bc66e738a7d98efabb60d755657fe1c9b32b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 330
      },
      {
        "segments": [
          {
            "segment_id": "85dd0372-5562-4718-a4a3-dc1113d852fb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 140,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "124 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nWhy is it called a Moment Generating Function?\r\nThis brings us to the second powerful application of MGFs. Many of the models we study have a\r\nsimple MGF, indeed, which permits us to determine the mean, variance, and even higher moments\r\nvery quickly. Let us see why. We already know that\r\nM(t) =\r\nX\r\nx∈S\r\ne\r\ntx f(x).\r\nTake the derivative with respect to t to get\r\nM0(t) =\r\nd\r\ndt\r\n\r\n\r\nX\r\nx∈S\r\ne\r\ntx f(x)\r\n\r\n =\r\nX\r\nx∈S\r\nd\r\ndt\r\n\u0010\r\ne\r\ntx f(x)\r\n\u0011\r\n=\r\nX\r\nx∈S\r\nxe\r\ntx f(x), (5.4.6)\r\nand so if we plug in zero for t we see\r\nM0(0) =\r\nX\r\nx∈S\r\nxe\r\n0\r\nf(x) =\r\nX\r\nx∈S\r\nx f(x) = µ = IE X. (5.4.7)\r\nSimilarly, M00(t) =\r\nP\r\nx\r\n2\r\ne\r\ntx f(x) so that M00(0) = IE X2\r\n. And in general, we can see2that\r\nM\r\n(r)\r\nX\r\n(0) = IE X\r\nr = r\r\nthmoment of Xabout the origin. (5.4.8)\r\nThese are also known as raw moments and are sometimes denoted µ\r\n0\r\nr\r\n. In addition to these are the\r\nso called central moments µr defined by\r\nµr = IE(X − µ)\r\nr\r\n, r = 1, 2, . . . (5.4.9)\r\nExample 5.17. Let X ∼ binom(size = n, prob = p) with M(t) = (q + pe\r\nt\r\n)\r\nn\r\n. We calculated the\r\nmean and variance of a binomial random variable in Section 5.3 by means of the binomial series.\r\nBut look how quickly we find the mean and variance with the moment generating function.\r\nM0(t) =n(q + pe\r\nt\r\n)\r\nn−1\r\npe\r\nt\r\n|t=0 ,\r\n=n · 1\r\nn−1\r\np,\r\n=np.\r\nAnd\r\nM00(0) =n(n − 1)[q + pe\r\nt\r\n]\r\nn−2\r\n(pe\r\nt\r\n)\r\n2 + n[q + pet\r\n]\r\nn−1\r\npe\r\nt\r\n|t=0 ,\r\nIE X\r\n2 =n(n − 1)p2 + np.\r\nTherefore\r\nσ\r\n2 = IE X2 − (IE X)2\r\n,\r\n=n(n − 1)p\r\n2 + np − n2\r\np\r\n2\r\n,\r\n=np − np2 = npq.\r\nSee how much easier that was?\r\n2We are glossing over some significant mathematical details in our derivation. Suffice it to say that when the MGF exists\r\nin a neighborhood of t = 0, the exchange of differentiation and summation is valid in that neighborhood, and our remarks\r\nhold true.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/85dd0372-5562-4718-a4a3-dc1113d852fb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7f4fe51f92efd42e381e5c6413bebc96c2a1f790bb0716c81c0400d19bdebe4e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 392
      },
      {
        "segments": [
          {
            "segment_id": "6d65e791-3786-4581-acc8-dea1a7311472",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 141,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.5. THE EMPIRICAL DISTRIBUTION 125\r\nRemark 5.18. We learned in this section that M(r)(0) = IE X\r\nr\r\n. We remember from Calculus II that\r\ncertain functions f can be represented by a Taylor series expansion about a point a, which takes the\r\nform\r\nf(x) =\r\nX∞\r\nr=0\r\nf\r\n(r)\r\n(a)\r\nr!\r\n(x − a)\r\nr\r\n, for all |x − a| < R, (5.4.10)\r\nwhere R is called the radius of convergence of the series (see Appendix E.3). We combine the two\r\nto say that if an MGF exists for all t in the interval (−\u000f, \u000f), then we can write\r\nMX(t) =\r\nX∞\r\nr=0\r\nIE X\r\nr\r\nr!\r\nt\r\nr\r\n, for all |t| < \u000f. (5.4.11)\r\n5.4.3 How to do it with R\r\nThe distrEx package provides an expectation operator E which can be used on random variables\r\nthat have been defined in the ordinary distr sense:\r\n> X <- Binom(size = 3, prob = 0.45)\r\n> library(distrEx)\r\n> E(X)\r\n[1] 1.35\r\n> E(3 * X + 4)\r\n[1] 8.05\r\nFor discrete random variables with finite support, the expectation is simply computed with\r\ndirect summation. In the case that the random variable has infinite support and the function is\r\ncrazy, then the expectation is not computed directly, rather, it is estimated by first generating a\r\nrandom sample from the underlying model and next computing a sample mean of the function of\r\ninterest.\r\nThere are methods for other population parameters:\r\n> var(X)\r\n[1] 0.7425\r\n> sd(X)\r\n[1] 0.8616844\r\nThere are even methods for IQR, mad, skewness, and kurtosis.\r\n5.5 The Empirical Distribution\r\nDo an experiment n times and observe n values x1, x2, . . . , xn of a random variable X. For simplicity\r\nin most of the discussion that follows it will be convenient to imagine that the observed values are\r\ndistinct, but the remarks are valid even when the observed values are repeated.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6d65e791-3786-4581-acc8-dea1a7311472.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c817d11d46c9fa0f133410f4e7580a19fef718426885aabdfcf206a38a7c0afd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 318
      },
      {
        "segments": [
          {
            "segment_id": "a0763629-1d0d-4c7c-9cd0-cc85c3bc9ad3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 142,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "126 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nDefinition 5.19. The empirical cumulative distribution function Fn (written ECDF) is the probabil\u0002ity distribution that places probability mass 1/n on each of the values x1, x2, . . . , xn. The empirical\r\nPMF takes the form\r\nfX(x) =\r\n1\r\nn\r\n, x ∈ {x1, x2, ..., xn} . (5.5.1)\r\nIf the value xiis repeated k times, the mass at xiis accumulated to k/n.\r\nThe mean of the empirical distribution is\r\nµ =\r\nX\r\nx∈S\r\nx fX(x) =\r\nXn\r\ni=1\r\nxi·\r\n1\r\nn\r\n(5.5.2)\r\nand we recognize this last quantity to be the sample mean, x. The variance of the empirical distri\u0002bution is\r\nσ\r\n2 =\r\nX\r\nx∈S\r\n(x − µ)\r\n2\r\nfX(x) =\r\nXn\r\ni=1\r\n(xi − x)\r\n2\r\n·\r\n1\r\nn\r\n(5.5.3)\r\nand this last quantity looks very close to what we already know to be the sample variance.\r\ns\r\n2 =\r\n1\r\nn − 1\r\nXn\r\ni=1\r\n(xi − x)\r\n2\r\n. (5.5.4)\r\nThe empirical quantile function is the inverse of the ECDF. See Section 6.3.1.\r\n5.5.1 How to do it with R\r\nThe empirical distribution is not directly available as a distribution in the same way that the other\r\nbase probability distributions are, but there are plenty of resources available for the determined\r\ninvestigator.\r\nGiven a data vector of observed values x, we can see the empirical CDF with the ecdf function:\r\n> x <- c(4, 7, 9, 11, 12)\r\n> ecdf(x)\r\nEmpirical CDF\r\nCall: ecdf(x)\r\nx[1:5] = 4, 7, 9, 11, 12\r\nThe above shows that the returned value of ecdf(x) is not a number but rather a function.\r\nThe ECDF is not usually used by itself in this form, by itself. More commonly it is used as an\r\nintermediate step in a more complicated calculation, for instance, in hypothesis testing (see Chapter\r\n10) or resampling (see Chapter 13). It is nevertheless instructive to see what the ecdf looks like,\r\nand there is a special plot method for ecdf objects.\r\n> plot(ecdf(x))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a0763629-1d0d-4c7c-9cd0-cc85c3bc9ad3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6880c0e58603742ae98045daeeb4179852e41f6f0d991bf1923c6a658d09fe24",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c3a5100b-3aca-4d73-9797-0d7779603cf1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 143,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.5. THE EMPIRICAL DISTRIBUTION 127\r\n2 4 6 8 10 12 14\r\n0.0 0.2 0.4 0.6 0.8 1.0\r\necdf(x)\r\nx\r\nFn(x)\r\n●\r\n●\r\n●\r\n●\r\n●\r\nFigure 5.5.1: The empirical CDF\r\nSee Figure 5.5.1. The graph is of a right-continuous function with jumps exactly at the locations\r\nstored in x. There are no repeated values in x so all of the jumps are equal to 1/5 = 0.2.\r\nThe empirical PDF is not usually of particular interest in itself, but if we really wanted we could\r\ndefine a function to serve as the empirical PDF:\r\n> epdf <- function(x) function(t){sum(x %in% t)/length(x)}\r\n> x <- c(0,0,1)\r\n> epdf(x)(0) # should be 2/3\r\n[1] 0.6666667\r\nTo simulate from the empirical distribution supported on the vector x, we use the sample\r\nfunction.\r\n> x <- c(0, 0, 1)\r\n> sample(x, size = 7, replace = TRUE)\r\n[1] 0 1 0 1 1 0 0",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c3a5100b-3aca-4d73-9797-0d7779603cf1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c0f217fbfb25732392c8ca60bb8c2671f33a17d1dd8e87f684f78eda142fc0c6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 484
      },
      {
        "segments": [
          {
            "segment_id": "d12e36fb-3f3d-47d5-b00c-2c0c05ae2454",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 144,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "128 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nWe can get the empirical quantile function in R with quantile(x, probs = p, type = 1);\r\nsee Section 6.3.1.\r\nAs we hinted above, the empirical distribution is significant more because of how and where it\r\nappears in more sophisticated applications. We will explore some of these in later chapters – see,\r\nfor instance, Chapter 13.\r\n5.6 Other Discrete Distributions\r\nThe binomial and discrete uniform distributions are popular, and rightly so; they are simple and\r\nform the foundation for many other more complicated distributions. But the particular uniform and\r\nbinomial models only apply to a limited range of problems. In this section we introduce situations\r\nfor which we need more than what the uniform and binomial offer.\r\n5.6.1 Dependent Bernoulli Trials\r\nThe Hypergeometric Distribution\r\nConsider an urn with 7 white balls and 5 black balls. Let our random experiment be to randomly\r\nselect 4 balls, without replacement, from the urn. Then the probability of observing 3 white balls\r\n(and thus 1 black ball) would be\r\nIP(3W, 1B) =\r\n\u0010\r\n7\r\n3\r\n\u0011\u00105\r\n1\r\n\u0011\r\n\u0010\r\n12\r\n4\r\n\u0011 . (5.6.1)\r\nMore generally, we sample without replacement K times from an urn with M white balls and N\r\nblack balls. Let X be the number of white balls in the sample. The PMF of X is\r\nfX(x) =\r\n\u0010\r\nM\r\nx\r\n\u0011\u0010 N\r\nK−x\r\n\u0011\r\n\u0010\r\nM+N\r\nK\r\n\u0011 . (5.6.2)\r\nWe say that X has a hypergeometric distribution and write X ∼ hyper(m = M, n = N, k = K).\r\nThe support set for the hypergeometric distribution is a little bit tricky. It is tempting to say that\r\nx should go from 0 (no white balls in the sample) to K (no black balls in the sample), but that does\r\nnot work if K > M, because it is impossible to have more white balls in the sample than there were\r\nwhite balls originally in the urn. We have the same trouble if K > N. The good news is that the\r\nmajority of examples we study have K ≤ M and K ≤ N and we will happily take the support to be\r\nx = 0, 1, . . . , K.\r\nIt is shown in Exercise 5.5 that\r\nµ = K\r\nM\r\nM + N\r\n, σ2 = K\r\nMN\r\n(M + N)\r\n2\r\nM + N − K\r\nM + N − 1\r\n. (5.6.3)\r\nThe associated R functions for the PMF and CDF are dhyper(x, m, n, k) and phyper,\r\nrespectively. There are two more functions: qhyper, which we will discuss in Section 6.3.1, and\r\nrhyper, discussed below.\r\nExample 5.20. Suppose in a certain shipment of 250 Pentium processors there are 17 defective\r\nprocessors. A quality control consultant randomly collects 5 processors for inspection to determine\r\nwhether or not they are defective. Let X denote the number of defectives in the sample.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d12e36fb-3f3d-47d5-b00c-2c0c05ae2454.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=964daf2935050693414004268665d49f431f26adc1dfd124725fee1c61fdd86b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 482
      },
      {
        "segments": [
          {
            "segment_id": "44e8ace6-9b27-49da-8ba9-b6ad757f320e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 145,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.6. OTHER DISCRETE DISTRIBUTIONS 129\r\n1. Find the probability of exactly 3 defectives in the sample, that is, find IP(X = 3).\r\nSolution: We know that X ∼ hyper(m = 17, n = 233, k = 5). So the required probability is\r\njust\r\nfX(3) =\r\n\u0010\r\n17\r\n3\r\n\u0011\u0010233\r\n2\r\n\u0011\r\n\u0010\r\n250\r\n5\r\n\u0011 .\r\nTo calculate it in R we just type\r\n> dhyper(3, m = 17, n = 233, k = 5)\r\n[1] 0.002351153\r\nTo find it with the R Commander we go Probability . Discrete Distributions . Hyperge\u0002ometric distribution . Hypergeometric probabilities. . . .We fill in the parameters m = 17,\r\nn = 233, and k = 5. Click OK, and the following table is shown in the window.\r\n> A <- data.frame(Pr = dhyper(0:4, m = 17, n = 233, k = 5))\r\n> rownames(A) <- 0:4\r\n> A\r\nPr\r\n0 7.011261e-01\r\n1 2.602433e-01\r\n2 3.620776e-02\r\n3 2.351153e-03\r\n4 7.093997e-05\r\nWe wanted IP(X = 3), and this is found from the table to be approximately 0.0024. The value\r\nis rounded to the fourth decimal place.\r\nWe know from our above discussion that the sample space should be x = 0, 1, 2, 3, 4, 5, yet,\r\nin the table the probabilities are only displayed for x = 1, 2, 3, and 4. What is happening?\r\nAs it turns out, the R Commander will only display probabilities that are 0.00005 or greater.\r\nSince x = 5 is not shown, it suggests that the outcome has a tiny probability. To find its exact\r\nvalue we use the dhyper function:\r\n> dhyper(5, m = 17, n = 233, k = 5)\r\n[1] 7.916049e-07\r\nIn other words, IP(X = 5) ≈ 0.0000007916049, a small number indeed.\r\n2. Find the probability that there are at most 2 defectives in the sample, that is, compute IP(X ≤\r\n2).\r\nSolution: Since IP(X ≤ 2) = IP(X = 0, 1, 2), one way to do this would be to add the 0, 1,\r\nand 2 entries in the above table. this gives 0.7011 + 0.2602 + 0.0362 = 0.9975. Our answer\r\nshould be correct up to the accuracy of 4 decimal places. However, a more precise method",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/44e8ace6-9b27-49da-8ba9-b6ad757f320e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=41df0ec1305db0b462cfe5d6c5ef2403af85e996580c80c10aee401d490dd165",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 365
      },
      {
        "segments": [
          {
            "segment_id": "5c1cca2f-00c9-4b4f-ab5f-e54d81684efe",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 146,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "130 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nis provided by the R Commander. Under the Hypergeometric distribution menu we select\r\nHypergeometric tail probabilities. . . . We fill in the parameters m, n, and k as before, but\r\nin the Variable value(s) dialog box we enter the value 2. We notice that the Lower tail\r\noption is checked, and we leave that alone. Click OK.\r\n> phyper(2, m = 17, n = 233, k = 5)\r\n[1] 0.9975771\r\nAnd thus IP(X ≤ 2) ≈ 0.9975771. We have confirmed that the above answer was correct up\r\nto four decimal places.\r\n3. Find IP(X > 1).\r\nThe table did not give us the explicit probability IP(X = 5), so we can not use the table to\r\ngive us this probability. We need to use another method. Since IP(X > 1) = 1 − IP(X ≤ 1) =\r\n1 − FX(1), we can find the probability with Hypergeometric tail probabilities. . . . We enter\r\n1 for Variable Value(s), we enter the parameters as before, and in this case we choose the\r\nUpper tail option. This results in the following output.\r\n> phyper(1, m = 17, n = 233, k = 5, lower.tail = FALSE)\r\n[1] 0.03863065\r\nIn general, the Upper tail option of a tail probabilities dialog computes IP(X > x) for all\r\ngiven Variable Value(s) x.\r\n4. Generate 100, 000 observations of the random variable X.\r\nWe can randomly simulate as many observations of X as we want in R Commander. Simply\r\nchoose Simulate hypergeometric variates. . . in the Hypergeometric distribution dialog.\r\nIn the Number of samples dialog, type 1. Enter the parameters as above. Under the Store\r\nValues section, make sure New Data set is selected. Click OK.\r\nA new dialog should open, with the default name Simset1. We could change this if we like,\r\naccording to the rules for R object names. In the sample size box, enter 100000. Click OK.\r\nIn the Console Window, R Commander should issue an alert that Simset1 has been initial\u0002ized, and in a few seconds, it should also state that 100,000 hypergeometric variates were\r\nstored in hyper.sim1. We can view the sample by clicking the View Data Set button on\r\nthe R Commander interface.\r\nWe know from our formulas that µ = K · M/(M + N) = 5 ∗ 17/250 = 0.34. We can check\r\nour formulas using the fact that with repeated observations of X we would expect about 0.34\r\ndefectives on the average. To see how our sample reflects the true mean, we can compute the\r\nsample mean\r\nRcmdr> mean(Simset2$hyper.sim1, na.rm=TRUE)\r\n[1] 0.340344\r\nRcmdr> sd(Simset2$hyper.sim1, na.rm=TRUE)\r\n[1] 0.5584982",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5c1cca2f-00c9-4b4f-ab5f-e54d81684efe.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5bc9ef724b4970f6cf2c3a3d78538ce1c41ad8e5d3eef05057e6674e28a763b6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 437
      },
      {
        "segments": [
          {
            "segment_id": "b0e28c32-7eeb-42aa-8f6b-082dee53fe07",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 147,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.6. OTHER DISCRETE DISTRIBUTIONS 131\r\n.\r\n.\r\n.\r\nWe see that when given many independent observations of X, the sample mean is very close\r\nto the true mean µ. We can repeat the same idea and use the sample standard deviation to\r\nestimate the true standard deviation of X. From the output above our estimate is 0.5584982,\r\nand from our formulas we get\r\nσ\r\n2 = K\r\nMN\r\n(M + N)\r\n2\r\nM + N − K\r\nM + N − 1\r\n≈ 0.3117896,\r\nwith σ =\r\n√\r\nσ2 ≈ 0.5583811944. Our estimate was pretty close.\r\nFrom the console we can generate random hypergeometric variates with the rhyper function,\r\nas demonstrated below.\r\n> rhyper(10, m = 17, n = 233, k = 5)\r\n[1] 0 0 0 0 0 2 0 0 0 1\r\nSampling With and Without Replacement\r\nSuppose that we have a large urn with, say, M white balls and N black balls. We take a sample of\r\nsize n from the urn, and let X count the number of white balls in the sample. If we sample\r\nwithout replacement, then X ∼ hyper(m =M, n = N, k = n) and has mean and variance\r\nµ =n\r\nM\r\nM + N\r\n,\r\nσ\r\n2 =n\r\nMN\r\n(M + N)\r\n2\r\nM + N − n\r\nM + N − 1\r\n,\r\n=n\r\nM\r\nM + N\r\n\u0012\r\n1 −\r\nM\r\nM + N\r\n\u0013 M + N − n\r\nM + N − 1\r\n.\r\nOn the other hand, if we sample\r\nwith replacement, then X ∼ binom(size = n, prob = M/(M + N)) with mean and variance\r\nµ =n\r\nM\r\nM + N\r\n,\r\nσ\r\n2 =n\r\nM\r\nM + N\r\n\u0012\r\n1 −\r\nM\r\nM + N\r\n\u0013\r\n.\r\nWe see that both sampling procedures have the same mean, and the method with the larger variance\r\nis the “with replacement” scheme. The factor by which the variances differ,\r\nM + N − n\r\nM + N − 1\r\n, (5.6.4)\r\nis called a finite population correction. For a fixed sample size n, as M, N → ∞ it is clear that the\r\ncorrection goes to 1, that is, for infinite populations the sampling schemes are essentially the same\r\nwith respect to mean and variance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b0e28c32-7eeb-42aa-8f6b-082dee53fe07.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=aef43a3fb72b439762af05d0c9e307977db4e015f7cfa29db6d7e187d190c0cd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 380
      },
      {
        "segments": [
          {
            "segment_id": "0c54d3ee-73e4-4b10-8a8b-45a26a9ce930",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 148,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "132 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\n5.6.2 Waiting Time Distributions\r\nAnother important class of problems is associated with the amount of time it takes for a specified\r\nevent of interest to occur. For example, we could flip a coin repeatedly until we observe Heads. We\r\ncould toss a piece of paper repeatedly until we make it in the trash can.\r\nThe Geometric Distribution\r\nSuppose that we conduct Bernoulli trials repeatedly, noting the successes and failures. Let X be the\r\nnumber of failures before a success. If IP(S ) = p then X has PMF\r\nfX(x) = p(1 − p)\r\nx\r\n, x = 0, 1, 2, . . . (5.6.5)\r\n(Why?) We say that X has a Geometric distribution and we write X ∼ geom(prob = p). The\r\nassociated R functions are dgeom(x, prob), pgeom, qgeom, and rhyper, which give the PMF,\r\nCDF, quantile function, and simulate random variates, respectively.\r\nAgain it is clear that f(x) ≥ 0 and we check that P\r\nf(x) = 1 (see Equation E.3.9 in Appendix\r\nE.3):\r\nX∞\r\nx=0\r\np(1 − p)\r\nx =p\r\nX∞\r\nx=0\r\nq\r\nx = p\r\n1\r\n1 − q\r\n= 1.\r\nWe will find in the next section that the mean and variance are\r\nµ =\r\n1 − p\r\np\r\n=\r\nq\r\np\r\nand σ\r\n2 =\r\nq\r\np\r\n2\r\n. (5.6.6)\r\nExample 5.21. The Pittsburgh Steelers place kicker, Jeff Reed, made 81.2% of his attempted field\r\ngoals in his career up to 2006. Assuming that his successive field goal attempts are approximately\r\nBernoulli trials, find the probability that Jeff misses at least 5 field goals before his first successful\r\ngoal.\r\nSolution: If X = the number of missed goals until Jeff’s first success, then X ∼ geom(prob =\r\n0.812) and we want IP(X ≥ 5) = IP(X > 4). We can find this in R with\r\n> pgeom(4, prob = 0.812, lower.tail = FALSE)\r\n[1] 0.0002348493\r\nNote 5.22. Some books use a slightly different definition of the geometric distribution. They con\u0002sider Bernoulli trials and let Y count instead the number of trials until a success, so that Y has\r\nPMF\r\nfY (y) = p(1 − p)\r\ny−1\r\n, y = 1, 2, 3, . . . (5.6.7)\r\nWhen they say “geometric distribution”, this is what they mean. It is not hard to see that the\r\ntwo definitions are related. In fact, if X denotes our geometric and Y theirs, then Y = X + 1.\r\nConsequently, they have µY = µX + 1 and σ\r\n2\r\nY\r\n= σ\r\n2\r\nX\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0c54d3ee-73e4-4b10-8a8b-45a26a9ce930.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11dfdf2bcefdb5a311fbab817faf6711113925e59f1f6beb53ca150b8fdd7c7d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 422
      },
      {
        "segments": [
          {
            "segment_id": "9f407f99-e340-4f3b-8ba1-4a2f22a01eb7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 149,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.6. OTHER DISCRETE DISTRIBUTIONS 133\r\nThe Negative Binomial Distribution\r\nWe may generalize the problem and consider the case where we wait for more than one success.\r\nSuppose that we conduct Bernoulli trials repeatedly, noting the respective successes and failures.\r\nLet X count the number of failures before r successes. If IP(S ) = p then X has PMF\r\nfX(x) =\r\n \r\nr + x − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\nx\r\n, x = 0, 1, 2, . . . (5.6.8)\r\nWe say that X has a Negative Binomial distribution and write X ∼ nbinom(size = r, prob = p).\r\nThe associated R functions are dnbinom(x, size, prob), pnbinom, qnbinom, and rnbinom,\r\nwhich give the PMF, CDF, quantile function, and simulate random variates, respectively.\r\nAs usual it should be clear that fX(x) ≥ 0 and the fact that PfX(x) = 1 follows from a general\u0002ization of the geometric series by means of a Maclaurin’s series expansion:\r\n1\r\n1 − t\r\n=\r\nX∞\r\nk=0\r\nt\r\nk\r\n, for −1 < t < 1, and (5.6.9)\r\n1\r\n(1 − t)\r\nr\r\n=\r\nX∞\r\nk=0\r\n \r\nr + k − 1\r\nr − 1\r\n!\r\nt\r\nk\r\n, for −1 < t < 1. (5.6.10)\r\nTherefore\r\nX∞\r\nx=0\r\nfX(x) = p\r\nr X∞\r\nx=0\r\n \r\nr + x − 1\r\nr − 1\r\n!\r\nq\r\nx = pr\r\n(1 − q)\r\n−r = 1, (5.6.11)\r\nsince |q| = |1 − p| < 1.\r\nExample 5.23. We flip a coin repeatedly and let X count the number of Tails until we get seven\r\nHeads. What is IP(X = 5)?\r\nSolution: We know that X ∼ nbinom(size = 7, prob = 1/2).\r\nIP(X = 5) = fX(5) =\r\n \r\n7 + 5 − 1\r\n7 − 1\r\n!\r\n(1/2)7(1/2)5 =\r\n \r\n11\r\n6\r\n!\r\n2\r\n−12\r\nand we can get this in R with\r\n> dnbinom(5, size = 7, prob = 0.5)\r\n[1] 0.1127930\r\nLet us next compute the MGF of X ∼ nbinom(size = r, prob = p).\r\nMX(t) =\r\nX∞\r\nx=0\r\ne\r\ntx \r\nr + x − 1\r\nr − 1\r\n!\r\np\r\nr\r\nq\r\nx\r\n=p\r\nr X∞\r\nx=0\r\n \r\nr + x − 1\r\nr − 1\r\n!\r\n[qe\r\nt\r\n]\r\nx\r\n=p\r\nr\r\n(1 − qet)\r\n−r\r\n, provided |qe\r\nt\r\n| < 1,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9f407f99-e340-4f3b-8ba1-4a2f22a01eb7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cf20b1279b9f055fcb4308c307d521062da248f66f6cd39fd471915697422d17",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 383
      },
      {
        "segments": [
          {
            "segment_id": "5bdc76a0-54d5-4eec-ac37-19a69811807d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 150,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "134 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nand so\r\nMX(t) =\r\n \r\np\r\n1 − qe\r\nt\r\n!r\r\n, for qe\r\nt < 1. (5.6.12)\r\nWe see that qe\r\nt < 1 when t < − ln(1 − p).\r\nLet X ∼ nbinom(size = r, prob = p) with M(t) = p\r\nr\r\n(1 − qe\r\nt\r\n)\r\n−r\r\n. We proclaimed above the\r\nvalues of the mean and variance. Now we are equipped with the tools to find these directly.\r\nM0(t) =p\r\nr\r\n(−r)(1 − qe\r\nt\r\n)\r\n−r−1\r\n(−qe\r\nt\r\n),\r\n=rqe\r\nt\r\np\r\nr\r\n(1 − qe\r\nt\r\n)\r\n−r−1\r\n,\r\n=\r\nrqe\r\nt\r\n1 − qe\r\nt M(t), and so\r\nM0(0) =\r\nrq\r\n1 − q\r\n· 1 =\r\nrq\r\np\r\n.\r\nThus µ = rq/p. We next find IE X\r\n2\r\n.\r\nM00(0) =\r\nrqe\r\nt\r\n(1 − qe\r\nt\r\n) − rqe\r\nt\r\n(−qe\r\nt\r\n)\r\n(1 − qe\r\nt\r\n)\r\n2 M(t) +\r\nrqe\r\nt\r\n1 − qe\r\nt M0\r\n(t)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\nt=0\r\n,\r\n=\r\nrqp + rq2\r\np\r\n2\r\n· 1 +\r\nrq\r\np\r\n \r\nrq\r\np\r\n!\r\n,\r\n=\r\nrq\r\np\r\n2\r\n+\r\n \r\nrq\r\np\r\n!2\r\n.\r\nFinally we may say σ\r\n2 = M00(0) − [M0\r\n(0)]2 = rq/p\r\n2\r\n.\r\nExample 5.24. A random variable has MGF\r\nMX(t) =\r\n \r\n0.19\r\n1 − 0.81et\r\n!31\r\n.\r\nThen X ∼ nbinom(size = 31, prob = 0.19).\r\nNote 5.25. As with the Geometric distribution, some books use a slightly different definition of the\r\nNegative Binomial distribution. They consider Bernoulli trials and let Y be the number of trials\r\nuntil r successes, so that Y has PMF\r\nfY (y) =\r\n \r\ny − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\ny−r\r\n, y = r,r + 1,r + 2, . . . (5.6.13)\r\nIt is again not hard to see that if X denotes our Negative Binomial and Y theirs, then Y = X + r.\r\nConsequently, they have µY = µX + r and σ\r\n2\r\nY\r\n= σ\r\n2\r\nX\r\n.\r\n5.6.3 Arrival Processes\r\nThe Poisson Distribution\r\nThis is a distribution associated with “rare events”, for reasons which will become clear in a mo\u0002ment. The events might be:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5bdc76a0-54d5-4eec-ac37-19a69811807d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cfef3a9122335aad9ad5584264e3d8f62ec836f4c6bdde144d2e846327f1d0c2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 364
      },
      {
        "segments": [
          {
            "segment_id": "ae963676-7fab-43ab-93fe-a44c0121760b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 151,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.6. OTHER DISCRETE DISTRIBUTIONS 135\r\n• traffic accidents,\r\n• typing errors, or\r\n• customers arriving in a bank.\r\nLet λ be the average number of events in the time interval [0, 1]. Let the random variable X count\r\nthe number of events occurring in the interval. Then under certain reasonable conditions it can be\r\nshown that\r\nfX(x) = IP(X = x) = e\r\n−λ λ\r\nx\r\nx!\r\n, x = 0, 1, 2, . . . (5.6.14)\r\nWe use the notation X ∼ pois(lambda = λ). The associated R functions are dpois(x, lambda),\r\nppois, qpois, and rpois, which give the PMF, CDF, quantile function, and simulate random\r\nvariates, respectively.\r\nWhat are the reasonable conditions? Divide [0, 1] into subintervals of length 1/n. A Poisson\r\nprocess satisfies the following conditions:\r\n• the probability of an event occurring in a particular subinterval is ≈ λ/n.\r\n• the probability of two or more events occurring in any subinterval is ≈ 0.\r\n• occurrences in disjoint subintervals are independent.\r\nRemark 5.26. If X counts the number of events in the interval [0, t] and λ is the average number\r\nthat occur in unit time, then X ∼ pois(lambda = λt), that is,\r\nIP(X = x) = e\r\n−λt\r\n(λt)\r\nx\r\nx!\r\n, x = 0, 1, 2, 3 . . . (5.6.15)\r\nExample 5.27. On the average, five cars arrive at a particular car wash every hour. Let X count the\r\nnumber of cars that arrive from 10AM to 11AM. Then X ∼ pois(lambda = 5). Also, µ = σ\r\n2 = 5.\r\nWhat is the probability that no car arrives during this period?\r\nSolution: The probability that no car arrives is\r\nIP(X = 0) = e\r\n−5\r\n5\r\n0\r\n0!\r\n= e\r\n−5 ≈ 0.0067.\r\nExample 5.28. Suppose the car wash above is in operation from 8AM to 6PM, and we let Y be the\r\nnumber of customers that appear in this period. Since this period covers a total of 10 hours, from\r\nRemark 5.26 we get that Y ∼ pois(lambda = 5 ∗ 10 = 50). What is the probability that there are\r\nbetween 48 and 50 customers, inclusive?\r\nSolution: We want IP(48 ≤ Y ≤ 50) = IP(X ≤ 50) − IP(X ≤ 47).\r\n> diff(ppois(c(47, 50), lambda = 50))\r\n[1] 0.1678485",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ae963676-7fab-43ab-93fe-a44c0121760b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=136b02e5ac2df064ef5c1b6d2db8733a03464c11f3512361bbb3bf127e6e79da",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 381
      },
      {
        "segments": [
          {
            "segment_id": "562ff477-96e7-4345-9794-a3a50c7c2b7a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 152,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "136 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\n5.7 Functions of Discrete Random Variables\r\nWe have built a large catalogue of discrete distributions, but the tools of this section will give us the\r\nability to consider infinitely many more. Given a random variable X and a given function h, we may\r\nconsider Y = h(X). Since the values of X are determined by chance, so are the values of Y. The\r\nquestion is, what is the PMF of the random variable Y? The answer, of course, depends on h. In\r\nthe case that h is one-to-one (see Appendix E.2), the solution can be found by simple substitution.\r\nExample 5.29. Let X ∼ nbinom(size = r, prob = p). We saw in 5.6 that X represents the\r\nnumber of failures until r successes in a sequence of Bernoulli trials. Suppose now that instead we\r\nwere interested in counting the number of trials (successes and failures) until the r\r\nth success occurs,\r\nwhich we will denote by Y. In a given performance of the experiment, the number of failures (X)\r\nand the number of successes (r) together will comprise the total number of trials (Y), or in other\r\nwords, X + r = Y. We may let h be defined by h(x) = x + r so that Y = h(X), and we notice that\r\nh is linear and hence one-to-one. Finally, X takes values 0, 1, 2, . . . implying that the support of Y\r\nwould be {r, r + 1, r + 2, . . .}. Solving for X we get X = Y − r. Examining the PMF of X\r\nfX(x) =\r\n \r\nr + x − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\nx\r\n, (5.7.1)\r\nwe can substitute x = y − r to get\r\nfY (y) = fX(y − r),\r\n=\r\n \r\nr + (y − r) − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\ny−r\r\n,\r\n=\r\n \r\ny − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\ny−r\r\n, y = r, r + 1, . . .\r\nEven when the function h is not one-to-one, we may still find the PMF of Y simply by accumu\u0002lating, for each y, the probability of all the x’s that are mapped to that y.\r\nProposition 5.30. Let X be a discrete random variable with PMF fX supported on the set S X. Let\r\nY = h(X) for some function h. Then Y has PMF fY defined by\r\nfY (y) =\r\nX\r\n{x∈S X| h(x)=y}\r\nfX(x) (5.7.2)\r\nExample 5.31. Let X ∼ binom(size = 4, prob = 1/2), and let Y = (X − 1)2. Consider the\r\nfollowing table:\r\nx 0 1 2 3 4\r\nfX(x) 1/16 1/4 6/16 1/4 1/16\r\ny = (x − 2)2 1 0 1 4 9\r\nFrom this we see that Y has support S Y = {0, 1, 4, 9}. We also see that h(x) = (x−1)2is not one\u0002to-one on the support of X, because both x = 0 and x = 2 are mapped by h to y = 1. Nevertheless,\r\nwe see that Y = 0 only when X = 1, which has probability 1/4; therefore, fY (0) should equal 1/4.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/562ff477-96e7-4345-9794-a3a50c7c2b7a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2bb1e5a3a8948fc03d07a5306c92f593927ba255c64f90928275c8dcb75001a3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 535
      },
      {
        "segments": [
          {
            "segment_id": "562ff477-96e7-4345-9794-a3a50c7c2b7a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 152,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "136 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\n5.7 Functions of Discrete Random Variables\r\nWe have built a large catalogue of discrete distributions, but the tools of this section will give us the\r\nability to consider infinitely many more. Given a random variable X and a given function h, we may\r\nconsider Y = h(X). Since the values of X are determined by chance, so are the values of Y. The\r\nquestion is, what is the PMF of the random variable Y? The answer, of course, depends on h. In\r\nthe case that h is one-to-one (see Appendix E.2), the solution can be found by simple substitution.\r\nExample 5.29. Let X ∼ nbinom(size = r, prob = p). We saw in 5.6 that X represents the\r\nnumber of failures until r successes in a sequence of Bernoulli trials. Suppose now that instead we\r\nwere interested in counting the number of trials (successes and failures) until the r\r\nth success occurs,\r\nwhich we will denote by Y. In a given performance of the experiment, the number of failures (X)\r\nand the number of successes (r) together will comprise the total number of trials (Y), or in other\r\nwords, X + r = Y. We may let h be defined by h(x) = x + r so that Y = h(X), and we notice that\r\nh is linear and hence one-to-one. Finally, X takes values 0, 1, 2, . . . implying that the support of Y\r\nwould be {r, r + 1, r + 2, . . .}. Solving for X we get X = Y − r. Examining the PMF of X\r\nfX(x) =\r\n \r\nr + x − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\nx\r\n, (5.7.1)\r\nwe can substitute x = y − r to get\r\nfY (y) = fX(y − r),\r\n=\r\n \r\nr + (y − r) − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\ny−r\r\n,\r\n=\r\n \r\ny − 1\r\nr − 1\r\n!\r\np\r\nr\r\n(1 − p)\r\ny−r\r\n, y = r, r + 1, . . .\r\nEven when the function h is not one-to-one, we may still find the PMF of Y simply by accumu\u0002lating, for each y, the probability of all the x’s that are mapped to that y.\r\nProposition 5.30. Let X be a discrete random variable with PMF fX supported on the set S X. Let\r\nY = h(X) for some function h. Then Y has PMF fY defined by\r\nfY (y) =\r\nX\r\n{x∈S X| h(x)=y}\r\nfX(x) (5.7.2)\r\nExample 5.31. Let X ∼ binom(size = 4, prob = 1/2), and let Y = (X − 1)2. Consider the\r\nfollowing table:\r\nx 0 1 2 3 4\r\nfX(x) 1/16 1/4 6/16 1/4 1/16\r\ny = (x − 2)2 1 0 1 4 9\r\nFrom this we see that Y has support S Y = {0, 1, 4, 9}. We also see that h(x) = (x−1)2is not one\u0002to-one on the support of X, because both x = 0 and x = 2 are mapped by h to y = 1. Nevertheless,\r\nwe see that Y = 0 only when X = 1, which has probability 1/4; therefore, fY (0) should equal 1/4.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/562ff477-96e7-4345-9794-a3a50c7c2b7a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2bb1e5a3a8948fc03d07a5306c92f593927ba255c64f90928275c8dcb75001a3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 535
      },
      {
        "segments": [
          {
            "segment_id": "81eb867e-015f-481d-8b20-8f88fd5f1a1f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 153,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.7. FUNCTIONS OF DISCRETE RANDOM VARIABLES 137\r\nA similar approach works for y = 4 and y = 9. And Y = 1 exactly when X = 0 or X = 2, which has\r\ntotal probability 7/16. In summary, the PMF of Y may be written:\r\ny 0 1 4 9\r\nfX(x) 1/4 7/16 1/4 1/16\r\nNote that there is not a special name for the distribution of Y, it is just an example of what to do\r\nwhen the transformation of a random variable is not one-to-one. The method is the same for more\r\ncomplicated problems.\r\nProposition 5.32. If X is a random variable with IE X = µ and Var(X) = σ\r\n2\r\n, then the mean and\r\nvariance of Y = mX + b is\r\nµY = mµ + b, σ2\r\nY = m\r\n2σ2\r\n, σY = |m|σ. (5.7.3)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/81eb867e-015f-481d-8b20-8f88fd5f1a1f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4495a272c08c497669e69168bbe229fd74af025b3126e02003b07b545bdba64c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "1dcf51f4-d090-4798-8440-788e02899d4a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 154,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "138 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\nChapter Exercises\r\nExercise 5.1. A recent national study showed that approximately 44.7% of college students have\r\nused Wikipedia as a source in at least one of their term papers. Let X equal the number of students\r\nin a random sample of size n = 31 who have used Wikipedia as a source.\r\n1. How is X distributed?\r\nX ∼ binom(size = 31, prob = 0.447)\r\n2. Sketch the probability mass function (roughly).\r\n5 10 15 20\r\n0.00 0.08\r\nBinomial Dist'n: Trials = 31, Prob of success = 0.447\r\nNumber of Successes\r\nProbability Mass\r\n● ● ● ●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ● ● ●\r\n3. Sketch the cumulative distribution function (roughly).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1dcf51f4-d090-4798-8440-788e02899d4a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c701446ec8de244669201c7c83c24d4770b6929cdc97ff82e391b6a29069b64c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "68010082-ff39-4a74-a815-203f42cde60e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 155,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.7. FUNCTIONS OF DISCRETE RANDOM VARIABLES 139\r\n5 10 15 20 25\r\n0.0 0.4 0.8\r\nBinomial Dist'n: Trials = 31, Prob of success = 0.447\r\nNumber of Successes\r\nCumulative Probability\r\n● ● ● ● ● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ● ● ● ● ● ● ●\r\n● ● ● ● ● ● ● ●\r\n●\r\n●\r\n●\r\n●\r\n● ● ● ● ● ● ●\r\n4. Find the probability that X is equal to 17.\r\n> dbinom(17, size = 31, prob = 0.447)\r\n[1] 0.07532248\r\n5. Find the probability that X is at most 13.\r\n> pbinom(13, size = 31, prob = 0.447)\r\n[1] 0.451357\r\n6. Find the probability that X is bigger than 11.\r\n> pbinom(11, size = 31, prob = 0.447, lower.tail = FALSE)\r\n[1] 0.8020339\r\n7. Find the probability that X is at least 15.\r\n> pbinom(14, size = 31, prob = 0.447, lower.tail = FALSE)\r\n[1] 0.406024\r\n8. Find the probability that X is between 16 and 19, inclusive.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/68010082-ff39-4a74-a815-203f42cde60e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f6aad699523c9dd957829543b11cf844d745c12ad70f71f2fbc3bfd95c098438",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 433
      },
      {
        "segments": [
          {
            "segment_id": "23d1025a-c089-4b8a-86bb-c5486a878e1e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 156,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "140 CHAPTER 5. DISCRETE DISTRIBUTIONS\r\n> sum(dbinom(16:19, size = 31, prob = 0.447))\r\n[1] 0.2544758\r\n> diff(pbinom(c(19, 15), size = 31, prob = 0.447, lower.tail = FALSE))\r\n[1] 0.2544758\r\n9. Give the mean of X, denoted IE X.\r\n> library(distrEx)\r\n> X = Binom(size = 31, prob = 0.447)\r\n> E(X)\r\n[1] 13.857\r\n10. Give the variance of X.\r\n> var(X)\r\n[1] 7.662921\r\n11. Give the standard deviation of X.\r\n> sd(X)\r\n[1] 2.768198\r\n12. Find IE(4X + 51.324)\r\n> E(4 * X + 51.324)\r\n[1] 106.752\r\nExercise 5.2. For the following situations, decide what the distribution of X should be. In nearly\r\nevery case, there are additional assumptions that should be made for the distribution to apply;\r\nidentify those assumptions (which may or may not hold in practice.)\r\n1. We shoot basketballs at a basketball hoop, and count the number of shots until we make a\r\ngoal. Let X denote the number of missed shots. On a normal day we would typically make\r\nabout 37% of the shots.\r\n2. In a local lottery in which a three digit number is selected randomly, let X be the number\r\nselected.\r\n3. We drop a Styrofoam cup to the floor twenty times, each time recording whether the cup\r\ncomes to rest perfectly right side up, or not. Let X be the number of times the cup lands\r\nperfectly right side up.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/23d1025a-c089-4b8a-86bb-c5486a878e1e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c62668c813caa11cce8de96e58d53c1be7507aabbccc1629d8130abbd84d94fb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 229
      },
      {
        "segments": [
          {
            "segment_id": "e2739f1a-2820-4f9b-9389-5fb020147b9b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 157,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "5.7. FUNCTIONS OF DISCRETE RANDOM VARIABLES 141\r\n4. We toss a piece of trash at the garbage can from across the room. If we miss the trash can,\r\nwe retrieve the trash and try again, continuing to toss until we make the shot. Let X denote\r\nthe number of missed shots.\r\n5. Working for the border patrol, we inspect shipping cargo as when it enters the harbor looking\r\nfor contraband. A certain ship comes to port with 557 cargo containers. Standard practice\r\nis to select 10 containers randomly and inspect each one very carefully, classifying it as\r\neither having contraband or not. Let X count the number of containers that illegally contain\r\ncontraband.\r\n6. At the same time every year, some migratory birds land in a bush outside for a short rest. On\r\na certain day, we look outside and let X denote the number of birds in the bush.\r\n7. We count the number of rain drops that fall in a circular area on a sidewalk during a ten\r\nminute period of a thunder storm.\r\n8. We count the number of moth eggs on our window screen.\r\n9. We count the number of blades of grass in a one square foot patch of land.\r\n10. We count the number of pats on a baby’s back until (s)he burps.\r\nExercise 5.3. Show that IE(X − µ)\r\n2 = IE X2 − µ2\r\n. Hint: expand the quantity (X − µ)\r\n2\r\nand distribute\r\nthe expectation over the resulting terms.\r\nExercise 5.4. If X ∼ binom(size = n, prob = p) show that IE X(X − 1) = n(n − 1)p\r\n2\r\n.\r\nExercise 5.5. Calculate the mean and variance of the hypergeometric distribution. Show that\r\nµ = K\r\nM\r\nM + N\r\n, σ2 = K\r\nMN\r\n(M + N)\r\n2\r\nM + N − K\r\nM + N − 1\r\n. (5.7.4)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e2739f1a-2820-4f9b-9389-5fb020147b9b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ce869d9fdecc037c1f7e25faad005a59bef7da90e1070f0ed9da8c7d2533b370",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "44ad0737-10c8-45df-8162-29b8d87d9cfa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 158,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "142 CHAPTER 5. DISCRETE DISTRIBUTIONS",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/44ad0737-10c8-45df-8162-29b8d87d9cfa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=972e98cf87a43b88209127d0350a0b958e54bc25b0cd2b9a3d1687dc4526f1f7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 318
      },
      {
        "segments": [
          {
            "segment_id": "eb067946-1f61-486d-b22d-bba1cf7c5621",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 159,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 6\r\nContinuous Distributions\r\nThe focus of the last chapter was on random variables whose support can be written down in a list\r\nof values (finite or countably infinite), such as the number of successes in a sequence of Bernoulli\r\ntrials. Now we move to random variables whose support is a whole range of values, say, an interval\r\n(a, b). It is shown in later classes that it is impossible to write all of the numbers down in a list;\r\nthere are simply too many of them.\r\nThis chapter begins with continuous random variables and the associated PDFs and CDFs The\r\ncontinuous uniform distribution is highlighted, along with the Gaussian, or normal, distribution.\r\nSome mathematical details pave the way for a catalogue of models.\r\nThe interested reader who would like to learn more about any of the assorted discrete distribu\u0002tions mentioned below should take a look at Continuous Univariate Distributions, Volumes 1 and\r\n2 by Johnson et al [47, 48].\r\nWhat do I want them to know?\r\n• how to choose a reasonable continuous model under a variety of physical circumstances\r\n• basic correspondence between continuous versus discrete random variables\r\n• the general tools of the trade for manipulation of continuous random variables, integration,\r\netc.\r\n• some details on a couple of continuous models, and exposure to a bunch of other ones\r\n• how to make new continuous random variables from old ones\r\n6.1 Continuous Random Variables\r\n6.1.1 Probability Density Functions\r\nContinuous random variables have supports that look like\r\nS X = [a, b] or (a, b), (6.1.1)\r\n143",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/eb067946-1f61-486d-b22d-bba1cf7c5621.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=06976c838c25a053f265b7708caf55f1744e9a6d9ff7e634ffe3ec346179a0d7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 259
      },
      {
        "segments": [
          {
            "segment_id": "d5052304-d2f6-487c-8296-9d5e388cb85c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 160,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "144 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nor unions of intervals of the above form. Examples of random variables that are often taken to be\r\ncontinuous are:\r\n• the height or weight of an individual,\r\n• other physical measurements such as the length or size of an object, and\r\n• durations of time (usually).\r\nEvery continuous random variable X has a probability density function (PDF) denoted fX associated\r\nwith it1that satisfies three basic properties:\r\n1. fX(x) > 0 for x ∈ S X,\r\n2. R\r\nx∈S X\r\nfX(x) dx = 1, and\r\n3. IP(X ∈ A) =\r\nR\r\nx∈A\r\nfX(x) dx, for an event A ⊂ S X.\r\nRemark 6.1. We can say the following about continuous random variables:\r\n• Usually, the set A in 3 takes the form of an interval, for example, A = [c, d], in which case\r\nIP(X ∈ A) =\r\nZ d\r\nc\r\nfX(x) dx. (6.1.2)\r\n• It follows that the probability that X falls in a given interval is simply the area under the\r\ncurve of fX over the interval.\r\n• Since the area of a line x = c in the plane is zero, IP(X = c) = 0 for any value c. In other\r\nwords, the chance that X equals a particular value c is zero, and this is true for any number\r\nc. Moreover, when a < b all of the following probabilities are the same:\r\nIP(a ≤ X ≤ b) = IP(a < X ≤ b) = IP(a ≤ X < b) = IP(a < X < b). (6.1.3)\r\n• The PDF fX can sometimes be greater than 1. This is in contrast to the discrete case; every\r\nnonzero value of a PMF is a probability which is restricted to lie in the interval [0, 1].\r\nWe met the cumulative distribution function, FX, in Chapter 5. Recall that it is defined by FX(t) =\r\nIP(X ≤ t), for −∞ < t < ∞. While in the discrete case the CDF is unwieldy, in the continuous case\r\nthe CDF has a relatively convenient form:\r\nFX(t) = IP(X ≤ t) =\r\nZ t\r\n−∞\r\nfX(x) dx, −∞ < t < ∞. (6.1.4)\r\nRemark 6.2. For any continuous CDF FX the following are true.\r\n• FX is nondecreasing , that is, t1 ≤ t2 implies FX(t1) ≤ FX(t2).\r\n1Not true. There are pathological random variables with no density function. (This is one of the crazy things that\r\ncan happen in the world of measure theory). But in this book we will not get even close to these anomalous beasts, and\r\nregardless it can be proved that the CDF always exists.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d5052304-d2f6-487c-8296-9d5e388cb85c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e6414c3ebd9f8e008d7187b383db377cf8068734a05ad3ec2829d7b7cb143fae",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 435
      },
      {
        "segments": [
          {
            "segment_id": "01410ce8-ccc2-4f55-9bbd-d478daf4e8a9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 161,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.1. CONTINUOUS RANDOM VARIABLES 145\r\n• FX is continuous (see Appendix E.2). Note the distinction from the discrete case: CDFs of\r\ndiscrete random variables are not continuous, they are only right continuous.\r\n• limt→−∞ FX(t) = 0 and limt→∞ FX(t) = 1.\r\nThere is a handy relationship between the CDF and PDF in the continuous case. Consider the\r\nderivative of FX:\r\nF\r\n0\r\nX\r\n(t) =\r\nd\r\ndt\r\nFX(t) =\r\nd\r\ndt\r\nZ t\r\n−∞\r\nfX(x) dx = fX(t), (6.1.5)\r\nthe last equality being true by the Fundamental Theorem of Calculus, part (2) (see Appendix E.2).\r\nIn short, (FX)\r\n0 = fX in the continuous case2\r\n.\r\n6.1.2 Expectation of Continuous Random Variables\r\nFor a continuous random variable X the expected value of g(X) is\r\nIE g(X) =\r\nZ\r\nx∈S\r\ng(x)fX(x) dx, (6.1.6)\r\nprovided the (potentially improper) integral R\r\nS\r\n|g(x)| f(x)dx is convergent. One important example\r\nis the mean µ, also known as IE X:\r\nµ = IE X =\r\nZ\r\nx∈S\r\nx fX(x) dx, (6.1.7)\r\nprovided R\r\nS\r\n|x| f(x)dx is finite. Also there is the variance\r\nσ\r\n2 = IE(X − µ)2 =\r\nZ\r\nx∈S\r\n(x − µ)\r\n2\r\nfX(x) dx, (6.1.8)\r\nwhich can be computed with the alternate formula σ\r\n2 = IE X2 − (IE X)2\r\n. In addition, there is the\r\nstandard deviation σ =\r\n√\r\nσ2. The moment generating function is given by\r\nMX(t) = IE etX =\r\nZ ∞\r\n−∞\r\ne\r\ntx fX(x) dx, (6.1.9)\r\nprovided the integral exists (is finite) for all t in a neighborhood of t = 0.\r\nExample 6.3. Let the continuous random variable X have PDF\r\nfX(x) = 3x\r\n2\r\n, 0 ≤ x ≤ 1.\r\nWe will see later that fX belongs to the Beta family of distributions. It is easy to see thatR ∞\r\n−∞\r\nf(x)dx =\r\n1.\r\nZ ∞\r\n−∞\r\nfX(x)dx =\r\nZ 1\r\n0\r\n3x\r\n2\r\ndx\r\n= x\r\n3\r\n\f\r\n\f\r\n\f\r\n1\r\nx=0\r\n= 1\r\n3 − 03\r\n= 1.\r\n2\r\nIn the discrete case, fX(x) = FX(x) − limt→x\r\n− FX(t).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/01410ce8-ccc2-4f55-9bbd-d478daf4e8a9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0afecce81be0ecbdd016b10ffc78a7701baa7ea33bb14f147737b87709baa44a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 338
      },
      {
        "segments": [
          {
            "segment_id": "b9e10aa3-d3bc-4bb8-ab20-4cb5227dc668",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 162,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "146 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nThis being said, we may find IP(0.14 ≤ X < 0.71).\r\nIP(0.14 ≤ X < 0.71) =\r\nZ 0.71\r\n0.14\r\n3x\r\n2\r\ndx,\r\n= x\r\n3\r\n\f\r\n\f\r\n\f\r\n0.71\r\nx=0.14\r\n= 0.713 − 0.143\r\n≈ 0.355167.\r\nWe can find the mean and variance in an identical manner.\r\nµ =\r\nZ ∞\r\n−∞\r\nx fX(x)dx =\r\nZ 1\r\n0\r\nx · 3x\r\n2\r\ndx,\r\n=\r\n3\r\n4\r\nx\r\n4\r\n|\r\n1\r\nx=0\r\n,\r\n=\r\n3\r\n4\r\n.\r\nIt would perhaps be best to calculate the variance with the shortcut formula σ\r\n2 = IE X2 − µ2\r\n:\r\nIE X\r\n2 =\r\nZ ∞\r\n−∞\r\nx\r\n2\r\nfX(x)dx =\r\nZ 1\r\n0\r\nx\r\n2\r\n· 3x\r\n2\r\ndx\r\n=\r\n3\r\n5\r\nx\r\n5\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\nx=0\r\n= 3/5.\r\nwhich gives σ\r\n2 = 3/5 − (3/4)2 = 3/80.\r\nExample 6.4. We will try one with unbounded support to brush up on improper integration. Let\r\nthe random variable X have PDF\r\nfX(x) =\r\n3\r\nx\r\n4\r\n, x > 1.\r\nWe can show that R ∞\r\n−∞\r\nf(x)dx = 1:\r\nZ ∞\r\n−∞\r\nfX(x)dx =\r\nZ ∞\r\n1\r\n3\r\nx\r\n4\r\ndx\r\n= lim\r\nt→∞ Z t\r\n1\r\n3\r\nx\r\n4\r\ndx\r\n= lim\r\nt→∞\r\n3\r\n1\r\n−3\r\nx\r\n−3\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\nt\r\nx=1\r\n= −\r\n \r\nlim\r\nt→∞\r\n1\r\nt\r\n3\r\n− 1\r\n!\r\n= 1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b9e10aa3-d3bc-4bb8-ab20-4cb5227dc668.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=404e62cf5ae9a298416c89756e7b49bfcc7515d6d7b5cc9fba757eb590816532",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 224
      },
      {
        "segments": [
          {
            "segment_id": "ed8d7e12-beca-45b2-9b84-8fb1052fc5f1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 163,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.1. CONTINUOUS RANDOM VARIABLES 147\r\nWe calculate IP(3.4 ≤ X < 7.1):\r\nIP(3.4 ≤ X < 7.1) =\r\nZ 7.1\r\n3.4\r\n3x\r\n−4\r\ndx\r\n= 3\r\n1\r\n−3\r\nx\r\n−3\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n7.1\r\nx=3.4\r\n= −1(7.1\r\n−3 − 3.4−3\r\n)\r\n≈ 0.0226487123.\r\nWe locate the mean and variance just like before.\r\nµ =\r\nZ ∞\r\n−∞\r\nx fX(x)dx =\r\nZ ∞\r\n1\r\nx ·\r\n3\r\nx\r\n4\r\ndx\r\n= 3\r\n1\r\n−2\r\nx\r\n−2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n∞\r\nx=1\r\n= −\r\n3\r\n2\r\n \r\nlim\r\nt→∞\r\n1\r\nt\r\n2\r\n− 1\r\n!\r\n=\r\n3\r\n2\r\n.\r\nAgain we use the shortcut σ\r\n2 = IE X2 − µ2\r\n:\r\nIE X\r\n2 =\r\nZ ∞\r\n−∞\r\nx\r\n2\r\nfX(x)dx =\r\nZ ∞\r\n1\r\nx\r\n2\r\n·\r\n3\r\nx\r\n4\r\ndx\r\n= 3\r\n1\r\n−1\r\nx\r\n−1\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n∞\r\nx=1\r\n= −3\r\n \r\nlim\r\nt→∞\r\n1\r\nt\r\n2\r\n− 1\r\n!\r\n= 3,\r\nwhich closes the example with σ\r\n2 = 3 − (3/2)2 = 3/4.\r\n6.1.3 How to do it with R\r\nThere exist utilities to calculate probabilities and expectations for general continuous random vari\u0002ables, but it is better to find a built-in model, if possible. Sometimes it is not possible. We show\r\nhow to do it the long way, and the distr package way.\r\nExample 6.5. Let X have PDF f(x) = 3x\r\n2\r\n, 0 < x < 1 and find IP(0.14 ≤ X ≤ 0.71). (We will\r\nignore that X is a beta random variable for the sake of argument.)\r\n> f <- function(x) 3 * x^2\r\n> integrate(f, lower = 0.14, upper = 0.71)\r\n0.355167 with absolute error < 3.9e-15\r\nCompare this to the answer we found in Example 6.3. We could integrate the function x f(x) =\r\n3*x^3 from zero to one to get the mean, and use the shortcut σ\r\n2 = IE X2 − (IE X)\r\n2\r\nfor the variance.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ed8d7e12-beca-45b2-9b84-8fb1052fc5f1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=29e0b23f43b80b120295bd4cb39c73002f906b0ee929fd828d97808a3b37bb10",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 309
      },
      {
        "segments": [
          {
            "segment_id": "0ba94d91-12a4-49b9-8589-9ce65c1714d8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 164,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "148 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nExample 6.6. Let X have PDF f(x) = 3/x\r\n4\r\n, x > 1. We may integrate the function x f(x) = 3/x^3\r\nfrom zero to infinity to get the mean of X.\r\n> g <- function(x) 3/x^3\r\n> integrate(g, lower = 1, upper = Inf)\r\n1.5 with absolute error < 1.7e-14\r\nCompare this to the answer we got in Example 6.4. Use -Inf for −∞.\r\nExample 6.7. Let us redo Example 6.3 with the distr package. The method is similar to that\r\nencountered in Section 5.1.3 in Chapter 5. We define an absolutely continuous random variable:\r\n> library(distr)\r\n> f <- function(x) 3 * x^2\r\n> X <- AbscontDistribution(d = f, low1 = 0, up1 = 1)\r\n> p(X)(0.71) - p(X)(0.14)\r\n[1] 0.355167\r\nCompare this answers we found earlier. Now let us try expectation with the distrEx package\r\n[74]:\r\n> library(distrEx)\r\n> E(X)\r\n[1] 0.7496337\r\n> var(X)\r\n[1] 0.03768305\r\n> 3/80\r\n[1] 0.0375\r\nCompare these answers to the ones we found in Example 6.3. Why are they different? Because\r\nthe distrEx package resorts to numerical methods when it encounters a model it does not recog\u0002nize. This means that the answers we get for calculations may not exactly match the theoretical\r\nvalues. Be careful.\r\n6.2 The Continuous Uniform Distribution\r\nA random variable X with the continuous uniform distribution on the interval (a, b) has PDF\r\nfX(x) =\r\n1\r\nb − a\r\n, a < x < b. (6.2.1)\r\nThe associated R function is dunif(min = a, max = b). We write X ∼ unif(min = a, max = b).\r\nDue to the particularly simple form of this PDF we can also write down explicitly a formula for the\r\nCDF FX:\r\nFX(t) =\r\n\r\n\r\n\r\n0, t < 0,\r\nt−a\r\nb−a\r\n, a ≤ t < b,\r\n1, t ≥ b.\r\n(6.2.2)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0ba94d91-12a4-49b9-8589-9ce65c1714d8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6245255065bb987bc5ee01b0f4b4b08bb91439e346364756cc8d96fc67b40203",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 307
      },
      {
        "segments": [
          {
            "segment_id": "82c4196b-51f0-4a0e-932e-c67d40dd748c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 165,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.3. THE NORMAL DISTRIBUTION 149\r\nThe continuous uniform distribution is the continuous analogue of the discrete uniform distribution;\r\nit is used to model experiments whose outcome is an interval of numbers that are “equally likely” in\r\nthe sense that any two intervals of equal length in the support have the same probability associated\r\nwith them.\r\nExample 6.8. Choose a number in [0,1] at random, and let X be the number chosen. Then X ∼\r\nunif(min = 0, max = 1).\r\nThe mean of X ∼ unif(min = a, max = b) is relatively simple to calculate:\r\nµ = IE X =\r\nZ ∞\r\n−∞\r\nx fX(x) dx,\r\n=\r\nZ b\r\na\r\nx\r\n1\r\nb − a\r\ndx,\r\n=\r\n1\r\nb − a\r\nx\r\n2\r\n2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\nb\r\nx=a\r\n,\r\n=\r\n1\r\nb − a\r\nb\r\n2 − a2\r\n2\r\n,\r\n=\r\nb + a\r\n2\r\n,\r\nusing the popular formula for the difference of squares. The variance is left to Exercise 6.4.\r\n6.3 The Normal Distribution\r\nWe say that X has a normal distribution if it has PDF\r\nfX(x) =\r\n1\r\nσ\r\n√\r\n2π\r\nexp (\r\n−(x − µ)\r\n2\r\n2σ2\r\n)\r\n, −∞ < x < ∞. (6.3.1)\r\nWe write X ∼ norm(mean = µ, sd = σ), and the associated R function is dnorm(x, mean = 0,\r\nsd = 1).\r\nThe familiar bell-shaped curve, the normal distribution is also known as the Gaussian dis\u0002tribution because the German mathematician C. F. Gauss largely contributed to its mathematical\r\ndevelopment. This distribution is by far the most important distribution, continuous or discrete.\r\nThe normal model appears in the theory of all sorts of natural phenomena, from to the way parti\u0002cles of smoke dissipate in a closed room, to the journey of a bottle in the ocean to the white noise\r\nof cosmic background radiation.\r\nWhen µ = 0 and σ = 1 we say that the random variable has a standard normal distribution and\r\nwe typically write Z ∼ norm(mean = 0, sd = 1). The lowercase Greek letter phi (φ) is used to\r\ndenote the standard normal PDF and the capital Greek letter phi Φ is used to denote the standard\r\nnormal CDF: for −∞ < z < ∞,\r\nφ(z) =\r\n1\r\n√\r\n2π\r\ne\r\n−z\r\n2\r\n/2\r\nand Φ(t) =\r\nZ t\r\n−∞\r\nφ(z) dz. (6.3.2)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/82c4196b-51f0-4a0e-932e-c67d40dd748c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=08dfd2c9baef6a02b62368435bb7fd09d7399b008704f14cac4b844d1f74658e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 386
      },
      {
        "segments": [
          {
            "segment_id": "25140778-d0e5-4369-920d-860dd7230483",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 166,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "150 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nProposition 6.9. If X ∼ norm(mean = µ, sd = σ) then\r\nZ =\r\nX − µ\r\nσ\r\n∼ norm(mean = 0, sd = 1). (6.3.3)\r\nThe MGF of Z ∼ norm(mean = 0, sd = 1) is relatively easy to derive:\r\nMZ(t) =\r\nZ ∞\r\n−∞\r\ne\r\ntz 1\r\n√\r\n2π\r\ne\r\n−z\r\n2\r\n/2\r\ndz,\r\n=\r\nZ ∞\r\n−∞\r\n1\r\n√\r\n2π\r\nexp (−\r\n1\r\n2\r\n\u0010\r\nz\r\n2 + 2tz + t2\r\n\u0011\r\n+\r\nt\r\n2\r\n2\r\n)\r\ndz,\r\n= e\r\nt\r\n2\r\n/2\r\n Z ∞\r\n−∞\r\n1\r\n√\r\n2π\r\ne\r\n−[z−(−t)]2/2\r\ndz\r\n!\r\n,\r\nand the quantity in the parentheses is the total area under a norm(mean = −t, sd = 1) density,\r\nwhich is one. Therefore,\r\nMZ(t) = e\r\n−t\r\n2\r\n/2\r\n, −∞ < t < ∞. (6.3.4)\r\nExample 6.10. The MGF of X ∼ norm(mean = µ, sd = σ) is then not difficult either because\r\nZ =\r\nX − µ\r\nσ\r\n, or rewriting, X = σZ + µ.\r\nTherefore:\r\nMX(t) = IE etX = IE et(σZ+µ) = IE eσtXe\r\nµ = etµMZ(σt),\r\nand we know that MZ(t) = e\r\nt\r\n2\r\n/2\r\n, thus substituting we get\r\nMX(t) = e\r\ntµ\r\ne\r\n(σt)\r\n2\r\n/2 = exp n\r\nµt + σ\r\n2\r\nt\r\n2\r\n/2\r\no\r\n,\r\nfor −∞ < t < ∞.\r\nFact 6.11. The same argument above shows that if X has MGF MX(t) then the MGF of Y = a + bX\r\nis\r\nMY (t) = e\r\ntaMX(bt). (6.3.5)\r\nExample 6.12. The 68-95-99.7 Rule. We saw in Section 3.3.6 that when an empirical distribution\r\nis approximately bell shaped there are specific proportions of the observations which fall at varying\r\ndistances from the (sample) mean. We can see where these come from – and obtain more precise\r\nproportions – with the following:\r\n> pnorm(1:3) - pnorm(-(1:3))\r\n[1] 0.6826895 0.9544997 0.9973002\r\nExample 6.13. Let the random experiment consist of a person taking an IQ test, and let X be the\r\nscore on the test. The scores on such a test are typically standardized to have a mean of 100 and a\r\nstandard deviation of 15. What is IP(85 ≤ X ≤ 115)?\r\nSolution: this one is easy because the limits 85 and 115 fall exactly one standard deviation\r\n(below and above, respectively) from the mean of 100. The answer is therefore approximately\r\n68%.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/25140778-d0e5-4369-920d-860dd7230483.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7963e8d4dc3ff9057469feb5abefbed85c62b5dc2680e53e70df5f7c933b157a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 398
      },
      {
        "segments": [
          {
            "segment_id": "3c0a2de7-f213-4935-9857-339cbe488873",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 167,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.3. THE NORMAL DISTRIBUTION 151\r\n6.3.1 Normal Quantiles and the Quantile Function\r\nUntil now we have been given two values and our task has been to find the area under the PDF\r\nbetween those values. In this section, we go in reverse: we are given an area, and we would like to\r\nfind the value(s) that correspond to that area.\r\nExample 6.14. Assuming the IQ model of Example 6.13, what is the lowest possible IQ score that\r\na person can have and still be in the top 1% of all IQ scores?\r\nSolution: If a person is in the top 1%, then that means that 99% of the people have lower IQ\r\nscores. So, in other words, we are looking for a value x such that F(x) = IP(X ≤ x) satisfies F(x) =\r\n0.99, or yet another way to say it is that we would like to solve the equation F(x)−0.99 = 0. For the\r\nsake of argument, let us see how to do this the long way. We define the function g(x) = F(x)−0.99,\r\nand then look for the root of g with the uniroot function. It uses numerical procedures to find\r\nthe root so we need to give it an interval of x values in which to search for the root. We can get\r\nan educated guess from the Empirical Rule 3.13; the root should be somewhere between two and\r\nthree standard deviations (15 each) above the mean (which is 100).\r\n> g <- function(x) pnorm(x, mean = 100, sd = 15) - 0.99\r\n> uniroot(g, interval = c(130, 145))\r\n$root\r\n[1] 134.8952\r\n$f.root\r\n[1] -4.873083e-09\r\n$iter\r\n[1] 6\r\n$estim.prec\r\n[1] 6.103516e-05\r\nThe answer is shown in $root which is approximately 134.8952, that is, a person with this IQ\r\nscore or higher falls in the top 1% of all IQ scores.\r\nThe discussion in example 6.14 was centered on the search for a value x that solved an equation\r\nF(x) = p, for some given probability p, or in mathematical parlance, the search for F\r\n−1\r\n, the inverse\r\nof the CDF of X, evaluated at p. This is so important that it merits a definition all its own.\r\nDefinition 6.15. The quantile function3 of a random variable X is the inverse of its cumulative\r\ndistribution function:\r\nQX(p) = min {x : FX(x) ≥ p} , 0 < p < 1. (6.3.6)\r\nRemark 6.16. Here are some properties of quantile functions:\r\n1. The quantile function is defined and finite for all 0 < p < 1.\r\n3The precise definition of the quantile function is QX(p) = inf {x : FX(x) ≥ p}, so at least it is well defined (though\r\nperhaps infinite) for the values p = 0 and p = 1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3c0a2de7-f213-4935-9857-339cbe488873.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ae36ece750c5fee41aa746a49f91b5f933e70807ec829575cc3037611be2a04b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 455
      },
      {
        "segments": [
          {
            "segment_id": "09303027-2b27-4e9e-877b-d9f875f55a8e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 168,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "152 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\n2. QX is left-continuous (see Appendix E.2). For discrete random variables it is a step function,\r\nand for continuous random variables it is a continuous function.\r\n3. In the continuous case the graph of QX may be obtained by reflecting the graph of FX about\r\nthe line y = x. In the discrete case, before reflecting one should: 1) connect the dots to get\r\nrid of the jumps – this will make the graph look like a set of stairs, 2) erase the horizontal\r\nlines so that only vertical lines remain, and finally 3) swap the open circles with the solid\r\ndots. Please see Figure 5.3.2 for a comparison.\r\n4. The two limits\r\nlim\r\np→0\r\n+\r\nQX(p) and lim\r\np→1\r\n−\r\nQX(p)\r\nalways exist, but may be infinite (that is, sometimes limp→0 Q(p) = −∞ and/or limp→1 Q(p) =\r\n∞).\r\nAs the reader might expect, the standard normal distribution is a very special case and has its\r\nown special notation.\r\nDefinition 6.17. For 0 < α < 1, the symbol zα denotes the unique solution of the equation IP(Z >\r\nzα) = α, where Z ∼ norm(mean = 0, sd = 1). It can be calculated in one of two equivalent ways:\r\nqnorm(1 − α) and qnorm(α, lower.tail = FALSE).\r\nThere are a few other very important special cases which we will encounter in later chapters.\r\n6.3.2 How to do it with R\r\nQuantile functions are defined for all of the base distributions with the q prefix to the distribution\r\nname, except for the ECDF whose quantile function is exactly the Qx(p) =quantile(x, probs\r\n= p , type = 1) function.\r\nExample 6.18. Back to Example 6.14, we are looking for QX(0.99), where X ∼ norm(mean =\r\n100, sd = 15). It could not be easier to do with R.\r\n> qnorm(0.99, mean = 100, sd = 15)\r\n[1] 134.8952\r\nCompare this answer to the one obtained earlier with uniroot.\r\nExample 6.19. Find the values z0.025, z0.01, and z0.005 (these will play an important role from Chap\u0002ter 9 onward).\r\n> qnorm(c(0.025, 0.01, 0.005), lower.tail = FALSE)\r\n[1] 1.959964 2.326348 2.575829\r\nNote the lower.tail argument. We would get the same answer with\r\nqnorm(c(0.975 , 0.99 , 0.995))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/09303027-2b27-4e9e-877b-d9f875f55a8e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=49ce460ac6402483857ca8601fe4f96973f21ba10c96f1b2704e9847bd11964a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 371
      },
      {
        "segments": [
          {
            "segment_id": "7b02c70d-a7b5-4e64-b836-e15287d9d6dd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 169,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.4. FUNCTIONS OF CONTINUOUS RANDOM VARIABLES 153\r\n6.4 Functions of Continuous Random Variables\r\nThe goal of this section is to determine the distribution of U = g(X) based on the distribution of\r\nX. In the discrete case all we needed to do was back substitute for x = g\r\n−1\r\n(u) in the PMF of X\r\n(sometimes accumulating probability mass along the way). In the continuous case, however, we\r\nneed more sophisticated tools. Now would be a good time to review Appendix E.2.\r\n6.4.1 The PDF Method\r\nProposition 6.20. Let X have PDF fX and let g be a function which is one-to-one with a differen\u0002tiable inverse g−1\r\n. Then the PDF of U = g(X) is given by\r\nfU(u) = fX\r\nh\r\ng\r\n−1\r\n(u)\r\ni\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\nd\r\ndu\r\ng\r\n−1\r\n(u)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n. (6.4.1)\r\nRemark 6.21. The formula in Equation 6.4.1 is nice, but does not really make any sense. It is better\r\nto write in the intuitive form\r\nfU(u) = fX(x)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\ndx\r\ndu\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n. (6.4.2)\r\nExample 6.22. Let X ∼ norm(mean = µ, sd = σ), and let Y = e\r\nX\r\n. What is the PDF of Y?\r\nSolution: Notice first that ex > 0 for any x, so the support of Y is (0, ∞). Since the transforma\u0002tion is monotone, we can solve y = e\r\nx\r\nfor x to get x = ln y, giving dx/dy = 1/y. Therefore, for any\r\ny > 0,\r\nfY (y) = fX(ln y) ·\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\ny\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n=\r\n1\r\nσ\r\n√\r\n2π\r\nexp (\r\n(ln y − µ)\r\n2\r\n2σ2\r\n)\r\n·\r\n1\r\ny\r\n,\r\nwhere we have dropped the absolute value bars since y > 0. The random variable Y is said to have\r\na lognormal distribution; see Section 6.5.\r\nExample 6.23. Suppose X ∼ norm(mean = 0, sd = 1) and let Y = 4 − 3X. What is the PDF of Y?\r\nThe support of X is (−∞, ∞), and as x goes from −∞ to ∞, the quantity y = 4−3x also traverses\r\n(−∞, ∞). Solving for x in the equation y = 4 − 3x yields x = −(y − 4)/3 giving dx/dy = −1/3. And\r\nsince\r\nfX(x) =\r\n1\r\n√\r\n2π\r\ne\r\n−x\r\n2\r\n/2\r\n, −∞ < x < ∞,\r\nwe have\r\nfY (y) = fX\r\n \r\ny − 4\r\n3\r\n!\r\n·\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n−\r\n1\r\n3\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n, −∞ < y < ∞,\r\n=\r\n1\r\n3\r\n√\r\n2π\r\ne\r\n−(y−4)2/2·3\r\n2\r\n, −∞ < y < ∞.\r\nWe recognize the PDF of Y to be that of a norm(mean = 4, sd = 3) distribution. Indeed, we may\r\nuse an identical argument as the above to prove the following fact:\r\nFact 6.24. If X ∼ norm(mean = µ, sd = σ) and if Y = a + bX for constants a and b, with b , 0,\r\nthen Y ∼ norm(mean = a + bµ, sd = |b|σ).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7b02c70d-a7b5-4e64-b836-e15287d9d6dd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c32bb6a7afc8d0872e7c787283cbe25558b99b48e40a89019f92909a01055a53",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 482
      },
      {
        "segments": [
          {
            "segment_id": "68a7bed5-50eb-4a11-87c7-66f4ff1424ca",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 170,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "154 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nNote that it is sometimes easier to postpone solving for the inverse transformation x = x(u).\r\nInstead, leave the transformation in the form u = u(x) and calculate the derivative of the original\r\ntransformation\r\ndu/dx = g\r\n0\r\n(x). (6.4.3)\r\nOnce this is known, we can get the PDF of U with\r\nfU(u) = fX(x)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\ndu/dx\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n. (6.4.4)\r\nIn many cases there are cancellations and the work is shorter. Of course, it is not always true that\r\ndx\r\ndu\r\n=\r\n1\r\ndu/dx\r\n, (6.4.5)\r\nbut for the well-behaved examples in this book the trick works just fine.\r\nRemark 6.25. In the case that g is not monotone we cannot apply Proposition 6.20 directly. How\u0002ever, hope is not lost. Rather, we break the support of X into pieces such that g is monotone on\r\neach one. We apply Proposition 6.20 on each piece, and finish up by adding the results together.\r\n6.4.2 The CDF method\r\nWe know from Section 6.1 that fX = F\r\n0\r\nX\r\nin the continuous case. Starting from the equation FY (y) =\r\nIP(Y ≤ y), we may substitute g(X) for Y, then solve for X to obtain IP[X ≤ g\r\n−1\r\n(y)], which is just\r\nanother way to write FX[g\r\n−1\r\n(y)]. Differentiating this last quantity with respect to y will yield the\r\nPDF of Y.\r\nExample 6.26. Suppose X ∼ unif(min = 0, max = 1) and suppose that we let Y = − ln X. What is\r\nthe PDF of Y?\r\nThe support set of X is (0, 1), and y traverses (0, ∞) as x ranges from 0 to 1, so the support set\r\nof Y is S Y = (0, ∞). For any y > 0, we consider\r\nFY (y) = IP(Y ≤ y) = IP(− ln X ≤ y) = IP(X ≥ e\r\n−y\r\n) = 1 − IP(X < e\r\n−y\r\n),\r\nwhere the next to last equality follows because the exponential function is monotone (this point will\r\nbe revisited later). Now since X is continuous the two probabilities IP(X < e\r\n−y\r\n) and IP(X ≤ e\r\n−y\r\n) are\r\nequal; thus\r\n1 − IP(X < e\r\n−y\r\n) = 1 − IP(X ≤ e\r\n−y\r\n) = 1 − FX(e−y).\r\nNow recalling that the CDF of a unif(min = 0, max = 1) random variable satisfies F(u) = u (see\r\nEquation 6.2.2), we can say\r\nFY (y) = 1 − FX(e−y) = 1 − e\r\n−y\r\n, for y > 0.\r\nWe have consequently found the formula for the CDF of Y; to obtain the PDF fY we need only\r\ndifferentiate FY :\r\nfY (y) =\r\nd\r\ndy\r\n\r\n1 − e\r\n−y\r\n\u0001\r\n= 0 − e\r\n−y\r\n(−1),\r\nor fY (y) = e\r\n−y\r\nfor y > 0. This turns out to be a member of the exponential family of distributions,\r\nsee Section 6.5.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/68a7bed5-50eb-4a11-87c7-66f4ff1424ca.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=273e978821b42f5d972e9458b91ecdc7496b12686fc4c3b518b256e1762567f6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 483
      },
      {
        "segments": [
          {
            "segment_id": "c92686ac-de72-4a05-acc8-96852597973a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 171,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.4. FUNCTIONS OF CONTINUOUS RANDOM VARIABLES 155\r\nExample 6.27. The Probability Integral Transform. Given a continuous random variable X with\r\nstrictly increasing CDF FX, let the random variable Y be defined by Y = FX(X). Then the distribu\u0002tion of Y is unif(min = 0, max = 1).\r\nProof. We employ the CDF method. First note that the support of Y is (0, 1). Then for any 0 < y <\r\n1,\r\nFY (y) = IP(Y ≤ y) = IP(FX(X) ≤ y).\r\nNow since FX is strictly increasing, it has a well defined inverse function F\r\n−1\r\nX\r\n. Therefore,\r\nIP(FX(X) ≤ y) = IP(X ≤ F\r\n−1\r\nX\r\n(y)) = FX[F\r\n−1\r\nX\r\n(y)] = y.\r\nSummarizing, we have seen that FY (y) = y, 0 < y < 1. But this is exactly the CDF of a unif(min =\r\n0, max = 1) random variable. \u0003\r\nFact 6.28. The Probability Integral Transform is true for all continuous random variables with\r\ncontinuous CDFs, not just for those with strictly increasing CDFs (but the proof is more compli\u0002cated). The transform is not true for discrete random variables, or for continuous random variables\r\nhaving a discrete component (that is, with jumps in their CDF).\r\nExample 6.29. Let Z ∼ norm(mean = 0, sd = 1) and let U = Z\r\n2\r\n. What is the PDF of U?\r\nNotice first that Z\r\n2 ≥ 0, and thus the support of U is [0, ∞). And for any u ≥ 0,\r\nFU(u) = IP(U ≤ u) = IP(Z\r\n2 ≤ u).\r\nBut Z\r\n2 ≤ u occurs if and only if −\r\n√\r\nu ≤ Z ≤\r\n√\r\nu. The last probability above is simply the area\r\nunder the standard normal PDF from −\r\n√\r\nu to √u, and since φ is symmetric about 0, we have\r\nIP(Z\r\n2 ≤ u) = 2 IP(0 ≤ Z ≤\r\n√\r\nu) = 2\r\nh\r\nFZ(\r\n√\r\nu) − FZ(0)i= 2Φ(\r\n√\r\nu) − 1,\r\nbecause Φ(0) = 1/2. To find the PDF of U we differentiate the CDF recalling that Φ0 = φ.\r\nfU(u) =\r\n\u0010\r\n2Φ(\r\n√\r\nu) − 1\r\n\u00110\r\n= 2φ(\r\n√\r\nu) ·\r\n1\r\n2\r\n√\r\nu\r\n= u\r\n−1/2\r\nφ(\r\n√\r\nu).\r\nSubstituting,\r\nfU(u) = u\r\n−1/2\r\n1\r\n√\r\n2π\r\ne\r\n−(\r\n√\r\nu)\r\n2\r\n/2 = (2πu)−1/2\r\ne\r\n−u\r\n, u > 0.\r\nThis is what we will later call a chi-square distribution with 1 degree of freedom. See Section 6.5.\r\n6.4.3 How to do it with R\r\nThe distr package has functionality to investigate transformations of univariate distributions.\r\nThere are exact results for ordinary transformations of the standard distributions, and distr takes\r\nadvantage of these in many cases. For instance, the distr package can handle the transformation\r\nin Example 6.23 quite nicely:\r\n> library(distr)\r\n> X <- Norm(mean = 0, sd = 1)\r\n> Y <- 4 - 3 * X\r\n> Y",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c92686ac-de72-4a05-acc8-96852597973a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cdbbd3af4d08d0afc91b2d1178e87f8e0462e7eca626b548b50edfbc85e7e973",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 485
      },
      {
        "segments": [
          {
            "segment_id": "3ae87669-6140-4a11-9d0b-2fc31c812837",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 172,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "156 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nDistribution Object of Class: Norm\r\nmean: 4\r\nsd: 3\r\nSo distr “knows” that a linear transformation of a normal random variable is again normal,\r\nand it even knows what the correct mean and sd should be. But it is impossible for distr to\r\nknow everything, and it is not long before we venture outside of the transformations that distr\r\nrecognizes. Let us try Example 6.22:\r\n> Y <- exp(X)\r\n> Y\r\nDistribution Object of Class: AbscontDistribution\r\nThe result is an object of class AbscontDistribution, which is one of the classes that distr\r\nuses to denote general distributions that it does not recognize (it turns out that Z has a lognormal\r\ndistribution; see Section 6.5). A simplified description of the process that distr undergoes when\r\nit encounters a transformation Y = g(X) that it does not recognize is\r\n1. Randomly generate many, many copies X1, X2, . . . , Xn from the distribution of X,\r\n2. Compute Y1 = g(X1), Y2 = g(X2), . . . , Yn = g(Xn) and store them for use.\r\n3. Calculate the PDF, CDF, quantiles, and random variates using the simulated values of Y.\r\nAs long as the transformation is sufficiently nice, such as a linear transformation, the exponential,\r\nabsolute value, etc., the d-p-q functions are calculated analytically based on the d-p-q functions\r\nassociated with X. But if we try a crazy transformation then we are greeted by a warning:\r\n> W <- sin(exp(X) + 27)\r\n> W\r\nDistribution Object of Class: AbscontDistribution\r\nThe warning confirms that the d-p-q functions are not calculated analytically, but are instead\r\nbased on the randomly simulated values of Y. We must be careful to remember this. The nature\r\nof random simulation means that we can get different answers to the same question: watch what\r\nhappens when we compute IP(W ≤ 0.5) using the W above, then define W again, and compute the\r\n(supposedly) same IP(W ≤ 0.5) a few moments later.\r\n> p(W)(0.5)\r\n[1] 0.5793242\r\n> W <- sin(exp(X) + 27)\r\n> p(W)(0.5)\r\n[1] 0.5793242\r\nThe answers are not the same! Furthermore, if we were to repeat the process we would get yet\r\nanother answer for IP(W ≤ 0.5).\r\nThe answers were close, though. And the underlying randomly generated X’s were not the\r\nsame so it should hardly be a surprise that the calculated W’s were not the same, either. This serves\r\nas a warning (in concert with the one that distr provides) that we should be careful to remember\r\nthat complicated transformations computed by R are only approximate and may fluctuate slightly\r\ndue to the nature of the way the estimates are calculated.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3ae87669-6140-4a11-9d0b-2fc31c812837.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a80a48614494b7df22fc837375688bb05391a0cf9babe19ebde972223beb464e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 441
      },
      {
        "segments": [
          {
            "segment_id": "c502e392-8cdb-4a32-8272-277854ce107f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 173,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.5. OTHER CONTINUOUS DISTRIBUTIONS 157\r\n6.5 Other Continuous Distributions\r\n6.5.1 Waiting Time Distributions\r\nIn some experiments, the random variable being measured is the time until a certain event occurs.\r\nFor example, a quality control specialist may be testing a manufactured product to see how long\r\nit takes until it fails. An efficiency expert may be recording the customer traffic at a retail store to\r\nstreamline scheduling of staff.\r\nThe Exponential Distribution\r\nWe say that X has an exponential distribution and write X ∼ exp(rate = λ).\r\nfX(x) = λe\r\n−λx\r\n, x > 0 (6.5.1)\r\nThe associated R functions are dexp(x, rate = 1), pexp, qexp, and rexp, which give the PDF,\r\nCDF, quantile function, and simulate random variates, respectively.\r\nThe parameter λ measures the rate of arrivals (to be described later) and must be positive. The\r\nCDF is given by the formula\r\nFX(t) = 1 − e\r\n−λt\r\n, t > 0. (6.5.2)\r\nThe mean is µ = 1/λ and the variance is σ\r\n2 = 1/λ2\r\n.\r\nThe exponential distribution is closely related to the Poisson distribution. If customers arrive\r\nat a store according to a Poisson process with rate λ and if Y counts the number of customers that\r\narrive in the time interval [0, t), then we saw in Section 5.6 that Y ∼ pois(lambda = λt). Now\r\nconsider a different question: let us start our clock at time 0 and stop the clock when the first\r\ncustomer arrives. Let X be the length of this random time interval. Then X ∼ exp(rate = λ).\r\nObserve the following string of equalities:\r\nIP(X > t) = IP(first arrival after time t),\r\n= IP(no events in [0,t)),\r\n= IP(Y = 0),\r\n= e\r\n−λt\r\n,\r\nwhere the last line is the PMF of Y evaluated at y = 0. In other words, IP(X ≤ t) = 1 − e\r\n−λt\r\n, which\r\nis exactly the CDF of an exp(rate = λ) distribution.\r\nThe exponential distribution is said to be memoryless because exponential random variables\r\n\"forget\" how old they are at every instant. That is, the probability that we must wait an additional\r\nfive hours for a customer to arrive, given that we have already waited seven hours, is exactly the\r\nprobability that we needed to wait five hours for a customer in the first place. In mathematical\r\nsymbols, for any s, t > 0,\r\nIP(X > s + t | X > t) = IP(X > s). (6.5.3)\r\nSee Exercise 6.5.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c502e392-8cdb-4a32-8272-277854ce107f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=51be042bc72c17ceaa6ba9fb77cb3d1054a8e76cede57bca917f24f4aa4235fd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 413
      },
      {
        "segments": [
          {
            "segment_id": "d107ef3a-9adc-4eeb-a426-4de7d9822aa4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 174,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "158 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nThe Gamma Distribution\r\nThis is a generalization of the exponential distribution. We say that X has a gamma distribution and\r\nwrite X ∼ gamma(shape = α, rate = λ). It has PDF\r\nfX(x) =\r\nλ\r\nα\r\nΓ(α)\r\nx\r\nα−1\r\ne\r\n−λx\r\n, x > 0. (6.5.4)\r\nThe associated R functions are dgamma(x, shape, rate = 1), pgamma, qgamma, and rgamma,\r\nwhich give the PDF, CDF, quantile function, and simulate random variates, respectively. If α = 1\r\nthen X ∼ exp(rate = λ). The mean is µ = α/λ and the variance is σ\r\n2 = α/λ2\r\n.\r\nTo motivate the gamma distribution recall that if X measures the length of time until the first\r\nevent occurs in a Poisson process with rate λ then X ∼ exp(rate = λ). If we let Y measure the\r\nlength of time until the α\r\nth event occurs then Y ∼ gamma(shape = α, rate = λ). When α is an\r\ninteger this distribution is also known as the Erlang distribution.\r\nExample 6.30. At a car wash, two customers arrive per hour on the average. We decide to\r\nmeasure how long it takes until the third customer arrives. If Y denotes this random time then\r\nY ∼ gamma(shape = 3, rate = 1/2).\r\n6.5.2 The Chi square, Student’s t, and Snedecor’s F Distributions\r\nThe Chi square Distribution\r\nA random variable X with PDF\r\nfX(x) =\r\n1\r\nΓ(p/2)2p/2\r\nx\r\np/2−1\r\ne\r\n−x/2\r\n, x > 0, (6.5.5)\r\nis said to have a chi-square distribution with p degrees of freedom. We write X ∼ chisq(df = p).\r\nThe associated R functions are dchisq(x, df), pchisq, qchisq, and rchisq, which give the\r\nPDF, CDF, quantile function, and simulate random variates, respectively. See Figure 6.5.1. In an\r\nobvious notation we may define χ\r\n2\r\nα\r\n(p) as the number on the x-axis such that there is exactly α area\r\nunder the chisq(df = p) curve to its right.\r\nThe code to produce Figure 6.5.1 is\r\n> curve(dchisq(x, df = 3), from = 0, to = 20, ylab = \"y\")\r\n> ind <- c(4, 5, 10, 15)\r\n> for (i in ind) curve(dchisq(x, df = i), 0, 20, add = TRUE)\r\nRemark 6.31. Here are some useful things to know about the chi-square distribution.\r\n1. If Z ∼ norm(mean = 0, sd = 1), then Z\r\n2 ∼ chisq(df = 1). We saw this in Example 6.29,\r\nand the fact is important when it comes time to find the distribution of the sample variance,\r\nS\r\n2\r\n. See Theorem 8.5 in Section 8.2.2.\r\n2. The chi-square distribution is supported on the positive x-axis, with a right-skewed distribu\u0002tion.\r\n3. The chisq(df = p) distribution is the same as a gamma(shape = p/2, rate = 1/2) distri\u0002bution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d107ef3a-9adc-4eeb-a426-4de7d9822aa4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7cfd313ad68dc31badd04de04804e2812b4cbfc321c0530da3a520c8d2d9e509",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c648d65b-8ed1-473f-8ad6-2fe85a15a649",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 175,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.5. OTHER CONTINUOUS DISTRIBUTIONS 159\r\n0 5 10 15 20\r\n0.00 0.05 0.10 0.15 0.20 0.25\r\nx\r\ny\r\nFigure 6.5.1: Chi square distribution for various degrees of freedom",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c648d65b-8ed1-473f-8ad6-2fe85a15a649.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fa942252ce6db6bb0f700d42c40b1eb3f95c90dec5973bc80009af48c966295b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 489
      },
      {
        "segments": [
          {
            "segment_id": "8334b1cf-9b45-486d-95d5-33db35788a71",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 176,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "160 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\n4. The MGF of X ∼ chisq(df = p) is\r\nMX(t) = (1 − 2t)\r\n−p\r\n, t < 1/2. (6.5.6)\r\nStudent’s t distribution\r\nA random variable X with PDF\r\nfX(x) =\r\nΓ [(r + 1)/2]\r\n√\r\nrπ Γ(r/2) \r\n1 +\r\nx\r\n2\r\nr\r\n!−(r+1)/2\r\n, −∞ < x < ∞ (6.5.7)\r\nis said to have Student’s t distribution with r degrees of freedom, and we write X ∼ t(df = r). The\r\nassociated R functions are dt, pt, qt, and rt, which give the PDF, CDF, quantile function, and\r\nsimulate random variates, respectively. See Section 8.2.\r\nSnedecor’s F distribution\r\nA random variable X with p.d.f.\r\nfX(x) =\r\nΓ[(m + n)/2]\r\nΓ(m/2)Γ(n/2) \u0012mn\r\n\u0013m/2\r\nx\r\nm/2−1\r\n\u0012\r\n1 +\r\nm\r\nn\r\nx\r\n\u0013−(m+n)/2\r\n, x > 0. (6.5.8)\r\nis said to have an F distribution with (m, n) degrees of freedom. We write X ∼ f(df1 = m, df2 = n).\r\nThe associated R functions are df(x, df1, df2), pf, qf, and rf, which give the PDF, CDF,\r\nquantile function, and simulate random variates, respectively. We define Fα(m, n) as the number on\r\nthe x-axis such that there is exactly α area under the f(df1 = m, df2 = n) curve to its right.\r\nRemark 6.32. Here are some notes about the F distribution.\r\n1. If X ∼ f(df1 = m, df2 = n) and Y = 1/X, then Y ∼ f(df1 = n, df2 = m). Historically,\r\nthis fact was especially convenient. In the old days, statisticians used printed tables for their\r\nstatistical calculations. Since the F tables were symmetric in m and n, it meant that publishers\r\ncould cut the size of their printed tables in half. It plays less of a role today now that personal\r\ncomputers are widespread.\r\n2. If X ∼ t(df = r), then X\r\n2 ∼ f(df1 = 1, df2 = r). We will see this again in Section 11.3.3.\r\n6.5.3 Other Popular Distributions\r\nThe Cauchy Distribution\r\nThis is a special case of the Student’s t distribution. It has PDF\r\nfX(x) =\r\n1\r\nβπ\r\n\r\n\r\n1 +\r\n \r\nx − m\r\nβ\r\n!2\r\n\r\n\r\n−1\r\n, −∞ < x < ∞ (6.5.9)\r\nWe write X ∼ cauchy(location = m, scale = β). The associated R function is dcauchy(x,\r\nlocation = 0, scale = 1).\r\nIt is easy to see that a cauchy(location = 0, scale = 1) distribution is the same as a t(df = 1)\r\ndistribution. The cauchy distribution looks like a norm distribution but with very heavy tails.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8334b1cf-9b45-486d-95d5-33db35788a71.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b4fc48e520c69373805db642939ae690ac601bcadc1e0355a750c18f1cd4de0e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 423
      },
      {
        "segments": [
          {
            "segment_id": "64f18a67-4a36-4178-9706-9828c2ff79ce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 177,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.5. OTHER CONTINUOUS DISTRIBUTIONS 161\r\nThe mean (and variance) do not exist, that is, they are infinite. The median is represented by the\r\nlocation parameter, and the scale parameter influences the spread of the distribution about its\r\nmedian.\r\nThe Beta Distribution\r\nThis is a generalization of the continuous uniform distribution.\r\nfX(x) =\r\nΓ(α + β)\r\nΓ(α)Γ(β)\r\nx\r\nα−1\r\n(1 − x)\r\nβ−1\r\n, 0 < x < 1 (6.5.10)\r\nWe write X ∼ beta(shape1 = α, shape2 = β). The associated R function is dbeta(x, shape1,\r\nshape2). The mean and variance are\r\nµ =\r\nα\r\nα + β\r\nand σ\r\n2 =\r\nαβ\r\n(α + β)\r\n2\r\n(α + β + 1)\r\n. (6.5.11)\r\nSee Example 6.3. This distribution comes up a lot in Bayesian statistics because it is a good model\r\nfor one’s prior beliefs about a population proportion p, 0 ≤ p ≤ 1.\r\nThe Logistic Distribution\r\nfX(x) =\r\n1\r\nσ\r\nexp \u0012−\r\nx − µ\r\nσ\r\n\u0013 \u00141 + exp \u0012\r\n−\r\nx − µ\r\nσ\r\n\u0013\u0015−2\r\n, −∞ < x < ∞. (6.5.12)\r\nWe write X ∼ logis(location = µ, scale = σ). The associated R function is dlogis(x,\r\nlocation = 0, scale = 1). The logistic distribution comes up in differential equations as a\r\nmodel for population growth under certain assumptions. The mean is µ and the variance is π\r\n2σ2\r\n/3.\r\nThe Lognormal Distribution\r\nThis is a distribution derived from the normal distribution (hence the name). If U ∼ norm(mean =\r\nµ, sd = σ), then X = e\r\nUhas PDF\r\nfX(x) =\r\n1\r\nσx\r\n√\r\n2π\r\nexp \"\r\n−(ln x − µ)\r\n2\r\n2σ2\r\n#\r\n, 0 < x < ∞. (6.5.13)\r\nWe write X ∼ lnorm(meanlog = µ, sdlog = σ). The associated R function is dlnorm(x,\r\nmeanlog = 0, sdlog = 1). Notice that the support is concentrated on the positive x axis; the\r\ndistribution is right-skewed with a heavy tail. See Example 6.22.\r\nThe Weibull Distribution\r\nThis has PDF\r\nfX(x) =\r\nα\r\nβ\r\n \r\nx\r\nβ\r\n!α−1\r\nexp \r\nx\r\nβ\r\n!α\r\n, x > 0. (6.5.14)\r\nWe write X ∼ weibull(shape = α, scale = β). The associated R function is dweibull(x,\r\nshape, scale = 1).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/64f18a67-4a36-4178-9706-9828c2ff79ce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=593a1598c147a6b12e153d88da8ee4b17edcefff12723c4ab773ad8cd23fcc12",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 365
      },
      {
        "segments": [
          {
            "segment_id": "eeb550de-4fc0-4e74-9ec4-c40dae365a46",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 178,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "162 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\n6.5.4 How to do it with R\r\nThere is some support of moments and moment generating functions for some continuous prob\u0002ability distributions included in the actuar package [25]. The convention is m in front of the\r\ndistribution name for raw moments, and mgf in front of the distribution name for the moment gen\u0002erating function. At the time of this writing, the following distributions are supported: gamma,\r\ninverse Gaussian, (non-central) chi-squared, exponential, and uniform.\r\nExample 6.33. Calculate the first four raw moments for X ∼ gamma(shape = 13, rate = 1) and\r\nplot the moment generating function.\r\nWe load the actuar package and use the functions mgamma and mgfgamma:\r\n> library(actuar)\r\n> mgamma(1:4, shape = 13, rate = 1)\r\n[1] 13 182 2730 43680\r\nFor the plot we can use the function in the following form:\r\n> plot(function(x) {\r\n+ mgfgamma(x, shape = 13, rate = 1)\r\n+ }, from = -0.1, to = 0.1, ylab = \"gamma mgf\")",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/eeb550de-4fc0-4e74-9ec4-c40dae365a46.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a5659bb0e8db858296ee79ff312e7a2aa87fbe3e5dec0eb5e78629a4b6aa1839",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "9c5d91d8-9bc5-4cdc-b99e-75ecc13680f1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 179,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "6.5. OTHER CONTINUOUS DISTRIBUTIONS 163\r\n−0.10 −0.05 0.00 0.05 0.10\r\n1 2\r\n3\r\n4\r\nx\r\ngamma mgf\r\nFigure 6.5.2: Plot of the gamma(shape = 13, rate = 1) MGF",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9c5d91d8-9bc5-4cdc-b99e-75ecc13680f1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=58b96fedcfdf030dba49550368c6f69c8d45d33fde9549f410aa273956f15240",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 192
      },
      {
        "segments": [
          {
            "segment_id": "8025174a-13dc-40aa-80b0-c7852a62486b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 180,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "164 CHAPTER 6. CONTINUOUS DISTRIBUTIONS\r\nChapter Exercises\r\nExercise 6.1. Find the constant c so that the given function is a valid PDF of a random variable X.\r\n1. f(x) = Cxn, 0 < x < 1.\r\n2. f(x) = Cxe\r\n−x\r\n, 0 < x < ∞.\r\n3. f(x) = e\r\n−(x−C)\r\n, 7 < x < ∞.\r\n4. f(x) = Cx3(1 − x)\r\n2\r\n, 0 < x < 1.\r\n5. f(x) = C(1 + x\r\n2\r\n/4)−1, −∞ < x < ∞.\r\nExercise 6.2. For the following random experiments, decide what the distribution of X should be.\r\nIn nearly every case, there are additional assumptions that should be made for the distribution to\r\napply; identify those assumptions (which may or may not strictly hold in practice).\r\n1. We throw a dart at a dart board. Let X denote the squared linear distance from the bulls-eye\r\nto the where the dart landed.\r\n2. We randomly choose a textbook from the shelf at the bookstore and let P denote the propor\u0002tion of the total pages of the book devoted to exercises.\r\n3. We measure the time it takes for the water to completely drain out of the kitchen sink.\r\n4. We randomly sample strangers at the grocery store and ask them how long it will take them\r\nto drive home.\r\nExercise 6.3. If Z is norm(mean = 0, sd = 1), find\r\n1. IP(Z > 2.64)\r\n> pnorm(2.64, lower.tail = FALSE)\r\n[1] 0.004145301\r\n2. IP(0 ≤ Z < 0.87)\r\n> pnorm(0.87) - 1/2\r\n[1] 0.3078498\r\n3. IP(|Z| > 1.39) (Hint: draw a picture!)\r\n> 2 * pnorm(-1.39)\r\n[1] 0.1645289\r\nExercise 6.4. Calculate the variance of X ∼ unif(min = a, max = b). Hint: First calculate IE X\r\n2\r\n.\r\ntype the exercise here\r\nExercise 6.5. Prove the memoryless property for exponential random variables. That is, for X ∼\r\nexp(rate = λ) show that for any s, t > 0,\r\nIP(X > s + t | X > t) = IP(X > s).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8025174a-13dc-40aa-80b0-c7852a62486b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f0f17f82e66bd3bccdc45022a63e1c04eb65c67f160c9274d39cd77f4a917c63",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 333
      },
      {
        "segments": [
          {
            "segment_id": "9c5751e2-ef69-417a-8fd2-187b1d41174b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 181,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 7\r\nMultivariate Distributions\r\nWe have built up quite a catalogue of distributions, discrete and continuous. They were all uni\u0002variate, however, meaning that we only considered one random variable at a time. We can imagine\r\nnevertheless many random variables associated with a single person: their height, their weight,\r\ntheir wrist circumference (all continuous), or their eye/hair color, shoe size, whether they are right\r\nhanded, left handed, or ambidextrous (all categorical), and we can even surmise reasonable proba\u0002bility distributions to associate with each of these variables.\r\nBut there is a difference: for a single person, these variables are related. For instance, a person’s\r\nheight betrays a lot of information about that person’s weight.\r\nThe concept we are hinting at is the notion of dependence between random variables. It is the\r\nfocus of this chapter to study this concept in some detail. Along the way, we will pick up additional\r\nmodels to add to our catalogue. Moreover, we will study certain classes of dependence, and clarify\r\nthe special case when there is no dependence, namely, independence.\r\nThe interested reader who would like to learn more about any of the below mentioned multi\u0002variate distributions should take a look at Discrete Multivariate Distributions by Johnson et al [49]\r\nor Continuous Multivariate Distributions [54] by Kotz et al.\r\nWhat do I want them to know?\r\n• the basic notion of dependence and how it is manifested with multiple variables (two, in\r\nparticular)\r\n• joint versus marginal distributions/expectation (discrete and continuous)\r\n• some numeric measures of dependence\r\n• conditional distributions, in the context of independence and exchangeability\r\n• some details of at least one multivariate model (discrete and continuous)\r\n• what it looks like when there are more than two random variables present\r\n165",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9c5751e2-ef69-417a-8fd2-187b1d41174b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=54ca97d6a128b0536ab0d3d352bd12f1a77008fecdc8c43e7edd3c4b8aee7d4a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 288
      },
      {
        "segments": [
          {
            "segment_id": "bbde0d64-7cdd-46f3-98fb-df75f6c80cf1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 182,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "166 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\n7.1 Joint and Marginal Probability Distributions\r\nConsider two discrete random variables X and Y with PMFs fX and fY that are supported on the\r\nsample spaces S X and S Y , respectively. Let S X,Y denote the set of all possible observed pairs (x, y),\r\ncalled the joint support set of X and Y. Then the joint probability mass function of X and Y is the\r\nfunction fX,Y defined by\r\nfX,Y (x, y) = IP(X = x, Y = y), for (x, y) ∈ S X,Y . (7.1.1)\r\nEvery joint PMF satisfies\r\nfX,Y (x, y) > 0 for all (x, y) ∈ S X,Y , (7.1.2)\r\nand\r\nX\r\n(x,y)∈S X,Y\r\nfX,Y (x, y) = 1. (7.1.3)\r\nIt is customary to extend the function fX,Y to be defined on all of R\r\n2 by setting fX,Y (x, y) = 0 for\r\n(x, y) < S X,Y .\r\nIn the context of this chapter, the PMFs fX and fY are called the marginal PMFs of X and Y,\r\nrespectively. If we are given only the joint PMF then we may recover each of the marginal PMFs\r\nby using the Theorem of Total Probability (see Equation4.4.5): observe\r\nfX(x) = IP(X = x), (7.1.4)\r\n=\r\nX\r\ny∈S Y\r\nIP(X = x, Y = y), (7.1.5)\r\n=\r\nX\r\ny∈S Y\r\nfX,Y (x, y). (7.1.6)\r\nBy interchanging the roles of X and Y it is clear that\r\nfY (y) =\r\nX\r\nx∈S Y\r\nfX,Y (x, y). (7.1.7)\r\nGiven the joint PMF we may recover the marginal PMFs, but the converse is not true. Even if we\r\nhave both marginal distributions they are not sufficient to determine the joint PMF; more informa\u0002tion is needed1\r\n.\r\nAssociated with the joint PMF is the joint cumulative distribution function FX,Y defined by\r\nFX,Y (x, y) = IP(X ≤ x, Y ≤ y), for (x, y) ∈ R\r\n2\r\n.\r\nThe bivariate joint CDF is not quite as tractable as the univariate CDFs, but in principle we could\r\ncalculate it by adding up quantities of the form in Equation 7.1.1. The joint CDF is typically not\r\nused in practice due to its inconvenient form; one can usually get by with the joint PMF alone.\r\nWe now introduce some examples of bivariate discrete distributions. The first we have seen\r\nbefore, and the second is based on the first.\r\n1We are not at a total loss, however. There are Frechet bounds which pose limits on how large (and small) the joint\r\ndistribution must be at each point.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/bbde0d64-7cdd-46f3-98fb-df75f6c80cf1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f09e52015950af1df03d0546d05d1918e5f944ab4e3e5987dac52f8bacefef1c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 420
      },
      {
        "segments": [
          {
            "segment_id": "dc3e6864-ea55-48e0-82b9-4d16cfad0fb8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 183,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.1. JOINT AND MARGINAL PROBABILITY DISTRIBUTIONS 167\r\nExample 7.1. Roll a fair die twice. Let X be the face shown on the first roll, and let Y be the face\r\nshown on the second roll. We have already seen this example in Chapter 4, Example 4.30. For this\r\nexample, it suffices to define\r\nfX,Y (x, y) =\r\n1\r\n36\r\n, x = 1, . . . , 6, y = 1, . . . , 6.\r\nThe marginal PMFs are given by fX(x) = 1/6, x = 1, 2, . . . , 6, and fY (y) = 1/6, y = 1, 2, . . . , 6,\r\nsince\r\nfX(x) =\r\nX\r\n6\r\ny=1\r\n1\r\n36\r\n=\r\n1\r\n6\r\n, x = 1, . . . , 6,\r\nand the same computation with the letters switched works for Y.\r\nIn the previous example, and in many other ones, the joint support can be written as a product\r\nset of the support of X “times” the support of Y, that is, it may be represented as a cartesian product\r\nset, or rectangle, S X,Y = S X × S Y , where S X × S Y = {(x, y) : x ∈ S X, y ∈ S Y }. As we shall see\r\npresently in Section 7.4, this form is a necessary condition for X and Y to be independent (or\r\nalternatively exchangeable when S X = S Y ). But please note that in general it is not required for\r\nS X,Y to be of rectangle form. We next investigate just such an example.\r\nExample 7.2. Let the random experiment again be to roll a fair die twice, except now let us define\r\nthe random variables U and V by\r\nU = the maximum of the two rolls, and\r\nV = the sum of the two rolls.\r\nWe see that the support of U is S U = {1, 2, . . . , 6} and the support of V is S V = {2, 3, . . . , 12}. We\r\nmay represent the sample space with a matrix, and for each entry in the matrix we may calculate\r\nthe value that U assumes. The result is in the left half of Table 7.1.\r\nWe can use the table to calculate the marginal PMF of U, because from Example 4.30 we know\r\nthat each entry in the matrix has probability 1/36 associated with it. For instance, there is only one\r\noutcome in the matrix with U = 1, namely, the top left corner. This single entry has probability\r\n1/36, therefore, it must be that fU(1) = IP(U = 1) = 1/36. Similarly we see that there are three\r\nentries in the matrix with U = 2, thus fU(2) = 3/36. Continuing in this fashion we will find the\r\nmarginal distribution of U may be written\r\nfU(u) =\r\n2u − 1\r\n36\r\n, u = 1, 2, . . . , 6. (7.1.8)\r\nWe may do a similar thing for V; see the right half of Table 7.1. Collecting all of the probability\r\nwe will find that the marginal PMF of V is\r\nfV(v) =\r\n6 − |v − 7|\r\n36\r\n, v = 2, 3, . . . , 12. (7.1.9)\r\nWe may collapse the two matrices from Table 7.1 into one, big matrix of pairs of values (u, v).\r\nThe result is shown in Table 7.2.\r\nAgain, each of these pairs has probability 1/36 associated with it and we are looking at the joint\r\nPDF of (U, V) albeit in an unusual form. Many of the pairs are repeated, but some of them are not:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/dc3e6864-ea55-48e0-82b9-4d16cfad0fb8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2b7b7d4f7ad3f52d88f18644b038b0adf42d74ce73a16726739c771e67992ea9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 608
      },
      {
        "segments": [
          {
            "segment_id": "dc3e6864-ea55-48e0-82b9-4d16cfad0fb8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 183,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.1. JOINT AND MARGINAL PROBABILITY DISTRIBUTIONS 167\r\nExample 7.1. Roll a fair die twice. Let X be the face shown on the first roll, and let Y be the face\r\nshown on the second roll. We have already seen this example in Chapter 4, Example 4.30. For this\r\nexample, it suffices to define\r\nfX,Y (x, y) =\r\n1\r\n36\r\n, x = 1, . . . , 6, y = 1, . . . , 6.\r\nThe marginal PMFs are given by fX(x) = 1/6, x = 1, 2, . . . , 6, and fY (y) = 1/6, y = 1, 2, . . . , 6,\r\nsince\r\nfX(x) =\r\nX\r\n6\r\ny=1\r\n1\r\n36\r\n=\r\n1\r\n6\r\n, x = 1, . . . , 6,\r\nand the same computation with the letters switched works for Y.\r\nIn the previous example, and in many other ones, the joint support can be written as a product\r\nset of the support of X “times” the support of Y, that is, it may be represented as a cartesian product\r\nset, or rectangle, S X,Y = S X × S Y , where S X × S Y = {(x, y) : x ∈ S X, y ∈ S Y }. As we shall see\r\npresently in Section 7.4, this form is a necessary condition for X and Y to be independent (or\r\nalternatively exchangeable when S X = S Y ). But please note that in general it is not required for\r\nS X,Y to be of rectangle form. We next investigate just such an example.\r\nExample 7.2. Let the random experiment again be to roll a fair die twice, except now let us define\r\nthe random variables U and V by\r\nU = the maximum of the two rolls, and\r\nV = the sum of the two rolls.\r\nWe see that the support of U is S U = {1, 2, . . . , 6} and the support of V is S V = {2, 3, . . . , 12}. We\r\nmay represent the sample space with a matrix, and for each entry in the matrix we may calculate\r\nthe value that U assumes. The result is in the left half of Table 7.1.\r\nWe can use the table to calculate the marginal PMF of U, because from Example 4.30 we know\r\nthat each entry in the matrix has probability 1/36 associated with it. For instance, there is only one\r\noutcome in the matrix with U = 1, namely, the top left corner. This single entry has probability\r\n1/36, therefore, it must be that fU(1) = IP(U = 1) = 1/36. Similarly we see that there are three\r\nentries in the matrix with U = 2, thus fU(2) = 3/36. Continuing in this fashion we will find the\r\nmarginal distribution of U may be written\r\nfU(u) =\r\n2u − 1\r\n36\r\n, u = 1, 2, . . . , 6. (7.1.8)\r\nWe may do a similar thing for V; see the right half of Table 7.1. Collecting all of the probability\r\nwe will find that the marginal PMF of V is\r\nfV(v) =\r\n6 − |v − 7|\r\n36\r\n, v = 2, 3, . . . , 12. (7.1.9)\r\nWe may collapse the two matrices from Table 7.1 into one, big matrix of pairs of values (u, v).\r\nThe result is shown in Table 7.2.\r\nAgain, each of these pairs has probability 1/36 associated with it and we are looking at the joint\r\nPDF of (U, V) albeit in an unusual form. Many of the pairs are repeated, but some of them are not:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/dc3e6864-ea55-48e0-82b9-4d16cfad0fb8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2b7b7d4f7ad3f52d88f18644b038b0adf42d74ce73a16726739c771e67992ea9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 608
      },
      {
        "segments": [
          {
            "segment_id": "f4e6e073-f213-4aac-81b4-75319d2d66ab",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 184,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "168 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nU 1 2 3 4 5 6\r\n1 1 2 3 4 5 6\r\n2 2 2 3 4 5 6\r\n3 3 3 3 4 5 6\r\n4 4 4 4 4 5 6\r\n5 5 5 5 5 5 6\r\n6 6 6 6 6 6 6\r\n(a) U = max(X, Y)\r\nV 1 2 3 4 5 6\r\n1 2 3 4 5 6 7\r\n2 3 4 5 6 7 8\r\n3 4 5 6 7 8 9\r\n4 5 6 7 8 9 10\r\n5 6 7 8 9 10 11\r\n6 7 8 9 10 11 12\r\n(b) V = X + Y\r\nTable 7.1: Maximum U and sum V of a pair of dice rolls (X, Y)\r\n(U, V) 1 2 3 4 5 6\r\n1 (1,2) (2,3) (3,4) (4,5) (5,6) (6,7)\r\n2 (2,3) (2,4) (3,5) (4,6) (5,7) (6,8)\r\n3 (3,4) (3,5) (3,6) (4,7) (5,8) (6,9)\r\n4 (4,5) (4,6) (4,7) (4,8) (5,9) (6,10)\r\n5 (5,6) (5,7) (5,8) (5,9) (5,10) (6,11)\r\n6 (6,7) (6,8) (6,9) (6,10) (6,11) (6,12)\r\nTable 7.2: Joint values of U = max(X, Y) and V = X + Y\r\n2 3 4 5 6 7 8 9 10 11 12 Total\r\n1 1/36 1/36\r\n2 2/36 1/36 3/36\r\n3 2/36 2/36 1/36 5/36\r\n4 2/36 2/36 2/36 1/36 7/36\r\n5 2/36 2/36 2/36 2/36 1/36 9/36\r\n6 2/36 2/36 2/36 2/36 2/36 1/36 11/36\r\nTotal 1/36 2/36 3/36 4/36 5/36 6/36 5/36 4/36 3/36 2/36 1/36 1\r\nTable 7.3: The joint PMF of (U, V)\r\nThe outcomes of U are along the left and the outcomes of V are along the top. Empty entries in the table have\r\nzero probability. The row totals (on the right) and column totals (on the bottom) correspond to the marginal\r\ndistribution of U and V, respectively.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f4e6e073-f213-4aac-81b4-75319d2d66ab.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f38185cd341c99880602d4edee33139e2cb26a28c538fb5332ea1ea18edffaf9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 307
      },
      {
        "segments": [
          {
            "segment_id": "92f1ab0e-9b79-44c4-993b-5c49cff4c99e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 185,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.1. JOINT AND MARGINAL PROBABILITY DISTRIBUTIONS 169\r\n(1, 2) appears twice, but (2, 3) appears only once. We can make more sense out of this by writing a\r\nnew table with U on one side and V along the top. We will accumulate the probability just like we\r\ndid in Example 7.1. See Table 7.3.\r\nThe joint support of (U, V) is concentrated along the main diagonal; note that the nonzero\r\nentries do not form a rectangle. Also notice that if we form row and column totals we are doing\r\nexactly the same thing as Equation 7.1.7, so that the marginal distribution of U is the list of totals\r\nin the right “margin” of the Table 7.3, and the marginal distribution of V is the list of totals in the\r\nbottom “margin”.\r\nContinuing the reasoning for the discrete case, given two continuous random variables X and\r\nY there similarly exists2a function fX,Y (x, y) associated with X and Y called the joint probability\r\ndensity function of X and Y. Every joint PDF satisfies\r\nfX,Y (x, y) ≥ 0 for all (x, y) ∈ S X,Y , (7.1.10)\r\nand\r\n\"\r\nS X,Y\r\nfX,Y (x, y) dx dy = 1. (7.1.11)\r\nIn the continuous case there is not such a simple interpretation for the joint PDF; however, we\r\ndo have one for the joint CDF, namely,\r\nFX,Y (x, y) = IP(X ≤ x, Y ≤ y) =\r\nZ x\r\n−∞\r\nZ y\r\n−∞\r\nfX,Y (u, v) dv du,\r\nfor (x, y) ∈ R\r\n2\r\n. If X and Y have the joint PDF fX,Y , then the marginal density of X may be recovered\r\nby\r\nfX(x) =\r\nZ\r\nS Y\r\nfX,Y (x, y) dy, x ∈ S X (7.1.12)\r\nand the marginal PDF of Y may be found with\r\nfY (y) =\r\nZ\r\nS X\r\nfX,Y (x, y) dx, y ∈ S Y . (7.1.13)\r\nExample 7.3. Let the joint PDF of (X, Y) be given by\r\nfX,Y (x, y) =\r\n6\r\n5\r\n\u0010\r\nx + y\r\n2\r\n\u0011\r\n, 0 < x < 1, 0 < y < 1.\r\nThe marginal PDF of X is\r\nfX(x) =\r\nZ 1\r\n0\r\n6\r\n5\r\n\u0010\r\nx + y\r\n2\r\n\u0011\r\ndy,\r\n=\r\n6\r\n5\r\n \r\nxy +\r\ny\r\n3\r\n3\r\n!\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\ny=0\r\n,\r\n=\r\n6\r\n5\r\n \r\nx +\r\n1\r\n3\r\n!\r\n,\r\n2Strictly speaking, the joint density function does not necessarily exist. But the joint CDF always exists.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/92f1ab0e-9b79-44c4-993b-5c49cff4c99e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=990ccf3732f64aa5d053065a53bd0c9ace0f2569657a05651edc2b052b5fc482",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 405
      },
      {
        "segments": [
          {
            "segment_id": "afefbeca-3f3d-45a1-a873-20cb4f35ab41",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 186,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "170 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nfor 0 < x < 1, and the marginal PDF of Y is\r\nfY (y) =\r\nZ 1\r\n0\r\n6\r\n5\r\n\u0010\r\nx + y\r\n2\r\n\u0011\r\ndx,\r\n=\r\n6\r\n5\r\n \r\nx\r\n2\r\n2\r\n+ xy2\r\n!\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\nx=0\r\n,\r\n=\r\n6\r\n5\r\n \r\n1\r\n2\r\n+ y\r\n2\r\n!\r\n,\r\nfor 0 < y < 1. In this example the joint support set was a rectangle [0, 1] × [0, 1], but it turns out\r\nthat X and Y are not independent. See Section 7.4.\r\n7.1.1 How to do it with R\r\nWe will show how to do Example 7.2 using R; it is much simpler to do it with R than without. First\r\nwe set up the sample space with the rolldie function. Next, we add random variables U and V\r\nwith the addrv function. We take a look at the very top of the data frame (probability space) to\r\nmake sure that everything is operating according to plan.\r\n> S <- rolldie(2, makespace = TRUE)\r\n> S <- addrv(S, FUN = max, invars = c(\"X1\", \"X2\"), name = \"U\")\r\n> S <- addrv(S, FUN = sum, invars = c(\"X1\", \"X2\"), name = \"V\")\r\n> head(S)\r\nX1 X2 U V probs\r\n1 1 1 1 2 0.02777778\r\n2 2 1 2 3 0.02777778\r\n3 3 1 3 4 0.02777778\r\n4 4 1 4 5 0.02777778\r\n5 5 1 5 6 0.02777778\r\n6 6 1 6 7 0.02777778\r\nYes, the U and V columns have been added to the data frame and have been computed correctly.\r\nThis result would be fine as it is, but the data frame has too many rows: there are repeated pairs\r\n(u, v) which show up as repeated rows in the data frame. The goal is to aggregate the rows of S\r\nsuch that the result has exactly one row for each unique pair (u, v) with positive probability. This\r\nsort of thing is exactly the task for which the marginal function was designed. We may take a\r\nlook at the joint distribution of U and V (we only show the first few rows of the data frame, but the\r\ncomplete one has 11 rows).\r\n> UV <- marginal(S, vars = c(\"U\", \"V\"))\r\n> head(UV)\r\nU V probs\r\n1 1 2 0.02777778\r\n2 2 3 0.05555556\r\n3 2 4 0.02777778\r\n4 3 4 0.05555556",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/afefbeca-3f3d-45a1-a873-20cb4f35ab41.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ea0f48fdb5092aa1f64a6c5e674a2b8975c79973d456cfbeca93b771e00a5ce0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 394
      },
      {
        "segments": [
          {
            "segment_id": "76f83e98-dab0-4491-bf57-3f833d9dfb2b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 187,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.1. JOINT AND MARGINAL PROBABILITY DISTRIBUTIONS 171\r\n5 3 5 0.05555556\r\n6 4 5 0.05555556\r\nThe data frame is difficult to understand. It would be better to have a tabular display like Table\r\n7.3. We can do that with the xtabs function.\r\n> xtabs(round(probs, 3) ~ U + V, data = UV)\r\nV\r\nU 2 3 4 5 6 7 8 9 10 11 12\r\n1 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\r\n2 0.000 0.056 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\r\n3 0.000 0.000 0.056 0.056 0.028 0.000 0.000 0.000 0.000 0.000 0.000\r\n4 0.000 0.000 0.000 0.056 0.056 0.056 0.028 0.000 0.000 0.000 0.000\r\n5 0.000 0.000 0.000 0.000 0.056 0.056 0.056 0.056 0.028 0.000 0.000\r\n6 0.000 0.000 0.000 0.000 0.000 0.056 0.056 0.056 0.056 0.056 0.028\r\nCompare these values to the ones shown in Table 7.3. We can repeat the process with marginal\r\nto get the univariate marginal distributions of U and V separately.\r\n> marginal(UV, vars = \"U\")\r\nU probs\r\n1 1 0.02777778\r\n2 2 0.08333333\r\n3 3 0.13888889\r\n4 4 0.19444444\r\n5 5 0.25000000\r\n6 6 0.30555556\r\n> head(marginal(UV, vars = \"V\"))\r\nV probs\r\n1 2 0.02777778\r\n2 3 0.05555556\r\n3 4 0.08333333\r\n4 5 0.11111111\r\n5 6 0.13888889\r\n6 7 0.16666667\r\nAnother way to do the same thing is with the rowSums and colSums of the xtabs object.\r\nCompare\r\n> temp <- xtabs(probs ~ U + V, data = UV)\r\n> rowSums(temp)\r\n1 2 3 4 5 6\r\n0.02777778 0.08333333 0.13888889 0.19444444 0.25000000 0.30555556\r\n> colSums(temp)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/76f83e98-dab0-4491-bf57-3f833d9dfb2b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c573765e3cd909903cef39ff6a1eb4bac1d50a10fde062d80db9f4199472145a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 260
      },
      {
        "segments": [
          {
            "segment_id": "6d5d37af-34bc-4e88-a283-dd43135363c8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 188,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "172 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\n2 3 4 5 6 7\r\n0.02777778 0.05555556 0.08333333 0.11111111 0.13888889 0.16666667\r\n8 9 10 11 12\r\n0.13888889 0.11111111 0.08333333 0.05555556 0.02777778\r\nYou should check that the answers that we have obtained exactly match the same (somewhat\r\nlaborious) calculations that we completed in Example 7.2.\r\n7.2 Joint and Marginal Expectation\r\nGiven a function g with arguments (x, y) we would like to know the long-run average behavior\r\nof g(X, Y) and how to mathematically calculate it. Expectation in this context is computed in the\r\npedestrian way. We simply integrate (sum) with respect to the joint probability density (mass)\r\nfunction.\r\nIE g(X, Y) =\r\n\"\r\nS X,Y\r\ng(x, y) fX,Y (x, y) dx dy, (7.2.1)\r\nor in the discrete case\r\nIE g(X, Y) =\r\nXX\r\n(x,y)∈S X,Y\r\ng(x, y) fX,Y (x, y). (7.2.2)\r\n7.2.1 Covariance and Correlation\r\nThere are two very special cases of joint expectation: the covariance and the correlation. These\r\nare measures which help us quantify the dependence between X and Y.\r\nDefinition 7.4. The covariance of X and Y is\r\nCov(X, Y) = IE(X − IE X)(Y − IE Y). (7.2.3)\r\nBy the way, there is a shortcut formula for covariance which is almost as handy as the shortcut\r\nfor the variance:\r\nCov(X, Y) = IE(XY) − (IE X)(IE Y). (7.2.4)\r\nThe proof is left to Exercise 7.1.\r\nThe Pearson product moment correlation between X and Y is the covariance between X and Y\r\nrescaled to fall in the interval [−1, 1]. It is formally defined by\r\nCorr(X, Y) =\r\nCov(X, Y)\r\nσXσY\r\n(7.2.5)\r\nThe correlation is usually denoted by ρX,Y or simply ρ if the random variables are clear from context.\r\nThere are some important facts about the correlation coefficient:\r\n1. The range of correlation is −1 ≤ ρX,Y ≤ 1.\r\n2. Equality holds above (ρX,Y = ±1) if and only if Y is a linear function of X with probability\r\none.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6d5d37af-34bc-4e88-a283-dd43135363c8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5e840b0d66a9d037837058f4d04bd91af448aa3ca9dc07bf0a690e592f4776c3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 320
      },
      {
        "segments": [
          {
            "segment_id": "8b4c68cd-3285-468c-a768-c1fc8896b6ae",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 189,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.2. JOINT AND MARGINAL EXPECTATION 173\r\nExample 7.5. We will compute the covariance for the discrete distribution in Example 7.2. The\r\nexpected value of U is\r\nIE U =\r\nX\r\n6\r\nu=1\r\nu fU(u) =\r\nX\r\n6\r\nu=1\r\nu\r\n2u − 1\r\n36\r\n= 1\r\n \r\n1\r\n36!\r\n+ 2\r\n \r\n3\r\n36!\r\n+ · · · + 6\r\n \r\n11\r\n36!\r\n=\r\n161\r\n36\r\n,\r\nand the expected value of V is\r\nIE V =\r\nX\r\n12\r\nv=2\r\nv\r\n6 − |7 − v|\r\n36\r\n= 2\r\n \r\n1\r\n36!\r\n+ 3\r\n \r\n2\r\n36!\r\n+ · · · + 12 \r\n1\r\n36!\r\n= 7,\r\nand the expected value of UV is\r\nIE UV =\r\nX\r\n6\r\nu=1\r\nX\r\n12\r\nv=2\r\nuv fU,V(u, v) = 1 · 2\r\n \r\n1\r\n36!\r\n+ 2 · 3\r\n \r\n2\r\n36!\r\n+ · · · + 6 · 12 \r\n1\r\n36!\r\n=\r\n308\r\n9\r\n.\r\nTherefore the covariance of (U, V) is\r\nCov(U, V) = IE UV − (IE U) (IE V) =\r\n308\r\n9\r\n−\r\n161\r\n36\r\n· 7 =\r\n35\r\n12\r\n.\r\nAll we need now are the standard deviations of U and V to calculate the correlation coefficient\r\n(omitted).\r\nWe will do a continuous example so that you can see how it works.\r\nExample 7.6. Let us find the covariance of the variables (X, Y) from Example 7.3. The expected\r\nvalue of X is\r\nIE X =\r\nZ 1\r\n0\r\nx ·\r\n6\r\n5\r\n \r\nx +\r\n1\r\n3\r\n!\r\ndx =\r\n2\r\n5\r\nx\r\n3 +\r\n1\r\n5\r\nx\r\n2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\nx=0\r\n=\r\n3\r\n5\r\n,\r\nand the expected value of Y is\r\nIE Y =\r\nZ 1\r\n0\r\ny ·\r\n6\r\n5\r\n \r\n1\r\n2\r\n+ y\r\n2\r\n!\r\ndx =\r\n3\r\n10\r\ny\r\n2 +\r\n3\r\n20\r\ny\r\n4\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\ny=0\r\n=\r\n9\r\n20\r\n.\r\nFinally, the expected value of XY is\r\nIE XY =\r\nZ 1\r\n0\r\nZ 1\r\n0\r\nxy\r\n6\r\n5\r\n\u0010\r\nx + y\r\n2\r\n\u0011\r\ndx dy,\r\n=\r\nZ 1\r\n0\r\n \r\n2\r\n5\r\nx\r\n3\r\ny +\r\n3\r\n10\r\nxy4\r\n!\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\nx=0\r\ndy,\r\n=\r\nZ 1\r\n0\r\n \r\n2\r\n5\r\ny +\r\n3\r\n10\r\ny\r\n4\r\n!\r\ndy,\r\n=\r\n1\r\n5\r\n+\r\n3\r\n50,\r\nwhich is 13/50. Therefore the covariance of (X, Y) is\r\nCov(X, Y) =\r\n13\r\n50 −\r\n \r\n3\r\n5\r\n! 9\r\n20!\r\n= −\r\n1\r\n100\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8b4c68cd-3285-468c-a768-c1fc8896b6ae.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2d3198a2e8e86221ff443711c1813f3b860024c22be1b555e9736fe03bad8d98",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 390
      },
      {
        "segments": [
          {
            "segment_id": "9a60f0ee-17d8-45fe-897a-e7b2082234f7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 190,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "174 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\n7.2.2 How to do it with R\r\nThere are not any specific functions in the prob package designed for multivariate expectation.\r\nThis is not a problem, though, because it is easy enough to do expectation the long way – with\r\ncolumn operations. We just need to keep the definition in mind. For instance, we may compute the\r\ncovariance of (U, V) from Example 7.5.\r\n> Eu <- sum(S$U * S$probs)\r\n> Ev <- sum(S$V * S$probs)\r\n> Euv <- sum(S$U * S$V * S$probs)\r\n> Euv - Eu * Ev\r\n[1] 2.916667\r\nCompare this answer to what we got in Example 7.5.\r\nTo do the continuous case we could use the computer algebra utilities of Yacas and the asso\u0002ciated R package Ryacas [35]. See Section 7.7.1 for another example where the Ryacas package\r\nappears.\r\n7.3 Conditional Distributions\r\nIf x ∈ S X is such that fX(x) > 0, then we define the conditional density of Y| X = x, denoted fY|x,\r\nby\r\nfY|x(y|x) =\r\nfX,Y (x, y)\r\nfX(x)\r\n, y ∈ S Y . (7.3.1)\r\nWe define fX|y in a similar fashion.\r\nExample 7.7. Let the joint PMF of X and Y be given by\r\nfX,Y (x, y) =\r\nExample 7.8. Let the joint PDF of X and Y be given by\r\nBayesian Connection\r\nConditional distributions play a fundamental role in Bayesian probability and statistics. There is\r\na parameter θ which is of primary interest, and about which we would like to learn. But rather\r\nthan observing θ directly, we instead observe a random variable X whose probability distribution\r\ndepends on θ. Using the information we provided by X, we would like to update the information\r\nthat we have about θ.\r\nOur initial beliefs about θ are represented by a probability distribution, called the prior distri\u0002bution, denoted by π. The PDF fX|θ is called the likelihood function, also called the likelihood of\r\nX conditional on θ. Given an observation X = x, we would like to update our beliefs π to a new\r\ndistribution, called the posterior distribution of θ given the observation X = x, denoted πθ|x. It\r\nmay seem a mystery how to obtain πθ|x based only on the information provided by π and fX|θ, but it\r\nshould not be. We have already studied this in Section 4.8 where it was called Bayes’ Rule:\r\nπ(θ|x) =\r\nπ(θ) f(x|θ)\r\nR\r\nπ(u) f(x|u)du\r\n. (7.3.2)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9a60f0ee-17d8-45fe-897a-e7b2082234f7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1fa46363c4e038019035c154fad533c63e0342a70a5d1b8970223a23d1bd5aca",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 400
      },
      {
        "segments": [
          {
            "segment_id": "ac1f41b5-bcff-4fd5-8eed-db83fbd6ce2d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 191,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.3. CONDITIONAL DISTRIBUTIONS 175\r\nCompare the above expression to Equation 4.8.1.\r\nExample 7.9. Suppose the parameter θ is the IP(Heads) for a biased coin. It could be any value\r\nfrom 0 to 1. Perhaps we have some prior information about this coin, for example, maybe we\r\nhave seen this coin before and we have reason to believe that it shows Heads less than half of the\r\ntime. Suppose that we represent our beliefs about θ with a beta(shape1 = 1, shape2 = 3) prior\r\ndistribution, that is, we assume\r\nθ ∼ π(θ) = 3(1 − θ)\r\n2\r\n, 0 < θ < 1.\r\nTo learn more about θ, we will do what is natural: flip the coin. We will observe a random variable X\r\nwhich takes the value 1 if the coin shows Heads, and 0 if the coin shows Tails. Under these circum\u0002stances, X will have a Bernoulli distribution, and in particular, X|θ ∼ binom(size = 1, prob = θ):\r\nfX|θ(x|θ) = θ\r\nx\r\n(1 − θ)\r\n1−x\r\n, x = 0, 1.\r\nBased on the observation X = x, we will update the prior distribution to the posterior distribution,\r\nand we will do so with Bayes’ Rule: it says\r\nπ(θ|x) ∝ π(θ) f(x|θ),\r\n= θ\r\nx\r\n(1 − θ)\r\n1−x\r\n· 3(1 − θ)\r\n2\r\n,\r\n= 3 θ\r\nx\r\n(1 − θ)\r\n3−x\r\n, 0 < θ < 1,\r\nwhere the constant of proportionality is given by\r\nZ\r\n3 u\r\nx\r\n(1 − u)\r\n3−x\r\ndu =\r\nZ\r\n3 u\r\n(1+x)−1\r\n(1 − u)\r\n(4−x)−1\r\ndu = 3\r\nΓ(1 + x)Γ(4 − x)\r\nΓ[(1 + x) + (4 − x)]\r\n,\r\nthe integral being calculated by inspection of the formula for a beta(shape1 = 1 + x, shape2 =\r\n4 − x) distribution. That is to say, our posterior distribution is precisely\r\nθ|x ∼ beta(shape1 = 1 + x, shape2 = 4 − x).\r\nThe Bayesian statistician uses the posterior distribution for all matters concerning inference\r\nabout θ.\r\nRemark 7.10. We usually do not restrict ourselves to the observation of only one X conditional\r\non θ. In fact, it is common to observe an entire sample X1, X2,. . . ,Xn conditional on θ (which\r\nitself is often multidimensional). Do not be frightened, however, because the intuition is the\r\nsame. There is a prior distribution π(θ), a likelihood f(x1, x2, . . . , xn|θ), and a posterior distribu\u0002tion π(θ|x1, x2, . . . , xn). Bayes’ Rule states that the relationship between the three is\r\nπ(θ|x1, x2, . . . , xn) ∝ π(θ) f(x1, x2, . . . , xn|θ),\r\nwhere the constant of proportionality isRπ(u) f(x1, x2, . . . , xn|u) du. Any good textbook on Bayesian\r\nStatistics will explain these notions in detail; to the interested reader I recommend Gelman [33] or\r\nLee [57].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ac1f41b5-bcff-4fd5-8eed-db83fbd6ce2d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9a2678531a41eaf68c82e11d17eec6d1c399b52bfcaed6fd03200847946b7877",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 472
      },
      {
        "segments": [
          {
            "segment_id": "958b1a9b-7fb8-44e7-a85f-96a18e35104c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 192,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "176 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\n7.4 Independent Random Variables\r\n7.4.1 Independent Random Variables\r\nWe recall from Chapter 4 that the events A and B are said to be independent when\r\nIP(A ∩ B) = IP(A) IP(B). (7.4.1)\r\nIf it happens that\r\nIP(X = x, Y = y) = IP(X = x) IP(Y = y), for every x ∈ S X, y ∈ S Y , (7.4.2)\r\nthen we say that X and Y are independent random variables. Otherwise, we say that X and Y are\r\ndependent. Using the PMF notation from above, we see that independent discrete random variables\r\nsatisfy\r\nfX,Y (x, y) = fX(x)fY (y) for every x ∈ S X, y ∈ S Y . (7.4.3)\r\nContinuing the reasoning, given two continuous random variables X and Y with joint PDF fX,Y and\r\nrespective marginal PDFs fX and fY that are supported on the sets S X and S Y , if it happens that\r\nfX,Y (x, y) = fX(x)fY (y) for every x ∈ S X, y ∈ S Y , (7.4.4)\r\nthen we say that X and Y are independent.\r\nExample 7.11. In Example 7.1 we considered the random experiment of rolling a fair die twice.\r\nThere we found the joint PMF to be\r\nfX,Y (x, y) =\r\n1\r\n36\r\n, x = 1, . . . , 6, y = 1, . . . , 6,\r\nand we found the marginal PMFs fX(x) = 1/6, x = 1, 2, . . . , 6, and fY (y) = 1/6, y = 1, 2, . . . , 6.\r\nTherefore in this experiment X and Y are independent since for every x and y in the joint support\r\nthe joint PMF satisfies\r\nfX,Y (x, y) =\r\n1\r\n36\r\n=\r\n \r\n1\r\n6\r\n! 1\r\n6\r\n!\r\n= fX(x) fY (y).\r\nExample 7.12. In Example 7.2 we considered the same experiment but different random variables\r\nU and V. We can prove that U and V are not independent if we can find a single pair (u, v) where\r\nthe independence equality does not hold. There are many such pairs. One of them is (6, 12):\r\nfU,V(6, 12) =\r\n1\r\n36\r\n,\r\n \r\n11\r\n36! 136!\r\n= fU(6) fV(12).\r\nIndependent random variables are very useful to the mathematician. They have many, many,\r\ntractable properties. We mention some of the more important ones.\r\nProposition 7.13. If X and Y are independent, then for any functions u and v,\r\nIE (u(X)v(Y)) = (IE u(X)) (IE v(Y)) . (7.4.5)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/958b1a9b-7fb8-44e7-a85f-96a18e35104c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b2875d89679931ce55e1bec20a9993b1643bd8d6d68bedaf07bb3c0848b2c948",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 415
      },
      {
        "segments": [
          {
            "segment_id": "a922bb18-47a6-4c93-92b6-f0ca040258e4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 193,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.4. INDEPENDENT RANDOM VARIABLES 177\r\nProof. This is straightforward from the definition.\r\nIE (u(X)v(Y)) =\r\n\"\r\nu(x)v(y) fX,Y (x, y) dxdy\r\n=\r\n\"\r\nu(x)v(y) fX(x) fY (y) dxdy\r\n=\r\nZ\r\nu(x) fX(x) dx\r\nZ\r\nv(y) fY (y) dy\r\nand this last quantity is exactly (IE u(X)) (IE v(Y)). \u0003\r\nNow that we have Proposition 7.13 we mention a corollary that will help us later to quickly\r\nidentify those random variables which are not independent.\r\nCorollary 7.14. If X and Y are independent, then Cov(X, Y) = 0, and consequently, Corr(X, Y) = 0.\r\nProof. When X and Y are independent then IE XY = IE X IE Y. And when the covariance is zero\r\nthe numerator of the correlation is 0. \u0003\r\nRemark 7.15. Unfortunately, the converse of Corollary 7.14 is not true. That is, there are many\r\nrandom variables which are dependent yet their covariance and correlation is zero. For more details,\r\nsee Casella and Berger [13].\r\nProposition 7.13 is useful to us and we will receive mileage out of it, but there is another fact\r\nwhich will play an even more important role. Unfortunately, the proof is beyond the techniques\r\npresented here. The inquisitive reader should consult Casella and Berger [13], Resnick [70], etc.\r\nFact 7.16. If X and Y are independent, then u(X) and v(Y) are independent for any functions u and\r\nv.\r\n7.4.2 Combining Independent Random Variables\r\nAnother important corollary of Proposition 7.13 will allow us to find the distribution of sums of\r\nrandom variables.\r\nCorollary 7.17. If X and Y are independent, then the moment generating function of X + Y is\r\nMX+Y (t) = MX(t) · MY (t). (7.4.6)\r\nProof. Choose u(x) = e\r\nx\r\nand v(y) = e\r\ny\r\nin Proposition 7.13, and remember the identity et(x+y) =\r\ne\r\ntx ety\r\n. \u0003\r\nLet us take a look at some examples of the corollary in action.\r\nExample 7.18. Let X ∼ binom(size = n1, prob = p) and Y ∼ binom(size = n2, prob = p) be\r\nindependent. Then X + Y has MGF\r\nMX+Y (t) = MX(t) MY (t) =\r\n\u0010\r\nq + pe\r\nt\r\n\u0011n1\u0010\r\nq + pe\r\nt\r\n\u0011n2\r\n=\r\n\u0010\r\nq + pe\r\nt\r\n\u0011n1+n2\r\n,\r\nwhich is the MGF of a binom(size = n1 + n2, prob = p) distribution. Therefore, X + Y ∼\r\nbinom(size = n1 + n2, prob = p).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a922bb18-47a6-4c93-92b6-f0ca040258e4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=be30ca0c85c3a7ef96b101a604fda39d3c90c83a28d3cf391c31f7473dcda552",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 395
      },
      {
        "segments": [
          {
            "segment_id": "c00e92ac-4b68-4311-a904-c52e61c4b36d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 194,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "178 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nExample 7.19. Let X ∼ norm(mean = µ1, sd = σ1) and Y ∼ norm(mean = µ2, sd = σ2) be\r\nindependent. Then X + Y has MGF\r\nMX(t) MY (t) = exp nµ1t + t\r\n2σ2\r\n1\r\n/2\r\no\r\nexp nµ2t + t\r\n2σ2\r\n2\r\n/2\r\no\r\n= exp n\r\n(µ1 + µ2) t + t\r\n2\r\n\u0010\r\nσ\r\n2\r\n1 + σ\r\n2\r\n2\r\n\u0011\r\n/2\r\no\r\n,\r\nwhich is the MGF of a norm(mean = µ1 + µ2, sd =\r\nq\r\nσ\r\n2\r\n1\r\n+ σ\r\n2\r\n2\r\n) distribution.\r\nEven when we cannot use the MGF trick to identify the exact distribution of a linear combina\u0002tion of random variables, we can still say something about its mean and variance.\r\nProposition 7.20. Let X1 and X2 be independent with respective population means µ1 and µ2 and\r\npopulation standard deviations σ1 and σ2. For given constants a1 and a2, define Y = a1X1 + a2X2.\r\nThen the mean and standard deviation of Y are given by the formulas\r\nµY = a1µ1 + a2µ2, σY =\r\n\u0010\r\na\r\n2\r\n1σ\r\n2\r\n1 + a\r\n2\r\n2σ\r\n2\r\n2\r\n\u00111/2\r\n. (7.4.7)\r\nProof. We use Proposition 5.11:\r\nIE Y = IE (a1X1 + a2X2) = a1 IE X1 + a2 IE X2 = a1µ1 + a2µ2.\r\nFor the standard deviation, we will find the variance and take the square root at the end. And\r\nto calculate the variance we will first compute IE Y\r\n2 with an eye toward using the identity σ2\r\nY\r\n=\r\nIE Y\r\n2 − (IE Y)\r\n2\r\nas a final step.\r\nIE Y\r\n2 = IE (a1X1 + a2X2)2 = IE \u0010\r\na\r\n2\r\n1X\r\n2\r\n1 + a\r\n2\r\n2X\r\n2\r\n2 + 2a1a2X1X2\r\n\u0011\r\n.\r\nUsing linearity of expectation the IE distributes through the sum. Now IE X\r\n2\r\ni\r\n= σ\r\n2\r\ni\r\n+ µ\r\n2\r\ni\r\n, for i = 1\r\nand 2 and IE X1X2 = IE X1 IE X2 = µ1µ2 because of independence. Thus\r\nIE Y\r\n2 = a2\r\n1\r\n(σ\r\n2\r\n1 + µ\r\n2\r\n1\r\n) + a\r\n2\r\n2\r\n(σ\r\n2\r\n2 + µ\r\n2\r\n2\r\n) + 2a1a2µ1µ2,\r\n= a\r\n2\r\n1σ\r\n2\r\n1 + a\r\n2\r\n2σ\r\n2\r\n2 +\r\n\u0010\r\na\r\n2\r\n1\r\nµ\r\n2\r\n1 + a\r\n2\r\n2\r\nµ\r\n2\r\n2 + 2a1a2µ1µ2\r\n\u0011\r\n.\r\nBut notice that the expression in the parentheses is exactly (a1µ1 + a2µ2)\r\n2 = (IE Y)2\r\n, so the proof is\r\ncomplete.. \u0003\r\n7.5 Exchangeable Random Variables\r\nTwo random variables X and Y are said to be exchangeable if their joint CDF is a symmetric\r\nfunction of its arguments:\r\nFX,Y (x, y) = FX,Y (y, x), for all (x, y) ∈ R\r\n2\r\n. (7.5.1)\r\nWhen the joint density f exists, we may equivalently say that X and Y are exchangeable if f(x, y) =\r\nf(y, x) for all (x, y).\r\nExchangeable random variables exhibit symmetry in the sense that a person may exchange one\r\nvariable for the other with no substantive changes to their joint random behavior. While indepen\u0002dence speaks to a lack of influence between the two variables, exchangeability aims to capture the\r\nsymmetry between them.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c00e92ac-4b68-4311-a904-c52e61c4b36d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=909f561f7124a3eb6aba7534ee3a7e0928fbec39a87d608276bd0964feccf72e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "c00e92ac-4b68-4311-a904-c52e61c4b36d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 194,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "178 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nExample 7.19. Let X ∼ norm(mean = µ1, sd = σ1) and Y ∼ norm(mean = µ2, sd = σ2) be\r\nindependent. Then X + Y has MGF\r\nMX(t) MY (t) = exp nµ1t + t\r\n2σ2\r\n1\r\n/2\r\no\r\nexp nµ2t + t\r\n2σ2\r\n2\r\n/2\r\no\r\n= exp n\r\n(µ1 + µ2) t + t\r\n2\r\n\u0010\r\nσ\r\n2\r\n1 + σ\r\n2\r\n2\r\n\u0011\r\n/2\r\no\r\n,\r\nwhich is the MGF of a norm(mean = µ1 + µ2, sd =\r\nq\r\nσ\r\n2\r\n1\r\n+ σ\r\n2\r\n2\r\n) distribution.\r\nEven when we cannot use the MGF trick to identify the exact distribution of a linear combina\u0002tion of random variables, we can still say something about its mean and variance.\r\nProposition 7.20. Let X1 and X2 be independent with respective population means µ1 and µ2 and\r\npopulation standard deviations σ1 and σ2. For given constants a1 and a2, define Y = a1X1 + a2X2.\r\nThen the mean and standard deviation of Y are given by the formulas\r\nµY = a1µ1 + a2µ2, σY =\r\n\u0010\r\na\r\n2\r\n1σ\r\n2\r\n1 + a\r\n2\r\n2σ\r\n2\r\n2\r\n\u00111/2\r\n. (7.4.7)\r\nProof. We use Proposition 5.11:\r\nIE Y = IE (a1X1 + a2X2) = a1 IE X1 + a2 IE X2 = a1µ1 + a2µ2.\r\nFor the standard deviation, we will find the variance and take the square root at the end. And\r\nto calculate the variance we will first compute IE Y\r\n2 with an eye toward using the identity σ2\r\nY\r\n=\r\nIE Y\r\n2 − (IE Y)\r\n2\r\nas a final step.\r\nIE Y\r\n2 = IE (a1X1 + a2X2)2 = IE \u0010\r\na\r\n2\r\n1X\r\n2\r\n1 + a\r\n2\r\n2X\r\n2\r\n2 + 2a1a2X1X2\r\n\u0011\r\n.\r\nUsing linearity of expectation the IE distributes through the sum. Now IE X\r\n2\r\ni\r\n= σ\r\n2\r\ni\r\n+ µ\r\n2\r\ni\r\n, for i = 1\r\nand 2 and IE X1X2 = IE X1 IE X2 = µ1µ2 because of independence. Thus\r\nIE Y\r\n2 = a2\r\n1\r\n(σ\r\n2\r\n1 + µ\r\n2\r\n1\r\n) + a\r\n2\r\n2\r\n(σ\r\n2\r\n2 + µ\r\n2\r\n2\r\n) + 2a1a2µ1µ2,\r\n= a\r\n2\r\n1σ\r\n2\r\n1 + a\r\n2\r\n2σ\r\n2\r\n2 +\r\n\u0010\r\na\r\n2\r\n1\r\nµ\r\n2\r\n1 + a\r\n2\r\n2\r\nµ\r\n2\r\n2 + 2a1a2µ1µ2\r\n\u0011\r\n.\r\nBut notice that the expression in the parentheses is exactly (a1µ1 + a2µ2)\r\n2 = (IE Y)2\r\n, so the proof is\r\ncomplete.. \u0003\r\n7.5 Exchangeable Random Variables\r\nTwo random variables X and Y are said to be exchangeable if their joint CDF is a symmetric\r\nfunction of its arguments:\r\nFX,Y (x, y) = FX,Y (y, x), for all (x, y) ∈ R\r\n2\r\n. (7.5.1)\r\nWhen the joint density f exists, we may equivalently say that X and Y are exchangeable if f(x, y) =\r\nf(y, x) for all (x, y).\r\nExchangeable random variables exhibit symmetry in the sense that a person may exchange one\r\nvariable for the other with no substantive changes to their joint random behavior. While indepen\u0002dence speaks to a lack of influence between the two variables, exchangeability aims to capture the\r\nsymmetry between them.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c00e92ac-4b68-4311-a904-c52e61c4b36d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=909f561f7124a3eb6aba7534ee3a7e0928fbec39a87d608276bd0964feccf72e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "7932d4cf-0121-4ac2-b42e-f138ff6bf46d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 195,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.6. THE BIVARIATE NORMAL DISTRIBUTION 179\r\nExample 7.21. Let X and Y have joint PDF\r\nfX,Y (x, y) = (1 + α)λ\r\n2\r\ne\r\n−λ(x+y) + α(2λ)2\r\ne\r\n−2λ(x+y) − 2αλ2\r\n\u0010\r\ne\r\n−λ(2x+y) + e−λ(x+2y)\r\n\u0011\r\n. (7.5.2)\r\nIt is straightforward and tedious to check that !f = 1. We may see immediately that fX,Y (x, y) =\r\nfX,Y (y, x) for all (x, y), which confirms that X and Y are exchangeable. Here, α is said to be an\r\nassociation parameter. This particular example is one from the Farlie-Gumbel-Morgenstern family\r\nof distributions; see [54].\r\nExample 7.22. Suppose X and Y are i.i.d. binom(size = n, prob = p). Then their joint PMF is\r\nfX,Y (x, y) = fX(x)fY (y)\r\n=\r\n \r\nn\r\nx\r\n!\r\np\r\nx\r\n(1 − p)\r\nn−x\r\n \r\nn\r\ny\r\n!\r\np\r\ny\r\n(1 − p)\r\nn−y\r\n,\r\n=\r\n \r\nn\r\nx\r\n! n\r\ny\r\n!\r\np\r\nx+y\r\n(1 − p)\r\n2n−(x+y)\r\n,\r\nand the value is the same if we exchange x and y. Therefore (X, Y) are exchangeable.\r\nLooking at Example 7.22 more closely we see that the fact that (X, Y) are exchangeable has\r\nnothing to do with the binom(size = n, prob = p) distribution; it only matters that they are\r\nindependent (so that the joint PDF factors) and they are identically distributed (in which case we\r\nmay swap letters to no effect). We could just have easily used any other marginal distribution. We\r\nwill take this as a proof of the following proposition.\r\nProposition 7.23. If X and Y are i.i.d. (with common marginal distribution F) then X and Y are\r\nexchangeable.\r\nExchangeability thus contains i.i.d. as a special case.\r\n7.6 The Bivariate Normal Distribution\r\nThe bivariate normal PDF is given by the unwieldy formula\r\nfX,Y (x, y) =\r\n1\r\n2π σXσY\r\np\r\n1 − ρ\r\n2\r\nexp\r\n\r\n\r\n\r\n−\r\n1\r\n2(1 − ρ\r\n2\r\n)\r\n\r\n\r\n \r\nx − µX\r\nσX\r\n!2\r\n+ · · ·\r\n· · · + 2ρ\r\n \r\nx − µX\r\nσX\r\n! y − µY\r\nσY\r\n!\r\n+\r\n \r\ny − µY\r\nσY\r\n!2\r\n\r\n\r\n\r\n\r\n\r\n, (7.6.1)\r\nfor (x, y) ∈ R\r\n2\r\n. We write (X, Y) ∼ mvnorm(mean = µ, sigma = Σ), where\r\nµ = (µX, µY )\r\nT\r\n,\r\nX\r\n=\r\n \r\nσ\r\n2\r\nX\r\nρσXσY\r\nρσXσY σ\r\n2\r\nY\r\n!\r\n. (7.6.2)\r\nSee Appendix E. The vector notation allows for a more compact rendering of the joint PDF:\r\nfX,Y (x) =\r\n1\r\n2π |Σ|\r\n1/2\r\nexp (−\r\n1\r\n2\r\n(x − µ)\r\n> Σ−1\r\n(x − µ)\r\n)\r\n, (7.6.3)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7932d4cf-0121-4ac2-b42e-f138ff6bf46d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=67f291f3e9ddbac7fa79f03d9c8c3d8dce1485279c7139ca0ca069add796b5dd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 431
      },
      {
        "segments": [
          {
            "segment_id": "ac6b0119-b377-4ee9-8ac6-55783e43b621",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 196,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "180 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nwhere in an abuse of notation we have written x for (x, y). Note that the formula only holds when\r\nρ , ±1.\r\nRemark 7.24. In Remark 7.15 we noted that just because random variables are uncorrelated it does\r\nnot necessarily mean that they are independent. However, there is an important exception to this\r\nrule: the bivariate normal distribution. Indeed, (X, Y) ∼ mvnorm(mean = µ, sigma = Σ) are\r\nindependent if and only if ρ = 0.\r\nRemark 7.25. Inspection of the joint PDF shows that if µX = µY and σX = σY then X and Y are\r\nexchangeable.\r\nThe bivariate normal MGF is\r\nMX,Y (t) = exp µ\r\n>\r\nt +\r\n1\r\n2\r\nt\r\n>Σt\r\n!\r\n, (7.6.4)\r\nwhere t = (t1, t2).\r\nThe bivariate normal distribution may be intimidating at first but it turns out to be very tractable\r\ncompared to other multivariate distributions. An example of this is the following fact about the\r\nmarginals.\r\nFact 7.26. If (X, Y) ∼ mvnorm(mean = µ, sigma = Σ) then\r\nX ∼ norm(mean = µX, sd = σX) and Y ∼ norm(mean = µY , sd = σY ). (7.6.5)\r\nFrom this we immediately get that IE X = µX and Var(X) = σ\r\n2\r\nX\r\n(and the same is true for Y with\r\nthe letters switched). And it should be no surprise that the correlation between X and Y is exactly\r\nCorr(X, Y) = ρ.\r\nProposition 7.27. The conditional distribution of Y| X = x is norm(mean = µY|x, sd = σY|x), where\r\nµY|x = µY + ρ\r\nσY\r\nσX\r\n(x − µX) , and σY|x = σY\r\nq\r\n1 − ρ\r\n2\r\n. (7.6.6)\r\nThere are a few things to note about Proposition 7.27 which will be important in Chapter 11.\r\nFirst, the conditional mean of Y|x is linear in x, with slope\r\nρ\r\nσY\r\nσX\r\n. (7.6.7)\r\nSecond, the conditional variance of Y|x is independent of x.\r\n7.6.1 How to do it with R\r\nThe multivariate normal distribution is implemented in both the mvtnorm package [34] and the\r\nmnormt package [17]. We use the mvtnorm package in this book simply because it is a dependency\r\nof another package used in the book.\r\nThe mvtnorm package has functions dmvnorm and rmvnorm for the PDF and to generate ran\u0002dom vectors, respectively. Let us get started with a graph of the bivariate normal PDF. We can\r\nmake the plot with the following code3, where the workhorse is the persp function in base R.\r\n3Another way to do this is with the curve3d function in the emdbook package [9]. It looks like this:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ac6b0119-b377-4ee9-8ac6-55783e43b621.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0c10cd9d5ecf7fd894e59502caea79e1035005846a0dcfd9b4d70314bc63761f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 439
      },
      {
        "segments": [
          {
            "segment_id": "b0e1df16-ec45-4811-9727-43d802b07626",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 197,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.7. BIVARIATE TRANSFORMATIONS OF RANDOM VARIABLES 181\r\n> library(mvtnorm)\r\n> x <- y <- seq(from = -3, to = 3, length.out = 30)\r\n> f <- function(x, y) dmvnorm(cbind(x, y), mean = c(0, 0),\r\n+ sigma = diag(2))\r\n> z <- outer(x, y, FUN = f)\r\n> persp(x, y, z, theta = -30, phi = 30, ticktype = \"detailed\")\r\nWe chose the standard bivariate normal, mvnorm(mean = 0, sigma = I), to display.\r\n7.7 Bivariate Transformations of Random Variables\r\nWe studied in Section 6.4 how to find the PDF of Y = g(X) given the PDF of X. But now we have\r\ntwo random variables X and Y, with joint PDF fX,Y , and we would like to consider the joint PDF of\r\ntwo new random variables\r\nU = g(X, Y) and V = h(X, Y), (7.7.1)\r\nwhere g and h are two given functions, typically “nice” in the sense of Appendix E.6.\r\nSuppose that the transformation (x, y) 7−→ (u, v) is one-to-one. Then an inverse transformation\r\nx = x(u, v) and y = y(u, v) exists, so let ∂(x, y)/∂(u, v) denote the Jacobian of the inverse transfor\u0002mation. Then the joint PDF of (U, V) is given by\r\nfU,V(u, v) = fX,Y\r\n\u0002\r\nx(u, v), y(u, v)\r\n\u0003\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n∂(x, y)\r\n∂(u, v)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n, (7.7.2)\r\nor we can rewrite more shortly as\r\nfU,V(u, v) = fX,Y (x, y)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n∂(x, y)\r\n∂(u, v)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n. (7.7.3)\r\nTake a moment and compare Equation 7.7.3 to Equation 6.4.2. Do you see the connection?\r\nRemark 7.28. It is sometimes easier to postpone solving for the inverse transformation x = x(u, v)\r\nand y = y(u, v). Instead, leave the transformation in the form u = u(x, y) and v = v(x, y) and\r\ncalculate the Jacobian of the original transformation\r\n∂(u, v)\r\n∂(x, y)\r\n=\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n∂u\r\n∂x\r\n∂u\r\n∂y\r\n∂v\r\n∂x\r\n∂v\r\n∂y\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n=\r\n∂u\r\n∂x\r\n∂v\r\n∂y\r\n−\r\n∂u\r\n∂y\r\n∂v\r\n∂x\r\n. (7.7.4)\r\nOnce this is known, we can get the PDF of (U, V) by\r\nfU,V(u, v) = fX,Y (x, y)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1\r\n∂(u,v)\r\n∂(x,y)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n. (7.7.5)\r\nlibrary(emdbook); library(mvtnorm) # note: the order matters\r\nmu <- c(0,0); sigma <- diag(2)\r\nf <- function(x,y) dmvnorm(c(x,y), mean = mu, sigma = sigma)\r\ncurve3d(f(x,y), from = c(-3,-3), to = c(3,3), theta = -30, phi = 30)\r\nThe code above is slightly shorter than that using persp and is easier to understand. One must be careful, however. If\r\nthe library calls are swapped then the code will not work because both packages emdbook and mvtnorm have a function\r\ncalled “dmvnorm”; one must load them to the search path in the correct order or R will use the wrong one (the arguments\r\nare named differently and the underlying algorithms are different).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b0e1df16-ec45-4811-9727-43d802b07626.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d759229ba28fe9e54e1549b40f5eae67f0af5c92edf978aa18795ec9e3bfb1aa",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "10546f24-8378-4705-a2db-b4ecf2863f22",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 198,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "182 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nx\r\n−3\r\n−2\r\n−1\r\n0\r\n1\r\n2\r\n3\r\ny\r\n−3\r\n−2\r\n−1\r\n0\r\n1\r\n2\r\n3\r\nz\r\n0.05\r\n0.10\r\n0.15\r\nFigure 7.6.1: Graph of a bivariate normal PDF",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/10546f24-8378-4705-a2db-b4ecf2863f22.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7da68eb3e9a001d16c47068d9d9240616d4bc4c13182e18035834cdb991b1c34",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 487
      },
      {
        "segments": [
          {
            "segment_id": "e416ab64-aab2-4401-b290-69b0040d2df9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 199,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.7. BIVARIATE TRANSFORMATIONS OF RANDOM VARIABLES 183\r\nIn some cases there will be a cancellation and the work will be a lot shorter. Of course, it is not\r\nalways true that\r\n∂(x, y)\r\n∂(u, v)\r\n=\r\n1\r\n∂(u,v)\r\n∂(x,y)\r\n, (7.7.6)\r\nbut for the well-behaved examples that we will see in this book it works just fine. . . do you see the\r\nconnection between Equations 7.7.6 and 6.4.5?\r\nExample 7.29. Let (X, Y) ∼ mvnorm(mean = 02×1, sigma = I2×2) and consider the transformation\r\nU = 3X + 4Y,\r\nV = 5X + 6Y.\r\nWe can solve the system of equations to find the inverse transformations; they are\r\nX = − 3U + 2V,\r\nY =\r\n5\r\n2\r\nU −\r\n3\r\n2\r\nV,\r\nin which case the Jacobian of the inverse transformation is\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n−3 2\r\n5\r\n2\r\n−\r\n3\r\n2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= 3\r\n \r\n−\r\n3\r\n2\r\n!\r\n− 2\r\n \r\n5\r\n2\r\n!\r\n= −\r\n1\r\n2\r\n.\r\nAs (x, y) traverses R\r\n2\r\n, so too does (u, v). Since the joint PDF of (X, Y) is\r\nfX,Y (x, y) =\r\n1\r\n2π\r\nexp (−\r\n1\r\n2\r\n\u0010\r\nx\r\n2 + y2\r\n\u0011\r\n)\r\n, (x, y) ∈ R\r\n2\r\n,\r\nwe get that the joint PDF of (U, V) is\r\nfU,V(u, v) =\r\n1\r\n2π\r\nexp\r\n\r\n\r\n\r\n−\r\n1\r\n2\r\n\r\n\r\n(−3u + 2v)\r\n2 +\r\n \r\n5u − 3v\r\n2\r\n!2\r\n\r\n\r\n\r\n\r\n\r\n·\r\n1\r\n2\r\n, (u, v) ∈ R\r\n2\r\n. (7.7.7)\r\nRemark 7.30. It may not be obvious, but Equation 7.7.7 is the PDF of a mvnorm distribution. For\r\na more general result see Theorem 7.34.\r\n7.7.1 How to do it with R\r\nIt is possible to do the computations above in R with the Ryacas package. The package is an inter\u0002face to the open-source computer algebra system, “Yacas”. The user installs Yacas, then employs\r\nRyacas to submit commands to Yacas, after which the output is displayed in the R console.\r\nThere are not yet any examples of Yacas in this book, but there are online materials to help the\r\ninterested reader: see http://code.google.com/p/ryacas/ to get started.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e416ab64-aab2-4401-b290-69b0040d2df9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a8e04868d5b4942540723fbee971b3a5ba004ebbc12326bde453ea7410348d4f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 357
      },
      {
        "segments": [
          {
            "segment_id": "2ac4ec45-e933-4ed8-9b1a-b3591aee0cdf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 200,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "184 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\n7.8 Remarks for the Multivariate Case\r\nThere is nothing spooky about n ≥ 3 random variables. We just have a whole bunch of them: X1,\r\nX2,. . . , Xn, which we can shorten to X = (X1, X2, . . . , Xn)\r\nT\r\nto make the formulas prettier (now may\r\nbe a good time to check out Appendix E.5). For X supported on the set S X, the joint PDF fX (if it\r\nexists) satisfies\r\nfX(x) > 0, for x ∈ S X, (7.8.1)\r\nand\r\nZZ · · · Z\r\nfX(x) dx1dx2 · · · dxn = 1, (7.8.2)\r\nor even shorter: RfX(x) dx = 1. The joint CDF FX is defined by\r\nFX(x) = IP(X1 ≤ x1, X2 ≤ x2, . . . , Xn ≤ xn), (7.8.3)\r\nfor x ∈ R\r\nn\r\n. The expectation of a function g(X) is defined just as we would imagine:\r\nIE g(X) =\r\nZ\r\ng(x) fX(x) dx. (7.8.4)\r\nprovided the integral exists and is finite. And the moment generating function in the multivariate\r\ncase is defined by\r\nMX(t) = IE exp nt\r\nTX\r\no\r\n, (7.8.5)\r\nwhenever the integral exists and is finite for all t in a neighborhood of 0n×1 (note that t\r\nTX is short\u0002hand for t1X1 + t2X2 + · · · + tnXn). The only difference in any of the above for the discrete case is\r\nthat integrals are replaced by sums.\r\nMarginal distributions are obtained by integrating out remaining variables from the joint distri\u0002bution. And even if we are given all of the univariate marginals it is not enough to determine the\r\njoint distribution uniquely.\r\nWe say that X1, X2, . . . , Xn are mutually independent if their joint PDF factors into the product\r\nof the marginals\r\nfX(x) = fX1\r\n(x1) fX2\r\n(x2) · · · fXn\r\n(xn), (7.8.6)\r\nfor every x in their joint support S X, and we say that X1, X2, . . . , Xn are exchangeable if their joint\r\nPDF (or CDF) is a symmetric function of its n arguments, that is, if\r\nfX(x\r\n∗\r\n) = fX(x), (7.8.7)\r\nfor any reordering x\r\n∗ of the elements of x = (x1, x2, . . . , xn) in the joint support.\r\nProposition 7.31. Let X1, X2, . . . , Xn be independent with respective population means µ1, µ2, . . . ,\r\nµn and standard deviations σ1, σ2, . . . , σn. For given constants a1, a2, . . . ,an define Y =\r\nPn\r\ni=1\r\naiXi.\r\nThen the mean and standard deviation of Y are given by the formulas\r\nµY =\r\nXn\r\ni=1\r\naiµi, σY =\r\n\r\n\r\nXn\r\ni=1\r\na\r\n2\r\ni σ\r\n2\r\ni\r\n\r\n\r\n1/2\r\n. (7.8.8)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/2ac4ec45-e933-4ed8-9b1a-b3591aee0cdf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ea7b0e8d40ce807408c3ada6b342d22c869df579890fb75c843cdbfc2043e9eb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 461
      },
      {
        "segments": [
          {
            "segment_id": "7db6fb8b-f862-452b-a49a-7a30d00d4ce4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 201,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.8. REMARKS FOR THE MULTIVARIATE CASE 185\r\nProof. The mean is easy:\r\nIE Y = IE\r\n\r\n\r\nXn\r\ni=1\r\naiXi\r\n\r\n =\r\nXn\r\ni=1\r\nai IE Xi =\r\nXn\r\ni=1\r\naiµi.\r\nThe variance is not too difficult to compute either. As an intermediate step, we calculate IE Y\r\n2\r\n.\r\nIE Y\r\n2 = IE\r\n\r\n\r\nXn\r\ni=1\r\naiXi\r\n\r\n\r\n2\r\n= IE\r\n\r\n\r\nXn\r\ni=1\r\na\r\n2\r\ni X\r\n2\r\ni + 2\r\nXn−1\r\ni=1\r\nXn\r\nj=i+1\r\naiajXiXj\r\n\r\n\r\n.\r\nUsing linearity of expectation the IE distributes through the sums. Now IE X\r\n2\r\ni\r\n= σ\r\n2\r\ni\r\n+ µ\r\n2\r\ni\r\nand\r\nIE XiXj = IE Xi IE Xj = µiµj when i , j because of independence. Thus\r\nIE Y\r\n2 =\r\nXn\r\ni=1\r\na\r\n2\r\ni\r\n(σ\r\n2\r\ni + µ\r\n2\r\ni\r\n) + 2\r\nXn−1\r\ni=1\r\nXn\r\nj=i+1\r\naiajµiµj\r\n=\r\nXn\r\ni=1\r\na\r\n2\r\ni σ\r\n2\r\ni +\r\n\r\n\r\nXn\r\ni=1\r\na\r\n2\r\ni\r\nµ\r\n2\r\ni + 2\r\nXn−1\r\ni=1\r\nXn\r\nj=i+1\r\naiajµiµj\r\n\r\n\r\nTo complete the proof, note that the expression in the parentheses is exactly (IE Y)\r\n2\r\n, and recall the\r\nidentity σ\r\n2\r\nY\r\n= IE Y\r\n2 − (IE Y)\r\n2\r\n. \u0003\r\nThere is a corresponding statement of Fact 7.16 for the multivariate case. The proof is also\r\nomitted here.\r\nFact 7.32. If X and Y are mutually independent random vectors, then u(X) and v(Y) are indepen\u0002dent for any functions u and v.\r\nBruno de Finetti was a strong proponent of the subjective approach to probability. He proved\r\nan important theorem in 1931 which illuminates the link between exchangeable random variables\r\nand independent random variables. Here it is in one of its simplest forms.\r\nTheorem 7.33. De Finetti’s Theorem. Let X1, X2, . . . be a sequence of binom(size = 1, prob =\r\np) random variables such that (X1, . . . , Xk) are exchangeable for every k. Then there exists a\r\nrandom variable Θ with support [0, 1] and PDF fΘ(θ) such that\r\nIP(X1 = x1, . . . , Xk = xk) =\r\nZ 1\r\n0\r\nθ\r\nP\r\nxi\r\n(1 − θ)\r\nk−\r\nP\r\nxi\r\nfΘ(θ) dθ, (7.8.9)\r\nfor all xi = 0, 1, i = 1, 2, . . . , k.\r\nTo get a handle on the intuitive content de Finetti’s theorem, imagine that we have a bunch\r\nof coins in our pocket with each having its own unique value of θ = IP(Heads). We reach into\r\nour pocket and select a coin at random according to some probability – say, fΘ(θ). We take the\r\nrandomly selected coin and flip it k times.\r\nThink carefully: the conditional probability of observing a sequence X1 = x1, . . . , Xk = xk,\r\ngiven a specific coin θ would just be θ\r\nP\r\nxi\r\n(1 − θ)\r\nk−\r\nP\r\nxi\r\n, because the coin flips are an independent",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7db6fb8b-f862-452b-a49a-7a30d00d4ce4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=afbcf8e6a0c275f5eb9c5ca0cd847dd9ba4d5346fee9dc6e24ffa7036ec1e595",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 496
      },
      {
        "segments": [
          {
            "segment_id": "12d3e3d7-a40a-46eb-ac76-b2e624312fca",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 202,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "186 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nsequence of Bernoulli trials. But the coin is random, so the Theorem of Total Probability says we\r\ncan get the unconditional probability IP(X1 = x1, . . . , Xk = xk) by adding up terms that look like\r\nθ\r\nP\r\nxi\r\n(1 − θ)\r\nk−\r\nP\r\nxi\r\nfΘ(θ), (7.8.10)\r\nwhere we sum over all possible coins. The right-hand side of Equation 7.8.9 is a sophisticated way\r\nto denote this process.\r\nOf course, the integral’s value does not change if we jumble the xi’s, so (X1, . . . , Xk) are clearly\r\nexchangeable. The power of de Finetti’s Theorem is that every infinite binary exchangeable se\u0002quence can be written in the above form.\r\nThe connection to subjective probability: our prior information about θ corresponds to fΘ(θ)\r\nand the likelihood of the sequence X1 = x1, . . . , Xk = xk (conditional on θ) corresponds to θ\r\nP\r\nxi\r\n(1 −\r\nθ)\r\nk−\r\nP\r\nxi\r\n. Compare Equation 7.8.9 to Section 4.8 and Section 7.3.\r\nThe multivariate normal distribution immediately generalizes from the bivariate case. If the\r\nmatrix Σ is nonsingular then the joint PDF of X ∼ mvnorm(mean = µ, sigma = Σ) is\r\nfX(x) =\r\n1\r\n(2π)\r\nn/2\r\n|Σ|\r\n1/2\r\nexp (−\r\n1\r\n2\r\n(x − µ)\r\n> Σ−1\r\n(x − µ)\r\n)\r\n, (7.8.11)\r\nand the MGF is\r\nMX(t) = exp (µ\r\n>\r\nt +\r\n1\r\n2\r\nt\r\n>Σt\r\n)\r\n. (7.8.12)\r\nWe will need the following in Chapter 12.\r\nTheorem 7.34. If X ∼ mvnorm(mean = µ, sigma = Σ) and A is any matrix, then the random\r\nvector Y = AX is distributed\r\nY ∼ mvnorm(mean = Aµ, sigma = AΣA\r\nT\r\n). (7.8.13)\r\nProof. Look at the MGF of Y:\r\nMY(t) = IE exp nt\r\nT\r\n(AX)\r\no\r\n,\r\n= IE exp n(A\r\nT\r\nt)\r\nTX\r\no\r\n,\r\n= exp (µ\r\nT\r\n(A\r\n>\r\nt) +\r\n1\r\n2\r\n(A\r\nT\r\nt)\r\nTΣ(AT\r\nt)\r\n)\r\n,\r\n= exp (\r\n(Aµ)\r\nT\r\nt +\r\n1\r\n2\r\nt\r\nT\r\n\u0010\r\nAΣA\r\nT\r\n\u0011\r\nt\r\n)\r\n,\r\nand the last expression is the MGF of an mvnorm(mean = Aµ, sigma = AΣA\r\nT\r\n) distribution. \u0003\r\n7.9 The Multinomial Distribution\r\nWe sample n times, with replacement, from an urn that contains balls of k different types. Let X1\r\ndenote the number of balls in our sample of type 1, let X2 denote the number of balls of type 2, . . .\r\n, and let Xk denote the number of balls of type k. Suppose the urn has proportion p1 of balls of type",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/12d3e3d7-a40a-46eb-ac76-b2e624312fca.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=96cc08441d6a2514867f26a71a99bf0b04dfda558cdf1c842a34b73ee2422bcc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 433
      },
      {
        "segments": [
          {
            "segment_id": "121efaa2-fa85-4cde-ad35-783557b5746e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 203,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.9. THE MULTINOMIAL DISTRIBUTION 187\r\n1, proportion p2 of balls of type 2, . . . , and proportion pk of balls of type k. Then the joint PMF of\r\n(X1, . . . , Xk) is\r\nfX1,...,Xk(x1, . . . , xk) =\r\n \r\nn\r\nx1 x2 · · · xk\r\n!\r\np\r\nx1\r\n1\r\np\r\nx2\r\n2\r\n· · · p\r\nxk\r\nk\r\n, (7.9.1)\r\nfor (x1, . . . , xk) in the joint support S X1,...XK. We write\r\n(X1, . . . , Xk) ∼ multinom(size = n, prob = pk×1). (7.9.2)\r\nSeveral comments are in order. First, the joint support set S X1,...XKcontains all nonnegative\r\ninteger k-tuples (x1, . . . , xk) such that x1 + x2 + · · · + xk = n. A support set like this is called a\r\nsimplex. Second, the proportions p1, p2, . . . , pk satisfy pi ≥ 0 for all i and p1 + p2 + · · · + pk = 1.\r\nFinally, the symbol\r\n \r\nn\r\nx1 x2 · · · xk\r\n!\r\n=\r\nn!\r\nx1! x2! · · · xk!\r\n(7.9.3)\r\nis called a multinomial coefficient which generalizes the notion of a binomial coefficient we saw in\r\nEquation 4.5.1.\r\nThe form and notation we have just described matches the R usage but is not standard among\r\nother texts. Most other books use the above for a k−1 dimension multinomial distribution, because\r\nthe linear constraint x1 + x2 +· · ·+ xk = n means that once the values of X1, X2, . . . , Xk−1 are known\r\nthe final value Xk is determined, not random. Another term used for this is a singular distribution.\r\nFor the most part we will ignore these difficulties, but the careful reader should keep them\r\nin mind. There is not much of a difference in practice, except that below we will use a two\u0002dimensional support set for a three-dimension multinomial distribution. See Figure 7.9.1.\r\nWhen k = 2, we have x1 = x and x2 = n−x, we have p1 = p and p2 = 1−p, and the multinomial\r\ncoefficient is literally a binomial coefficient. In the previous notation we have thus shown that the\r\nmultinom(size = n, prob = p2×1) distribution is the same as a binom(size = n, prob = p)\r\ndistribution.\r\nExample 7.35. Dinner with Barack Obama. During the 2008 U.S. presidential primary, Barack\r\nObama offered to have dinner with three randomly selected monetary contributors to his campaign.\r\nImagine the thousands of people in the contributor database. For the sake of argument, Suppose that\r\nthe database was approximately representative of the U.S. population as a whole, Suppose Barack\r\nObama wants to have dinner http://pewresearch.org/pubs/773/fewer-voters-identify-as-republican\r\n36 democrat, 27 republican , 37 independent.\r\nRemark 7.36. Here are some facts about the multinomial distribution.\r\n1. The expected value of (X1, X2, . . . , Xk) is npk×1.\r\n2. The variance-covariance matrix Σ is symmetric with diagonal entries σ\r\n2\r\ni\r\n= npi(1 − pi),\r\ni = 1, 2, . . . , k and off-diagonal entries Cov(Xi, Xj) = −npipj, for i , j. The correlation\r\nbetween Xi and Xjis therefore Corr(Xi, Xj) = −\r\np\r\npipj/(1 − pi)(1 − pj).\r\n3. The marginal distribution of (X1, X2, . . . , Xk−1) is multinom(size = n, prob = p(k−1)×1)\r\nwith\r\np(k−1)×1 = (p1, p2, . . . , pk−2, pk−1 + pk) , (7.9.4)\r\nand in particular, Xi ∼ binom(size = n, prob = pi).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/121efaa2-fa85-4cde-ad35-783557b5746e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=859cc4e9b455fb265093709db93d9dab68977a2b93acdec1b2e7a41285b25466",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 580
      },
      {
        "segments": [
          {
            "segment_id": "121efaa2-fa85-4cde-ad35-783557b5746e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 203,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.9. THE MULTINOMIAL DISTRIBUTION 187\r\n1, proportion p2 of balls of type 2, . . . , and proportion pk of balls of type k. Then the joint PMF of\r\n(X1, . . . , Xk) is\r\nfX1,...,Xk(x1, . . . , xk) =\r\n \r\nn\r\nx1 x2 · · · xk\r\n!\r\np\r\nx1\r\n1\r\np\r\nx2\r\n2\r\n· · · p\r\nxk\r\nk\r\n, (7.9.1)\r\nfor (x1, . . . , xk) in the joint support S X1,...XK. We write\r\n(X1, . . . , Xk) ∼ multinom(size = n, prob = pk×1). (7.9.2)\r\nSeveral comments are in order. First, the joint support set S X1,...XKcontains all nonnegative\r\ninteger k-tuples (x1, . . . , xk) such that x1 + x2 + · · · + xk = n. A support set like this is called a\r\nsimplex. Second, the proportions p1, p2, . . . , pk satisfy pi ≥ 0 for all i and p1 + p2 + · · · + pk = 1.\r\nFinally, the symbol\r\n \r\nn\r\nx1 x2 · · · xk\r\n!\r\n=\r\nn!\r\nx1! x2! · · · xk!\r\n(7.9.3)\r\nis called a multinomial coefficient which generalizes the notion of a binomial coefficient we saw in\r\nEquation 4.5.1.\r\nThe form and notation we have just described matches the R usage but is not standard among\r\nother texts. Most other books use the above for a k−1 dimension multinomial distribution, because\r\nthe linear constraint x1 + x2 +· · ·+ xk = n means that once the values of X1, X2, . . . , Xk−1 are known\r\nthe final value Xk is determined, not random. Another term used for this is a singular distribution.\r\nFor the most part we will ignore these difficulties, but the careful reader should keep them\r\nin mind. There is not much of a difference in practice, except that below we will use a two\u0002dimensional support set for a three-dimension multinomial distribution. See Figure 7.9.1.\r\nWhen k = 2, we have x1 = x and x2 = n−x, we have p1 = p and p2 = 1−p, and the multinomial\r\ncoefficient is literally a binomial coefficient. In the previous notation we have thus shown that the\r\nmultinom(size = n, prob = p2×1) distribution is the same as a binom(size = n, prob = p)\r\ndistribution.\r\nExample 7.35. Dinner with Barack Obama. During the 2008 U.S. presidential primary, Barack\r\nObama offered to have dinner with three randomly selected monetary contributors to his campaign.\r\nImagine the thousands of people in the contributor database. For the sake of argument, Suppose that\r\nthe database was approximately representative of the U.S. population as a whole, Suppose Barack\r\nObama wants to have dinner http://pewresearch.org/pubs/773/fewer-voters-identify-as-republican\r\n36 democrat, 27 republican , 37 independent.\r\nRemark 7.36. Here are some facts about the multinomial distribution.\r\n1. The expected value of (X1, X2, . . . , Xk) is npk×1.\r\n2. The variance-covariance matrix Σ is symmetric with diagonal entries σ\r\n2\r\ni\r\n= npi(1 − pi),\r\ni = 1, 2, . . . , k and off-diagonal entries Cov(Xi, Xj) = −npipj, for i , j. The correlation\r\nbetween Xi and Xjis therefore Corr(Xi, Xj) = −\r\np\r\npipj/(1 − pi)(1 − pj).\r\n3. The marginal distribution of (X1, X2, . . . , Xk−1) is multinom(size = n, prob = p(k−1)×1)\r\nwith\r\np(k−1)×1 = (p1, p2, . . . , pk−2, pk−1 + pk) , (7.9.4)\r\nand in particular, Xi ∼ binom(size = n, prob = pi).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/121efaa2-fa85-4cde-ad35-783557b5746e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=859cc4e9b455fb265093709db93d9dab68977a2b93acdec1b2e7a41285b25466",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 580
      },
      {
        "segments": [
          {
            "segment_id": "e8b3f241-6ac8-4399-9c2a-70a7b60dfe8e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 204,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "188 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\n7.9.1 How to do it with R\r\nThere is support for the multinomial distribution in base R, namely in the stats package. The\r\ndmultinom function represents the PMF and the rmultinom function generates random variates.\r\n> library(combinat)\r\n> tmp <- t(xsimplex(3, 6))\r\n> p <- apply(tmp, MARGIN = 1, FUN = dmultinom, prob = c(36,\r\n+ 27, 37))\r\n> library(prob)\r\n> S <- probspace(tmp, probs = p)\r\n> ProbTable <- xtabs(probs ~ X1 + X2, data = S)\r\n> round(ProbTable, 3)\r\nX2\r\nX1 0 1 2 3 4 5 6\r\n0 0.003 0.011 0.020 0.020 0.011 0.003 0.000\r\n1 0.015 0.055 0.080 0.058 0.021 0.003 0.000\r\n2 0.036 0.106 0.116 0.057 0.010 0.000 0.000\r\n3 0.047 0.103 0.076 0.018 0.000 0.000 0.000\r\n4 0.034 0.050 0.018 0.000 0.000 0.000 0.000\r\n5 0.013 0.010 0.000 0.000 0.000 0.000 0.000\r\n6 0.002 0.000 0.000 0.000 0.000 0.000 0.000\r\nDo some examples of rmultinom.\r\nHere is another way to do it4.\r\n4Another way to do the plot is with the scatterplot3d function in the scatterplot3d package [61]. It looks like\r\nthis:\r\nlibrary(scatterplot3d)\r\nX <- t(as.matrix(expand.grid(0:6, 0:6)))\r\nX <- X[ , colSums(X) <= 6]; X <- rbind(X, 6 - colSums(X))\r\nZ <- round(apply(X, 2, function(x) dmultinom(x, prob = 1:3)), 3)\r\nA <- data.frame(x = X[1, ], y = X[2, ], probability = Z)\r\nscatterplot3d(A, type = “h”, lwd = 3, box = FALSE)\r\nThe scatterplot3d graph looks better in this example, but the code is clearly more difficult to understand. And with\r\ncloud one can easily do conditional plots of the form cloud(z ~x + y|f), where f is a factor.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e8b3f241-6ac8-4399-9c2a-70a7b60dfe8e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ef321d60a3880b143b1e7e5de06ee847be6319bfeb4fe0c7b90f0ced16fa8673",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c834c3a8-785f-4508-a85c-11d6bfa15e4c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 205,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "7.9. THE MULTINOMIAL DISTRIBUTION 189\r\n● ● ●\r\n● ● ● ● ● ●\r\n● ●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\nX1 X2\r\nprobs Figure 7.9.1: Plot of a multinomial PMF",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c834c3a8-785f-4508-a85c-11d6bfa15e4c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=190d3e496f9f8caa3f41429df587e1d2b0f3b354eff8bbbdff2622762b02b3fc",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "f629b207-094a-49f3-a29d-41c342eab514",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 206,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "190 CHAPTER 7. MULTIVARIATE DISTRIBUTIONS\r\nChapter Exercises\r\nExercise 7.1. Prove that Cov(X, Y) = IE(XY) − (IE X)(IE Y).\r\nExercise 7.2. Suppose X ∼ chisq(df = p1) and Y ∼ chisq(df = p2) are independent. Find the\r\ndistribution of X + Y (you may want to refer to Equation 6.5.6).\r\nExercise 7.3. Show that when X and Y are independent the MGF of X − Y is MX(t)MY (−t). Use\r\nthis to find the distribution of X − Y when X ∼ norm(mean = µ1, sd = σ1) and Y ∼ norm(mean =\r\nµ2, sd = σ2) are independent.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f629b207-094a-49f3-a29d-41c342eab514.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=17e669031019d417a134b501c3083b429cca27eb86f7f37ae6432487a33f1b79",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 414
      },
      {
        "segments": [
          {
            "segment_id": "271f3afe-dbba-4033-88d0-d639c2531302",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 207,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 8\r\nSampling Distributions\r\nThis is an important chapter; it is the bridge from probability and descriptive statistics that we\r\nstudied in Chapters 3 through 7 to inferential statistics which forms the latter part of this book.\r\nHere is the link: we are presented with a population about which we would like to learn. And\r\nwhile it would be desirable to examine every single member of the population, we find that it is\r\neither impossible or infeasible to for us to do so, thus, we resort to collecting a sample instead. We\r\ndo not lose heart. Our method will suffice, provided the sample is representative of the population.\r\nA good way to achieve this is to sample randomly from the population.\r\nSupposing for the sake of argument that we have collected a random sample, the next task\r\nis to make some sense out of the data because the complete list of sample information is usually\r\ncumbersome, unwieldy. We summarize the data set with a descriptive statistic, a quantity calculated\r\nfrom the data (we saw many examples of these in Chapter 3). But our sample was random. . .\r\ntherefore, it stands to reason that our statistic will be random, too. How is the statistic distributed?\r\nThe probability distribution associated with the population (from which we sample) is called\r\nthe population distribution, and the probability distribution associated with our statistic is called its\r\nsampling distribution; clearly, the two are interrelated. To learn about the population distribution,\r\nit is imperative to know everything we can about the sampling distribution. Such is the goal of this\r\nchapter.\r\nWe begin by introducing the notion of simple random samples and cataloguing some of their\r\nmore convenient mathematical properties. Next we focus on what happens in the special case of\r\nsampling from the normal distribution (which, again, has several convenient mathematical prop\u0002erties), and in particular, we meet the sampling distribution of X and S\r\n2\r\n. Then we explore what\r\nhappens to X’s sampling distribution when the population is not normal and prove one of the most\r\nremarkable theorems in statistics, the Central Limit Theorem (CLT).\r\nWith the CLT in hand, we then investigate the sampling distributions of several other popular\r\nstatistics, taking full advantage of those with a tractable form. We finish the chapter with an ex\u0002ploration of statistics whose sampling distributions are not quite so tractable, and to accomplish\r\nthis goal we will use simulation methods that are grounded in all of our work in the previous four\r\nchapters.\r\nWhat do I want them to know?\r\n191",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/271f3afe-dbba-4033-88d0-d639c2531302.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6c8cf86aa4b87bff4c7132e057c15593ada90afc9a7acbb5631d8d570d1da067",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 423
      },
      {
        "segments": [
          {
            "segment_id": "6a5512ad-4f2c-4f6b-9864-81c1ebe080b7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 208,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "192 CHAPTER 8. SAMPLING DISTRIBUTIONS\r\n• the notion of population versus simple random sample, parameter versus statistic, and popu\u0002lation distribution versus sampling distribution\r\n• the classical sampling distributions of the standard one and two sample statistics\r\n• how to generate a simulated sampling distribution when the statistic is crazy\r\n• the Central Limit Theorem, period.\r\n• some basic concepts related to sampling distribution utility, such as bias and variance\r\n8.1 Simple Random Samples\r\n8.1.1 Simple Random Samples\r\nDefinition 8.1. If X1, X2, . . . , Xn are independent with Xi ∼ f for i = 1, 2, . . . , n, then we say\r\nthat X1, X2, . . . , Xn are independent and identically distributed (i.i.d.) from the population f or\r\nalternatively we say that X1, X2, . . . , Xn are a simple random sample of size n, denoted S RS (n),\r\nfrom the population f .\r\nProposition 8.2. Let X1, X2, . . . , Xn be a S RS (n) from a population distribution with mean µ and\r\nfinite standard deviation σ. Then the mean and standard deviation of X are given by the formulas\r\nµX = µ and σX = σ/ √n.\r\nProof. Plug in a1 = a2 = · · · = an = 1/n in Proposition 7.31. \u0003\r\nThe next fact will be useful to us when it comes time to prove the Central Limit Theorem in\r\nSection 8.3.\r\nProposition 8.3. Let X1, X2, . . . , Xn be a S RS (n) from a population distribution with MGF M(t).\r\nThen the MGF of X is given by\r\nMX(t) =\r\n\u0014\r\nM\r\n\u0012\r\nt\r\nn\r\n\u0013\u0015n\r\n. (8.1.1)\r\nProof. Go from the definition:\r\nMX(t) = IE etX\r\n,\r\n= IE et(X1+···+Xn)/n,\r\n= IE etX1/ne\r\ntX2/n\r\n· · · e\r\ntXn/n\r\n.\r\nAnd because X1, X2, . . . , Xn are independent, Proposition 7.13 allows us to distribute the expecta\u0002tion among each term in the product, which is\r\nIE etX1/nIE etX2/n· · · IE etXn/n.\r\nThe last step is to recognize that each term in last product above is exactly M(t/n). \u0003",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6a5512ad-4f2c-4f6b-9864-81c1ebe080b7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=08f0563ed13a7cc3302221496194a523cbe0632f37d06c9abe34e06e1104ae90",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 354
      },
      {
        "segments": [
          {
            "segment_id": "186c7df8-f4cd-454c-a3ec-849e4957195d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 209,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "8.2. SAMPLING FROM A NORMAL DISTRIBUTION 193\r\n8.2 Sampling from a Normal Distribution\r\n8.2.1 The Distribution of the Sample Mean\r\nProposition 8.4. Let X1, X2, . . . , Xn be a S RS (n) from a norm(mean = µ, sd = σ) distribution.\r\nThen the sample mean X has a norm(mean = µ, sd = σ/ √\r\nn) sampling distribution.\r\nProof. The mean and standard deviation of X follow directly from Proposition 8.2. To address the\r\nshape, first remember from Section 6.3 that the norm(mean = µ, sd = σ) MGF is of the form\r\nM(t) = exp nµt + σ\r\n2\r\nt\r\n2\r\n/2\r\no\r\n.\r\nNow use Proposition 8.3 to find\r\nMX\r\n(t) =\r\n\u0014\r\nM\r\n\u0012\r\nt\r\nn\r\n\u0013\u0015n\r\n,\r\n=\r\nh\r\nexp nµ(t/n) + σ\r\n2\r\n(t/n)\r\n2\r\n/2\r\noin\r\n,\r\n= exp nn ·\r\nh\r\nµ(t/n) + σ\r\n2\r\n(t/n)\r\n2\r\n/2\r\nio\r\n,\r\n= exp nµt + (σ/ √\r\nn)\r\n2\r\nt\r\n2\r\n/2\r\no\r\n,\r\nand we recognize this last quantity as the MGF of a norm(mean = µ, sd = σ/ √n) distribution. \u0003\r\n8.2.2 The Distribution of the Sample Variance\r\nTheorem 8.5. Let X1, X2, . . . , Xn be a S RS (n) from a norm(mean = µ, sd = σ) distribution, and\r\nlet\r\nX =\r\nXn\r\ni=1\r\nXi and S 2 =\r\n1\r\nn − 1\r\nXn\r\ni=1\r\n(Xi − X)\r\n2\r\n. (8.2.1)\r\nThen\r\n1. X and S 2 are independent, and\r\n2. The rescaled sample variance\r\n(n − 1)\r\nσ2\r\nS\r\n2 =\r\nPn\r\ni=1\r\n(Xi − X)\r\n2\r\nσ2\r\n(8.2.2)\r\nhas a chisq(df = n − 1) sampling distribution.\r\nProof. The proof is beyond the scope of the present book, but the theorem is simply too important\r\nto be omitted. The interested reader could consult Casella and Berger [13], or Hogg et al [43]. \u0003\r\n8.2.3 The Distribution of Student’s T Statistic\r\nProposition 8.6. Let X1, X2, . . . , Xn be a S RS (n) from a norm(mean = µ, sd = σ) distribution.\r\nThen the quantity\r\nT =\r\nX − µ\r\nS/\r\n√\r\nn\r\n(8.2.3)\r\nhas a t(df = n − 1) sampling distribution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/186c7df8-f4cd-454c-a3ec-849e4957195d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7e44c4817238320ad0fd2d21062bfd0899eac4fb505f52fa38e94a25ac9efc70",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 363
      },
      {
        "segments": [
          {
            "segment_id": "75d2ce7a-e333-41ad-a602-dacbf5ef869d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 210,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "194 CHAPTER 8. SAMPLING DISTRIBUTIONS\r\nProof. Divide the numerator and denominator by σ and rewrite\r\nT =\r\nX−µ\r\nσ/ √n\r\nS/σ\r\n=\r\nX−µ\r\nσ/ √n\r\nq\r\n(n−1)S\r\n2\r\nσ2\r\n.\r\n(n − 1)\r\n.\r\nNow let\r\nZ =\r\nX − µ\r\nσ/ √n\r\nand V =\r\n(n − 1)S\r\n2\r\nσ2\r\n,\r\nso that\r\nT =\r\nZ\r\n√\r\nV/r\r\n, (8.2.4)\r\nwhere r = n − 1.\r\nWe know from Section 8.2.1 that Z ∼ norm(mean = 0, sd = 1) and we know from Section 8.2.2\r\nthat V ∼ chisq(df = n−1). Further, since we are sampling from a normal distribution, Theorem 8.5\r\ngives that X and S\r\n2\r\nare independent and by Fact 7.16 so are Z and V. In summary, the distribution\r\nof T is the same as the distribution of the quantity Z/\r\n√\r\nV/r, where Z ∼ norm(mean = 0, sd = 1)\r\nand V ∼ chisq(df = r) are independent. This is in fact the definition of Student’s t distribution. \u0003\r\nThis distribution was first published by W. S. Gosset (1900) under the pseudonym Student, and\r\nthe distribution has consequently come to be known as Student’s t distribution. The PDF of T can\r\nbe derived explicitly using the techniques of Section 6.4; it takes the form\r\nfX(x) =\r\nΓ[(r + 1)/2]\r\n√\r\nrπ Γ(r/2) \r\n1 +\r\nx\r\n2\r\nr\r\n!−(r+1)/2\r\n, −∞ < x < ∞ (8.2.5)\r\nAny random variable X with the preceding PDF is said to have Student’s t distribution with r\r\ndegrees of freedom, and we write X ∼ t(df = r). The shape of the PDF is similar to the normal,\r\nbut the tails are considerably heavier. See Figure 8.2.1. As with the normal distribution, there are\r\nfour functions in R associated with the t distribution, namely dt, pt, qt, and rt, which compute\r\nthe PDF, CDF, quantile function, and generate random variates, respectively.\r\nThe code to produce Figure 8.2.1 is\r\n> curve(dt(x, df = 30), from = -3, to = 3, lwd = 3, ylab = \"y\")\r\n> ind <- c(1, 2, 3, 5, 10)\r\n> for (i in ind) curve(dt(x, df = i), -3, 3, add = TRUE)\r\nSimilar to that done for the normal we may define tα(df = n − 1) as the number on the x-axis\r\nsuch that there is exactly α area under the t(df = n − 1) curve to its right.\r\nExample 8.7. Find t0.01(df = 23) with the quantile function.\r\n> qt(0.01, df = 23, lower.tail = FALSE)\r\n[1] 2.499867\r\nRemark 8.8. There are a few things to note about the t(df = r) distribution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/75d2ce7a-e333-41ad-a602-dacbf5ef869d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11326b525cf3d72729ea62597383ab305c503c9e2fe56b6f8a8da8f0fec48e7f",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "9e195dba-6a65-478a-9bf2-ab6e801fe514",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 211,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "8.2. SAMPLING FROM A NORMAL DISTRIBUTION 195\r\n−3 −2 −1 0 1 2 3\r\n0.0 0.1 0.2 0.3 0.4\r\nx\r\ny\r\nFigure 8.2.1: Student’s t distribution for various degrees of freedom",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9e195dba-6a65-478a-9bf2-ab6e801fe514.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=99fae36bb56ef40b74d9dbb2979be1a0f0652d728b73c5689f0c63502f07d5c7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 465
      },
      {
        "segments": [
          {
            "segment_id": "01bfa11f-db72-41b3-b239-25eed473e38f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 212,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "196 CHAPTER 8. SAMPLING DISTRIBUTIONS\r\n1. The t(df = 1) distribution is the same as the cauchy(location = 0, scale = 1) distribu\u0002tion. The Cauchy distribution is rather pathological and is a counterexample to many famous\r\nresults.\r\n2. The standard deviation of t(df = r) is undefined (that is, infinite) unless r > 2. When r is\r\nmore than 2, the standard deviation is always bigger than one, but decreases to 1 as r → ∞.\r\n3. As r → ∞, the t(df = r) distribution approaches the norm(mean = 0, sd = 1) distribution.\r\n8.3 The Central Limit Theorem\r\nIn this section we study the distribution of the sample mean when the underlying distribution is not\r\nnormal. We saw in Section 8.2 that when X1, X2, . . . , Xn is a S RS (n) from a norm(mean = µ, sd =\r\nσ) distribution then X ∼ norm(mean = µ, sd = σ/ √\r\nn). In other words, we may say (owing to Fact\r\n6.24) when the underlying population is normal that the sampling distribution of Z defined by\r\nZ =\r\nX − µ\r\nσ/ √n\r\n(8.3.1)\r\nis norm(mean = 0, sd = 1).\r\nHowever, there are many populations that are not normal. . . and the statistician often finds\r\nherself sampling from such populations. What can be said in this case? The surprising answer is\r\ncontained in the following theorem.\r\nTheorem 8.9. The Central Limit Theorem. Let X1, X2, . . . , Xn be a S RS (n) from a population\r\ndistribution with mean µ and finite standard deviation σ. Then the sampling distribution of\r\nZ =\r\nX − µ\r\nσ/ √n\r\n(8.3.2)\r\napproaches a norm(mean = 0, sd = 1) distribution as n → ∞.\r\nRemark 8.10. We suppose that X1, X2, . . . , Xn are i.i.d., and we learned in Section 8.1.1 that X has\r\nmean µ and standard deviation σ/ √\r\nn, so we already knew that Z has mean 0 and standard deviation\r\n1. The beauty of the CLT is that it addresses the shape of Z’s distribution when the sample size is\r\nlarge.\r\nRemark 8.11. Notice that the shape of the underlying population’s distribution is not mentioned in\r\nTheorem 8.9; indeed, the result is true for any population that is well-behaved enough to have a\r\nfinite standard deviation. In particular, if the population is normally distributed then we know from\r\nSection 8.2.1 that the distribution of X (and Z by extension) is exactly normal, for every n.\r\nRemark 8.12. How large is “sufficiently large”? It is here that the shape of the underlying popula\u0002tion distribution plays a role. For populations with distributions that are approximately symmetric\r\nand mound-shaped, the samples may need to be only of size four or five, while for highly skewed or\r\nheavy-tailed populations the samples may need to be much larger for the distribution of the sample\r\nmeans to begin to show a bell-shape. Regardless, for a given population distribution (with finite\r\nstandard deviation) the approximation tends to be better for larger sample sizes.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/01bfa11f-db72-41b3-b239-25eed473e38f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7cb056183d912fa3dcbe132c8bbe419ca31ac95cc7e8bf3ac7dfe83179c920f5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 509
      },
      {
        "segments": [
          {
            "segment_id": "1669584c-9942-4ecc-86de-6a7ae4a749a4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 213,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "8.4. SAMPLING DISTRIBUTIONS OF TWO-SAMPLE STATISTICS 197\r\n8.3.1 How to do it with R\r\nThe TeachingDemos package [79] has clt.examp and the distrTeach [74] package has illustrateCLT.\r\nTry the following at the command line (output omitted):\r\n> library(TeachingDemos)\r\n> example(clt.examp)\r\nand\r\n> library(distrTeach)\r\n> example(illustrateCLT)\r\nThe IPSUR package has the functions clt1, clt2, and clt3 (see Exercise 8.2 at the end of\r\nthis chapter). Its purpose is to investigate what happens to the sampling distribution of X when the\r\npopulation distribution is mound shaped, finite support, and skewed, namely t(df = 3), unif(a =\r\n0, b = 10) and gamma(shape = 1.21, scale = 1/2.37), respectively.\r\nFor example, when the command clt1() is issued a plot window opens to show a graph of the\r\nPDF of a t(df = 3) distribution. On the display are shown numerical values of the population mean\r\nand variance. While the students examine the graph the computer is simulating random samples of\r\nsize sample.size = 2 from the population = \"rt\" distribution a total of N.iter = 100000\r\ntimes, and sample means are calculated of each sample. Next follows a histogram of the simulated\r\nsample means, which closely approximates the sampling distribution of X, see Section 8.5. Also\r\nshow are the sample mean and sample variance of all of the simulated Xs. As a final step, when the\r\nstudent clicks the second plot, a normal curve with the same mean and variance as the simulated\r\nXs is superimposed over the histogram. Students should compare the population theoretical mean\r\nand variance to the simulated mean and variance of the sampling distribution. They should also\r\ncompare the shape of the simulated sampling distribution to the shape of the normal distribution.\r\nThe three separate clt1, clt2, and clt3 functions were written so that students could compare\r\nwhat happens overall when the shape of the population distribution changes. It would be possible\r\nto combine all three into one big function, clt which covers all three cases (and more).\r\n8.4 Sampling Distributions of Two-Sample Statistics\r\nThere are often two populations under consideration, and it sometimes of interest to compare prop\u0002erties between groups. To do so we take independent samples from each population and calculate\r\nrespective sample statistics for comparison. In some simple cases the sampling distribution of the\r\ncomparison is known and easy to derive; such cases are the subject of the present section.\r\n8.4.1 Difference of Independent Sample Means\r\nProposition 8.13. Let X1, X2, . . . , Xn1\r\nbe an S RS (n1) from a norm(mean = µX, sd = σX) dis\u0002tribution and let Y1, Y2, . . . , Yn2\r\nbe an S RS (n2) from a norm(mean = µY , sd = σY ) distribution.\r\nSuppose that X1, X2, . . . , Xn1and Y1, Y2, . . . , Yn2are independent samples. Then the quantity\r\nX − Y − (µX − µY )\r\nq\r\nσ\r\n2\r\nX\r\n.\r\nn1 + σ\r\n2\r\nY\r\n.\r\nn2\r\n(8.4.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1669584c-9942-4ecc-86de-6a7ae4a749a4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bc2c47a15004e02555708dd5a15312bbfbdd176ec7a022444852fc4a7a6d1882",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 491
      },
      {
        "segments": [
          {
            "segment_id": "393160a0-6cba-4fea-bb2d-a171f5315fc3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 214,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "198 CHAPTER 8. SAMPLING DISTRIBUTIONS\r\nhas a norm(mean = 0, sd = 1) sampling distribution. Equivalently, X − Y has a norm(mean =\r\nµX − µY , sd =\r\nq\r\nσ\r\n2\r\nX\r\n.\r\nn1 + σ\r\n2\r\nY\r\n.\r\nn2) sampling distribution.\r\nProof. We know that X is norm(mean = µX, sd = σX/\r\n√\r\nn1) and we also know that Y is norm(mean =\r\nµY , sd = σY /\r\n√\r\nn2). And since the samples X1, X2, . . . , Xn1\r\nand Y1, Y2, . . . , Yn2\r\nare independent,\r\nso too are X and Y. The distribution of their difference is thus normal as well, and the mean and\r\nstandard deviation are given by Proposition 7.20. \u0003\r\nRemark 8.14. Even if the distribution of one or both of the samples is not normal, the quantity in\r\nEquation 8.4.1 will be approximately normal provided both sample sizes are large.\r\nRemark 8.15. For the special case of µX = µY we have shown that\r\nX − Y\r\nq\r\nσ\r\n2\r\nX\r\n/n1 + σ\r\n2\r\nY\r\n/n2\r\n(8.4.2)\r\nhas a norm(mean = 0, sd = 1) sampling distribution, or in other words, X − Y has a norm(mean =\r\n0, sd =\r\nq\r\nσ\r\n2\r\nX\r\n/n1 + σ\r\n2\r\nY\r\n/n2) sampling distribution. This will be important when it comes time to do\r\nhypothesis tests; see Section 9.3.\r\n8.4.2 Difference of Independent Sample Proportions\r\nProposition 8.16. Let X1, X2, . . . , Xn1\r\nbe an S RS (n1) from a binom(size = 1, prob = p1) dis\u0002tribution and let Y1, Y2, . . . , Yn2\r\nbe an S RS (n2) from a binom(size = 1, prob = p2) distribution.\r\nSuppose that X1, X2, . . . , Xn1\r\nand Y1, Y2, . . . , Yn2\r\nare independent samples. Define\r\npˆ1 =\r\n1\r\nn1\r\nXn1\r\ni=1\r\nXi and pˆ2 =\r\n1\r\nn2\r\nXn2\r\nj=1\r\nYj. (8.4.3)\r\nThen the sampling distribution of\r\npˆ1 − pˆ2 − (p1 − p2)\r\nq\r\np1(1−p1)\r\nn1\r\n+\r\np2(1−p2)\r\nn2\r\n(8.4.4)\r\napproaches a norm(mean = 0, sd = 1) distribution as both n1, n2 → ∞. In other words, the\r\nsampling distribution of pˆ1 − pˆ2 is approximately\r\nnorm\r\n\r\n\r\nmean = p1 − p2, sd =\r\nr\r\np1(1 − p1)\r\nn1\r\n+\r\np2(1 − p2)\r\nn2\r\n\r\n , (8.4.5)\r\nprovided both n1 and n2 are sufficiently large.\r\nProof. We know that ˆp1 is approximately normal for n1 sufficiently large by the CLT, and we know\r\nthat ˆp2 is approximately normal for n2 sufficiently large, also by the CLT. Further, ˆp1 and ˆp2 are\r\nindependent since they are derived from independent samples. And a difference of independent",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/393160a0-6cba-4fea-bb2d-a171f5315fc3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4daa66ce0aa74677d5089e443e1363550ff1517b71dfa0dc33863ffe63dd09ce",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 450
      },
      {
        "segments": [
          {
            "segment_id": "238acb87-dce2-4876-9e42-e1c975364fd1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 215,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "8.4. SAMPLING DISTRIBUTIONS OF TWO-SAMPLE STATISTICS 199\r\n(approximately) normal distributions is (approximately) normal, by Exercise 7.31. The expressions\r\nfor the mean and standard deviation follow immediately from Proposition 7.20 combined with the\r\nformulas for the binom(size = 1, prob = p) distribution from Chapter 5. \u0003\r\n8.4.3 Ratio of Independent Sample Variances\r\nProposition 8.18. Let X1, X2, . . . , Xn1be an S RS (n1) from a norm(mean = µX, sd = σX) dis\u0002tribution and let Y1, Y2, . . . , Yn2\r\nbe an S RS (n2) from a norm(mean = µY , sd = σY ) distribution.\r\nSuppose that X1, X2, . . . , Xn1and Y1, Y2, . . . , Yn2are independent samples. Then the ratio\r\nF =\r\nσ\r\n2\r\nY\r\nS\r\n2\r\nX\r\nσ\r\n2\r\nX\r\nS\r\n2\r\nY\r\n(8.4.6)\r\nhas an f(df1 = n1 − 1, df2 = n2 − 1) sampling distribution.\r\nProof. We know from Theorem 8.5 that (n1 − 1)S\r\n2\r\nX\r\n/σ2\r\nX\r\nis distributed chisq(df = n1 − 1) and\r\n(n2 − 1)S\r\n2\r\nY\r\n/σ2\r\nY\r\nis distributed chisq(df = n2 − 1). Now write\r\nF =\r\nσ\r\n2\r\nY\r\nS\r\n2\r\nX\r\nσ\r\n2\r\nX\r\nS\r\n2\r\nY\r\n=\r\n(n1 − 1)S\r\n2\r\nY\r\n.\r\n(n1 − 1)\r\n(n2 − 1)S\r\n2\r\nY\r\n.\r\n(n2 − 1)\r\n·\r\n1/ σ\r\n2\r\nX\r\n1/ σ\r\n2\r\nY\r\n,\r\nby multiplying and dividing the numerator with n1 −1 and doing likewise for the denominator with\r\nn2 − 1. Now we may regroup the terms into\r\nF =\r\n(n1−1)S\r\n2\r\nX\r\nσ\r\n2\r\nX\r\n\u001e\r\n(n1 − 1)\r\n(n2−1)S\r\n2\r\nY\r\nσ\r\n2\r\nY\r\n\u001e\r\n(n2 − 1)\r\n,\r\nand we recognize F to be the ratio of independent chisq distributions, each divided by its respective\r\nnumerator df = n1 − 1 and denominator df = n1 − 1 degrees of freedom. This is, indeed, the\r\ndefinition of Snedecor’s F distribution. \u0003\r\nRemark 8.19. For the special case of σX = σY we have shown that\r\nF =\r\nS\r\n2\r\nX\r\nS\r\n2\r\nY\r\n(8.4.7)\r\nhas an f(df1 = n1 − 1, df2 = n2 − 1) sampling distribution. This will be important in Chapters 9\r\nonward.\r\n1\r\nRemark 8.17. This does not explicitly follow, because of our cavalier use of “approximately” in too many places. To be\r\nmore thorough, however, would require more concepts than we can afford at the moment. The interested reader may consult\r\na more advanced text, specifically the topic of weak convergence, that is, convergence in distribution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/238acb87-dce2-4876-9e42-e1c975364fd1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e587bffdd7543d45e60395d9d8808a174c62b10a62e97ed27436b4b3140273fb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 423
      },
      {
        "segments": [
          {
            "segment_id": "b0ddd53a-bea4-4112-b975-7599b104be15",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 216,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "200 CHAPTER 8. SAMPLING DISTRIBUTIONS\r\n8.5 Simulated Sampling Distributions\r\nSome comparisons are meaningful, but their sampling distribution is not quite so tidy to describe\r\nanalytically. What do we do then?\r\nAs it turns out, we do not need to know the exact analytical form of the sampling distribution;\r\nsometimes it is enough to approximate it with a simulated distribution. In this section we will show\r\nyou how. Note that R is particularly well suited to compute simulated sampling distributions, much\r\nmore so than, say, SPSS or SAS.\r\n8.5.1 The Interquartile Range\r\n> iqrs <- replicate(100, IQR(rnorm(100)))\r\nWe can look at the mean of the simulated values\r\n> mean(iqrs) # close to 1\r\n[1] 1.339361\r\nand we can see the standard deviation\r\n> sd(iqrs)\r\n[1] 0.1613081\r\nNow let’s take a look at a plot of the simulated values\r\n8.5.2 The Median Absolute Deviation\r\n> mads <- replicate(100, mad(rnorm(100)))\r\nWe can look at the mean of the simulated values\r\n> mean(mads) # close to 1.349\r\n[1] 1.003938\r\nand we can see the standard deviation\r\n> sd(mads)\r\n[1] 0.1213011\r\nNow let’s take a look at a plot of the simulated values",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b0ddd53a-bea4-4112-b975-7599b104be15.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ecf2cad6495fe45e2474490e6247489dfe3e3d3ea270c302aae38447273cf34",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "71dd3eee-fa9e-477d-b8a3-b4f040896ef4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 217,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "8.5. SIMULATED SAMPLING DISTRIBUTIONS 201\r\nHistogram of iqrs\r\niqrs\r\nFrequency\r\n1.0 1.2 1.4 1.6 1.8\r\n0\r\n2\r\n4\r\n6\r\n8 10\r\nFigure 8.5.1: Plot of simulated IQRs",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/71dd3eee-fa9e-477d-b8a3-b4f040896ef4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fb484e89deecdfaa15bb4f7ae78f7ac2fa9a6a981f7351473a070c6c0a855562",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "1787d2bd-d483-4724-862b-6750854d3d6f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 218,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "202 CHAPTER 8. SAMPLING DISTRIBUTIONS\r\nHistogram of mads\r\nmads\r\nFrequency\r\n0.8 0.9 1.0 1.1 1.2 1.3\r\n0\r\n5 10 15\r\nFigure 8.5.2: Plot of simulated MADs",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1787d2bd-d483-4724-862b-6750854d3d6f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0a3dff51efada738b63d0578ebc97aa04ade9620e54b6789559adaa9bf39320e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 241
      },
      {
        "segments": [
          {
            "segment_id": "fb1bdc5d-0c15-42d7-840c-eedf9565e935",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 219,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "8.5. SIMULATED SAMPLING DISTRIBUTIONS 203\r\nChapter Exercises\r\nExercise 8.1. Suppose that we observe a random sample X1, X2, . . . , Xn of size S RS (n =26) from\r\na norm(mean =19) distribution.\r\n1. What is the mean of X?\r\n2. What is the standard deviation of X?\r\n3. What is the distribution of X? (approximately)\r\n4. Find IP(a < X ≤ b)\r\n5. Find IP(X > c).\r\nExercise 8.2. In this exercise we will investigate how the shape of the population distribution\r\naffects the time until the distribution of X is acceptably normal.\r\nAnswer the questions and write a report about what you have learned. Use plots and histograms to\r\nsupport your conclusions. See Appendix F for instructions about writing reports with R. For these\r\nproblems, the discussion/interpretation parts are the most important, so be sure to ANSWER THE\r\nWHOLE QUESTION.\r\nThe Central Limit Theorem\r\nFor Questions 1-3, we assume that we have observed random variables X1, X2, . . . ,Xn that are\r\nan S RS (n) from a given population (depending on the problem) and we want to investigate the\r\ndistribution of X as the sample size n increases.\r\n1. The population of interest in this problem has a Student’s t distribution with r = 3 degrees of\r\nfreedom. We begin our investigation with a sample size of n = 2. Open an R session, make\r\nsure to type library(IPSUR) and then follow that with clt1().\r\n(a) Look closely and thoughtfully at the first graph. How would you describe the popula\u0002tion distribution? Think back to the different properties of distributions in Chapter 3.\r\nIs the graph symmetric? Skewed? Does it have heavy tails or thin tails? What else can\r\nyou say?\r\n(b) What is the population mean µ and the population variance σ\r\n2\r\n? (Read these from the\r\nfirst graph.)\r\n(c) The second graph shows (after a few seconds) a relative frequency histogram which\r\nclosely approximates the distribution of X. Record the values of mean(xbar) and\r\nvar(xbar), where xbar denotes the vector that contains the simulated sample means.\r\nUse the answers from part (b) to calculate what these estimates should be, based on\r\nwhat you know about the theoretical mean and variance of X. How well do your an\u0002swers to parts (b) and (c) agree?\r\n(d) Click on the histogram to superimpose a red normal curve, which is the theoretical\r\nlimit of the distribution of X as n → ∞. How well do the histogram and the normal\r\ncurve match? Describe the differences between the two distributions. When judging",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/fb1bdc5d-0c15-42d7-840c-eedf9565e935.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1991087a5133588b2431b0f3b49b4e8e39f49b7b3e1237f8a4aa1ef76b0b43b6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 424
      },
      {
        "segments": [
          {
            "segment_id": "c4650031-c6b1-4410-802e-dbbe4189d3b9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 220,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "204 CHAPTER 8. SAMPLING DISTRIBUTIONS\r\nbetween the two, do not worry so much about the scale (the graphs are being rescaled\r\nautomatically, anyway). Rather, look at the peak: does the histogram poke through the\r\ntop of the normal curve? How about on the sides: are there patches of white space\r\nbetween the histogram and line on either side (or both)? How do the curvature of the\r\nhistogram and the line compare? Check down by the tails: does the red line drop off\r\nvisibly below the level of the histogram, or do they taper off at the same height?\r\n(e) We can increase our sample size from 2 to 11 with the command clt1(sample.size\r\n= 11). Return to the command prompt to do this. Answer parts (b) and (c) for this new\r\nsample size.\r\n(f) Go back to clt1 and increase the sample.size from 11 to 31. Answer parts (b) and\r\n(c) for this new sample size.\r\n(g) Comment on whether it appears that the histogram and the red curve are “noticeably\r\ndifferent” or whether they are “essentially the same” for the largest sample size n = 31.\r\nIf they are still “noticeably different” at n = 31, how large does n need to be until they\r\nare “essentially the same”? (Experiment with different values of n).\r\n2. Repeat Question 1 for the function clt2. In this problem, the population of interest has a\r\nunif(min = 0, max = 10) distribution.\r\n3. Repeat Question 1 for the function clt3. In this problem, the population of interest has a\r\ngamma(shape = 1.21, rate = 1/2.37) distribution.\r\n4. Summarize what you have learned. In your own words, what is the general trend that is\r\nbeing displayed in these histograms, as the sample size n increases from 2 to 11, on to 31,\r\nand onward?\r\n5. How would you describe the relationship between the shape of the population distribution\r\nand the speed at which X’s distribution converges to normal? In particular, consider a pop\u0002ulation which is highly skewed. Will we need a relatively large sample size or a relatively\r\nsmall sample size in order for X’s distribution to be approximately bell shaped?\r\nExercise 8.3. Let X1,. . . , X25 be a random sample from a norm(mean = 37, sd = 45) distribution,\r\nand let X be the sample mean of these n = 25 observations. Find the following probabilities.\r\n1. How is X distributed?\r\nnorm(mean = 37, sd = 45/\r\n√\r\n25)\r\n2. Find IP(X > 43.1).\r\n> pnorm(43.1, mean = 37, sd = 9, lower.tail = FALSE)\r\n[1] 0.2489563",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c4650031-c6b1-4410-802e-dbbe4189d3b9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d3254a3459a6159e75f5a5e84c73e9de7320687cec28290af361be6a925acabc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 427
      },
      {
        "segments": [
          {
            "segment_id": "dfc0ba84-3ccb-498d-96a0-ae539a2caf81",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 221,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 9\r\nEstimation\r\nWe will discuss two branches of estimation procedures: point estimation and interval estimation.\r\nWe briefly discuss point estimation first and then spend the rest of the chapter on interval estimation.\r\nWe find an estimator with the methods of Section 9.1. We make some assumptions about the\r\nunderlying population distribution and use what we know from Chapter 8 about sampling distribu\u0002tions both to study how the estimator will perform, and to find intervals of confidence for underlying\r\nparameters associated with the population distribution. Once we have confidence intervals we can\r\ndo inference in the form of hypothesis tests in the next chapter.\r\nWhat do I want them to know?\r\n• how to look at a problem, identify a reasonable model, and estimate a parameter associated\r\nwith the model\r\n• about maximum likelihood, and in particular, how to\r\n◦ eyeball a likelihood to get a maximum\r\n◦ use calculus to find an MLE for one-parameter families\r\n• about properties of the estimators they find, such as bias, minimum variance, MSE\r\n• point versus interval estimation, and how to find and interpret confidence intervals for basic\r\nexperimental designs\r\n• the concept of margin of error and its relationship to sample size\r\n9.1 Point Estimation\r\nThe following example is how I was introduced to maximum likelihood.\r\nExample 9.1. Suppose we have a small pond in our backyard, and in the pond there live some fish.\r\nWe would like to know how many fish live in the pond. How can we estimate this? One procedure\r\ndeveloped by researchers is the capture-recapture method. Here is how it works.\r\n205",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/dfc0ba84-3ccb-498d-96a0-ae539a2caf81.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1b6de84482fea8fa2c04c5f4b4020c836679d602d53593bb3a065bca6ad85ddd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 266
      },
      {
        "segments": [
          {
            "segment_id": "67388b38-59f1-4ae5-96ff-7572cf8294d4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 222,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "206 CHAPTER 9. ESTIMATION\r\nWe will fish from the pond and suppose that we capture M = 7 fish. On each caught fish we\r\nattach an unobtrusive tag to the fish’s tail, and release it back into the water.\r\nNext, we wait a few days for the fish to remix and become accustomed to their new tag. Then\r\nwe go fishing again. On the second trip some of the fish we catch may be tagged; some may not be.\r\nLet X denote the number of caught fish which are tagged1, and suppose for the sake of argument\r\nthat we catch K = 4 fish and we find that 3 of them are tagged.\r\nNow let F denote the (unknown) total number of fish in the pond. We know that F ≥ 7, because\r\nwe tagged that many on the first trip. In fact, if we let N denote the number of untagged fish in the\r\npond, then F = M + N. We have sampled K = 4 times, without replacement, from an urn which\r\nhas M = 7 white balls and N = F − M black balls, and we have observed x = 3 of them to be white.\r\nWhat is the probability of this?\r\nLooking back to Section 5.6, we see that the random variable X has a hyper(m = M, n =\r\nF − M, k = K) distribution. Therefore, for an observed value X = x the probability would be\r\nIP(X = x) =\r\n\u0010\r\nM\r\nx\r\n\u0011\u0010F−M\r\nK−x\r\n\u0011\r\n\u0010\r\nF\r\nK\r\n\u0011 .\r\nFirst we notice that F must be at least 7. Could F be equal to seven? If F = 7 then all of the\r\nfish would have been tagged on the first run, and there would be no untagged fish in the pond, thus,\r\nIP(3 successes in 4 trials) = 0.\r\nWhat about F = 8; what would be the probability of observing X = 3 tagged fish?\r\nIP(3 successes in 4 trials) =\r\n\u0010\r\n7\r\n3\r\n\u0011\u00101\r\n1\r\n\u0011\r\n\u0010\r\n8\r\n4\r\n\u0011 =\r\n35\r\n70\r\n= 0.5.\r\nSimilarly, if F = 9 then the probability of observing X = 3 tagged fish would be\r\nIP(3 successes in 4 trials) =\r\n\u0010\r\n7\r\n3\r\n\u0011\u00102\r\n1\r\n\u0011\r\n\u0010\r\n9\r\n4\r\n\u0011 =\r\n70\r\n126\r\n≈ 0.556.\r\nWe can see already that the observed data X = 3 is more likely when F = 9 than it is when F = 8.\r\nAnd here lies the genius of Sir Ronald Aylmer Fisher: he asks, “What is the value of F which has\r\nthe highest likelihood?” In other words, for all of the different possible values of F, which one\r\nmakes the above probability the biggest? We can answer this question with a plot of IP(X = x)\r\nversus F. See Figure 9.1.1.\r\nExample 9.2. In the last example we were only concerned with how many fish were in the pond,\r\nbut now, we will ask a different question. Suppose it is known that there are only two species of\r\nfish in the pond: smallmouth bass (Micropterus dolomieu) and bluegill (Lepomis macrochirus);\r\nperhaps we built the pond some years ago and stocked it with only these two species. We would\r\nlike to estimate the proportion of fish in the pond which are bass.\r\nLet p = the proportion of bass. Without any other information, it is conceivable for p to be\r\nany value in the interval [0, 1], but for the sake of argument we will suppose that p falls strictly\r\n1\r\nIt is theoretically possible that we could catch the same tagged fish more than once, which would inflate our count of\r\ntagged fish. To avoid this difficulty, suppose that on the second trip we use a tank on the boat to hold the caught fish until\r\ndata collection is completed.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/67388b38-59f1-4ae5-96ff-7572cf8294d4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=550b0a6534932ee53c6701c52dcf37833952f24dcab84a578ccbe1751d220d7b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 641
      },
      {
        "segments": [
          {
            "segment_id": "67388b38-59f1-4ae5-96ff-7572cf8294d4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 222,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "206 CHAPTER 9. ESTIMATION\r\nWe will fish from the pond and suppose that we capture M = 7 fish. On each caught fish we\r\nattach an unobtrusive tag to the fish’s tail, and release it back into the water.\r\nNext, we wait a few days for the fish to remix and become accustomed to their new tag. Then\r\nwe go fishing again. On the second trip some of the fish we catch may be tagged; some may not be.\r\nLet X denote the number of caught fish which are tagged1, and suppose for the sake of argument\r\nthat we catch K = 4 fish and we find that 3 of them are tagged.\r\nNow let F denote the (unknown) total number of fish in the pond. We know that F ≥ 7, because\r\nwe tagged that many on the first trip. In fact, if we let N denote the number of untagged fish in the\r\npond, then F = M + N. We have sampled K = 4 times, without replacement, from an urn which\r\nhas M = 7 white balls and N = F − M black balls, and we have observed x = 3 of them to be white.\r\nWhat is the probability of this?\r\nLooking back to Section 5.6, we see that the random variable X has a hyper(m = M, n =\r\nF − M, k = K) distribution. Therefore, for an observed value X = x the probability would be\r\nIP(X = x) =\r\n\u0010\r\nM\r\nx\r\n\u0011\u0010F−M\r\nK−x\r\n\u0011\r\n\u0010\r\nF\r\nK\r\n\u0011 .\r\nFirst we notice that F must be at least 7. Could F be equal to seven? If F = 7 then all of the\r\nfish would have been tagged on the first run, and there would be no untagged fish in the pond, thus,\r\nIP(3 successes in 4 trials) = 0.\r\nWhat about F = 8; what would be the probability of observing X = 3 tagged fish?\r\nIP(3 successes in 4 trials) =\r\n\u0010\r\n7\r\n3\r\n\u0011\u00101\r\n1\r\n\u0011\r\n\u0010\r\n8\r\n4\r\n\u0011 =\r\n35\r\n70\r\n= 0.5.\r\nSimilarly, if F = 9 then the probability of observing X = 3 tagged fish would be\r\nIP(3 successes in 4 trials) =\r\n\u0010\r\n7\r\n3\r\n\u0011\u00102\r\n1\r\n\u0011\r\n\u0010\r\n9\r\n4\r\n\u0011 =\r\n70\r\n126\r\n≈ 0.556.\r\nWe can see already that the observed data X = 3 is more likely when F = 9 than it is when F = 8.\r\nAnd here lies the genius of Sir Ronald Aylmer Fisher: he asks, “What is the value of F which has\r\nthe highest likelihood?” In other words, for all of the different possible values of F, which one\r\nmakes the above probability the biggest? We can answer this question with a plot of IP(X = x)\r\nversus F. See Figure 9.1.1.\r\nExample 9.2. In the last example we were only concerned with how many fish were in the pond,\r\nbut now, we will ask a different question. Suppose it is known that there are only two species of\r\nfish in the pond: smallmouth bass (Micropterus dolomieu) and bluegill (Lepomis macrochirus);\r\nperhaps we built the pond some years ago and stocked it with only these two species. We would\r\nlike to estimate the proportion of fish in the pond which are bass.\r\nLet p = the proportion of bass. Without any other information, it is conceivable for p to be\r\nany value in the interval [0, 1], but for the sake of argument we will suppose that p falls strictly\r\n1\r\nIt is theoretically possible that we could catch the same tagged fish more than once, which would inflate our count of\r\ntagged fish. To avoid this difficulty, suppose that on the second trip we use a tank on the boat to hold the caught fish until\r\ndata collection is completed.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/67388b38-59f1-4ae5-96ff-7572cf8294d4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=550b0a6534932ee53c6701c52dcf37833952f24dcab84a578ccbe1751d220d7b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 641
      },
      {
        "segments": [
          {
            "segment_id": "a22d37e1-2fd8-42ef-9786-d23bac0e3229",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 223,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.1. POINT ESTIMATION 207\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n6 8 10 12 14\r\n0.0 0.1 0.2 0.3 0.4 0.5\r\nnumber of fish in pond\r\nLikelihood\r\nF\r\n^\r\n= 9\r\nFigure 9.1.1: Capture-recapture experiment",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a22d37e1-2fd8-42ef-9786-d23bac0e3229.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=daf00dd0878938707b89ba2ff4316abf2ce0d856f47d8577314db983a8680276",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 39
      },
      {
        "segments": [
          {
            "segment_id": "fba96a67-c1be-48af-8297-b29f50a198de",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 224,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "208 CHAPTER 9. ESTIMATION\r\nbetween 0 and 1. How can we learn about the true value of p? Go fishing! As before, we will use\r\ncatch-and-release, but unlike before, we will not tag the fish. We will simply note the species of\r\nany caught fish before returning it to the pond.\r\nSuppose we catch n fish. Let\r\nXi =\r\n\r\n\r\n\r\n1, if the ith fish is a bass,\r\n0, if the ith fish is a bluegill.\r\nSince we are returning the fish to the pond once caught, we may think of this as a sampling\r\nscheme with replacement where the proportion of bass p does not change. Given that we allow\r\nthe fish sufficient time to “mix” once returned, it is not completely unreasonable to model our\r\nfishing experiment as a sequence of Bernoulli trials, so that the Xi’s would be i.i.d. binom(size =\r\n1, prob = p). Under those assumptions we would have\r\nIP(X1 = x1, X2 = x2, . . . , Xn = xn) = IP(X1 = x1) IP(X2 = x2) · · · IP(Xn = xn),\r\n= p\r\nx1\r\n(1 − p)\r\nx1 p\r\nx2\r\n(1 − p)\r\nx2\r\n· · · p\r\nxn\r\n(1 − p)\r\nxn\r\n,\r\n= p\r\nP\r\nxi\r\n(1 − p)\r\nn−\r\nP\r\nxi\r\n.\r\nThat is,\r\nIP(X1 = x1, X2 = x2, . . . , Xn = xn) = p\r\nP\r\nxi\r\n(1 − p)\r\nn−\r\nP\r\nxi\r\n.\r\nThis last quantity is a function of p, called the likelihood function L(p):\r\nL(p) = p\r\nP\r\nxi\r\n(1 − p)\r\nn−\r\nP\r\nxi\r\n.\r\nA graph of L for values of Pxi = 3, 4, and 5 when n = 7 is shown in Figure 9.1.2.\r\n> curve(x^5 * (1 - x)^2, from = 0, to = 1, xlab = \"p\", ylab = \"L(p)\")\r\n> curve(x^4 * (1 - x)^3, from = 0, to = 1, add = TRUE)\r\n> curve(x^3 * (1 - x)^4, 0, 1, add = TRUE)\r\nWe want the value of p which has the highest likelihood, that is, we again wish to maximize\r\nthe likelihood. We know from calculus (see Appendix E.2) to differentiate L and set L\r\n0 = 0 to find\r\na maximum.\r\nL\r\n0\r\n(p) =\r\n\u0010X\r\nxi\r\n\u0011\r\np\r\nP\r\nxi−1\r\n(1 − p)\r\nn−\r\nP\r\nxi + p\r\nP\r\nxi\r\n\u0010\r\nn −\r\nX\r\nxi\r\n\u0011\r\n(1 − p)\r\nn−\r\nP\r\nxi−1\r\n(−1).\r\nThe derivative vanishes (L\r\n0 = 0) when\r\n\u0010X\r\nxi\r\n\u0011\r\np\r\nP\r\nxi−1\r\n(1 − p)\r\nn−\r\nP\r\nxi = p\r\nP\r\nxi\r\n\u0010\r\nn −\r\nX\r\nxi\r\n\u0011\r\n(1 − p)\r\nn−\r\nP\r\nxi−1\r\n, X\r\nxi(1 − p) =\r\n\u0010\r\nn −\r\nX\r\nxi\r\n\u0011\r\np, X\r\nxi − p\r\nX\r\nxi = np − p\r\nX\r\nxi,\r\n1\r\nn\r\nXn\r\ni=1\r\nxi = p.\r\nThis “best” p, the one which maximizes the likelihood, is called the maximum likelihood estimator\r\n(MLE) of p and is denoted ˆp. That is,\r\npˆ =\r\nPn\r\ni=1\r\nxi\r\nn\r\n= x. (9.1.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/fba96a67-c1be-48af-8297-b29f50a198de.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d93e6e934811b512c7128ed7c658fbace4ffa768199b8e73476ff39104ae5007",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 507
      },
      {
        "segments": [
          {
            "segment_id": "8b57ce62-0e20-4aae-b423-25c938b8800d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 225,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.1. POINT ESTIMATION 209\r\n0.0 0.2 0.4 0.6 0.8 1.0\r\n0.000 0.005 0.010 0.015\r\np\r\nL(p)\r\nFigure 9.1.2: Assorted likelihood functions for fishing, part two\r\nThree graphs are shown of L when Pxi equals 3, 4, and 5, respectively, from left to right. We pick an L that\r\nmatches the observed data and then maximize L as a function of p. If Pxi = 4, then the maximum appears to\r\noccur somewhere around p ≈ 0.6.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8b57ce62-0e20-4aae-b423-25c938b8800d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a71bb7e5705296f160ed7d00f537a75933f917d99c1ea5e9be17141a3f7088a8",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "7673ae2d-ef95-47d8-b379-35fd5905f8c6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 226,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "210 CHAPTER 9. ESTIMATION\r\n0.0 0.2 0.4 0.6 0.8 1.0\r\n0e+00 4e−09 8e−09\r\nparameter space\r\nLikelihood\r\nθ\r\n^\r\n= 0.4444\r\nFigure 9.1.3: Species maximum likelihood\r\nRemark 9.3. Strictly speaking we have only shown that the derivative equals zero at ˆp, so it is\r\ntheoretically possible that the critical value ˆp = x is located at a minimum2\r\ninstead of a maximum!\r\nWe should be thorough and check that L\r\n0 > 0 when p < x and L\r\n0 < 0 when p > x. Then by the\r\nFirst Derivative Test (Theorem E.6) we could be certain that ˆp = x is indeed a maximum likelihood\r\nestimator, and not a minimum likelihood estimator.\r\nThe result is shown in Figure 9.1.3.\r\nIn general, we have a family of PDFs f(x|θ) indexed by a parameter θ in some parameter space\r\nΘ. We want to learn about θ. We take a S RS (n):\r\nX1, X2, . . . , Xn which are i.i.d. f(x|θ). (9.1.2)\r\n2We can tell from the graph that our value of ˆp is a maximum instead of a minimum so we do not really need to worry\r\nfor this example. Other examples are not so easy, however, and we should be careful to be cognizant of this extra step.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7673ae2d-ef95-47d8-b379-35fd5905f8c6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=98f391eb92962c8d9dd9d35628346342ec403e4113fb6756b28f3ade2aad2cd0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 287
      },
      {
        "segments": [
          {
            "segment_id": "21c78fbc-d981-486c-9413-07826b507022",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 227,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.1. POINT ESTIMATION 211\r\nDefinition 9.4. Given the observed data x1, x2, . . . , xn, the likelihood function L is defined by\r\nL(θ) =\r\nYn\r\ni=1\r\nf(xi|θ), θ ∈ Θ.\r\nThe next step is to maximize L. The method we will use in this book is to find the derivative L\r\n0\r\nand solve the equation L\r\n0\r\n(θ) = 0. Call a solution θˆ. We will check that L is maximized at θˆ using\r\nthe First Derivative Test or the Second Derivative Test \u0010L\r\n00(θˆ) < 0\r\n\u0011\r\n.\r\nDefinition 9.5. A value θ that maximizes L is called a maximum likelihood estimator (MLE) and\r\nis denoted θˆ. It is a function of the sample, θˆ = θˆ (X1, X2, . . . , Xn), and is called a point estimator\r\nof θ.\r\nRemark 9.6. Some comments about maximum likelihood estimators:\r\n• Often it is easier to maximize the log-likelihood l(θ) = ln L(θ) instead of the likelihood L.\r\nSince the logarithmic function y = ln x is a monotone transformation, the solutions to both\r\nproblems are the same.\r\n• MLEs do not always exist (for instance, sometimes the likelihood has a vertical asymptote),\r\nand even when they do exist, they are not always unique (imagine a function with a bunch of\r\nhumps of equal height). For any given problem, there could be zero, one, or any number of\r\nvalues of θ for which L(θ) is a maximum.\r\n• The problems we encounter in this book are all very nice with likelihood functions that\r\nhave closed form representations and which are optimized by some calculus acrobatics. In\r\npractice, however, likelihood functions are sometimes nasty in which case we are obliged to\r\nuse numerical methods to find maxima (if there are any).\r\n• MLEs are just one of many possible estimators. One of the more popular alternatives are the\r\nmethod of moments estimators; see Casella and Berger [13] for more.\r\nNotice, in Example 9.2 we had Xii.i.d. binom(size = 1, prob = p), and we saw that the MLE\r\nwas ˆp = X. But further\r\nIE X = IE\r\nX1 + X2 + · · · + Xn\r\nn\r\n,\r\n=\r\n1\r\nn\r\n(IE X1 + IE X2 + · · · + IE Xn) ,\r\n=\r\n1\r\nn\r\n(np) ,\r\n= p,\r\nwhich is exactly the same as the parameter which we estimated. More concisely, IE ˆp = p, that is,\r\non the average, the estimator is exactly right.\r\nDefinition 9.7. Let s(X1, X2, . . . , Xn) be a statistic which estimates θ. If\r\nIE s(X1, X2, . . . , Xn) = θ,\r\nthen the statistic s(X1, X2, . . . , Xn) is said to be an unbiased estimator of θ. Otherwise, it is biased.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/21c78fbc-d981-486c-9413-07826b507022.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8a51c3ebbc6df9a287297d00173a69a3fa116c34a0fb23ad4bee354fdd1fd89d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 462
      },
      {
        "segments": [
          {
            "segment_id": "f4268b61-e4cd-4368-9421-9619ae7d172b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 228,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "212 CHAPTER 9. ESTIMATION\r\nExample 9.8. Let X1, X2, . . . , Xn be an S RS (n) from a norm(mean = µ, sd = σ) distribution. It\r\ncan be shown (in Exercise 9.1) that if θ = (µ, σ2) then the MLE of θ is\r\nθˆ = ( ˆµ,σˆ\r\n2\r\n), (9.1.3)\r\nwhere ˆµ = X and\r\nσˆ2 =\r\n1\r\nn\r\nXn\r\ni=1\r\n\u0010\r\nXi − X\r\n\u00112\r\n=\r\nn − 1\r\nn\r\nS\r\n2\r\n. (9.1.4)\r\nWe of course know from 8.2 that ˆµ is unbiased. What about σˆ2? Let us check:\r\nIE σˆ2 = IE\r\nn − 1\r\nn\r\nS\r\n2\r\n= IE \r\nσ\r\n2\r\nn\r\n(n − 1)S\r\n2\r\nσ2\r\n!\r\n=\r\nσ\r\n2\r\nn\r\nIE chisq(df = n − 1)\r\n=\r\nσ\r\n2\r\nn\r\n(n − 1),\r\nfrom which we may conclude two things:\r\n1. σˆ2is a biased estimator of σ\r\n2\r\n, and\r\n2. S\r\n2 = nσˆ2/(n − 1) is an unbiased estimator of σ2\r\n.\r\nOne of the most common questions in an introductory statistics class is, “Why do we divide\r\nby n − 1 when we compute the sample variance? Why do we not divide by n?” We see now that\r\ndivision by n amounts to the use of a biased estimator for σ\r\n2\r\n, that is, if we divided by n then on\r\nthe average we would underestimate the true value of σ\r\n2\r\n. We use n − 1 so that, on the average, our\r\nestimator of σ\r\n2 will be exactly right.\r\n9.1.1 How to do it with R\r\nR can be used to find maximum likelihood estimators in a lot of diverse settings. We will discuss\r\nonly the most basic here and will leave the rest to more sophisticated texts.\r\nFor one parameter estimation problems we may use the optimize function to find MLEs. The\r\narguments are the function to be maximized (the likelihood function), the range over which the\r\noptimization is to take place, and optionally any other arguments to be passed to the likelihood if\r\nneeded.\r\nLet us see how to do Example 9.2. Recall that our likelihood function was given by\r\nL(p) = p\r\nP\r\nxi\r\n(1 − p)\r\nn−\r\nP\r\nxi\r\n. (9.1.5)\r\nNotice that the likelihood is just a product of binom(size = 1, prob = p) PMFs. We first give\r\nsome sample data (in the vector datavals), next we define the likelihood function L, and finally\r\nwe optimize L over the range c(0,1).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f4268b61-e4cd-4368-9421-9619ae7d172b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e8d3590543fef0b2db913e1b72198a77e0681b6fff9f6324ce3572c4ecf8e788",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 414
      },
      {
        "segments": [
          {
            "segment_id": "b656f775-4d52-42cb-99a7-969fa4e57e4f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 229,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.1. POINT ESTIMATION 213\r\n> x <- mtcars$am\r\n> L <- function(p, x) prod(dbinom(x, size = 1, prob = p))\r\n> optimize(L, interval = c(0, 1), x = x, maximum = TRUE)\r\n$maximum\r\n[1] 0.4062458\r\n$objective\r\n[1] 4.099989e-10\r\nNote that the optimize function by default minimizes the function L, so we have to set\r\nmaximum = TRUE to get an MLE. The returned value of $maximum gives an approximate value\r\nof the MLE to be 0.406 and $objective gives L evaluated at the MLE which is approximately 0.\r\nWe previously remarked that it is usually more numerically convenient to maximize the log\u0002likelihood (or minimize the negative log-likelihood), and we can just as easily do this with R. We\r\njust need to calculate the log-likelihood beforehand which (for this example) is\r\n−l(p) = −\r\nX\r\nxiln p −\r\n\u0010\r\nn −\r\nX\r\nxi\r\n\u0011\r\nln(1 − p).\r\nIt is done in R with\r\n> minuslogL <- function(p, x) -sum(dbinom(x, size = 1, prob = p,\r\n+ log = TRUE))\r\n> optimize(minuslogL, interval = c(0, 1), x = x)\r\n$minimum\r\n[1] 0.4062525\r\n$objective\r\n[1] 21.61487\r\nNote that we did not need maximum = TRUE because we minimized the negative log-likelihood.\r\nThe answer for the MLE is essentially the same as before, but the $objective value was different,\r\nof course.\r\nFor multiparameter problems we may use a similar approach by way of the mle function in the\r\nstats4 package.\r\nExample 9.9. Plant Growth. We will investigate the weight variable of the PlantGrowth\r\ndata. We will suppose that the weights constitute a random observations X1, X2,. . . , Xn that are\r\ni.i.d. norm(mean = µ, sd = σ) which is not unreasonable based on a histogram and other ex\u0002ploratory measures. We will find the MLE of θ = (µ, σ2\r\n). We claimed in Example 9.8 that\r\nθˆ = ( ˆµ,σˆ\r\n2\r\n) had the form given above. Let us check whether this is plausible numerically. The\r\nnegative log-likelihood function is\r\n> minuslogL <- function(mu, sigma2){\r\n+ -sum(dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE))\r\n+ }",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b656f775-4d52-42cb-99a7-969fa4e57e4f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=05b57f7e2469ec6557ac72fbe83e811703c4f6f2f622601c0ff04731d2de20a9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 346
      },
      {
        "segments": [
          {
            "segment_id": "a3c38f2a-3931-44d1-9857-bc6b165849df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 230,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "214 CHAPTER 9. ESTIMATION\r\nNote that we omitted the data as an argument to the log-likelihood function; the only arguments\r\nwere the parameters over which the maximization is to take place. Now we will simulate some\r\ndata and find the MLE. The optimization algorithm requires starting values (intelligent guesses)\r\nfor the parameters. We choose values close to the sample mean and variance (which turn out to be\r\napproximately 5 and 0.5, respectively) to illustrate the procedure.\r\n> x <- PlantGrowth$weight\r\n> library(stats4)\r\n> MaxLikeEst <- mle(minuslogL, start = list(mu = 5, sigma2 = 0.5))\r\n> summary(MaxLikeEst)\r\nMaximum likelihood estimation\r\nCall:\r\nmle(minuslogl = minuslogL, start = list(mu = 5, sigma2 = 0.5))\r\nCoefficients:\r\nEstimate Std. Error\r\nmu 5.0729848 0.1258666\r\nsigma2 0.4752721 0.1227108\r\n-2 log L: 62.82084\r\nThe outputted MLEs are shown above, and mle even gives us estimates for the standard errors\r\nof ˆµ and ˆσ\r\n2\r\n(which were obtained by inverting the numerical Hessian matrix at the optima; see\r\nAppendix E.6). Let us check how close the numerical MLEs came to the theoretical MLEs:\r\n> mean(x)\r\n[1] 5.073\r\n> var(x) * 29/30\r\n[1] 0.475281\r\n> sd(x)/sqrt(30)\r\n[1] 0.1280195\r\nThe numerical MLEs were very close to the theoretical MLEs. We already knew that the\r\nstandard error of ˆµ = X is σ/ √n, and the numerical estimate of this was very close too.\r\nThere is functionality in the distrTest package [74] to calculate theoretical MLEs; we will\r\nskip examples of these for the time being.\r\n9.2 Confidence Intervals for Means\r\nWe are given X1, X2, . . . , Xn that are an S RS (n) from a norm(mean = µ, sd = σ) distribution,\r\nwhere µ is unknown. We know that we may estimate µ with X, and we have seen that this estimator\r\nis the MLE. But how good is our estimate? We know that\r\nX − µ\r\nσ/ √n\r\n∼ norm(mean = 0, sd = 1). (9.2.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a3c38f2a-3931-44d1-9857-bc6b165849df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ecf7dc76d760d922326809aca2a96995e074f5f9e06c42f4317fb47006493224",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 319
      },
      {
        "segments": [
          {
            "segment_id": "6b52a535-5816-416b-a974-f5070b5fe8ae",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 231,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.2. CONFIDENCE INTERVALS FOR MEANS 215\r\nFor a big probability 1 − α, for instance, 95%, we can calculate the quantile zα/2. Then\r\nIP\r\n\r\n−zα/2 ≤\r\nX − µ\r\nσ/ √n\r\n≤ zα/2\r\n\r\n = 1 − α. (9.2.2)\r\nBut now consider the following string of equivalent inequalities:\r\n−zα/2 ≤\r\nX − µ\r\nσ/ √n\r\n≤ zα/2,\r\n−zα/2\r\n \r\nσ\r\n√\r\nn\r\n!\r\n≤ X − µ ≤ zα/2\r\n \r\nσ\r\n√\r\nn\r\n!\r\n,\r\n−X − zα/2\r\n \r\nσ\r\n√\r\nn\r\n!\r\n≤ −µ ≤ −X + zα/2\r\n \r\nσ\r\n√\r\nn\r\n!\r\n,\r\nX − zα/2\r\n \r\nσ\r\n√\r\nn\r\n!\r\n≤ µ ≤ X + zα/2\r\n \r\nσ\r\n√\r\nn\r\n!\r\n.\r\nThat is,\r\nIP \r\nX − zα/2\r\nσ\r\n√\r\nn\r\n≤ µ ≤ X + zα/2\r\nσ\r\n√\r\nn\r\n!\r\n= 1 − α. (9.2.3)\r\nDefinition 9.10. The interval\r\n\"\r\nX − zα/2\r\nσ\r\n√\r\nn\r\n, X + zα/2\r\nσ\r\n√\r\nn\r\n#\r\n(9.2.4)\r\nis a 100(1 − α)% confidence interval for µ. The quantity 1 − α is called the confidence coefficient.\r\nRemark 9.11. The interval is also sometimes written more compactly as\r\nX ± zα/2\r\nσ\r\n√\r\nn\r\n. (9.2.5)\r\nThe interpretation of confidence intervals is tricky and often mistaken by novices. When I am\r\nteaching the concept “live” during class, I usually ask the students to imagine that my piece of\r\nchalk represents the “unknown” parameter, and I lay it down on the desk in front of me. Once the\r\nchalk has been lain, it is fixed; it does not move. Our goal is to estimate the parameter. For the\r\nestimator I pick up a sheet of loose paper lying nearby. The estimation procedure is to randomly\r\ndrop the piece of paper from above, and observe where it lands. If the piece of paper covers the\r\npiece of chalk, then we are successful – our estimator covers the parameter. If it falls off to one side\r\nor the other, then we are unsuccessful; our interval fails to cover the parameter.\r\nThen I ask them: suppose we were to repeat this procedure hundreds, thousands, millions of\r\ntimes. Suppose we kept track of how many times we covered and how many times we did not.\r\nWhat percentage of the time would we be successful?\r\nIn the demonstration, the parameter corresponds to the chalk, the sheet of paper corresponds\r\nto the confidence interval, and the random experiment corresponds to dropping the sheet of paper.\r\nThe percentage of the time that we are successful exactly corresponds to the confidence coefficient.\r\nThat is, if we use a 95% confidence interval, then we can say that, in the long run, approximately\r\n95% of our intervals will cover the true parameter (which is fixed, but unknown).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6b52a535-5816-416b-a974-f5070b5fe8ae.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63fa28eed8ffdd893631bfc3cea7e913aacc5100d39253c02fac90da345697ec",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 453
      },
      {
        "segments": [
          {
            "segment_id": "1f43a74b-1010-4b39-8072-dcf804ac3f9f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 232,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "216 CHAPTER 9. ESTIMATION\r\n95 100 105\r\n0 10 20 30 40 50\r\nConfidence Interval\r\nIndex\r\nConfidence intervals based on z distribution\r\n| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\r\n| | | | | | | | | | |\r\n| | | | |\r\nFigure 9.2.1: Simulated confidence intervals\r\nThe graph was generated by the ci.examp function from the TeachingDemos package. Fifty (50) samples\r\nof size twenty five (25) were generated from a norm(mean = 100, sd = 10) distribution, and each sample\r\nwas used to find a 95% confidence interval for the population mean using Equation 9.2.5. The 50 confidence\r\nintervals are represented above by horizontal lines, and the respective sample means are denoted by vertical\r\nslashes. Confidence intervals that “cover” the true mean value of 100 are plotted in black; those that fail to\r\ncover are plotted in a lighter color. In the plot we see that only one (1) of the simulated intervals out of the 50\r\nfailed to cover µ = 100, which is a success rate of 98%. If the number of generated samples were to increase\r\nfrom 50 to 500 to 50000, . . . , then we would expect our success rate to approach the exact value of 95%.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1f43a74b-1010-4b39-8072-dcf804ac3f9f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=98ae9bd33e74d8cdeca0ebc0ce0fac76131f3a85516eebc90206ee1265a1f9d3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 234
      },
      {
        "segments": [
          {
            "segment_id": "02fb4c32-3d86-4486-8412-10bd29caa60a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 233,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.2. CONFIDENCE INTERVALS FOR MEANS 217\r\nSee Figure 9.2.1, which is a graphical display of these ideas.\r\nUnder the above framework, we can reason that an “interval” with a larger confidence coef\u0002ficient corresponds to a wider sheet of paper. Furthermore, the width of the confidence interval\r\n(sheet of paper) should be somehow related to the amount of information contained in the random\r\nsample, X1, X2, . . . , Xn. The following remarks makes these notions precise.\r\nRemark 9.12. For a fixed confidence coefficient 1 − α,\r\nif n increases, then the confidence interval gets SHORTER. (9.2.6)\r\nRemark 9.13. For a fixed sample size n,\r\nif 1 − α increases, then the confidence interval gets WIDER. (9.2.7)\r\nExample 9.14. Results from an Experiment on Plant Growth. The PlantGrowth data frame\r\ngives the results of an experiment to measure plant yield (as measured by the weight of the plant).\r\nWe would like to a 95% confidence interval for the mean weight of the plants. Suppose that we\r\nknow from prior research that the true population standard deviation of the plant weights is 0.7 g.\r\nThe parameter of interest is µ, which represents the true mean weight of the population of all\r\nplants of the particular species in the study. We will first take a look at a stemplot of the data:\r\n> library(aplpack)\r\n> with(PlantGrowth, stem.leaf(weight))\r\n1 | 2: represents 1.2\r\nleaf unit: 0.1\r\nn: 30\r\n1 f | 5\r\ns |\r\n2 3. | 8\r\n4 4* | 11\r\n5 t | 3\r\n8 f | 455\r\n10 s | 66\r\n13 4. | 889\r\n(4) 5* | 1111\r\n13 t | 2233\r\n9 f | 555\r\ns |\r\n6 5. | 88\r\n4 6* | 011\r\n1 t | 3\r\nThe data appear to be approximately normal with no extreme values. The data come from a\r\ndesigned experiment, so it is reasonable to suppose that the observations constitute a simple random\r\nsample of weights3. We know the population standard deviation σ = 0.70 from prior research. We\r\nare going to use the one-sample z-interval.\r\n3Actually we will see later that there is reason to believe that the observations are simple random samples from three\r\ndistinct populations. See Section 10.6.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/02fb4c32-3d86-4486-8412-10bd29caa60a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9842afda488d91c68aee9265841deb3b45603c787e24be0bdec8ffab9e1595d0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 370
      },
      {
        "segments": [
          {
            "segment_id": "99cbb814-058b-47f7-afe0-534b2c9a37e8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 234,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "218 CHAPTER 9. ESTIMATION\r\n> dim(PlantGrowth) # sample size is first entry\r\n[1] 30 2\r\n> with(PlantGrowth, mean(weight))\r\n[1] 5.073\r\n> qnorm(0.975)\r\n[1] 1.959964\r\nWe find the sample mean of the data to be x = 5.073 and zα/2 = z0.025 ≈ 1.96. Our interval is\r\ntherefore\r\nx ± zα/2\r\nσ\r\n√\r\nn\r\n= 5.073 ± 1.96 ·\r\n0.70\r\n√\r\n30\r\n,\r\nwhich comes out to approximately [4.823, 5.323]. In conclusion, we are 95% confident that the\r\ntrue mean weight µ of all plants of this species lies somewhere between 4.823 g and 5.323 g, that\r\nis, we are 95% confident that the interval [4.823, 5.323] covers µ. See Figure\r\nExample 9.15. Give some data with X1, X2, . . . , Xn an S RS (n) from a norm(mean = µ, sd = σ)\r\ndistribution. Maybe small sample?\r\n1. What is the parameter of interest? in the context of the problem. Give a point estimate for µ.\r\n2. What are the assumptions being made in the problem? Do they meet the conditions of the\r\ninterval?\r\n3. Calculate the interval.\r\n4. Draw the conclusion.\r\nRemark 9.16. What if σ is unknown? We instead use the interval\r\nX ± zα/2\r\nS\r\n√\r\nn\r\n, (9.2.8)\r\nwhere S is the sample standard deviation.\r\n• If n is large, then X will have an approximately normal distribution regardless of the under\u0002lying population (by the CLT) and S will be very close to the parameter σ (by the SLLN);\r\nthus the above interval will have approximately 100(1 − α)% confidence of covering µ.\r\n• If n is small, then\r\n◦ If the underlying population is normal then we may replace zα/2 with tα/2(df = n − 1).\r\nThe resulting 100(1 − α)% confidence interval is\r\nX ± tα/2(df = n − 1)\r\nS\r\n√\r\nn\r\n(9.2.9)\r\n◦ if the underlying population is not normal, but approximately normal, then we may\r\nuse the t interval, Equation 9.2.9. The interval will have approximately 100(1 − α)%\r\nconfidence of covering µ. However, if the population is highly skewed or the data have\r\noutliers, then we should ask a professional statistician for advice.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/99cbb814-058b-47f7-afe0-534b2c9a37e8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=08ee5f1d032276c7cfba3c7daeb77ea5aea64c4663714d42a06b0fa86ac5bdc6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 355
      },
      {
        "segments": [
          {
            "segment_id": "d0ec6ba8-6f39-42f8-afb7-5ee13155aa02",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 235,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.2. CONFIDENCE INTERVALS FOR MEANS 219\r\n4.8 5.0 5.2 5.4\r\n95% Normal Confidence Limits: σx = 0.128, n = 30\r\nf(z)\r\n0.0\r\n0.5\r\n1.0\r\n1.5\r\n2.0\r\n2.5\r\n3.0\r\n−3 −2 −1 0 1 2 3\r\n0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\ng( x ) = f(( x\r\n−\r\nµ )i σ x) σ x\r\nf(z)\r\n−1.96 1.96\r\nz\r\nz\r\nshaded area\r\nConf Level= 0.9500\r\n4.823 5.323 x\r\nx\r\nµ x 5.073\r\nFigure 9.2.2: Confidence interval plot for the PlantGrowth data\r\nThe shaded portion represents 95% of the total area under the curve, and the upper and lower bounds are the\r\nlimits of the one-sample 95% confidence interval. The graph is centered at the observed sample mean. It was\r\ngenerated by computing a z.test from the TeachingDemos package, storing the resulting htest object, and\r\nplotting it with the normal.and.t.dist function from the HH package. See the remarks in the “How to do it\r\nwith R” discussion later in this section.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d0ec6ba8-6f39-42f8-afb7-5ee13155aa02.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7a2a9ef880fd7cfe122ae8bb01e03a54c33156c96dfda205237abbeacfcf2cf1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 158
      },
      {
        "segments": [
          {
            "segment_id": "5bb4fe9c-724e-4893-9041-3925c6fcc95b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 236,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "220 CHAPTER 9. ESTIMATION\r\nThe author learned of a handy acronym from AP Statistics Exam graders that summarizes the\r\nimportant parts of confidence interval estimation, which is PANIC: Parameter, Assumptions, Name,\r\nInterval, and Conclusion.\r\nParameter: identify the parameter of interest with the proper symbols. Write down what the\r\nparameter means in the context of the problem.\r\nAssumptions: list any assumptions made in the experiment. If there are any other assumptions\r\nneeded or that were not checked, state what they are and why they are important.\r\nName: choose a statistical procedure from your bag of tricks based on the answers to the previous\r\ntwo parts. The assumptions of the procedure you choose should match those of the problem;\r\nif they do not match then either pick a different procedure or openly admit that the results\r\nmay not be reliable. Write down any underlying formulas used.\r\nInterval: calculate the interval from the sample data. This can be done by hand but will more\r\noften be done with the aid of a computer. Regardless of the method, all calculations or code\r\nshould be shown so that the entire process is repeatable by a subsequent reader.\r\nConclusion: state the final results, using language in the context of the problem. Include the\r\nappropriate interpretation of the interval, making reference to the confidence coefficient.\r\nRemark 9.17. All of the above intervals for µ were two-sided, but there are also one-sided intervals\r\nfor µ. They look like\r\n\"\r\nX − zα\r\nσ\r\n√\r\nn\r\n, ∞\r\n!\r\nor −∞, X + zα\r\nσ\r\n√\r\nn\r\n#\r\n(9.2.10)\r\nand satisfy\r\nIP \r\nX − zα\r\nσ\r\n√\r\nn\r\n≤ µ\r\n!\r\n= 1 − α and IP \r\nX + zα\r\nσ\r\n√\r\nn\r\n≥ µ\r\n!\r\n= 1 − α. (9.2.11)\r\nExample 9.18. Small sample, some data with X1, X2, . . . , Xn an S RS (n) from a norm(mean =\r\nµ, sd = σ) distribution.\r\n1. PANIC\r\n9.2.1 How to do it with R\r\nWe can do Example 9.14 with the following code.\r\n> library(TeachingDemos)\r\n> temp <- with(PlantGrowth, z.test(weight, stdev = 0.7))\r\n> temp\r\nOne Sample z-test\r\ndata: weight\r\nz = 39.6942, n = 30.000, Std. Dev. = 0.700, Std. Dev. of the\r\nsample mean = 0.128, p-value < 2.2e-16\r\nalternative hypothesis: true mean is not equal to 0\r\n95 percent confidence interval:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5bb4fe9c-724e-4893-9041-3925c6fcc95b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2e2ec14245d2f99827d74d6d95005b94b93ddcd94602f22e8ee0f73726eb4b59",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 388
      },
      {
        "segments": [
          {
            "segment_id": "10397d19-5369-4c06-bd3c-a4c7b5543573",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 237,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.3. CONFIDENCE INTERVALS FOR DIFFERENCES OF MEANS 221\r\n4.822513 5.323487\r\nsample estimates:\r\nmean of weight\r\n5.073\r\nThe confidence interval bounds are shown in the sixth line down of the output (please disregard\r\nall of the additional output information for now – we will use it in Chapter 10). We can make the\r\nplot for Figure 9.2.2 with\r\n> library(IPSUR)\r\n> plot(temp, \"Conf\")\r\n9.3 Confidence Intervals for Differences of Means\r\nLet X1, X2, . . . , Xn be a S RS (n) from a norm(mean = µX, sd = σX) distribution and let Y1, Y2, . . . ,\r\nYm be a S RS (m) from a norm(mean = µY , sd = σY ) distribution. Further, assume that the X1, X2,\r\n. . . , Xn sample is independent of the Y1, Y2, . . . , Ym sample.\r\nSuppose that σX and σY are known. We would like a confidence interval for µX − µY . We know\r\nthat\r\nX − Y ∼ norm\r\n\r\n\r\nmean = µX − µY , sd =\r\ns\r\nσ\r\n2\r\nX\r\nn\r\n+\r\nσ\r\n2\r\nY\r\nm\r\n\r\n\r\n. (9.3.1)\r\nTherefore, a 100(1 − α)% confidence interval for µX − µY is given by\r\n\u0010\r\nX − Y\r\n\u0011\r\n± zα/2\r\ns\r\nσ\r\n2\r\nX\r\nn\r\n+\r\nσ\r\n2\r\nY\r\nm\r\n. (9.3.2)\r\nUnfortunately, most of the time the values of σX and σY are unknown. This leads us to the follow\u0002ing:\r\n• If both sample sizes are large, then we may appeal to the CLT/SLLN (see 8.3) and substitute\r\nS\r\n2\r\nX\r\nand S\r\n2\r\nY\r\nfor σ\r\n2\r\nX\r\nand σ\r\n2\r\nY\r\nin the interval 9.3.2. The resulting confidence interval will have\r\napproximately 100(1 − α)% confidence.\r\n• If one or more of the sample sizes is small then we are in trouble, unless\r\n◦ the underlying populations are both normal and σX = σY . In this case (setting σ =\r\nσX = σY ),\r\nX − Y ∼ norm\r\n\r\n\r\nmean = µX − µY , sd = σ\r\nr\r\n1\r\nn\r\n+\r\n1\r\nm\r\n\r\n\r\n. (9.3.3)\r\nNow let\r\nU =\r\nn − 1\r\nσ2\r\nS\r\n2\r\nX +\r\nm − 1\r\nσ2\r\nS\r\n2\r\nY\r\n. (9.3.4)\r\nThen by Exercise 7.2 we know that U ∼ chisq(df = n + m − 2) and is not a large leap\r\nto believe that U is independent of X − Y; thus\r\nT =\r\nZ\r\n√\r\nU/ (n + m − 2)\r\n∼ t(df = n + m − 2). (9.3.5)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/10397d19-5369-4c06-bd3c-a4c7b5543573.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f7d88b4f76514bdae5db02eff4afde54a88750920629771bad368894d171f390",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 428
      },
      {
        "segments": [
          {
            "segment_id": "e237f7c3-dfe2-49e6-8806-1fd3791dce36",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 238,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "222 CHAPTER 9. ESTIMATION\r\nBut\r\nT =\r\nX−Y−(µX−µY )\r\nσ\r\n√1\r\nn\r\n+\r\n1\r\nm q\r\nn−1\r\nσ2 S\r\n2\r\nX\r\n+\r\nm−1\r\nσ2 S\r\n2\r\nY\r\n.\r\n(n + m − 2)\r\n,\r\n=\r\nX − Y − (µX − µY )\r\nr\u0010\r\n1\r\nn\r\n+\r\n1\r\nm\r\n\u0011\r\n\u0012\r\n(n−1)S\r\n2\r\nX\r\n+(m−1)S\r\n2\r\nY\r\nn+m−2\r\n\u0013\r\n,\r\n∼ t(df = n + m − 2).\r\nTherefore a 100(1 − α)% confidence interval for µX − µY is given by\r\n\u0010\r\nX − Y\r\n\u0011\r\n± tα/2(df = n + m − 2) S p\r\nr\r\n1\r\nn\r\n+\r\n1\r\nm\r\n, (9.3.6)\r\nwhere\r\nS p =\r\ns\r\n(n − 1)S\r\n2\r\nX\r\n+ (m − 1)S\r\n2\r\nY\r\nn + m − 2\r\n(9.3.7)\r\nis called the “pooled” estimator of σ.\r\n◦ if one of the samples is small, and both underlying populations are normal, but σX ,\r\nσY , then we may use a Welch (or Satterthwaite) approximation to the degrees of free\u0002dom. See Welch [88], Satterthwaite [76], or Neter et al [67]. The idea is to use an\r\ninterval of the form\r\n\u0010\r\nX − Y\r\n\u0011\r\n± tα/2(df = r)\r\ns\r\nS\r\n2\r\nX\r\nn\r\n+\r\nS\r\n2\r\nY\r\nm\r\n, (9.3.8)\r\nwhere the degrees of freedom r is chosen so that the interval has nice statistical proper\u0002ties. It turns out that a good choice for r is given by\r\nr =\r\n\u0010\r\nS\r\n2\r\nX\r\n/n + S\r\n2\r\nY\r\n/m\r\n\u00112\r\n1\r\nn−1\r\n\u0010\r\nS\r\n2\r\nX\r\n/n\r\n\u00112\r\n+\r\n1\r\nm−1\r\n\u0010\r\nS\r\n2\r\nY\r\n/m\r\n\u00112\r\n, (9.3.9)\r\nwhere we understand that r is rounded down to the nearest integer. The resulting inter\u0002val has approximately 100(1 − α)% confidence.\r\n9.3.1 How to do it with R\r\nThe basic function is t.test which has a var.equal argument that may be set to TRUE or FALSE.\r\nThe confidence interval is shown as part of the output, although there is a lot of additional informa\u0002tion that is not needed until Chapter 10.\r\nThere is not any specific functionality to handle the z-interval for small samples, but if the\r\nsamples are large then t.test with var.equal = FALSE will be essentially the same thing. The\r\nstandard deviations are never (?) known in advance anyway so it does not really matter in practice.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e237f7c3-dfe2-49e6-8806-1fd3791dce36.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c7a243e1677de6aeb2bdd462c5dda7487d7b1a1a2bd8f9d0e39fb1d75186a117",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 388
      },
      {
        "segments": [
          {
            "segment_id": "a5be2904-31f3-4fdb-b0c2-4a9b016c7c98",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 239,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.4. CONFIDENCE INTERVALS FOR PROPORTIONS 223\r\n9.4 Confidence Intervals for Proportions\r\nWe would like to know p which is the “proportion of successes”. For instance, p could be:\r\n• the proportion of U.S. citizens that support Obama,\r\n• the proportion of smokers among adults age 18 or over,\r\n• the proportion of people worldwide infected by the H1N1 virus.\r\nWe are given an S RS (n) X1, X2, . . . , Xn distributed binom(size = 1, prob = p). Recall from\r\nSection 5.3 that the common mean of these variables is IE X = p and the variance is IE(X − p)\r\n2 =\r\np(1 − p). If we let Y =\r\nP\r\nXi, then from Section 5.3 we know that Y ∼ binom(size = n, prob = p)\r\nand that\r\nX =\r\nY\r\nn\r\nhas IE X = p and Var(X) =\r\np(1 − p)\r\nn\r\n.\r\nThus if n is large (here is the CLT) then an approximate 100(1 − α)% confidence interval for p\r\nwould be given by\r\nX ± zα/2\r\nr\r\np(1 − p)\r\nn\r\n. (9.4.1)\r\nOOPS. . . ! Equation 9.4.1 is of no use to us because the unknown parameter p is in the formula!\r\n(If we knew what p was to plug in the formula then we would not need a confidence interval in the\r\nfirst place.) There are two solutions to this problem.\r\n1. Replace p with ˆp = X. Then an approximate 100(1 − α)% confidence interval for p is given\r\nby\r\npˆ ± zα/2\r\nr\r\npˆ(1 − pˆ)\r\nn\r\n. (9.4.2)\r\nThis approach is called the Wald interval and is also known as the asymptotic interval because\r\nit appeals to the CLT for large sample sizes.\r\n2. Go back to first principles. Note that\r\n−zα/2 ≤\r\nY/n − p\r\np\r\np(1 − p)/n\r\n≤ zα/2\r\nexactly when the function f defined by\r\nf(p) = (Y/n − p)\r\n2 − z2\r\nα/2\r\np(1 − p)\r\nn\r\nsatisfies f(p) ≤ 0. But f is quadratic in p so its graph is a parabola; it has two roots, and\r\nthese roots form the limits of the confidence interval. We can find them with the quadratic\r\nformula (see Exercise 9.2):\r\n\r\n\r\n\r\n\r\npˆ +\r\nz\r\n2\r\nα/2\r\n2n\r\n\r\n\r\n± zα/2\r\ns\r\npˆ(1 − pˆ)\r\nn\r\n+\r\nz\r\n2\r\nα/2\r\n(2n)\r\n2\r\n\r\n\r\n, \r\n\r\n1 +\r\nz\r\n2\r\nα/2\r\nn\r\n\r\n\r\n(9.4.3)\r\nThis approach is called the score interval because it is based on the inversion of the “Score\r\ntest”. See Chapter 14. It is also known as the Wilson interval; see Agresti [3].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a5be2904-31f3-4fdb-b0c2-4a9b016c7c98.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eeea1b0bf303270a431b518c9088a74ed33c645171796afcac00b594a1d76941",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 439
      },
      {
        "segments": [
          {
            "segment_id": "1cc8b929-0bdd-4d99-8ebd-23d5450b3151",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 240,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "224 CHAPTER 9. ESTIMATION\r\nFor two proportions p1 and p2, we may collect independent binom(size = 1, prob = p) samples\r\nof size n1 and n2, respectively. Let Y1 and Y2 denote the number of successes in the respective\r\nsamples.\r\nWe know that\r\nY1\r\nn1\r\n≈ norm\r\n\r\n\r\nmean = p1, sd =\r\nr\r\np1(1 − p1)\r\nn1\r\n\r\n\r\nand\r\nY2\r\nn2\r\n≈ norm\r\n\r\n\r\nmean = p2, sd =\r\nr\r\np2(1 − p2)\r\nn2\r\n\r\n\r\nso it stands to reason that an approximate 100(1 − α)% confidence interval for p1 − p2 is given by\r\n(pˆ1 − pˆ2) ± zα/2\r\nr\r\npˆ1(1 − pˆ1)\r\nn1\r\n+\r\npˆ2(1 − pˆ2)\r\nn2\r\n, (9.4.4)\r\nwhere ˆp1 = Y1/n1 and ˆp2 = Y2/n2.\r\nRemark 9.19. When estimating a single proportion, one-sided intervals are sometimes needed.\r\nThey take the form\r\n\r\n\r\n0, pˆ + zα/2\r\nr\r\npˆ(1 − pˆ)\r\nn\r\n\r\n\r\n(9.4.5)\r\nor\r\n\r\n\r\npˆ − zα/2\r\nr\r\npˆ(1 − pˆ)\r\nn\r\n, 1\r\n\r\n\r\n(9.4.6)\r\nor in other words, we know in advance that the true proportion is restricted to the interval [0, 1], so\r\nwe can truncate our confidence interval to those values on either side.\r\n9.4.1 How to do it with R\r\n> library(Hmisc)\r\n> binconf(x = 7, n = 25, method = \"asymptotic\")\r\nPointEst Lower Upper\r\n0.28 0.1039957 0.4560043\r\n> binconf(x = 7, n = 25, method = \"wilson\")\r\nPointEst Lower Upper\r\n0.28 0.1428385 0.4757661\r\nThe default value of the method argument is wilson.\r\nAn alternate way is\r\n> tab <- xtabs(~gender, data = RcmdrTestDrive)\r\n> prop.test(rbind(tab), conf.level = 0.95, correct = FALSE)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1cc8b929-0bdd-4d99-8ebd-23d5450b3151.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5aaf400d666f706aed97f622c1cc44edb4fe3119e95c461c65bcea7d93c80924",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "16cc8ff0-5941-48bc-9cc6-87d4f6d3a9a7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 241,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.5. CONFIDENCE INTERVALS FOR VARIANCES 225\r\n1-sample proportions test without continuity correction\r\ndata: rbind(tab), null probability 0.5\r\nX-squared = 2.881, df = 1, p-value = 0.08963\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n0.4898844 0.6381406\r\nsample estimates:\r\np\r\n0.5654762\r\n> A <- as.data.frame(Titanic)\r\n> library(reshape)\r\n> B <- with(A, untable(A, Freq))\r\n9.5 Confidence Intervals for Variances\r\nI am thinking one and two sample problems here.\r\n9.5.1 How to do it with R\r\nI am thinking about sigma.test in the TeachingDemos package and var.test in base R here.\r\n9.6 Fitting Distributions\r\n9.6.1 How to do it with R\r\nI am thinking about fitdistr from the MASS package [84].\r\n9.7 Sample Size and Margin of Error\r\nSections 9.2 through 9.5 all began the same way: we were given the sample size n and the confi\u0002dence coefficient 1 − α, and our task was to find a margin of error E so that\r\nθˆ ± E is a 100(1 − α)% confidence interval for θ.\r\nSome examples we saw were:\r\n• E = zα/2σ/ √\r\nn, in the one-sample z-interval,\r\n• E = tα/2(df = n + m − 2)S p\r\n√\r\nn\r\n−1 + m−1\r\n, in the two-sample pooled t-interval.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/16cc8ff0-5941-48bc-9cc6-87d4f6d3a9a7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63b756f765094d5765b4ac46cce9f54b153a3bfe7b72cf9a428e02c7338f7d42",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 477
      },
      {
        "segments": [
          {
            "segment_id": "f1e360cd-78c1-4b9e-8fcd-49e738a1bcea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 242,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "226 CHAPTER 9. ESTIMATION\r\nWe already know (we can see in the formulas above) that E decreases as n increases. Now we\r\nwould like to use this information to our advantage: suppose that we have a fixed margin of error\r\nE, say E = 3, and we want a 100(1 − α)% confidence interval for µ. The question is: how big does\r\nn have to be?\r\nFor the case of a population mean the answer is easy: we set up an equation and solve for n.\r\nExample 9.20. Given a situation, given σ, given E, we would like to know how big n has to be to\r\nensure that X ± 5 is a 95% confidence interval for µ.\r\nRemark 9.21.\r\n1. Always round up any decimal values of n, no matter how small the decimal is.\r\n2. Another name for E is the “maximum error of the estimate”.\r\nFor proportions, recall that the asymptotic formula to estimate p was\r\npˆ ± zα/2\r\nr\r\npˆ(1 − pˆ)\r\nn\r\n.\r\nReasoning as above we would want\r\nE = zα/2\r\nr\r\npˆ(1 − pˆ)\r\nn\r\n, or (9.7.1)\r\nn = z\r\n2\r\nα/2\r\npˆ(1 − pˆ)\r\nE2\r\n. (9.7.2)\r\nOOPS! Recall that ˆp = Y/n, which would put the variable n on both sides of Equation 9.7.2. Again,\r\nthere are two solutions to the problem.\r\n1. If we have a good idea of what p is, say p\r\n∗\r\nthen we can plug it in to get\r\nn = z\r\n2\r\nα/2\r\np\r\n∗\r\n(1 − p\r\n∗\r\n)\r\nE2\r\n. (9.7.3)\r\n2. Even if we have no idea what p is, we do know from calculus that p(1− p) ≤ 1/4 because the\r\nfunction f(x) = x(1− x) is quadratic (so its graph is a parabola which opens downward) with\r\nmaximum value attained at x = 1/2. Therefore, regardless of our choice for p\r\n∗\r\nthe sample\r\nsize must satisfy\r\nn = z\r\n2\r\nα/2\r\np\r\n∗\r\n(1 − p\r\n∗\r\n)\r\nE2\r\n≤\r\nz\r\n2\r\nα/2\r\n4E2\r\n. (9.7.4)\r\nThe quantity z\r\n2\r\nα/2\r\n/4E\r\n2\r\nis large enough to guarantee 100(1 − α)% confidence.\r\nExample 9.22. Proportion example\r\nRemark 9.23. For very small populations sometimes the value of n obtained from the formula is\r\ntoo big. In this case we should use the hypergeometric distribution for a sampling model rather than\r\nthe binomial model. With this modification the formulas change to the following: if N denotes the\r\npopulation size then let\r\nm = z\r\n2\r\nα/2\r\np\r\n∗\r\n(1 − p\r\n∗\r\n)\r\nE2\r\n(9.7.5)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f1e360cd-78c1-4b9e-8fcd-49e738a1bcea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c88460fbbef666016db8234d14fdca70a99073ca7dbf17b44962179a13209d95",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "2fca5aa8-e283-4ab1-a794-a24d84f4cbed",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 243,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "9.8. OTHER TOPICS 227\r\nand the sample size needed to ensure 100(1 − α)% confidence is achieved is\r\nn =\r\nm\r\n1 +\r\nm−1\r\nN\r\n. (9.7.6)\r\nIf we do not have a good value for the estimate p\r\n∗\r\nthen we may use p\r\n∗ = 1/2.\r\n9.7.1 How to do it with R\r\nI am thinking about power.t.test, power.prop.test, power.anova.test, and I am also thinking about\r\nreplicate.\r\n9.8 Other Topics\r\nMention mle from the stats4 package.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/2fca5aa8-e283-4ab1-a794-a24d84f4cbed.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=12cb1de8b48a8fdd9fffea52a8285653aadcfc5081f530d50bd4b39a6d0e0eea",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 504
      },
      {
        "segments": [
          {
            "segment_id": "d56032ca-b843-4356-9203-41220b95b048",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 244,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "228 CHAPTER 9. ESTIMATION\r\nChapter Exercises\r\nExercise 9.1. Let X1, X2, . . . , Xn be an S RS (n) from a norm(mean = µ, sd = σ) distribution. Find\r\na two-dimensional MLE for θ = (µ, σ).\r\nExercise 9.2. Find the upper and lower limits for the confidence interval procedure by finding the\r\nroots of f defined by\r\nf(p) = (Y/n − p)\r\n2 − z2\r\nα/2\r\np(1 − p)\r\nn\r\n.\r\nYou are going to need the quadratic formula.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d56032ca-b843-4356-9203-41220b95b048.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6e12557837af9e8a664174405db2053cb21274b14527da954944eb57802d58ed",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "022b81f8-6e06-4438-8a36-d1f323f4fcf4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 245,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 10\r\nHypothesis Testing\r\nWhat do I want them to know?\r\n• basic terminology and philosophy of the Neyman-Pearson paradigm\r\n• classical hypothesis tests for the standard one and two sample problems with means, vari\u0002ances, and proportions\r\n• the notion of between versus within group variation and how it plays out with one-way\r\nANOVA\r\n• the concept of statistical power and its relation to sample size\r\n10.1 Introduction\r\nI spent a week during the summer of 2005 at the University of Nebraska at Lincoln grading Ad\u0002vanced Placement Statistics exams, and while I was there I attended a presentation by Dr. Roxy\r\nPeck. At the end of her talk she described an activity she had used with students to introduce the\r\nbasic concepts of hypothesis testing. I was impressed by the activity and have used it in my own\r\nclasses several times since.\r\nThe instructor (with a box of cookies in hand) enters a class of fifteen or more students and\r\nproduces a brand-new, sealed deck of ordinary playing cards. The instructor asks for a student\r\nvolunteer to break the seal, and then the instructor prominently shuffles the deck1several times in\r\nfront of the class, after which time the students are asked to line up in a row. They are going to\r\nplay a game. Each student will draw a card from the top of the deck, in turn. If the card is black,\r\nthen the lucky student will get a cookie. If the card is red, then the unlucky student will sit down\r\nempty-handed. Let the game begin.\r\nThe first student draws a card: red. There are jeers and outbursts, and the student slinks off to\r\nhis/her chair. (S)he is disappointed, of course, but not really. After all, (s)he had a 50-50 chance of\r\ngetting black, and it did not happen. Oh well.\r\n1The jokers are removed before shuffling.\r\n229",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/022b81f8-6e06-4438-8a36-d1f323f4fcf4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb122c68654c2d567761f274d0b7427e9d8d6d3fc1895b6b5b87b77d9254823d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 392
      },
      {
        "segments": [
          {
            "segment_id": "38eedcc9-c429-40e3-8102-cb40d4b04fd7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 246,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "230 CHAPTER 10. HYPOTHESIS TESTING\r\nThe second student draws a card: red, again. There are more jeers, and the second student\r\nslips away. This student is also disappointed, but again, not so much, because it is probably his/her\r\nunlucky day. On to the next student.\r\nThe student draws: red again! There are a few wiseguys who yell (happy to make noise, more\r\nthan anything else), but there are a few other students who are not yelling any more – they are\r\nthinking. This is the third red in a row, which is possible, of course, but what is going on, here?\r\nThey are not quite sure. They are now concentrating on the next card. . . it is bound to be black,\r\nright?\r\nThe fourth student draws: red. Hmmm. . . now there are groans instead of outbursts. A few of\r\nthe students at the end of the line shrug their shoulders and start to make their way back to their\r\ndesk, complaining that the teacher does not want to give away any cookies. There are still some\r\nstudents in line though, salivating, waiting for the inevitable black to appear.\r\nThe fifth student draws red. Now it isn’t funny any more. As the remaining students make their\r\nway back to their seats an uproar ensues, from an entire classroom demanding cookies.\r\nKeep the preceding experiment in the back of your mind as you read the following sections.\r\nWhen you have finished the entire chapter, come back and read this introduction again. All of the\r\nmathematical jargon that follows is connected to the above paragraphs. In the meantime, I will get\r\nyou started:\r\nNull hypothesis: it is an ordinary deck of playing cards, shuffled thoroughly.\r\nAlternative hypothesis: either it is a trick deck of cards, or the instructor did some fancy shuffle\u0002work.\r\nObserved data: a sequence of draws from the deck, five reds in a row.\r\nIf it were truly an ordinary, well-shuffled deck of cards, the probability of observing zero blacks\r\nout of a sample of size five (without replacement) from a deck with 26 black cards and 26 red cards\r\nwould be\r\n> dhyper(0, m = 26, n = 26, k = 5)\r\n[1] 0.02531012\r\nThere are two very important final thoughts. First, everybody gets a cookie in the end. Second,\r\nthe students invariably (and aggressively) attempt to get me to open up the deck and reveal the true\r\nnature of the cards. I never do.\r\n10.2 Tests for Proportions\r\nExample 10.1. We have a machine that makes widgets.\r\n• Under normal operation, about 0.10 of the widgets produced are defective.\r\n• Go out and purchase a torque converter.\r\n• Install the torque converter, and observe n = 100 widgets from the machine.\r\n• Let Y = number of defective widgets observed.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/38eedcc9-c429-40e3-8102-cb40d4b04fd7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e25a22bba8fe590803358b156056c077ac4ab5b1bf2b3adba715063178342ec1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 463
      },
      {
        "segments": [
          {
            "segment_id": "2d6d360b-4a17-4a28-81a0-0e4d494155be",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 247,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.2. TESTS FOR PROPORTIONS 231\r\nIf\r\n• Y = 0, then the torque converter is great!\r\n• Y = 4, then the torque converter seems to be helping.\r\n• Y = 9, then there is not much evidence that the torque converter helps.\r\n• Y = 17, then throw away the torque converter.\r\nLet p denote the proportion of defectives produced by the machine. Before the installation of the\r\ntorque converter p was 0.10. Then we installed the torque converter. Did p change? Did it go up or\r\ndown? We use statistics to decide. Our method is to observe data and construct a 95% confidence\r\ninterval for p,\r\npˆ ± zα/2\r\nr\r\npˆ(1 − pˆ)\r\nn\r\n. (10.2.1)\r\nIf the confidence interval is\r\n• [0.01, 0.05], then we are 95% confident that 0.01 ≤ p ≤ 0.05, so there is evidence that the\r\ntorque converter is helping.\r\n• [0.15, 0.19], then we are 95% confident that 0.15 ≤ p ≤ 0.19, so there is evidence that the\r\ntorque converter is hurting.\r\n• [0.07, 0.11], then there is not enough evidence to conclude that the torque converter is doing\r\nanything at all, positive or negative.\r\n10.2.1 Terminology\r\nThe null hypothesis H0 is a “nothing” hypothesis, whose interpretation could be that nothing has\r\nchanged, there is no difference, there is nothing special taking place, etc.. In Example 10.1 the\r\nnull hypothesis would be H0 : p = 0.10. The alternative hypothesis H1 is the hypothesis that\r\nsomething has changed, in this case, H1 : p , 0.10. Our goal is to statistically test the hypothesis\r\nH0 : p = 0.10 versus the alternative H1 : p , 0.10. Our procedure will be:\r\n1. Go out and collect some data, in particular, a simple random sample of observations from the\r\nmachine.\r\n2. Suppose that H0 is true and construct a 100(1 − α)% confidence interval for p.\r\n3. If the confidence interval does not cover p = 0.10, then we rejectH0. Otherwise, we fail to\r\nrejectH0.\r\nRemark 10.2. Every time we make a decision it is possible to be wrong, and there are two possible\r\nmistakes that we could make. We have committed a\r\nType I Error if we reject H0 when in fact H0 is true. This would be akin to convicting an innocent\r\nperson for a crime (s)he did not commit.\r\nType II Error if we fail to reject H0 when in fact H1 is true. This is analogous to a guilty person\r\nescaping conviction.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/2d6d360b-4a17-4a28-81a0-0e4d494155be.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e853bc6d03677114309d7d173d3468d0050f2614c81a321d9c9d11087a937d46",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 414
      },
      {
        "segments": [
          {
            "segment_id": "b5a19ef4-7628-4b74-9a1e-2e4993fdac66",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 248,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "232 CHAPTER 10. HYPOTHESIS TESTING\r\nType I Errors are usually considered worse2, and we design our statistical procedures to control the\r\nprobability of making such a mistake. We define the\r\nsignificance level of the test = IP(Type I Error) = α. (10.2.2)\r\nWe want α to be small which conventionally means, say, α = 0.05, α = 0.01, or α = 0.005 (but\r\ncould mean anything, in principle).\r\n• The rejection region (also known as the critical region) for the test is the set of sample values\r\nwhich would result in the rejection of H0. For Example 10.1, the rejection region would be\r\nall possible samples that result in a 95% confidence interval that does not cover p = 0.10.\r\n• The above example with H1 : p , 0.10 is called a two-sided test. Many times we are\r\ninterested in a one-sided test, which would look like H1 : p < 0.10 or H1 : p > 0.10.\r\nWe are ready for tests of hypotheses for one proportion.\r\nTable here.\r\nDon’t forget the assumptions.\r\nExample 10.3. Find\r\n1. The null and alternative hypotheses\r\n2. Check your assumptions.\r\n3. Define a critical region with an α = 0.05 significance level.\r\n4. Calculate the value of the test statistic and state your conclusion.\r\nExample 10.4. Suppose p = the proportion of students who are admitted to the graduate school\r\nof the University of California at Berkeley, and suppose that a public relations officer boasts that\r\nUCB has historically had a 40% acceptance rate for its graduate school. Consider the data stored in\r\nthe table UCBAdmissions from 1973. Assuming these observations constituted a simple random\r\nsample, are they consistent with the officer’s claim, or do they provide evidence that the acceptance\r\nrate was significantly less than 40%? Use an α = 0.01 significance level.\r\nOur null hypothesis in this problem is H0 : p = 0.4 and the alternative hypothesis is H1 : p <\r\n0.4. We reject the null hypothesis if ˆp is too small, that is, if\r\npˆ − 0.4\r\n√\r\n0.4(1 − 0.4)/n\r\n< −zα, (10.2.3)\r\nwhere α = 0.01 and −z0.01 is\r\n> -qnorm(0.99)\r\n[1] -2.326348\r\n2There is no mathematical difference between the errors, however. The bottom line is that we choose one type of\r\nerror to control with an iron fist, and we try to minimize the probability of making the other type. That being said, null\r\nhypotheses are often by design to correspond to the “simpler” model, so it is often easier to analyze (and thereby control)\r\nthe probabilities associated with Type I Errors.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b5a19ef4-7628-4b74-9a1e-2e4993fdac66.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=472317e4b4e55bc47e11d6261483bc768e9be6bbd42ccae70817ab1c07435f12",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 428
      },
      {
        "segments": [
          {
            "segment_id": "b96ff5d8-0ce9-4490-9634-a729546c4993",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 249,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.2. TESTS FOR PROPORTIONS 233\r\nOur only remaining task is to find the value of the test statistic and see where it falls relative to\r\nthe critical value. We can find the number of people admitted and not admitted to the UCB graduate\r\nschool with the following.\r\n> A <- as.data.frame(UCBAdmissions)\r\n> head(A)\r\nAdmit Gender Dept Freq\r\n1 Admitted Male A 512\r\n2 Rejected Male A 313\r\n3 Admitted Female A 89\r\n4 Rejected Female A 19\r\n5 Admitted Male B 353\r\n6 Rejected Male B 207\r\n> xtabs(Freq ~ Admit, data = A)\r\nAdmit\r\nAdmitted Rejected\r\n1755 2771\r\nNow we calculate the value of the test statistic.\r\n> phat <- 1755/(1755 + 2771)\r\n> (phat - 0.4)/sqrt(0.4 * 0.6/(1755 + 2771))\r\n[1] -1.680919\r\nOur test statistic is not less than −2.32, so it does not fall into the critical region. Therefore, we\r\nfail to reject the null hypothesis that the true proportion of students admitted to graduate school is\r\nless than 40% and say that the observed data are consistent with the officer’s claim at the α = 0.01\r\nsignificance level.\r\nExample 10.5. We are going to do Example 10.4 all over again. Everything will be exactly the\r\nsame except for one change. Suppose we choose significance level α = 0.05 instead of α = 0.01.\r\nAre the 1973 data consistent with the officer’s claim?\r\nOur null and alternative hypotheses are the same. Our observed test statistic is the same: it was\r\napproximately −1.68. But notice that our critical value has changed: α = 0.05 and −z0.05 is\r\n> -qnorm(0.95)\r\n[1] -1.644854\r\nOur test statistic is less than −1.64 so it now falls into the critical region! We now reject the null\r\nhypothesis and conclude that the 1973 data provide evidence that the true proportion of students\r\nadmitted to the graduate school of UCB in 1973 was significantly less than 40%. The data are not\r\nconsistent with the officer’s claim at the α = 0.05 significance level.\r\nWhat is going on, here? If we choose α = 0.05 then we reject the null hypothesis, but if we\r\nchoose α = 0.01 then we fail to reject the null hypothesis. Our final conclusion seems to depend\r\non our selection of the significance level. This is bad; for a particular test, we never know whether\r\nour conclusion would have been different if we had chosen a different significance level.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b96ff5d8-0ce9-4490-9634-a729546c4993.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5e36d2ad5017e13121335d6c1e9e9b5fb46593db79c8e9dca2a477f33ca125b5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 398
      },
      {
        "segments": [
          {
            "segment_id": "162c25a9-5d6d-4a2b-8b89-78193899d2b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 250,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "234 CHAPTER 10. HYPOTHESIS TESTING\r\nOr do we?\r\nClearly, for some significance levels we reject, and for some significance levels we do not.\r\nWhere is the boundary? That is, what is the significance level for which we would reject at any sig\u0002nificance level bigger, and we would fail to reject at any significance level smaller? This boundary\r\nvalue has a special name: it is called the p-value of the test.\r\nDefinition 10.6. The p-value, or observed significance level, of a hypothesis test is the probability\r\nwhen the null hypothesis is true of obtaining the observed value of the test statistic (such as ˆp) or\r\nvalues more extreme – meaning, in the direction of the alternative hypothesis3.\r\nExample 10.7. Calculate the p-value for the test in Examples 10.4 and 10.5.\r\nThe p-value for this test is the probability of obtaining a z-score equal to our observed test\r\nstatistic (which had z-score ≈ −1.680919) or more extreme, which in this example is less than the\r\nobserved test statistic. In other words, we want to know the area under a standard normal curve on\r\nthe interval (−∞, −1.680919]. We can get this easily with\r\n> pnorm(-1.680919)\r\n[1] 0.04638932\r\nWe see that the p-value is strictly between the significance levels α = 0.01 and α = 0.05.\r\nThis makes sense: it has to be bigger than α = 0.01 (otherwise we would have rejected H0 in\r\nExample 10.4) and it must also be smaller than α = 0.05 (otherwise we would not have rejected\r\nH0 in Example 10.5). Indeed, p-values are a characteristic indicator of whether or not we would\r\nhave rejected at assorted significance levels, and for this reason a statistician will often skip the\r\ncalculation of critical regions and critical values entirely. If (s)he knows the p-value, then (s)he\r\nknows immediately whether or not (s)he would have rejected at any given significance level.\r\nThus, another way to phrase our significance test procedure is: we will reject H0 at the α-level\r\nof significance if the p-value is less than α.\r\nRemark 10.8. If we have two populations with proportions p1 and p2 then we can test the null\r\nhypothesis H0 : p1 = p2.\r\nTable Here.\r\nExample 10.9. Example.\r\n10.2.2 How to do it with R\r\nThe following does the test.\r\n> prop.test(1755, 1755 + 2771, p = 0.4, alternative = \"less\",\r\n+ conf.level = 0.99, correct = FALSE)\r\n1-sample proportions test without continuity correction\r\ndata: 1755 out of 1755 + 2771, null probability 0.4\r\nX-squared = 2.8255, df = 1, p-value = 0.04639\r\n3Bickel and Doksum [7] state the definition particularly well: the p-value is “the smallest level of significance α at which\r\nan experimenter using [the test statistic] T would reject [H0] on the basis of the observed [sample] outcome x”.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/162c25a9-5d6d-4a2b-8b89-78193899d2b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9e6363cfb8b592a1cd49e601196f8ae164b6864152150c34302df96c9573dbe0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 460
      },
      {
        "segments": [
          {
            "segment_id": "8a90fc09-2251-43fa-b503-6ac1a5a18895",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 251,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.3. ONE SAMPLE TESTS FOR MEANS AND VARIANCES 235\r\nalternative hypothesis: true p is less than 0.4\r\n99 percent confidence interval:\r\n0.0000000 0.4047326\r\nsample estimates:\r\np\r\n0.3877596\r\nDo the following to make the plot.\r\n> library(IPSUR)\r\n> library(HH)\r\n> temp <- prop.test(1755, 1755 + 2771, p = 0.4, alternative = \"less\",\r\n+ conf.level = 0.99, correct = FALSE)\r\n> plot(temp, \"Hypoth\")\r\nUse Yates’ continuity correction when the expected frequency of successes is less than 10. You\r\ncan use it all of the time, but you will have a decrease in power. For large samples the correction\r\ndoes not matter.\r\nWith the R Commander If you already know the number of successes and failures, then you\r\ncan use the menu Statistics . Proportions . IPSUR Enter table for single sample. . .\r\nOtherwise, your data – the raw successes and failures – should be in a column of the Active\r\nData Set. Furthermore, the data must be stored as a “factor” internally. If the data are not a factor\r\nbut are numeric then you can use the menu Data . Manage variables in active data set . Convert\r\nnumeric variables to factors. . . to convert the variable to a factor. Or, you can always use the\r\nfactor function.\r\nOnce your unsummarized data is a column, then you can use the menu Statistics . Proportions\r\n. Single-sample proportion test. . .\r\n10.3 One Sample Tests for Means and Variances\r\n10.3.1 For Means\r\nHere, X1, X2, . . . , Xn are a S RS (n) from a norm(mean = µ, sd = σ) distribution. We would like to\r\ntest H0 : µ = µ0.\r\nCase A: Suppose σ is known. Then under H0,\r\nZ =\r\nX − µ0\r\nσ/ √n\r\n∼ norm(mean = 0, sd = 1).\r\nTable here.\r\nCase B: When σ is unknown, under H0\r\nT =\r\nX − µ0\r\nS/\r\n√\r\nn\r\n∼ t(df = n − 1).\r\nTable here.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8a90fc09-2251-43fa-b503-6ac1a5a18895.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=12597e302dfa80ba639eaef7f26a584a41d2f80a785aa623ba5b7c5fca24135d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 322
      },
      {
        "segments": [
          {
            "segment_id": "be82fe4a-1fe6-44fd-a5b7-e1e5d0792993",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 252,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "236 CHAPTER 10. HYPOTHESIS TESTING\r\n0.38 0.39 0.40 0.41 0.42\r\nnormal density: σx = 0.007, n = 1\r\nf(z)\r\n0\r\n10\r\n20\r\n30\r\n40\r\n50\r\n−3 −2 −1 0 1 2 3\r\n0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\ng( x ) = f(( x\r\n−\r\nµ )i σ x) σ x\r\nf(z)\r\n−2.326\r\nz\r\nz\r\nshaded area\r\nα = 0.0100\r\n0.383 x\r\nx\r\nµ x 0.388 0.4\r\nz −1.681 p = 0.0464\r\nFigure 10.2.1: Hypothesis test plot based on normal.and.t.dist from the HH package\r\nThis plot shows all of the important features of hypothesis tests in one magnificent display. The (asymptotic)\r\ndistribution of the test statistic (under the null hypothesis) is standard normal, represented by the bell curve,\r\nabove. We see the critical region to the left, and the blue shaded area is the significance level, which for this\r\nexample is α = 0.05. The area outlined in green is the p-value, and the observed test statistic determines the\r\nupper bound of this region. We can see clearly that the p-value is larger than the significance level, thus, we\r\nwould not reject the null hypothesis. There are all sorts of tick marks shown below the graph which detail how\r\nthe different pieces are measured on different scales (the original data scale, the standardized scale, etc.). The\r\nworkhorse behind the plot is the normal.and.t.dist function from the HH package. See the discussion\r\nin “How to do it with R” for the exact sequence of commands to generate the plot.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/be82fe4a-1fe6-44fd-a5b7-e1e5d0792993.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9899946b81b76c16a96f48864d6ed35a40074560fd350f98588ad2d15ff4cb59",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 247
      },
      {
        "segments": [
          {
            "segment_id": "132e5d61-fa5e-40a4-81e4-c8f5a8d5f756",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 253,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.3. ONE SAMPLE TESTS FOR MEANS AND VARIANCES 237\r\nRemark 10.10. If σ is unknown but n is large then we can use the z-test.\r\nExample 10.11. In this example we\r\n1. Find the null and alternative hypotheses.\r\n2. Choose a test and find the critical region.\r\n3. Calculate the value of the test statistic and state the conclusion.\r\n4. Find the p-value.\r\nRemark 10.12. Remarks\r\n• p-values are also known as tail end probabilities. We reject H0 when the p-value is small.\r\n• σ/ √\r\nn when σ is known, is called the standard error of the sample mean. In general, if we\r\nhave an estimator θˆ then σθˆ is called the standard error of θˆ. We usually need to estimate σθˆ\r\nwith ˆσθˆ .\r\n10.3.2 How to do it with R\r\nI am thinking z.test in TeachingDemos, t.test in base R.\r\n> x <- rnorm(37, mean = 2, sd = 3)\r\n> library(TeachingDemos)\r\n> z.test(x, mu = 1, sd = 3, conf.level = 0.9)\r\nOne Sample z-test\r\ndata: x\r\nz = 2.8126, n = 37.000, Std. Dev. = 3.000, Std. Dev. of the sample\r\nmean = 0.493, p-value = 0.004914\r\nalternative hypothesis: true mean is not equal to 1\r\n90 percent confidence interval:\r\n1.575948 3.198422\r\nsample estimates:\r\nmean of x\r\n2.387185\r\nThe RcmdrPlugin.IPSUR package does not have a menu for z.test yet.\r\n> x <- rnorm(13, mean = 2, sd = 3)\r\n> t.test(x, mu = 0, conf.level = 0.9, alternative = \"greater\")\r\nOne Sample t-test\r\ndata: x\r\nt = 1.2949, df = 12, p-value = 0.1099\r\nalternative hypothesis: true mean is greater than 0\r\n90 percent confidence interval:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/132e5d61-fa5e-40a4-81e4-c8f5a8d5f756.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=56d03b0986d43f726ae08dfbd475377455d9ca27680b3fe3fac57daec5ff1653",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "62ed8619-3728-4f6c-bbd9-24bbe26c1bb6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 254,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "238 CHAPTER 10. HYPOTHESIS TESTING\r\n0.38 0.39 0.40 0.41 0.42\r\nnormal density: σx = 0.007, n = 1\r\nf(z)\r\n0\r\n10\r\n20\r\n30\r\n40\r\n50\r\n−3 −2 −1 0 1 2 3\r\n0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\ng( x ) = f(( x\r\n−\r\nµ )i σ x) σ x\r\nf(z)\r\n−2.326\r\nz\r\nz\r\nshaded area\r\nα = 0.0100\r\n0.383 x\r\nx\r\nµ x 0.388 0.4\r\nz −1.681 p = 0.0464\r\nFigure 10.3.1: Hypothesis test plot based on normal.and.t.dist from the HH package\r\nThis plot shows the important features of hypothesis tests.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/62ed8619-3728-4f6c-bbd9-24bbe26c1bb6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=10cc5df77a77490f0aac62e9511dc3e3ebd3437e61c7c1178c8be10ab6bf97f1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 363
      },
      {
        "segments": [
          {
            "segment_id": "f17d1f86-81cb-499c-bf3d-732b98c14824",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 255,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.4. TWO-SAMPLE TESTS FOR MEANS AND VARIANCES 239\r\n-0.05064006 Inf\r\nsample estimates:\r\nmean of x\r\n1.068850\r\nWith the R Commander Your data should be in a single numeric column (a variable) of the\r\nActive Data Set. Use the menu Statistics . Means . Single-sample t-test. . .\r\n10.3.3 Tests for a Variance\r\nHere, X1, X2, . . . , Xn are a S RS (n) from a norm(mean = µ, sd = σ) distribution. We would like to\r\ntest H0 : σ\r\n2 = σ0. We know that under H0,\r\nX\r\n2 =\r\n(n − 1)S\r\n2\r\nσ2\r\n∼ chisq(df = n − 1).\r\nTable here.\r\nExample 10.13. Give some data and a hypothesis.\r\n1. Give an α-level and test the critical region way.\r\n2. Find the p-value for the test.\r\n10.3.4 How to do it with R\r\nI am thinking about sigma.test in the TeachingDemos package.\r\n> library(TeachingDemos)\r\n> sigma.test(women$height, sigma = 8)\r\nOne sample Chi-squared test for variance\r\ndata: women$height\r\nX-squared = 4.375, df = 14, p-value = 0.01449\r\nalternative hypothesis: true variance is not equal to 64\r\n95 percent confidence interval:\r\n10.72019 49.74483\r\nsample estimates:\r\nvar of women$height\r\n20\r\n10.4 Two-Sample Tests for Means and Variances\r\nThe basic idea for this section is the following. We have X ∼ norm(mean = µX, sd = σX) and\r\nY ∼ norm(mean = µY , sd = σY ). distributed independently. We would like to know whether X\r\nand Y come from the same population distribution, that is, we would like to know:\r\nDoes X\r\nd\r\n= Y? (10.4.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f17d1f86-81cb-499c-bf3d-732b98c14824.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7b2fb81d4c18df9d21f8ed4a638b494626dedb18b15fa4475bc8a8e041035097",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 259
      },
      {
        "segments": [
          {
            "segment_id": "beed326c-9d1a-48da-be2f-b52c9483aed6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 256,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "240 CHAPTER 10. HYPOTHESIS TESTING\r\nwhere the symbol d= means equality of probability distributions.\r\nSince both X and Y are normal, we may rephrase the question:\r\nDoes µX = µY and σX = σY ? (10.4.2)\r\nSuppose first that we do not know the values of σX and σY , but we know that they are equal,\r\nσX = σY . Our test would then simplify to H0 : µX = µY . We collect data X1, X2, . . . , Xn and Y1,\r\nY2, . . . , Ym, both simple random samples of size n and m from their respective normal distributions.\r\nThen under H0 (that is, assuming H0 is true) we have µX = µY or rewriting, µX − µY = 0, so\r\nT =\r\nX − Y\r\nS p\r\nq\r\n1\r\nn\r\n+\r\n1\r\nm\r\n=\r\nX − Y − (µX − µY )\r\nS p\r\nq\r\n1\r\nn\r\n+\r\n1\r\nm\r\n∼ t(df = n + m − 2). (10.4.3)\r\n10.4.1 Independent Samples\r\nRemark 10.14. If the values of σX and σY are known, then we can plug them in to our statistic:\r\nZ =\r\nX − Y\r\nq\r\nσ\r\n2\r\nX\r\n/n + σ\r\n2\r\nY\r\n/m\r\n; (10.4.4)\r\nthe result will have a norm(mean = 0, sd = 1) distribution when H0 : µX = µY is true.\r\nRemark 10.15. Even if the values of σX and σY are not known, if both n and m are large then we\r\ncan plug in the sample estimates and the result will have approximately a norm(mean = 0, sd = 1)\r\ndistribution when H0 : µX = µY is true.\r\nZ =\r\nX − Y\r\nq\r\nS\r\n2\r\nX\r\n/n + S\r\n2\r\nY\r\n/m\r\n. (10.4.5)\r\nRemark 10.16. It is usually important to construct side-by-side boxplots and other visual displays\r\nin concert with the hypothesis test. This gives a visual comparison of the samples and helps to\r\nidentify departures from the test’s assumptions – such as outliers.\r\nRemark 10.17. WATCH YOUR ASSUMPTIONS.\r\n• The normality assumption can be relaxed as long as the population distributions are not\r\nhighly skewed.\r\n• The equal variance assumption can be relaxed as long as both sample sizes n and m are large.\r\nHowever, if one (or both) samples is small, then the test does not perform well; we should\r\ninstead use the methods of Chapter 13.\r\nFor a nonparametric alternative to the two-sample F test see Chapter 15.\r\n10.4.2 Paired Samples\r\n10.4.3 How to do it with R\r\n> t.test(extra ~ group, data = sleep, paired = TRUE)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/beed326c-9d1a-48da-be2f-b52c9483aed6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=754cc7e810bf3baee3d97e3d796fb3361231b459fb6586ae4398da76557039df",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 431
      },
      {
        "segments": [
          {
            "segment_id": "c8af0005-70ef-42aa-91fc-e5dede7afc57",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 257,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.5. OTHER HYPOTHESIS TESTS 241\r\nPaired t-test\r\ndata: extra by group\r\nt = -4.0621, df = 9, p-value = 0.002833\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n-2.4598858 -0.7001142\r\nsample estimates:\r\nmean of the differences\r\n-1.58\r\n10.5 Other Hypothesis Tests\r\n10.5.1 Kolmogorov-Smirnov Goodness-of-Fit Test\r\n10.5.2 How to do it with R\r\n> ks.test(randu$x, \"punif\")\r\nOne-sample Kolmogorov-Smirnov test\r\ndata: randu$x\r\nD = 0.0555, p-value = 0.1697\r\nalternative hypothesis: two-sided\r\n10.5.3 Shapiro-Wilk Normality Test\r\n10.5.4 How to do it with R\r\n> shapiro.test(women$height)\r\nShapiro-Wilk normality test\r\ndata: women$height\r\nW = 0.9636, p-value = 0.7545\r\n10.6 Analysis of Variance\r\n10.6.1 How to do it with R\r\nI am thinking\r\n> with(chickwts, by(weight, feed, shapiro.test))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c8af0005-70ef-42aa-91fc-e5dede7afc57.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b4d377140c22a58a102a280f6dd1874a5c3a35ca356447414bd8b3a7001ee3ae",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "add1e8b8-5ae4-4c1a-8f42-8bb312683cac",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 258,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "242 CHAPTER 10. HYPOTHESIS TESTING\r\nfeed: casein\r\nShapiro-Wilk normality test\r\ndata: dd[x, ]\r\nW = 0.9166, p-value = 0.2592\r\n--------------------------------------------------------\r\nfeed: horsebean\r\nShapiro-Wilk normality test\r\ndata: dd[x, ]\r\nW = 0.9376, p-value = 0.5265\r\n--------------------------------------------------------\r\nfeed: linseed\r\nShapiro-Wilk normality test\r\ndata: dd[x, ]\r\nW = 0.9693, p-value = 0.9035\r\n--------------------------------------------------------\r\nfeed: meatmeal\r\nShapiro-Wilk normality test\r\ndata: dd[x, ]\r\nW = 0.9791, p-value = 0.9612\r\n--------------------------------------------------------\r\nfeed: soybean\r\nShapiro-Wilk normality test\r\ndata: dd[x, ]\r\nW = 0.9464, p-value = 0.5064\r\n--------------------------------------------------------\r\nfeed: sunflower\r\nShapiro-Wilk normality test\r\ndata: dd[x, ]\r\nW = 0.9281, p-value = 0.3603",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/add1e8b8-5ae4-4c1a-8f42-8bb312683cac.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f82f77edbebf038fd0f27faa8d2148a7a313da0d77e83257e3a23b0249702657",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "3f5523d6-60d9-42c9-b6bd-abcb2e2dda2d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 259,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.7. SAMPLE SIZE AND POWER 243\r\nand\r\n> temp <- lm(weight ~ feed, data = chickwts)\r\nand\r\n> anova(temp)\r\nAnalysis of Variance Table\r\nResponse: weight\r\nDf Sum Sq Mean Sq F value Pr(>F)\r\nfeed 5 231129 46226 15.365 5.936e-10 ***\r\nResiduals 65 195556 3009\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nPlot for the intuition of between versus within group variation.\r\nPlots for the hypothesis tests:\r\n10.7 Sample Size and Power\r\nThe power function of a test for a parameter θ is\r\nβ(θ) = IP\r\nθ\r\n(Reject H0), −∞ < θ < ∞.\r\nHere are some properties of power functions:\r\n1. β(θ) ≤ α for any θ ∈ Θ0, and β(θ0) = α. We interpret this by saying that no matter what value\r\nθ takes inside the null parameter space, there is never more than a chance of α of rejecting\r\nthe null hypothesis. We have controlled the Type I error rate to be no greater than α.\r\n2. limn→∞ β(θ) = 1 for any fixed θ ∈ Θ1. In other words, as the sample size grows without\r\nbound we are able to detect a nonnull value of θ with increasing accuracy, no matter how\r\nclose it lies to the null parameter space. This may appear to be a good thing at first glance,\r\nbut it often turns out to be a curse. For another interpretation is that our Type II error rate\r\ngrows as the sample size increases.\r\n10.7.1 How to do it with R\r\nI am thinking about replicate here, and also power.examp from the TeachingDemos package.\r\nThere is an even better plot in upcoming work from the HH package.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3f5523d6-60d9-42c9-b6bd-abcb2e2dda2d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=96a7f95adc18de4347632c37f58f9349e2ebb79167c9de067181002de3d46016",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 493
      },
      {
        "segments": [
          {
            "segment_id": "24841d02-1db3-4506-aeb7-1a6eec0f2313",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 260,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "244 CHAPTER 10. HYPOTHESIS TESTING\r\n0 5 10 15 20 25\r\n0.0 0.1 0.2 0.3 0.4\r\nIndex\r\ny1\r\nFigure 10.6.1: Between group versus within group variation",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/24841d02-1db3-4506-aeb7-1a6eec0f2313.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ee7ec21864c877376c3d9dbff2456bfa27d144bd41a6da599a182a952f8dc9c3",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "eb7a2731-64d1-42cb-9576-d91f8bfa6cc5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 261,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.7. SAMPLE SIZE AND POWER 245\r\nHistogram of y2\r\ny2\r\nDensity\r\n2 3 4 5 6 7\r\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\r\nFigure 10.6.2: Between group versus within group variation",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/eb7a2731-64d1-42cb-9576-d91f8bfa6cc5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b3156141c454528574244a896722a2b57c6b0d512fd23c9e2f881e8df0af8b17",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "34a9b7b5-3eaf-440d-9e5e-fd8d9582c6bd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 262,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "246 CHAPTER 10. HYPOTHESIS TESTING\r\n0 1 2 3 4 5\r\nF density : ν1 = 5 ν2 = 30\r\nF density\r\n0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\nf\r\nf 2.534\r\nshaded area\r\n0.05\r\n3\r\nf 3\r\nFigure 10.6.3: Some F plots from the HH package",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/34a9b7b5-3eaf-440d-9e5e-fd8d9582c6bd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cdb7ba7e2ba2a57ad40d6d1ca8a771270c0f35e29007b36ba66b3dd99d7a116c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "9469d311-363f-4219-8905-4409cace871a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 263,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "10.7. SAMPLE SIZE AND POWER 247\r\n−2 −1 0 1 2 3 4\r\n0.0 0.3\r\nNull Distribution\r\nx\r\n−−> rejection region\r\n1.644854\r\nα\r\n−2 −1 0 1 2 3 4\r\n0.0 0.3\r\nAlternative Distribution\r\nx\r\n−−> rejection region\r\n1.644854\r\nPower\r\nse = 1.00 z* = 1.64 power = 0.26 \r\n n = 1 sd = 1.00 diff = 1.00 alpha = 0.050\r\nFigure 10.7.1: Plot of significance level and power\r\nThis graph was generated by the power.examp function from the TeachingDemos package. The plot corre\u0002sponds to the hypothesis test H0 : µ = µ0 versus H1 : µ = µ1 (where µ0 = 0 and µ1 = 1, by default) based on\r\na single observation X ∼ norm(mean = µ, sd = σ). The top graph is of the H0 density while the bottom is of\r\nthe H1 density. The significance level is set at α = 0.05, the sample size is n = 1, and the standard deviation\r\nis σ = 1. The pink area is the significance level, and the critical value z0.05 ≈ 1.645 is marked at the left\r\nboundary – this defines the rejection region. When H0 is true, the probability of falling in the rejection region\r\nis exactly α = 0.05. The same rejection region is marked on the bottom graph, and the probability of falling in\r\nit (when H1 is true) is the blue area shown at the top of the display to be approximately 0.26. This probability\r\nrepresents the power to detect a non-null mean value of µ = 1. With the command the run.power.examp()\r\nat the command line the same plot opens, but in addition, there are sliders available that allow the user to inter\u0002actively change the sample size n, the standard deviation σ, the true difference between the means µ1 −µ0, and\r\nthe significance level α. By playing around the student can investigate the effect each of the aforementioned\r\nparameters has on the statistical power. Note that you need the tkrplot package for run.power.examp.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9469d311-363f-4219-8905-4409cace871a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b9836b8c8c979dac781b1f5cbdaae5ccde50acc7fc07c9e65802953bccf92839",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "4d246460-b730-4d73-af2b-52a0aab5cfb8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 264,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "248 CHAPTER 10. HYPOTHESIS TESTING\r\nChapter Exercises",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4d246460-b730-4d73-af2b-52a0aab5cfb8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d8dfcddc29d4793167c9a34d641a771a6dc504d46353ddc9ab5d3af325e3207b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 442
      },
      {
        "segments": [
          {
            "segment_id": "3e3b3896-2f1e-4acd-b9d8-ec871ef1e0be",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 265,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 11\r\nSimple Linear Regression\r\nWhat do I want them to know?\r\n• basic philosophy of SLR and the regression assumptions\r\n• point and interval estimation of the model parameters, and how to use it to make predictions\r\n• point and interval estimation of future observations from the model\r\n• regression diagnostics, including R\r\n2\r\nand basic residual analysis\r\n• the concept of influential versus outlying observations, and how to tell the difference\r\n11.1 Basic Philosophy\r\nHere we have two variables X and Y. For our purposes, X is not random (so we will write x), but Y\r\nis random. We believe that Y depends in some way on x. Some typical examples of (x, Y) pairs are\r\n• x = study time and Y = score on a test.\r\n• x = height and Y = weight.\r\n• x = smoking frequency and Y = age of first heart attack.\r\nGiven information about the relationship between x and Y, we would like to predict future values\r\nof Y for particular values of x. This turns out to be a difficult problem1, so instead we first tackle\r\nan easier problem: we estimate IE Y. How can we accomplish this? Well, we know that Y depends\r\nsomehow on x, so it stands to reason that\r\nIE Y = µ(x), a function of x. (11.1.1)\r\nBut we should be able to say more than that. To focus our efforts we impose some structure on the\r\nfunctional form of µ. For instance,\r\n• if µ(x) = β0 + β1 x, we try to estimate β0 and β1.\r\n1Yogi Berra once said, “It is always difficult to make predictions, especially about the future.”\r\n249",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3e3b3896-2f1e-4acd-b9d8-ec871ef1e0be.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ae72781aa49cd749767d28c548e536e5f14ccf9d81dc7b9d87625d3893d85bc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 280
      },
      {
        "segments": [
          {
            "segment_id": "89c69853-484e-4c86-809d-f44f97267b0f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 266,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "250 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\n• if µ(x) = β0 + β1 x + β2 x\r\n2\r\n, we try to estimate β0, β1, and β2.\r\n• if µ(x) = β0e\r\nβ1 x\r\n, we try to estimate β0 and β1.\r\nThis helps us in the sense that we concentrate on the estimation of just a few parameters, β0 and\r\nβ1, say, rather than some nebulous function. Our modus operandi is simply to perform the random\r\nexperiment n times and observe the n ordered pairs of data (x1, Y1), (x2, Y2), . . . ,(xn, Yn). We use\r\nthese n data points to estimate the parameters.\r\nMore to the point, there are three simple linear regression (SLR) assumptions that will form the\r\nbasis for the rest of this chapter:\r\nAssumption 11.1. We assume that µ is a linear function of x, that is,\r\nµ(x) = β0 + β1 x, (11.1.2)\r\nwhere β0 and β1 are unknown constants to be estimated.\r\nAssumption 11.2. We further assume that Yiis µ(xi) – the “signal” – plus some “error” (repre\u0002sented by the symbol \u000fi):\r\nYi = β0 + β1 xi + \u000fi, i = 1, 2, . . . , n. (11.1.3)\r\nAssumption 11.3. We lastly assume that the errors are i.i.d. normal with mean 0 and variance σ\r\n2\r\n:\r\n\u000f1, \u000f2, . . . , \u000fn ∼ norm(mean = 0, sd = σ). (11.1.4)\r\nRemark 11.4. We assume both the normality of the errors \u000f and the linearity of the mean function\r\nµ. Recall from Proposition 7.27 of Chapter 7 that if (X, Y) ∼ mvnorm then the mean of Y|x is a\r\nlinear function of x. This is not a coincidence. In more advanced classes we study the case that\r\nboth X and Y are random, and in particular, when they are jointly normally distributed.\r\nWhat does it all mean?\r\nSee Figure 11.1.1. Shown in the figure is a solid line, the regression line µ, which in this display\r\nhas slope 0.5 and y-intercept 2.5, that is, µ(x) = 2.5+0.5x. The intuition is that for each given value\r\nof x, we observe a random value of Y which is normally distributed with a mean equal to the height\r\nof the regression line at that x value. Normal densities are superimposed on the plot to drive this\r\npoint home; in principle, the densities stand outside of the page, perpendicular to the plane of the\r\npaper. The figure shows three such values of x, namely, x = 1, x = 2.5, and x = 4. Not only do we\r\nassume that the observations at the three locations are independent, but we also assume that their\r\ndistributions have the same spread. In mathematical terms this means that the normal densities all\r\nalong the line have identical standard deviations – there is no “fanning out” or “scrunching in” of\r\nthe normal densities as x increases2.\r\nExample 11.5. Speed and stopping distance of cars. We will use the data frame cars from the\r\ndatasets package. It has two variables: speed and dist. We can take a look at some of the\r\nvalues in the data frame:\r\n2\r\nIn practical terms, this constant variance assumption is often violated, in that we often observe scatterplots that fan out\r\nfrom the line as x gets large or small. We say under those circumstances that the data show heteroscedasticity. There are\r\nmethods to address it, but they fall outside the realm of SLR.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/89c69853-484e-4c86-809d-f44f97267b0f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=285ac27f0dfbbf28806ea4881dba60f00b0abce8f0454fb46535557063ebe050",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 574
      },
      {
        "segments": [
          {
            "segment_id": "89c69853-484e-4c86-809d-f44f97267b0f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 266,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "250 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\n• if µ(x) = β0 + β1 x + β2 x\r\n2\r\n, we try to estimate β0, β1, and β2.\r\n• if µ(x) = β0e\r\nβ1 x\r\n, we try to estimate β0 and β1.\r\nThis helps us in the sense that we concentrate on the estimation of just a few parameters, β0 and\r\nβ1, say, rather than some nebulous function. Our modus operandi is simply to perform the random\r\nexperiment n times and observe the n ordered pairs of data (x1, Y1), (x2, Y2), . . . ,(xn, Yn). We use\r\nthese n data points to estimate the parameters.\r\nMore to the point, there are three simple linear regression (SLR) assumptions that will form the\r\nbasis for the rest of this chapter:\r\nAssumption 11.1. We assume that µ is a linear function of x, that is,\r\nµ(x) = β0 + β1 x, (11.1.2)\r\nwhere β0 and β1 are unknown constants to be estimated.\r\nAssumption 11.2. We further assume that Yiis µ(xi) – the “signal” – plus some “error” (repre\u0002sented by the symbol \u000fi):\r\nYi = β0 + β1 xi + \u000fi, i = 1, 2, . . . , n. (11.1.3)\r\nAssumption 11.3. We lastly assume that the errors are i.i.d. normal with mean 0 and variance σ\r\n2\r\n:\r\n\u000f1, \u000f2, . . . , \u000fn ∼ norm(mean = 0, sd = σ). (11.1.4)\r\nRemark 11.4. We assume both the normality of the errors \u000f and the linearity of the mean function\r\nµ. Recall from Proposition 7.27 of Chapter 7 that if (X, Y) ∼ mvnorm then the mean of Y|x is a\r\nlinear function of x. This is not a coincidence. In more advanced classes we study the case that\r\nboth X and Y are random, and in particular, when they are jointly normally distributed.\r\nWhat does it all mean?\r\nSee Figure 11.1.1. Shown in the figure is a solid line, the regression line µ, which in this display\r\nhas slope 0.5 and y-intercept 2.5, that is, µ(x) = 2.5+0.5x. The intuition is that for each given value\r\nof x, we observe a random value of Y which is normally distributed with a mean equal to the height\r\nof the regression line at that x value. Normal densities are superimposed on the plot to drive this\r\npoint home; in principle, the densities stand outside of the page, perpendicular to the plane of the\r\npaper. The figure shows three such values of x, namely, x = 1, x = 2.5, and x = 4. Not only do we\r\nassume that the observations at the three locations are independent, but we also assume that their\r\ndistributions have the same spread. In mathematical terms this means that the normal densities all\r\nalong the line have identical standard deviations – there is no “fanning out” or “scrunching in” of\r\nthe normal densities as x increases2.\r\nExample 11.5. Speed and stopping distance of cars. We will use the data frame cars from the\r\ndatasets package. It has two variables: speed and dist. We can take a look at some of the\r\nvalues in the data frame:\r\n2\r\nIn practical terms, this constant variance assumption is often violated, in that we often observe scatterplots that fan out\r\nfrom the line as x gets large or small. We say under those circumstances that the data show heteroscedasticity. There are\r\nmethods to address it, but they fall outside the realm of SLR.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/89c69853-484e-4c86-809d-f44f97267b0f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=285ac27f0dfbbf28806ea4881dba60f00b0abce8f0454fb46535557063ebe050",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 574
      },
      {
        "segments": [
          {
            "segment_id": "f30778b7-c6a8-4e7b-bd94-dd2fd547fa7d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 267,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.1. BASIC PHILOSOPHY 251\r\n0 1 2 3 4 5\r\n0 1 2\r\n3\r\n4\r\n5\r\n6\r\nx\r\ny\r\nFigure 11.1.1: Philosophical foundations of SLR",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f30778b7-c6a8-4e7b-bd94-dd2fd547fa7d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d2a67f747900a664b692836843cdaac47c7065341848cb0db48f6ed165221e02",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "57486f72-2259-4059-a899-46f90d15edce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 268,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "252 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n5 10 15 20 25\r\n0 20 40 60 80 100 120\r\nspeed\r\ndist\r\nFigure 11.1.2: Scatterplot of dist versus speed for the cars data\r\n> head(cars)\r\nspeed dist\r\n1 4 2\r\n2 4 10\r\n3 7 4\r\n4 7 22\r\n5 8 16\r\n6 9 10\r\nThe speed represents how fast the car was going (x) in miles per hour and dist (Y) measures\r\nhow far it took the car to stop, in feet. We can make a simple scatterplot of the data with the\r\ncommand plot(dist ~speed, data = cars).\r\nYou can see the output in Figure 11.1.2, which was produced by the following code.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/57486f72-2259-4059-a899-46f90d15edce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9afb4301a88a8a269d8ee5526065bc7506f208a2b4cadd108ee8e3e42e4f10c0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 186
      },
      {
        "segments": [
          {
            "segment_id": "c7c9bec5-8688-4df3-b446-6a8ff8ad474f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 269,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.2. ESTIMATION 253\r\n> plot(dist ~ speed, data = cars)\r\nThere is a pronounced upward trend to the data points, and the pattern looks approximately\r\nlinear. There does not appear to be substantial fanning out of the points or extreme values.\r\n11.2 Estimation\r\n11.2.1 Point Estimates of the Parameters\r\nWhere is µ(x)? In essence, we would like to “fit” a line to the points. But how do we determine a\r\n“good” line? Is there a best line? We will use maximum likelihood to find it. We know:\r\nYi = β0 + β1 xi + \u000fi, i = 1, . . . , n, (11.2.1)\r\nwhere the \u000fi’s are i.i.d. norm(mean = 0, sd = σ). Thus Yi ∼ norm(mean = β0 + β1 xi, sd = σ), i =\r\n1, . . . , n. Furthermore, Y1, . . . , Yn are independent – but not identically distributed. The likelihood\r\nfunction is:\r\nL(β0, β1, σ) =\r\nYn\r\ni=1\r\nfYi(yi), (11.2.2)\r\n=\r\nYn\r\ni=1\r\n(2πσ2)\r\n−1/2\r\nexp (\r\n−(yi − β0 − β1 xi)\r\n2\r\n2σ2\r\n)\r\n, (11.2.3)\r\n=(2πσ2)\r\n−n/2\r\nexp (\r\n−\r\nPn\r\ni=1\r\n(yi − β0 − β1 xi)\r\n2\r\n2σ2\r\n)\r\n. (11.2.4)\r\nWe take the natural logarithm to get\r\nln L(β0, β1, σ) = −\r\nn\r\n2\r\nln(2πσ2) −\r\nPn\r\ni=1\r\n(yi − β0 − β1 xi)\r\n2\r\n2σ2\r\n. (11.2.5)\r\nWe would like to maximize this function of β0 and β1. See Appendix E.6 which tells us that we\r\nshould find critical points by means of the partial derivatives. Let us start by differentiating with\r\nrespect to β0:\r\n∂\r\n∂β0\r\nln L = 0 −\r\n1\r\n2σ2\r\nXn\r\ni=1\r\n2(yi − β0 − β1 xi)(−1), (11.2.6)\r\nand the partial derivative equals zero when Pn\r\ni=1\r\n(yi − β0 − β1 xi) = 0, that is, when\r\nnβ0 + β1\r\nXn\r\ni=1\r\nxi =\r\nXn\r\ni=1\r\nyi. (11.2.7)\r\nMoving on, we next take the partial derivative of ln L (Equation 11.2.5) with respect to β1 to get\r\n∂\r\n∂β1\r\nln L = 0 −\r\n1\r\n2σ2\r\nXn\r\ni=1\r\n2(yi − β0 − β1 xi)(−xi), (11.2.8)\r\n=\r\n1\r\nσ2\r\nXn\r\ni=1\r\n\u0010\r\nxiyi − β0 xi − β1 x\r\n2\r\ni\r\n\u0011\r\n, (11.2.9)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c7c9bec5-8688-4df3-b446-6a8ff8ad474f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2396b24a8173edc1ba009a5b2faa9e7f89942c656f2ea14b1190ddff1e545c99",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 368
      },
      {
        "segments": [
          {
            "segment_id": "cef977f3-2836-442f-8a83-3327a1f3d5ae",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 270,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "254 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nand this equals zero when the last sum equals zero, that is, when\r\nβ0\r\nXn\r\ni=1\r\nxi + β1\r\nXn\r\ni=1\r\nx\r\n2\r\ni =\r\nXn\r\ni=1\r\nxiyi. (11.2.10)\r\nSolving the system of equations 11.2.7 and 11.2.10\r\nnβ0 + β1\r\nXn\r\ni=1\r\nxi =\r\nXn\r\ni=1\r\nyi (11.2.11)\r\nβ0\r\nXn\r\ni=1\r\nxi + β1\r\nXn\r\ni=1\r\nx\r\n2\r\ni =\r\nXn\r\ni=1\r\nxiyi (11.2.12)\r\nfor β0 and β1 (in Exercise 11.2) gives\r\nβˆ\r\n1 =\r\nPn\r\ni=1\r\nxiyi −\r\n\u0010Pn\r\ni=1\r\nxi\r\n\u0011 \u0010Pn\r\ni=1\r\nyi\r\n\u0011. n\r\nPn\r\ni=1\r\nx\r\n2\r\ni\r\n−\r\n\u0010Pn\r\ni=1\r\nxi\r\n\u00112\r\n\u001e\r\nn\r\n(11.2.13)\r\nand\r\nβˆ\r\n0 = y − βˆ\r\n1 x. (11.2.14)\r\nThe conclusion? To estimate the mean line\r\nµ(x) = β0 + β1 x, (11.2.15)\r\nwe use the “line of best fit”\r\nµˆ(x) = βˆ\r\n0 + βˆ1 x, (11.2.16)\r\nwhere βˆ\r\n0 and βˆ1 are given as above. For notation we will usually write b0 = βˆ0 and b1 = βˆ1 so that\r\nµˆ(x) = b0 + b1 x.\r\nRemark 11.6. The formula for b1 in Equation 11.2.13 gets the job done but does not really make\r\nany sense. There are many equivalent formulas for b1 that are more intuitive, or at the least are\r\neasier to remember. One of the author’s favorites is\r\nb1 = r\r\nsy\r\nsx\r\n, (11.2.17)\r\nwhere r, sy, and sx are the sample correlation coefficient and the sample standard deviations of the\r\nY and x data, respectively. See Exercise 11.3. Also, notice the similarity between Equation 11.2.17\r\nand Equation 7.6.7.\r\nHow to do it with R\r\nHere we go. R will calculate the linear regression line with the lm function. We will store the result\r\nin an object which we will call cars.lm. Here is how it works:\r\n> cars.lm <- lm(dist ~ speed, data = cars)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/cef977f3-2836-442f-8a83-3327a1f3d5ae.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e57b3ba78540d850eb5b3bdee656da1dc63e4d06b800950ba111ef6b30b6a1e6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 310
      },
      {
        "segments": [
          {
            "segment_id": "4a5b3a0c-5a85-4a1f-9750-3e2faaf37d48",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 271,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.2. ESTIMATION 255\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n5 10 15 20 25\r\n0 20 40 60 80 100 120\r\nspeed\r\ndist\r\nFigure 11.2.1: Scatterplot with added regression line for the cars data\r\nThe first part of the input to the lm function, dist~speed, is a model formula, read as “dist is\r\ndescribed by speed”. The data = cars argument tells R where to look for the variables quoted in\r\nthe model formula. The output object cars.lm contains a multitude of information. Let’s first take\r\na look at the coefficients of the fitted regression line, which are extracted by the coef function3:\r\n> coef(cars.lm)\r\n(Intercept) speed\r\n-17.579095 3.932409\r\nThe parameter estimates b0 and b1 for the intercept and slope, respectively, are shown above.\r\nThe regression line is thus given by ˆµ(speed) = -17.58 + 3.93speed.\r\nIt is good practice to visually inspect the data with the regression line added to the plot. To do\r\nthis we first scatterplot the original data and then follow with a call to the abline function. The\r\ninputs to abline are the coefficients of cars.lm (see Figure 11.2.1):\r\n3Alternatively, we could just type cars.lm to see the same thing.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4a5b3a0c-5a85-4a1f-9750-3e2faaf37d48.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1a8c30e9a7822c949c7c778e5624aaa69c789c4401f78e3fcad3e9f978fa5fca",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 237
      },
      {
        "segments": [
          {
            "segment_id": "0bb8a86c-2a86-4af3-b188-6441d311ba71",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 272,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "256 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\n> plot(dist ~ speed, data = cars, pch = 16)\r\n> abline(coef(cars))\r\nTo calculate points on the regression line we may simply plug the desired x value(s) into ˆµ,\r\neither by hand, or with the predict function. The inputs to predict are the fitted linear model\r\nobject, cars.lm, and the desired x value(s) represented by a data frame. See the example below.\r\nExample 11.7. Using the regression line for the cars data:\r\n1. What is the meaning of µ(60) = β0 + β1(8)?\r\nThis represents the average stopping distance (in feet) for a car going 8 mph.\r\n2. Interpret the slope β1.\r\nThe true slope β1 represents the increase in average stopping distance for each mile per\r\nhour faster that the car drives. In this case, we estimate the car to take approximately 3.93\r\nadditional feet to stop for each additional mph increase in speed.\r\n3. Interpret the intercept β0.\r\nThis would represent the mean stopping distance for a car traveling 0 mph (which our regres\u0002sion line estimates to be -17.58). Of course, this interpretation does not make any sense for\r\nthis example, because a car travelling 0 mph takes 0 ft to stop (it was not moving in the first\r\nplace)! What went wrong? Looking at the data, we notice that the smallest speed for which\r\nwe have measured data is 4 mph. Therefore, if we predict what would happen for slower\r\nspeeds then we would be extrapolating, a dangerous practice which often gives nonsensical\r\nresults.\r\n11.2.2 Point Estimates of the Regression Line\r\nWe said at the beginning of the chapter that our goal was to estimate µ = IE Y, and the arguments\r\nin Section 11.2.1 showed how to obtain an estimate ˆµ of µ when the regression assumptions hold.\r\nNow we will reap the benefits of our work in more ways than we previously disclosed. Given a\r\nparticular value x0, there are two values we would like to estimate:\r\n1. the mean value of Y at x0, and\r\n2. a future value of Y at x0.\r\nThe first is a number, µ(x0), and the second is a random variable, Y(x0), but our point estimate is\r\nthe same for both: ˆµ(x0).\r\nExample 11.8. We may use the regression line to obtain a point estimate of the mean stopping\r\ndistance for a car traveling 8 mph: ˆµ(15) = b0 + 8b1 ≈ -17.58 +(8) (3.93)≈ 13.88. We would also\r\nuse 13.88 as a point estimate for the stopping distance of a future car traveling 8 mph.\r\nNote that we actually have observed data for a car traveling 8 mph; its stopping distance was\r\n16 ft as listed in the fifth row of the cars data:\r\n> cars[5, ]",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0bb8a86c-2a86-4af3-b188-6441d311ba71.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=829d942c03ad421642da26b2b1c22758cc93d4c906a878bc2ccb5f6489f3f40e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 455
      },
      {
        "segments": [
          {
            "segment_id": "ab91c3f0-7094-45fc-8efa-f2f399e3acfb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 273,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.2. ESTIMATION 257\r\nspeed dist\r\n5 8 16\r\nThere is a special name for estimates ˆµ(x0) when x0 matches an observed value xi from the data\r\nset. They are called fitted values, they are denoted by Yˆ\r\n1, Yˆ2, . . . , Yˆn (ignoring repetition), and they\r\nplay an important role in the sections that follow.\r\nIn an abuse of notation we will sometimes write Yˆ or Yˆ(x0) to denote a point on the regression\r\nline even when x0 does not belong to the original data if the context of the statement obviates any\r\ndanger of confusion.\r\nWe saw in Example 11.7 that spooky things can happen when we are cavalier about point\r\nestimation. While it is usually acceptable to predict/estimate at values of x0 that fall within the\r\nrange of the original x data, it is reckless to use ˆµ for point estimates at locations outside that range.\r\nSuch estimates are usually worthless. Do not extrapolate unless there are compelling external\r\nreasons, and even then, temper it with a good deal of caution.\r\nHow to do it with R\r\nThe fitted values are automatically computed as a byproduct of the model fitting procedure and\r\nare already stored as a component of the cars.lm object. We may access them with the fitted\r\nfunction (we only show the first five entries):\r\n> fitted(cars.lm)[1:5]\r\n1 2 3 4 5\r\n-1.849460 -1.849460 9.947766 9.947766 13.880175\r\nPredictions at x values that are not necessarily part of the original data are done with the\r\npredict function. The first argument is the original cars.lm object and the second argument\r\nnewdata accepts a dataframe (in the same form that was used to fit cars.lm) that contains the\r\nlocations at which we are seeking predictions.\r\nLet us predict the average stopping distances of cars traveling 6 mph, 8 mph, and 21 mph:\r\n> predict(cars.lm, newdata = data.frame(speed = c(6, 8, 21)))\r\n1 2 3\r\n6.015358 13.880175 65.001489\r\nNote that there were no observed cars that traveled 6 mph or 21 mph. Also note that our estimate\r\nfor a car traveling 8 mph matches the value we computed by hand in Example 11.8.\r\n11.2.3 Mean Square Error and Standard Error\r\nTo find the MLE of σ\r\n2 we consider the partial derivative\r\n∂\r\n∂σ2\r\nln L =\r\nn\r\n2σ2\r\n−\r\n1\r\n2(σ2)\r\n2\r\nXn\r\ni=1\r\n(yi − β0 − β1 xi)\r\n2\r\n, (11.2.18)\r\nand after plugging in βˆ\r\n0 and βˆ1 and setting equal to zero we get\r\nσˆ2 =\r\n1\r\nn\r\nXn\r\ni=1\r\n(yi − βˆ\r\n0 − βˆ1 xi)\r\n2 =\r\n1\r\nn\r\nXn\r\ni=1\r\n[yi − µˆ(xi)]2. (11.2.19)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ab91c3f0-7094-45fc-8efa-f2f399e3acfb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3a28cd54afb3ae15401e496fd82d82a261463d7a2e196c212e6564eeab7f6bb6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 434
      },
      {
        "segments": [
          {
            "segment_id": "a0bb0070-bd81-4f88-91c0-d126bbc890cc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 274,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "258 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nWe write Yiˆ = µˆ(xi), and we let Ei = Yi − Yˆ\r\ni be the i\r\nth residual. We see\r\nnσˆ2 =\r\nXn\r\ni=1\r\nE\r\n2\r\ni = S S E = the sum of squared errors. (11.2.20)\r\nFor a point estimate of σ\r\n2 we use the mean square error S 2 defined by\r\nS\r\n2 =\r\nS S E\r\nn − 2\r\n, (11.2.21)\r\nand we estimate σ with the standard error S =\r\n√\r\nS\r\n2\r\n.\r\n4\r\nHow to do it with R\r\nThe residuals for the model may be obtained with the residuals function; we only show the first\r\nfew entries in the interest of space:\r\n> residuals(cars.lm)[1:5]\r\n1 2 3 4 5\r\n3.849460 11.849460 -5.947766 12.052234 2.119825\r\nIn the last section, we calculated the fitted value for x = 8 and found it to be approximately\r\nµˆ(8) ≈13.88. Now, it turns out that there was only one recorded observation at x = 8, and we have\r\nseen this value in the output of head(cars) in Example 11.5; it was dist = 16 ft for a car with\r\nspeed = 8 mph. Therefore, the residual should be E = Y − Yˆ which is E ≈ 16−13.88. Now take a\r\nlook at the last entry of residuals(cars.lm), above. It is not a coincidence.\r\nThe estimate S for σ is called the Residual standard error and for the cars data is shown\r\na few lines up on the summary(cars.lm) output (see How to do it with R in Section 11.2.4). We\r\nmay read it from there to be S ≈ 15.38, or we can access it directly from the summary object.\r\n> carsumry <- summary(cars.lm)\r\n> carsumry$sigma\r\n[1] 15.37959\r\n11.2.4 Interval Estimates of the Parameters\r\nWe discussed general interval estimation in Chapter 9. There we found that we could use what we\r\nknow about the sampling distribution of certain statistics to construct confidence intervals for the\r\nparameter being estimated. We will continue in that vein, and to get started we will determine the\r\nsampling distributions of the parameter estimates, b1 and b0.\r\nTo that end, we can see from Equation 11.2.13 (and it is made clear in Chapter 12) that b1 is\r\njust a linear combination of normally distributed random variables, so b1 is normally distributed\r\ntoo. Further, it can be shown that\r\nb1 ∼ norm mean = β1, sd = σb1\r\n\u0001\r\n(11.2.22)\r\n4Be careful not to confuse the mean square error S2 with the sample variance S2\r\nin Chapter 3. Other notation the reader\r\nmay encounter is the lowercase s\r\n2 or the bulky MS E.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a0bb0070-bd81-4f88-91c0-d126bbc890cc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=93a85b89aa7c4c46f0b624833a40c7afcab4c7a1321bc64ddc50b1bec96ac795",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 439
      },
      {
        "segments": [
          {
            "segment_id": "c00bd5c7-1429-4b53-9720-77ebae69409d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 275,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.2. ESTIMATION 259\r\nwhere\r\nσb1 =\r\nσ\r\npPn\r\ni=1\r\n(xi − x)\r\n2\r\n(11.2.23)\r\nis called the standard error of b1 which unfortunately depends on the unknown value of σ. We do\r\nnot lose heart, though, because we can estimate σ with the standard error S from the last section.\r\nThis gives us an estimate S b1\r\nfor σb1\r\ndefined by\r\nS b1 =\r\nS\r\npPn\r\ni=1\r\n(xi − x)\r\n2\r\n. (11.2.24)\r\nNow, it turns out that b0, b1, and S are mutually independent (see the footnote in Section 12.2.7).\r\nTherefore, the quantity\r\nT =\r\nb1 − β1\r\nS b1\r\n(11.2.25)\r\nhas a t(df = n − 2) distribution. Therefore, a 100(1 − α)% confidence interval for β1 is given by\r\nb1 ± tα/2(df = n − 1) S b1\r\n(11.2.26)\r\nIt is also sometimes of interest to construct a confidence interval for β0 in which case we will\r\nneed the sampling distribution of b0. It is shown in Chapter 12 that\r\nb0 ∼ norm mean = β0, sd = σb0\r\n\u0001\r\n, (11.2.27)\r\nwhere σb0is given by\r\nσb0 = σ\r\ns\r\n1\r\nn\r\n+\r\nx\r\n2\r\nPn\r\ni=1\r\n(xi − x)\r\n2\r\n, (11.2.28)\r\nand which we estimate with the S b0defined by\r\nS b0 = S\r\ns\r\n1\r\nn\r\n+\r\nx\r\n2\r\nPn\r\ni=1\r\n(xi − x)\r\n2\r\n. (11.2.29)\r\nThus the quantity\r\nT =\r\nb0 − β0\r\nS b0\r\n(11.2.30)\r\nhas a t(df = n − 2) distribution and a 100(1 − α)% confidence interval for β0 is given by\r\nb0 ± tα/2(df = n − 1) S b0(11.2.31)\r\nHow to do it with R\r\nLet us take a look at the output from summary(cars.lm):\r\n> summary(cars.lm)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c00bd5c7-1429-4b53-9720-77ebae69409d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3a034cae9ddcc0ff76017ad78eed339c6da1143982fd2781886f285dea7e48a8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 281
      },
      {
        "segments": [
          {
            "segment_id": "1a449e49-6535-4dac-a575-5678b0049254",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 276,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "260 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nCall:\r\nlm(formula = dist ~ speed, data = cars)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-29.069 -9.525 -2.272 9.215 43.201\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) -17.5791 6.7584 -2.601 0.0123 *\r\nspeed 3.9324 0.4155 9.464 1.49e-12 ***\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 15.38 on 48 degrees of freedom\r\nMultiple R-squared: 0.6511, Adjusted R-squared: 0.6438\r\nF-statistic: 89.57 on 1 and 48 DF, p-value: 1.490e-12\r\nIn the Coefficients section we find the parameter estimates and their respective standard\r\nerrors in the second and third columns; the other columns are discussed in Section 11.3. If we\r\nwanted, say, a 95% confidence interval for β1 we could use b1 = 3.932 and S b1 = 0.416 together\r\nwith a t0.025(df = 23) critical value to calculate b1 ± t0.025(df = 23)S b1.\r\nOr, we could use the confint function.\r\n> confint(cars.lm)\r\n2.5 % 97.5 %\r\n(Intercept) -31.167850 -3.990340\r\nspeed 3.096964 4.767853\r\nWith 95% confidence, the random interval [3.097, 4.768] covers the parameter β1.\r\n11.2.5 Interval Estimates of the Regression Line\r\nWe have seen how to estimate the coefficients of regression line with both point estimates and\r\nconfidence intervals. We even saw how to estimate a value ˆµ(x) on the regression line for a given\r\nvalue of x, such as x = 15.\r\nBut how good is our estimate ˆµ(15)? How much confidence do we have in this estimate?\r\nFurthermore, suppose we were going to observe another value of Y at x = 15. What could we say?\r\nIntuitively, it should be easier to get bounds on the mean (average) value of Y at x0 (called a\r\nconfidence interval for the mean value of Y at x0) than it is to get bounds on a future observation\r\nof Y (called a prediction interval for Y at x0). As we shall see, the intuition serves us well and\r\nconfidence intervals are shorter for the mean value, longer for the individual value.\r\nOur point estimate of µ(x0) is of course Yˆ = Yˆ(x0), so for a confidence interval we will need to\r\nknow Yˆ’s sampling distribution. It turns out (see Section ) that Yˆ = µˆ(x0) is distributed\r\nYˆ ∼ norm\r\n\r\n\r\nmean = µ(x0), sd = σ\r\ns\r\n1\r\nn\r\n+\r\n(x0 − x)\r\n2\r\nPn\r\ni=1\r\n(xi − x)\r\n2\r\n\r\n\r\n. (11.2.32)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1a449e49-6535-4dac-a575-5678b0049254.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=01925b519dde14332c723818351846da4c8e51d00e1c7ba5a26a7b041ad25ee9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 401
      },
      {
        "segments": [
          {
            "segment_id": "83ea9805-67b1-4d95-aa0f-4e04c3e5595f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 277,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.2. ESTIMATION 261\r\nSince σ is unknown we estimate it with S (we should expect the appearance of a t(df = n − 2)\r\ndistribution in the near future).\r\nA 100(1 − α)% confidence interval (CI) for µ(x0) is given by\r\nYˆ ± tα/2(df = n − 2) S\r\ns\r\n1\r\nn\r\n+\r\n(x0 − x\r\n2\r\n)\r\nPn\r\ni=1\r\n(xi − x)\r\n2\r\n. (11.2.33)\r\nIt is time for prediction intervals, which are slightly different. In order to find confidence bounds\r\nfor a new observation of Y (we will denote it Ynew) we use the fact that\r\nYnew ∼ norm\r\n\r\n\r\nmean = µ(x0), sd = σ\r\ns\r\n1 +\r\n1\r\nn\r\n+\r\n(x0 − x)\r\n2\r\nPn\r\ni=1\r\n(xi − x)\r\n2\r\n\r\n\r\n. (11.2.34)\r\nOf course σ is unknown and we estimate it with S . Thus, a 100(1 − α)% prediction interval (PI)\r\nfor a future value of Y at x0 is given by\r\nYˆ(x0) ± tα/2(df = n − 1) S\r\ns\r\n1 +\r\n1\r\nn\r\n+\r\n(x0 − x)\r\n2\r\nPn\r\ni=1\r\n(xi − x)\r\n2\r\n. (11.2.35)\r\nWe notice that the prediction interval in Equation 11.2.35 is wider than the confidence interval in\r\nEquation 11.2.33, as we expected at the beginning of the section.\r\nHow to do it with R\r\nConfidence and prediction intervals are calculated in R with the predict function, which we\r\nencountered in Section 11.2.2. There we neglected to take advantage of its additional interval\r\nargument. The general syntax follows.\r\nExample 11.9. We will find confidence and prediction intervals for the stopping distance of a car\r\ntravelling 5, 6, and 21 mph (note from the graph that there are no collected data for these speeds).\r\nWe have computed cars.lm earlier, and we will use this for input to the predict function. Also,\r\nwe need to tell R the values of x0 at which we want the predictions made, and store the x0 values\r\nin a data frame whose variable is labeled with the correct name. This is important.\r\n> new <- data.frame(speed = c(5, 6, 21))\r\nNext we instruct R to calculate the intervals. Confidence intervals are given by\r\n> predict(cars.lm, newdata = new, interval = \"confidence\")\r\nfit lwr upr\r\n1 2.082949 -7.644150 11.81005\r\n2 6.015358 -2.973341 15.00406\r\n3 65.001489 58.597384 71.40559\r\nPrediction intervals are given by\r\n> predict(cars.lm, newdata = new, interval = \"prediction\")",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/83ea9805-67b1-4d95-aa0f-4e04c3e5595f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=747ed11730f37364efdb94cb04f7b734332fcea7267de9742d0b6e423099a353",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 397
      },
      {
        "segments": [
          {
            "segment_id": "02b15b6a-5b14-4596-a13f-985cb654d16b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 278,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "262 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nfit lwr upr\r\n1 2.082949 -30.33359 34.49948\r\n2 6.015358 -26.18731 38.21803\r\n3 65.001489 33.42257 96.58040\r\nThe type of interval is dictated by the interval argument (which is none by default), and the\r\ndefault confidence level is 95% (which can be changed with the level argument).\r\nExample 11.10. Using the cars data,\r\n1. Report a point estimate of and a 95% confidence interval for the mean stopping distance for\r\na car travelling 5 mph.\r\nThe fitted value for x = 5 is 2.08, so a point estimate would be 2.08 ft. The 95% CI is\r\ngiven by [-7.64, 11.81], so with 95% confidence the mean stopping distance lies somewhere\r\nbetween -7.64 ft and 11.81 ft.\r\n2. Report a point prediction for and a 95% prediction interval for the stopping distance of a\r\nhypothetical car travelling 21 mph.\r\nThe fitted value for x = 21 is 65, so a point prediction for the stopping distance is 65 ft.\r\nThe 95% PI is given by [33.42, 96.58], so with 95% confidence we may assert that the\r\nhypothetical stopping distance for a car travelling 21 mph would lie somewhere between\r\n33.42 ft and 96.58 ft.\r\nGraphing the Confidence and Prediction Bands\r\nWe earlier guessed that a bound on the value of a single new observation would be inherently less\r\ncertain than a bound for an average (mean) value; therefore, we expect the CIs for the mean to be\r\ntighter than the PIs for a new observation. A close look at the standard deviations in Equations\r\n11.2.33 and 11.2.35 confirms our guess, but we would like to see a picture to drive the point home.\r\nWe may plot the confidence and prediction intervals with one fell swoop using the ci.plot\r\nfunction from the HH package [40]. The graph is displayed in Figure 11.2.2.\r\n> library(HH)\r\n> ci.plot(cars.lm)\r\nNotice that the bands curve outward away from the regression line as the x values move away\r\nfrom the center. This is expected once we notice the (x0 − x)\r\n2\r\nterm in the standard deviation\r\nformulas in Equations 11.2.33 and 11.2.35.\r\n11.3 Model Utility and Inference\r\n11.3.1 Hypothesis Tests for the Parameters\r\nMuch of the attention of SLR is directed toward β1 because when β1 , 0 the mean value of Y\r\nincreases (or decreases) as x increases. Further, if β1 = 0 then the mean value of Y remains the\r\nsame, regardless of the value of x (when the regression assumptions hold, of course). It is thus very",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/02b15b6a-5b14-4596-a13f-985cb654d16b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fe5625dfa257b0fe4219742dc198a1822158c1e43948c843da25abcb038c19d6",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "34d444ce-0679-42f3-a4b3-d59fcd096865",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 279,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.3. MODEL UTILITY AND INFERENCE 263\r\n95% confidence and prediction intervals for cars.lm\r\nspeed\r\ndist\r\n0\r\n50\r\n100\r\n5 10 15 20 25\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\nx\r\nx\r\nobserved\r\nfit\r\nconf int\r\npred int\r\n●\r\nFigure 11.2.2: Scatterplot with confidence/prediction bands for the cars data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/34d444ce-0679-42f3-a4b3-d59fcd096865.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fe4dd8918295405e830f916c4887dc1ccdf6a42bb3ac633f084c6165da822928",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 508
      },
      {
        "segments": [
          {
            "segment_id": "28ffd941-0b09-4db9-bf59-c5d3acfdebbf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 280,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "264 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nimportant to decide whether or not β1 = 0. We address the question with a statistical test of the\r\nnull hypothesis H0 : β1 = 0 versus the alternative hypothesis H1 : β1 , 0, and to do that we need to\r\nknow the sampling distribution of b1 when the null hypothesis is true.\r\nTo this end we already know from Section 11.2.4 that the quantity\r\nT =\r\nb1 − β1\r\nS b1\r\n(11.3.1)\r\nhas a t(df = n − 2) distribution; therefore, when β1 = 0 the quantity b1/S b1\r\nhas a t(df = n − 2)\r\ndistribution and we can compute a p-value by comparing the observed value of b1/S b1 with values\r\nunder a t(df = n − 2) curve.\r\nSimilarly, we may test the hypothesis H0 : β0 = 0 versus the alternative H1 : β0 , 0 with the\r\nstatistic T = b0/S b0\r\n, where S b0\r\nis given in Section 11.2.4. The test is conducted the same way as\r\nfor β1.\r\nHow to do it with R\r\nLet us take another look at the output from summary(cars.lm):\r\n> summary(cars.lm)\r\nCall:\r\nlm(formula = dist ~ speed, data = cars)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-29.069 -9.525 -2.272 9.215 43.201\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) -17.5791 6.7584 -2.601 0.0123 *\r\nspeed 3.9324 0.4155 9.464 1.49e-12 ***\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 15.38 on 48 degrees of freedom\r\nMultiple R-squared: 0.6511, Adjusted R-squared: 0.6438\r\nF-statistic: 89.57 on 1 and 48 DF, p-value: 1.490e-12\r\nIn the Coefficients section we find the t statistics and the p-values associated with the tests\r\nthat the respective parameters are zero in the fourth and fifth columns. Since the p-values are\r\n(much) less than 0.05, we conclude that there is strong evidence that the parameters β1 , 0 and\r\nβ0 , 0, and as such, we say that there is a statistically significant linear relationship between dist\r\nand speed.\r\n11.3.2 Simple Coefficient of Determination\r\nIt would be nice to have a single number that indicates how well our linear regression model is\r\ndoing, and the simple coefficient of determination is designed for that purpose. In what follows,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/28ffd941-0b09-4db9-bf59-c5d3acfdebbf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ed496940508f7888ea966860dda5369772b7b92b41ff0fe4611ba8d95eb82541",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 378
      },
      {
        "segments": [
          {
            "segment_id": "c54ad8b9-1942-4d11-8592-c90e78c0efaa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 281,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.3. MODEL UTILITY AND INFERENCE 265\r\nwe observe the values Y1, Y2, . . . ,Yn, and the goal is to estimate µ(x0), the mean value of Y at the\r\nlocation x0.\r\nIf we disregard the dependence of Y and x and base our estimate only on the Y values then a\r\nreasonable choice for an estimator is just the MLE of µ, which is Y. Then the errors incurred by the\r\nestimate are just Yi − Y and the variation about the estimate as measured by the sample variance is\r\nproportional to\r\nS S TO =\r\nXn\r\ni=1\r\n(Yi − Y)\r\n2\r\n. (11.3.2)\r\nHere, S S TO is an acronym for the total sum of squares.\r\nBut we do have additional information, namely, we have values xi associated with each value\r\nof Yi. We have seen that this information leads us to the estimate Yˆ\r\ni and the errors incurred are just\r\nthe residuals, Ei = Yi − Yˆ\r\ni\r\n. The variation associated with these errors can be measured with\r\nS S E =\r\nXn\r\ni=1\r\n(Yi − Yˆ\r\ni)\r\n2\r\n. (11.3.3)\r\nWe have seen the S S E before, which stands for the sum of squared errors or error sum of squares.\r\nOf course, we would expect the error to be less in the latter case, since we have used more informa\u0002tion. The improvement in our estimation as a result of the linear regression model can be measured\r\nwith the difference\r\n(Yi − Y) − (Yi − Yˆ\r\ni) = Yˆi − Y,\r\nand we measure the variation in these errors with\r\nS S R =\r\nXn\r\ni=1\r\n(Yˆ\r\ni − Y)\r\n2\r\n, (11.3.4)\r\nalso known as the regression sum of squares. It is not obvious, but some algebra proved a famous\r\nresult known as the ANOVA Equality:\r\nXn\r\ni=1\r\n(Yi − Y)\r\n2 =\r\nXn\r\ni=1\r\n(Yˆ\r\ni − Y)\r\n2 +\r\nXn\r\ni=1\r\n(Yi − Yˆ\r\ni)\r\n2\r\n(11.3.5)\r\nor in other words,\r\nS S TO = S S R + S S E. (11.3.6)\r\nThis equality has a nice interpretation. Consider S S TO to be the total variation of the errors. Think\r\nof a decomposition of the total variation into pieces: one piece measuring the reduction of error\r\nfrom using the linear regression model, or explained variation (S S R), while the other represents\r\nwhat is left over, that is, the errors that the linear regression model doesn’t explain, or unexplained\r\nvariation (S S E). In this way we see that the ANOVA equality merely partitions the variation into\r\ntotal variation = explained variation + unexplained variation.\r\nFor a single number to summarize how well our model is doing we use the simple coefficient of\r\ndetermination r\r\n2\r\n, defined by\r\nr\r\n2 = 1 −\r\nS S E\r\nS S TO\r\n. (11.3.7)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c54ad8b9-1942-4d11-8592-c90e78c0efaa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c2399ae8ad9a13fa431a5155ebe3dca2324feea8d010549fb5057ca089e68fb9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 474
      },
      {
        "segments": [
          {
            "segment_id": "1b20416f-7a22-4a26-97d7-6ab928695646",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 282,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "266 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nWe interpret r\r\n2\r\nas the proportion of total variation that is explained by the simple linear regression\r\nmodel. When r\r\n2\r\nis large, the model is doing a good job; when r\r\n2\r\nis small, the model is not doing a\r\ngood job.\r\nRelated to the simple coefficient of determination is the sample correlation coefficient, r. As\r\nyou can guess, the way we get r is by the formula |r| =\r\n√\r\nr\r\n2\r\n. But how do we get the sign? It is\r\nequal the sign of the slope estimate b1. That is, if the regression line ˆµ(x) has positive slope, then\r\nr =\r\n√\r\nr\r\n2\r\n. Likewise, if the slope of ˆµ(x) is negative, then r = −\r\n√\r\nr\r\n2\r\n.\r\nHow to do it with R\r\nThe primary method to display partitioned sums of squared errors is with an ANOVA table. The\r\ncommand in R to produce such a table is anova. The input to anova is the result of an lm call\r\nwhich for the cars data is cars.lm.\r\n> anova(cars.lm)\r\nAnalysis of Variance Table\r\nResponse: dist\r\nDf Sum Sq Mean Sq F value Pr(>F)\r\nspeed 1 21186 21185.5 89.567 1.490e-12 ***\r\nResiduals 48 11354 236.5\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nThe output gives\r\nr\r\n2 = 1 −\r\nS S E\r\nS S R + S S E\r\n= 1 −\r\n11353.5\r\n21185.5 + 11353.5\r\n≈ 0.65.\r\nThe interpretation should be: “The linear regression line accounts for approximately 65% of the\r\nvariation of dist as explained by speed”.\r\nThe value of r\r\n2\r\nis stored in the r.squared component of summary(cars.lm), which we\r\ncalled carsumry.\r\n> carsumry$r.squared\r\n[1] 0.6510794\r\nWe already knew this. We saw it in the next to the last line of the summary(cars.lm) output\r\nwhere it was called “Multiple R-squared”. Listed right beside it is the Adjusted R-squared\r\nwhich we will discuss in Chapter 12.\r\nFor the cars data, we find r to be\r\n> sqrt(carsumry$r.squared)\r\n[1] 0.8068949\r\nWe choose the principal square root because the slope of the regression line is positive.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1b20416f-7a22-4a26-97d7-6ab928695646.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=07a9fbf834b2f4e996a4896053699f946242fb25b41ca6de44a6e2e2d472747a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 360
      },
      {
        "segments": [
          {
            "segment_id": "61e80c7e-261a-4e18-8f67-b4d86cc4bea5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 283,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.4. RESIDUAL ANALYSIS 267\r\n11.3.3 Overall F statistic\r\nThere is another way to test the significance of the linear regression model. In SLR, the new way\r\nalso tests the hypothesis H0 : β1 = 0 versus H1 : β1 , 0, but it is done with a new test statistic\r\ncalled the overall F statistic. It is defined by\r\nF =\r\nS S R\r\nS S E/(n − 2)\r\n. (11.3.8)\r\nUnder the regression assumptions and when H0 is true, the F statistic has an f(df1 = 1, df2 =\r\nn − 2) distribution. We reject H0 when F is large – that is, when the explained variation is large\r\nrelative to the unexplained variation.\r\nAll this being said, we have not yet gained much from the overall F statistic because we already\r\nknew from Section 11.3.1 how to test H0 : β1 = 0. . . we use the Student’s t statistic. What is worse\r\nis that (in the simple linear regression model) it can be proved that the F in Equation 11.3.8 is\r\nexactly the Student’s t statistic for β1 squared,\r\nF =\r\n \r\nb1\r\nS b1\r\n!2\r\n. (11.3.9)\r\nSo why bother to define the F statistic? Why not just square the t statistic and be done with it? The\r\nanswer is that the F statistic has a more complicated interpretation and plays a more important role\r\nin the multiple linear regression model which we will study in Chapter 12. See Section 12.3.3 for\r\ndetails.\r\n11.3.4 How to do it with R\r\nThe overall F statistic and p-value are displayed in the bottom line of the summary(cars.lm)\r\noutput. It is also shown in the final columns of anova(cars.lm):\r\n> anova(cars.lm)\r\nAnalysis of Variance Table\r\nResponse: dist\r\nDf Sum Sq Mean Sq F value Pr(>F)\r\nspeed 1 21186 21185.5 89.567 1.490e-12 ***\r\nResiduals 48 11354 236.5\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nHere we see that the F statistic is 89.57 with a p-value very close to zero. The conclusion:\r\nthere is very strong evidence that H0 : β1 = 0 is false, that is, there is strong evidence that β1 , 0.\r\nMoreover, we conclude that the regression relationship between dist and speed is significant.\r\nNote that the value of the F statistic is the same as the Student’s t statistic for speed squared.\r\n11.4 Residual Analysis\r\nWe know from our model that Y = µ(x) + \u000f, or in other words, \u000f = Y − µ(x). Further, we know that\r\n\u000f ∼ norm(mean = 0, sd = σ). We may estimate \u000fi with the residual Ei = Yi − Yˆ\r\ni\r\n, where Yˆ\r\ni = µˆ(xi).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/61e80c7e-261a-4e18-8f67-b4d86cc4bea5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cf405e390ac13715951eec7bb0d32be453eaf8e7a2ed7bfb3f2645124112fdb2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 449
      },
      {
        "segments": [
          {
            "segment_id": "93e50821-21bb-4b07-82f8-5fd35ff2babb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 284,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "268 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nIf the regression assumptions hold, then the residuals should be normally distributed. We check\r\nthis in Section 11.4.1. Further, the residuals should have mean zero with constant variance σ\r\n2\r\n, and\r\nwe check this in Section 11.4.2. Last, the residuals should be independent, and we check this in\r\nSection 11.4.3.\r\nIn every case, we will begin by looking at residual plots – that is, scatterplots of the residuals\r\nEi versus index or predicted values Yˆ\r\ni – and follow up with hypothesis tests.\r\n11.4.1 Normality Assumption\r\nWe can assess the normality of the residuals with graphical methods and hypothesis tests. To check\r\ngraphically whether the residuals are normally distributed we may look at histograms or q-q plots.\r\nWe first examine a histogram in Figure 11.4.1. There we see that the distribution of the residuals\r\nappears to be mound shaped, for the most part. We can plot the order statistics of the sample\r\nversus quantiles from a norm(mean = 0, sd = 1) distribution with the command plot(cars.lm,\r\nwhich = 2), and the results are in Figure 11.4.1. If the assumption of normality were true, then\r\nwe would expect points randomly scattered about the dotted straight line displayed in the figure. In\r\nthis case, we see a slight departure from normality in that the dots show systematic clustering on\r\none side or the other of the line. The points on the upper end of the plot also appear begin to stray\r\nfrom the line. We would say there is some evidence that the residuals are not perfectly normal.\r\nTesting the Normality Assumption\r\nEven though we may be concerned about the plots, we can use tests to determine if the evidence\r\npresent is statistically significant, or if it could have happened merely by chance. There are many\r\nstatistical tests of normality. We will use the Shapiro-Wilk test, since it is known to be a good\r\ntest and to be quite powerful. However, there are many other fine tests of normality including the\r\nAnderson-Darling test and the Lillefors test, just to mention two of them.\r\nThe Shapiro-Wilk test is based on the statistic\r\nW =\r\n\u0010Pn\r\ni=1\r\naiE(i)\r\n\u00112\r\nPn\r\nj=1 E\r\n2\r\nj\r\n, (11.4.1)\r\nwhere the E(i) are the ordered residuals and the ai are constants derived from the order statistics of\r\na sample of size n from a normal distribution. See Section 10.5.3.\r\nWe perform the Shapiro-Wilk test below, using the shapiro.test function from the stats\r\npackage. The hypotheses are\r\nH0 : the residuals are normally distributed\r\nversus\r\nH1 : the residuals are not normally distributed.\r\nThe results from R are\r\n> shapiro.test(residuals(cars.lm))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/93e50821-21bb-4b07-82f8-5fd35ff2babb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2120de0d50cc76c95c823bf470006cec5b1eecb9be2b466651907c1c5ca2ccad",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 440
      },
      {
        "segments": [
          {
            "segment_id": "38f6fdcd-908f-4df1-83fe-2213c9a5846d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 285,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.4. RESIDUAL ANALYSIS 269\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n−2 −1 0 1 2\r\n−2 −1\r\n0 1 2\r\n3\r\nTheoretical Quantiles\r\nStandardized residuals\r\nlm(dist ~ speed)\r\nNormal Q−Q\r\n49 23\r\n35\r\nFigure 11.4.1: Normal q-q plot of the residuals for the cars data\r\nUsed for checking the normality assumption. Look out for any curvature or substantial departures from the\r\nstraight line; hopefully the dots hug the line closely.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/38f6fdcd-908f-4df1-83fe-2213c9a5846d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0e413c672c0a308153c2dd34b266875e17a78774742452a50e15d6669c9e7280",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 112
      },
      {
        "segments": [
          {
            "segment_id": "37d739b4-d4ae-4441-b6ca-e867dfb4517c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 286,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "270 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nShapiro-Wilk normality test\r\ndata: residuals(cars.lm)\r\nW = 0.9451, p-value = 0.02153\r\nFor these data we would reject the assumption of normality of the residuals at the α = 0.05\r\nsignificance level, but do not lose heart, because the regression model is reasonably robust to de\u0002partures from the normality assumption. As long as the residual distribution is not highly skewed,\r\nthen the regression estimators will perform reasonably well. Moreover, departures from constant\r\nvariance and independence will sometimes affect the quantile plots and histograms, therefore it is\r\nwise to delay final decisions regarding normality until all diagnostic measures have been investi\u0002gated.\r\n11.4.2 Constant Variance Assumption\r\nWe will again go to residual plots to try and determine if the spread of the residuals is changing\r\nover time (or index). However, it is unfortunately not that easy because the residuals do not have\r\nconstant variance! In fact, it can be shown that the variance of the residual Eiis\r\nVar(Ei) = σ\r\n2\r\n(1 − hii), i = 1, 2, . . . , n, (11.4.2)\r\nwhere hii is a quantity called the leverage which is defined below. Consequently, in order to check\r\nthe constant variance assumption we must standardize the residuals before plotting. We estimate the\r\nstandard error of Ei with sEi = s\r\n√\r\n(1 − hii) and define the standardized residuals Ri, i = 1, 2, . . . , n,\r\nby\r\nRi =\r\nEi\r\ns\r\n√\r\n1 − hii\r\n, i = 1, 2, . . . , n. (11.4.3)\r\nFor the constant variance assumption we do not need the sign of the residual so we will plot\r\n√\r\n|Ri| versus the fitted values. As we look at a scatterplot of √|Ri| versus Yˆ\r\ni we would expect\r\nunder the regression assumptions to see a constant band of observations, indicating no change in\r\nthe magnitude of the observed distance from the line. We want to watch out for a fanning-out of\r\nthe residuals, or a less common funneling-in of the residuals. Both patterns indicate a change in the\r\nresidual variance and a consequent departure from the regression assumptions, the first an increase,\r\nthe second a decrease.\r\nIn this case, we plot the standardized residuals versus the fitted values. The graph may be seen\r\nin Figure 11.4.2. For these data there does appear to be somewhat of a slight fanning-out of the\r\nresiduals.\r\nTesting the Constant Variance Assumption\r\nWe will use the Breusch-Pagan test to decide whether the variance of the residuals is nonconstant.\r\nThe null hypothesis is that the variance is the same for all observations, and the alternative hypoth\u0002esis is that the variance is not the same for all observations. The test statistic is found by fitting a\r\nlinear model to the centered squared residuals\r\nWi = E\r\n2\r\ni −\r\nS S E\r\nn\r\n, i = 1, 2, . . . , n. (11.4.4)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/37d739b4-d4ae-4441-b6ca-e867dfb4517c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5028a5d68c058eb74e184fe36a6108ac881136d7820d1a3d0e8534a8b66079fe",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 482
      },
      {
        "segments": [
          {
            "segment_id": "5388432b-2bcd-4902-9485-f722d772ce58",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 287,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.4. RESIDUAL ANALYSIS 271\r\n0 20 40 60 80\r\n0.0 0.5 1.0 1.5\r\nFitted values\r\nSta\r\nn\r\nd\r\nardiz\r\ne\r\nd re\r\nsid\r\nu\r\nals\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\nlm(dist ~ speed)\r\nScale−Location\r\n49 23\r\n35\r\nFigure 11.4.2: Plot of standardized residuals against the fitted values for the cars data\r\nUsed for checking the constant variance assumption. Watch out for any fanning out (or in) of the dots;\r\nhopefully they fall in a constant band.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5388432b-2bcd-4902-9485-f722d772ce58.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=58084ffd6d12dceb8dc7c8049b0625328d60f8228cb1bf2ebb67a552c17eb50e",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "a203774a-3e20-423d-bdbd-07072a5ad06e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 288,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "272 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nBy default the same explanatory variables are used in the new model which produces fitted values\r\nWˆ\r\ni\r\n, i = 1, 2, . . . , n. The Breusch-Pagan test statistic in R is then calculated with\r\nBP = n\r\nXn\r\ni=1\r\nWˆ\r\n2\r\ni ÷\r\nXn\r\ni=1\r\nW2\r\ni\r\n. (11.4.5)\r\nWe reject the null hypothesis if BP is too large, which happens when the explained variation in the\r\nnew model is large relative to the unexplained variation in the original model.\r\nWe do it in R with the bptest function from the lmtest package [93].\r\n> library(lmtest)\r\n> bptest(cars.lm)\r\nstudentized Breusch-Pagan test\r\ndata: cars.lm\r\nBP = 3.2149, df = 1, p-value = 0.07297\r\nFor these data we would not reject the null hypothesis at the α = 0.05 level. There is relatively\r\nweak evidence against the assumption of constant variance.\r\n11.4.3 Independence Assumption\r\nOne of the strongest of the regression assumptions is the one regarding independence. Departures\r\nfrom the independence assumption are often exhibited by correlation (or autocorrelation, literally,\r\nself-correlation) present in the residuals. There can be positive or negative correlation.\r\nPositive correlation is displayed by positive residuals followed by positive residuals, and neg\u0002ative residuals followed by negative residuals. Looking from left to right, this is exhibited by a\r\ncyclical feature in the residual plots, with long sequences of positive residuals being followed by\r\nlong sequences of negative ones.\r\nOn the other hand, negative correlation implies positive residuals followed by negative residu\u0002als, which are then followed by positive residuals, etc. Consequently, negatively correlated resid\u0002uals are often associated with an alternating pattern in the residual plots. We examine the residual\r\nplot in Figure 11.4.3. There is no obvious cyclical wave pattern or structure to the residual plot.\r\nTesting the Independence Assumption\r\nWe may statistically test whether there is evidence of autocorrelation in the residuals with the\r\nDurbin-Watson test. The test is based on the statistic\r\nD =\r\nPn\r\ni=2\r\n(Ei − Ei−1)\r\n2\r\nPn\r\nj=1 E\r\n2\r\nj\r\n. (11.4.6)\r\nExact critical values are difficult to obtain, but R will calculate the p-value to great accuracy. It is\r\nperformed with the dwtest function from the lmtest package. We will conduct a two sided test\r\nthat the correlation is not zero, which is not the default (the default is to test that the autocorrelation\r\nis positive).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a203774a-3e20-423d-bdbd-07072a5ad06e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6bfd223f120fe46e2dfae66ca145cd70008e64753c8bf8c28189ad322bc10dd7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "346ddf28-d725-4b08-ba9e-2cd7fc93fc2d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 289,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.4. RESIDUAL ANALYSIS 273\r\n0 20 40 60 80\r\n−20\r\n0 20 40\r\nFitted values\r\nResiduals\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\nlm(dist ~ speed)\r\nResiduals vs Fitted\r\n23 49\r\n35\r\nFigure 11.4.3: Plot of the residuals versus the fitted values for the cars data\r\nUsed for checking the independence assumption. Watch out for any patterns or structure; hopefully the points\r\nare randomly scattered on the plot.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/346ddf28-d725-4b08-ba9e-2cd7fc93fc2d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5bfa18a297631405280fcd52124c93378f404aa4d8460c5bcb5755dd859c458a",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c341faed-bda0-42a1-8f04-7b518b4122c9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 290,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "274 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\n> library(lmtest)\r\n> dwtest(cars.lm, alternative = \"two.sided\")\r\nDurbin-Watson test\r\ndata: cars.lm\r\nDW = 1.6762, p-value = 0.1904\r\nalternative hypothesis: true autocorelation is not 0\r\nIn this case we do not reject the null hypothesis at the α = 0.05 significance level; there is very\r\nlittle evidence of nonzero autocorrelation in the residuals.\r\n11.4.4 Remedial Measures\r\nWe often find problems with our model that suggest that at least one of the three regression as\u0002sumptions is violated. What do we do then? There are many measures at the statistician’s disposal,\r\nand we mention specific steps one can take to improve the model under certain types of violation.\r\nMean response is not linear. We can directly modify the model to better approximate the mean\r\nresponse. In particular, perhaps a polynomial regression function of the form\r\nµ(x) = β0 + β1 x1 + β2 x\r\n2\r\n1\r\nwould be appropriate. Alternatively, we could have a function of the form\r\nµ(x) = β0e\r\nβ1 x\r\n.\r\nModels like these are studied in nonlinear regression courses.\r\nError variance is not constant. Sometimes a transformation of the dependent variable will take\r\ncare of the problem. There is a large class of them called Box-Cox transformations. They\r\ntake the form\r\nY\r\n∗ = Yλ\r\n, (11.4.7)\r\nwhere λ is a constant. (The method proposed by Box and Cox will determine a suitable value\r\nof λ automatically by maximum likelihood). The class contains the transformations\r\nλ = 2, Y\r\n∗ = Y2\r\nλ = 0.5, Y\r\n∗ =\r\n√\r\nY\r\nλ = 0, Y\r\n∗ = ln Y\r\nλ = −1, Y\r\n∗ = 1/Y\r\nAlternatively, we can use the method of weighted least squares. This is studied in more detail\r\nin later classes.\r\nError distribution is not normal. The same transformations for stabilizing the variance are equally\r\nappropriate for smoothing the residuals to a more Gaussian form. In fact, often we will kill\r\ntwo birds with one stone.\r\nErrors are not independent. There is a large class of autoregressive models to be used in this\r\nsituation which occupy the latter part of Chapter 16.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c341faed-bda0-42a1-8f04-7b518b4122c9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e7396923348d651b8c00e4448c043c06ec86401319dfacf7fc7eaa7743800aca",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 461
      },
      {
        "segments": [
          {
            "segment_id": "731b3b8c-d90e-4054-8f8d-5d6103f344d6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 291,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.5. OTHER DIAGNOSTIC TOOLS 275\r\n11.5 Other Diagnostic Tools\r\nThere are two types of observations with which we must be especially careful:\r\nInfluential observations are those that have a substantial effect on our estimates, predictions, or\r\ninferences. A small change in an influential observation is followed by a large change in the\r\nparameter estimates or inferences.\r\nOutlying observations are those that fall fall far from the rest of the data. They may be indicating\r\na lack of fit for our regression model, or they may just be a mistake or typographical error\r\nthat should be corrected. Regardless, special attention should be given to these observations.\r\nAn outlying observation may or may not be influential.\r\nWe will discuss outliers first because the notation builds sequentially in that order.\r\n11.5.1 Outliers\r\nThere are three ways that an observation (xi, yi) may be an outlier: it can have an xi value which\r\nfalls far from the other x values, it can have a yi value which falls far from the other y values, or it\r\ncan have both its xi and yi values falling far from the other x and y values.\r\nLeverage\r\nLeverage statistics are designed to identify observations which have x values that are far away from\r\nthe rest of the data. In the simple linear regression model the leverage of xiis denoted by hii and\r\ndefined by\r\nhii =\r\n1\r\nn\r\n+\r\n(xi − x)\r\n2\r\nPn\r\nk=1\r\n(xk − x)\r\n2\r\n, i = 1, 2, . . . , n. (11.5.1)\r\nThe formula has a nice interpretation in the SLR model: if the distance from xito x is large relative\r\nto the other x’s then hii will be close to 1.\r\nLeverages have nice mathematical properties; for example, they satisfy\r\n0 ≤ hii ≤ 1, (11.5.2)\r\nand their sum is\r\nXn\r\ni=1\r\nhii =\r\nXn\r\ni=1\r\n\"\r\n1\r\nn\r\n+\r\n(xi − x)\r\n2\r\nPn\r\nk=1\r\n(xk − x)\r\n2\r\n#\r\n, (11.5.3)\r\n=\r\nn\r\nn\r\n+\r\nP\r\ni\r\n(xi − x)\r\n2\r\nP\r\nk\r\n(xk − x)\r\n2\r\n, (11.5.4)\r\n= 2. (11.5.5)\r\nA rule of thumb is to consider leverage values to be large if they are more than double their average\r\nsize (which is 2/n according to Equation 11.5.5). So leverages larger than 4/n are suspect. Another\r\nrule of thumb is to say that values bigger than 0.5 indicate high leverage, while values between 0.3\r\nand 0.5 indicate moderate leverage.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/731b3b8c-d90e-4054-8f8d-5d6103f344d6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7a7093f7ccfc6459ea3c63a9a9d71a7dc71a4468d75e5a45110355a4e3d3ace8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 405
      },
      {
        "segments": [
          {
            "segment_id": "dac91b11-e2e4-4e70-880a-80b66b989920",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 292,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "276 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nStandardized and Studentized Deleted Residuals\r\nWe have already encountered the standardized residuals riin Section 11.4.2; they are merely resid\u0002uals that have been divided by their respective standard deviations:\r\nRi =\r\nEi\r\nS\r\n√\r\n1 − hii\r\n, i = 1, 2, . . . , n. (11.5.6)\r\nValues of |Ri| > 2 are extreme and suggest that the observation has an outlying y-value.\r\nNow delete the i\r\nth case and fit the regression function to the remaining n − 1 cases, producing a\r\nfitted value Yˆ\r\n(i) with deleted residual Di = Yi − Yˆ(i)\r\n. It is shown in later classes that\r\nVar(Di) =\r\nS\r\n2\r\n(i)\r\n1 − hii\r\n, i = 1, 2, . . . , n, (11.5.7)\r\nso that the studentized deleted residuals ti defined by\r\nti =\r\nDi\r\nS (i)/(1 − hii)\r\n, i = 1, 2, . . . , n, (11.5.8)\r\nhave a t(df = n − 3) distribution and we compare observed values of tito this distribution to decide\r\nwhether or not an observation is extreme.\r\nThe folklore in regression classes is that a test based on the statistic in Equation 11.5.8 can be\r\ntoo liberal. A rule of thumb is if we suspect an observation to be an outlier before seeing the data\r\nthen we say it is significantly outlying if its two-tailed p-value is less than α, but if we suspect an\r\nobservation to be an outlier after seeing the data then we should only say it is significantly outlying\r\nif its two-tailed p-value is less than α/n. The latter rule of thumb is called the Bonferroni approach\r\nand can be overly conservative for large data sets. The responsible statistician should look at the\r\ndata and use his/her best judgement, in every case.\r\n11.5.2 How to do it with R\r\nWe can calculate the standardized residuals with the rstandard function. The input is the lm\r\nobject, which is cars.lm.\r\n> sres <- rstandard(cars.lm)\r\n> sres[1:5]\r\n1 2 3 4 5\r\n0.2660415 0.8189327 -0.4013462 0.8132663 0.1421624\r\nWe can find out which observations have studentized residuals larger than two with the com\u0002mand\r\n> sres[which(abs(sres) > 2)]\r\n23 35 49\r\n2.795166 2.027818 2.919060\r\nIn this case, we see that observations 23, 35, and 49 are potential outliers with respect to their\r\ny-value.\r\nWe can compute the studentized deleted residuals with rstudent:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/dac91b11-e2e4-4e70-880a-80b66b989920.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7acaee7f984424e02d1788942b4d3d48149e817730d0a4ddf85bc9fd1dc07b85",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 395
      },
      {
        "segments": [
          {
            "segment_id": "f6664550-10b2-45aa-a588-982e8bd68cab",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 293,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.5. OTHER DIAGNOSTIC TOOLS 277\r\n> sdelres <- rstudent(cars.lm)\r\n> sdelres[1:5]\r\n1 2 3 4 5\r\n0.2634500 0.8160784 -0.3978115 0.8103526 0.1407033\r\nWe should compare these values with critical values from a t(df = n − 3) distribution, which in\r\nthis case is t(df = 50 − 3 = 47). We can calculate a 0.005 quantile and check with\r\n> t0.005 <- qt(0.005, df = 47, lower.tail = FALSE)\r\n> sdelres[which(abs(sdelres) > t0.005)]\r\n23 49\r\n3.022829 3.184993\r\nThis means that observations 23 and 49 have a large studentized deleted residual. The leverages\r\ncan be found with the hatvalues function:\r\n> leverage <- hatvalues(cars.lm)\r\n> leverage[1:5]\r\n1 2 3 4 5\r\n0.11486131 0.11486131 0.07150365 0.07150365 0.05997080\r\n> leverage[which(leverage > 4/50)]\r\n1 2 50\r\n0.11486131 0.11486131 0.08727007\r\nHere we see that observations 1, 2, and 50 have leverages bigger than double their mean value.\r\nThese observations would be considered outlying with respect to their x value (although they may\r\nor may not be influential).\r\n11.5.3 Influential Observations\r\nDFBET AS and DFF IT S\r\nAnytime we do a statistical analysis, we are confronted with the variability of data. It is always\r\na concern when an observation plays too large a role in our regression model, and we would not\r\nlike or procedures to be overly influenced by the value of a single observation. Hence, it becomes\r\ndesirable to check to see how much our estimates and predictions would change if one of the\r\nobservations were not included in the analysis. If an observation changes the estimates/predictions\r\na large amount, then the observation is influential and should be subjected to a higher level of\r\nscrutiny.\r\nWe measure the change in the parameter estimates as a result of deleting an observation with\r\nDFBET AS . The DFBET AS for the intercept b0 are given by\r\n(DFBET AS )0(i) =\r\nb0 − b0(i)\r\nS (i)\r\nq\r\n1\r\nn\r\n+\r\nx\r\n2\r\nPn\r\ni=1\r\n(xi−x)\r\n2\r\n, i = 1, 2, . . . , n. (11.5.9)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f6664550-10b2-45aa-a588-982e8bd68cab.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=30c3bff371d5e519f834016e0de6813903d04ff7744cf13d7494c5b24cf3d408",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 329
      },
      {
        "segments": [
          {
            "segment_id": "ad6b95cf-048e-41f6-80d5-6ba0e92358f0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 294,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "278 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nand the DFBET AS for the slope b1 are given by\r\n(DFBET AS )1(i) =\r\nb1 − b1(i)\r\nS (i)\r\nhPn\r\ni=1\r\n(xi − x)\r\n2\r\ni−1/2\r\n, i = 1, 2, . . . , n. (11.5.10)\r\nSee Section 12.8 for a better way to write these. The signs of the DFBET AS indicate whether the\r\ncoefficients would increase or decrease as a result of including the observation. If the DFBET AS\r\nare large, then the observation has a large impact on those regression coefficients. We label obser\u0002vations as suspicious if their DFBET AS have magnitude greater 1 for small data or 2/\r\n√\r\nn for large\r\ndata sets.\r\nWe can calculate the DFBET AS with the dfbetas function (some output has been omitted):\r\n> dfb <- dfbetas(cars.lm)\r\n> head(dfb)\r\n(Intercept) speed\r\n1 0.09440188 -0.08624563\r\n2 0.29242487 -0.26715961\r\n3 -0.10749794 0.09369281\r\n4 0.21897614 -0.19085472\r\n5 0.03407516 -0.02901384\r\n6 -0.11100703 0.09174024\r\nWe see that the inclusion of the first observation slightly increases the Intercept and slightly\r\ndecreases the coefficient on speed.\r\nWe can measure the influence that an observation has on its fitted value with DFF IT S . These\r\nare calculated by deleting an observation, refitting the model, recalculating the fit, then standardiz\u0002ing. The formula is\r\n(DFF IT S )i =\r\nYˆ\r\ni − Yˆ(i)\r\nS (i)\r\n√\r\nhii\r\n, i = 1, 2, . . . , n. (11.5.11)\r\nThe value represents the number of standard deviations of Yˆ\r\ni\r\nthat the fitted value Yˆ\r\ni\r\nincreases or\r\ndecreases with the inclusion of the i\r\nth observation. We can compute them with the dffits function.\r\n> dff <- dffits(cars.lm)\r\n> dff[1:5]\r\n1 2 3 4 5\r\n0.09490289 0.29397684 -0.11039550 0.22487854 0.03553887\r\nA rule of thumb is to flag observations whose DFF IT exceeds one in absolute value, but there\r\nare none of those in this data set.\r\nCook’s Distance\r\nThe DFF IT S are good for measuring the influence on a single fitted value, but we may want to\r\nmeasure the influence an observation has on all of the fitted values simultaneously. The statistics\r\nused for measuring this are Cook’s distances which may be calculated5 by the formula\r\n5Cook’s distances are actually defined by a different formula than the one shown. The formula in Equation 11.5.12 is\r\nalgebraically equivalent to the defining formula and is, in the author’s opinion, more transparent.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ad6b95cf-048e-41f6-80d5-6ba0e92358f0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5408d7ebb6dcb1bd4968c0f3e8a8ea855facdabe13715459924b7b1b82d4c92a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 399
      },
      {
        "segments": [
          {
            "segment_id": "e03072d0-731e-4b81-b161-3026f42cc729",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 295,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.5. OTHER DIAGNOSTIC TOOLS 279\r\nDi =\r\nE\r\n2\r\ni\r\n(p + 1)S\r\n2\r\n·\r\nhii\r\n(1 − hii)\r\n2\r\n, i = 1, 2, . . . , n. (11.5.12)\r\nIt shows that Cook’s distance depends both on the residual Ei and the leverage hii and in this way\r\nDi contains information about outlying x and y values.\r\nTo assess the significance of D, we compare to quantiles of an f(df1 = 2, df2 = n − 2)\r\ndistribution. A rule of thumb is to classify observations falling higher than the 50th percentile as\r\nbeing extreme.\r\n11.5.4 How to do it with R\r\nWe can calculate the Cook’s Distances with the cooks.distance function.\r\n> cooksD <- cooks.distance(cars.lm)\r\n> cooksD[1:5]\r\n1 2 3 4 5\r\n0.0045923121 0.0435139907 0.0062023503 0.0254673384 0.0006446705\r\nWe can look at a plot of the Cook’s distances with the command plot(cars.lm, which =\r\n4).\r\nObservations with the largest Cook’s D values are labeled, hence we see that observations 23,\r\n39, and 49 are suspicious. However, we need to compare to the quantiles of an f(df1 = 2, df2 =\r\n48) distribution:\r\n> F0.50 <- qf(0.5, df1 = 2, df2 = 48)\r\n> cooksD[which(cooksD > F0.50)]\r\nnamed numeric(0)\r\nWe see that with this data set there are no observations with extreme Cook’s distance, after all.\r\n11.5.5 All Influence Measures Simultaneously\r\nWe can display the result of diagnostic checking all at once in one table, with potentially influential\r\npoints displayed. We do it with the command influence.measures(cars.lm):\r\n> influence.measures(cars.lm)\r\nThe output is a huge matrix display, which we have omitted in the interest of brevity. A point\r\nis identified if it is classified to be influential with respect to any of the diagnostic measures. Here\r\nwe see that observations 2, 11, 15, and 18 merit further investigation.\r\nWe can also look at all diagnostic plots at once with the commands\r\n> par(mfrow = c(2, 2))\r\n> plot(cars.lm)\r\n> par(mfrow = c(1, 1))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e03072d0-731e-4b81-b161-3026f42cc729.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3fb5b686614a72a90f8ada6848e659d2b5ef4f244d8897fbdb7912dc69f9e156",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "8dedd035-c988-4d03-a9b7-6d1b20223319",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 296,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "280 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\n0 10 20 30 40 50\r\n0.0 0.1 0.2 0.3\r\nObs. number\r\nCook's distance\r\nlm(dist ~ speed)\r\nCook's distance\r\n49\r\n23\r\n39\r\nFigure 11.5.1: Cook’s distances for the cars data\r\nUsed for checking for influential and/our outlying observations. Values with large Cook’s distance merit\r\nfurther investigation.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8dedd035-c988-4d03-a9b7-6d1b20223319.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63b3931d1a9ce0658c230679c637684ce4a809a037dc38bf78daf5bdc145a56e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 375
      },
      {
        "segments": [
          {
            "segment_id": "7cbc3130-a309-4bbe-945f-b42d24ee3c9d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 297,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.5. OTHER DIAGNOSTIC TOOLS 281\r\n0 20 40 60 80\r\n−20\r\n0 20 40\r\nFitted values\r\nResiduals\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\nResiduals vs Fitted\r\n23 49\r\n35\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ● ●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ● ●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n−2 −1 0 1 2\r\n−2\r\n0 1 2\r\n3\r\nTheoretical Quantiles\r\nStandardized residuals\r\nNormal Q−Q\r\n4923\r\n35\r\n0 20 40 60 80\r\n0.0 0.5 1.0 1.5\r\nFitted values\r\nStand\r\nardize\r\nd residuals\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\nScale−Location\r\n49 23\r\n35\r\n0.00 0.04 0.08\r\n−2\r\n0 1 2\r\n3\r\nLeverage\r\nStandardized residuals\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\nCook's distance\r\n0.5\r\nResiduals vs Leverage\r\n49 23\r\n39\r\nFigure 11.5.2: Diagnostic plots for the cars data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7cbc3130-a309-4bbe-945f-b42d24ee3c9d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5a6072aea660e3ceab14d5605e84467e91aaf5b0ccddb99447eae0a3d10bff9e",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "5ceeb7d3-4f6e-4a60-a910-2db23dff62d4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 298,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "282 CHAPTER 11. SIMPLE LINEAR REGRESSION\r\nThe par command is used so that 2 × 2 = 4 plots will be shown on the same display. The\r\ndiagnostic plots for the cars data are shown in Figure 11.5.2:\r\nWe have discussed all of the plots except the last, which is possibly the most interesting. It\r\nshows Residuals vs. Leverage, which will identify outlying y values versus outlying x values. Here\r\nwe see that observation 23 has a high residual, but low leverage, and it turns out that observations\r\n1 and 2 have relatively high leverage but low/moderate leverage (they are on the right side of the\r\nplot, just above the horizontal line). Observation 49 has a large residual with a comparatively large\r\nleverage.\r\nWe can identify the observations with the identify command; it allows us to display the\r\nobservation number of dots on the plot. First, we plot the graph, then we call identify:\r\n> plot(cars.lm, which = 5) # std'd resids vs lev plot\r\n> identify(leverage, sres, n = 4) # identify 4 points\r\nThe graph with the identified points is omitted (but the plain plot is shown in the bottom right\r\ncorner of Figure 11.5.2). Observations 1 and 2 fall on the far right side of the plot, near the\r\nhorizontal axis.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5ceeb7d3-4f6e-4a60-a910-2db23dff62d4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fadfa058e24b8dd3ef1e751332425d6670b48e27c30f3e136f87ac59907f93fc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 492
      },
      {
        "segments": [
          {
            "segment_id": "299331e7-e7b9-4303-8b83-8320f382aeed",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 299,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "11.5. OTHER DIAGNOSTIC TOOLS 283\r\nChapter Exercises\r\nExercise 11.1. Prove the ANOVA equality, Equation 11.3.5. Hint: show that\r\nXn\r\ni=1\r\n(Yi − Yˆ\r\ni)(Yˆi − Y) = 0.\r\nExercise 11.2. Solve the following system of equations for β1 and β0 to find the MLEs for slope\r\nand intercept in the simple linear regression model.\r\nnβ0 + β1\r\nXn\r\ni=1\r\nxi =\r\nXn\r\ni=1\r\nyi\r\nβ0\r\nXn\r\ni=1\r\nxi + β1\r\nXn\r\ni=1\r\nx\r\n2\r\ni =\r\nXn\r\ni=1\r\nxiyi\r\nExercise 11.3. Show that the formula given in Equation 11.2.17 is equivalent to\r\nβˆ\r\n1 =\r\nPn\r\ni=1\r\nxiyi −\r\n\u0010Pn\r\ni=1\r\nxi\r\n\u0011 \u0010Pn\r\ni=1\r\nyi\r\n\u0011. n\r\nPn\r\ni=1\r\nx\r\n2\r\ni\r\n−\r\n\u0010Pn\r\ni=1\r\nxi\r\n\u00112\r\n\u001e\r\nn\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/299331e7-e7b9-4303-8b83-8320f382aeed.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=478b84b49887efe024ef3cae4da28ff0f6d836a0eab0a2eee1b0a5045478aa92",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "0e7835eb-a894-4d38-93f0-9637df08afbc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 300,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "284 CHAPTER 11. SIMPLE LINEAR REGRESSION",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0e7835eb-a894-4d38-93f0-9637df08afbc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d86ce2ca88675946318fcafb05d8959f9edeb13002f76cc51d56ae2ade58b657",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "ec26ce67-e993-4777-be08-c958a0cb0795",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 301,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 12\r\nMultiple Linear Regression\r\nWe know a lot about simple linear regression models, and a next step is to study multiple regression\r\nmodels that have more than one independent (explanatory) variable. In the discussion that follows\r\nwe will assume that we have p explanatory variables, where p > 1.\r\nThe language is phrased in matrix terms – for two reasons. First, it is quicker to write and\r\n(arguably) more pleasant to read. Second, the matrix approach will be required for later study of\r\nthe subject; the reader might as well be introduced to it now.\r\nMost of the results are stated without proof or with only a cursory justification. Those yearning\r\nfor more should consult an advanced text in linear regression for details, such as Applied Linear\r\nRegression Models [67]or Linear Models: Least Squares and Alternatives [69].\r\nWhat do I want them to know?\r\n• the basic MLR model, and how it relates to the SLR\r\n• how to estimate the parameters and use those estimates to make predictions\r\n• basic strategies to determine whether or not the model is doing a good job\r\n• a few thoughts about selected applications of the MLR, such as polynomial, interaction, and\r\ndummy variable models\r\n• some of the uses of residuals to diagnose problems\r\n• hints about what will be coming later\r\n12.1 The Multiple Linear Regression Model\r\nThe first thing to do is get some better notation. We will write\r\nYn×1 =\r\n\r\n\r\ny1\r\ny2\r\n.\r\n.\r\n.\r\nyn\r\n\r\n\r\n, and Xn×(p+1) =\r\n\r\n\r\n1 x11 x21 · · · xp1\r\n1 x12 x22 · · · xp2\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n1 x1n x2n · · · xpn\r\n\r\n\r\n. (12.1.1)\r\n285",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ec26ce67-e993-4777-be08-c958a0cb0795.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b96007f8add8b2fd06f1a6279d337f0c4c2fe21ca56523a2883258020fe36374",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 426
      },
      {
        "segments": [
          {
            "segment_id": "e5b97ecc-a72f-4feb-b814-3da352fddc23",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 302,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "286 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nThe vector Y is called the response vector and the matrix X is called the model matrix. As in\r\nChapter 11, the most general assumption that relates Y to X is\r\nY = µ(X) + \u000f, (12.1.2)\r\nwhere µ is some function (the signal) and \u000f is the noise (everything else). We usually impose some\r\nstructure on µ and \u000f. In particular, the standard multiple linear regression model assumes\r\nY = Xβ + \u000f, (12.1.3)\r\nwhere the parameter vector β looks like\r\nβ(p+1)×1 =\r\nh\r\nβ0 β1 · · · βp\r\niT\r\n, (12.1.4)\r\nand the random vector \u000fn×1 =\r\nh\r\n\u000f1 \u000f2 · · · \u000fn\r\niT\r\nis assumed to be distributed\r\n\u000f ∼ mvnorm \u0010mean = 0n×1, sigma = σ\r\n2\r\nIn×n\r\n\u0011\r\n. (12.1.5)\r\nThe assumption on \u000f is equivalent to the assumption that \u000f1, \u000f2, . . . , \u000fn are i.i.d. norm(mean =\r\n0, sd = σ). It is a linear model because the quantity µ(X) = Xβ is linear in the parameters β0,\r\nβ1,. . . , βp. It may be helpful to see the model in expanded form; the above matrix formulation is\r\nequivalent to the more lengthy\r\nYi = β0 + β1 x1i + β2 x2i + · · · + βp xpi + \u000fi, i = 1, 2, . . . , n. (12.1.6)\r\nExample 12.1. Girth, Height, and Volume for Black Cherry trees.Measurements were made\r\nof the girth, height, and volume of timber in 31 felled black cherry trees. Note that girth is the\r\ndiameter of the tree (in inches) measured at 4 ft 6 in above the ground. The variables are\r\n1. Girth: tree diameter in inches (denoted x1)\r\n2. Height: tree height in feet (x2).\r\n3. Volume: volume of the tree in cubic feet. (y)\r\nThe data are in the datasets package and are already on the search path; they can be viewed with\r\n> head(trees)\r\nGirth Height Volume\r\n1 8.3 70 10.3\r\n2 8.6 65 10.3\r\n3 8.8 63 10.2\r\n4 10.5 72 16.4\r\n5 10.7 81 18.8\r\n6 10.8 83 19.7\r\nLet us take a look at a visual display of the data. For multiple variables, instead of a simple\r\nscatterplot we use a scatterplot matrix which is made with the splom function in the lattice\r\npackage [75] as shown below. The plot is shown in Figure 12.1.1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e5b97ecc-a72f-4feb-b814-3da352fddc23.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6bb6ffbbb3ac2741801b57e2f1f093170681a42bef6f34604dc02d7815ea644e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 398
      },
      {
        "segments": [
          {
            "segment_id": "d929cbe4-1983-4771-8134-db50d06dfaa2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 303,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.1. THE MULTIPLE LINEAR REGRESSION MODEL 287\r\nScatter Plot Matrix\r\n14 Girth\r\n16\r\n18\r\n20 14161820\r\n8\r\n10\r\n12\r\n14\r\n8 101214\r\n● ● ●\r\n● ● ●● ●●● ● ● ● ●\r\n● ● ●● ● ● ●●\r\n● ●\r\n●●●\r\n●\r\n●●●\r\n●●●●●●●●●●●\r\n●● ● ●● ●●\r\n●\r\n●●\r\n●●●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n75Height\r\n80\r\n85 75 80 85\r\n65\r\n70\r\n75\r\n65 70 75\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n● ●\r\n●\r\n●●●\r\n●●●\r\n●\r\n●\r\n●●\r\n●●●●●\r\n●\r\n●●●\r\n●●\r\n● ●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●● ●\r\n● ●● ● ● ● ● ● ● ●●●\r\n●\r\n● ● ●\r\n●●\r\n●●\r\n●\r\n●●●\r\n●\r\n●\r\nVolume 50\r\n60\r\n70\r\n80\r\n50 70\r\n10\r\n20\r\n30\r\n40\r\n10 30 Figure 12.1.1: Scatterplot matrix of trees data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d929cbe4-1983-4771-8134-db50d06dfaa2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb9d23faa3ab9a0be53dd304f6fff99e22a67e26a7d58c8070ca921b2133ccff",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 177
      },
      {
        "segments": [
          {
            "segment_id": "4717a3f1-982a-45c7-a1e6-3fb45fe5a28d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 304,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "288 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\n> library(lattice)\r\n> splom(trees)\r\nThe dependent (response) variable Volume is listed in the first row of the scatterplot matrix.\r\nMoving from left to right, we see an approximately linear relationship between Volume and the\r\nindependent (explanatory) variables Height and Girth. A first guess at a model for these data\r\nmight be\r\nY = β0 + β1 x1 + β2 x2 + \u000f, (12.1.7)\r\nin which case the quantity µ(x1, x2) = β0 + β1 x1 + β2 x2 would represent the mean value of Y at the\r\npoint (x1, x2).\r\nWhat does it mean?\r\nThe interpretation is simple. The intercept β0 represents the mean Volume when all other indepen\u0002dent variables are zero. The parameter βi represents the change in mean Volume when there is a\r\nunit increase in xi, while the other independent variable is held constant. For the trees data, β1\r\nrepresents the change in average Volume as Girth increases by one unit when the Height is held\r\nconstant, and β2 represents the change in average Volume as Height increases by one unit when\r\nthe Girth is held constant.\r\nIn simple linear regression, we had one independent variable and our linear regression surface\r\nwas 1D, simply a line. In multiple regression there are many independent variables and so our\r\nlinear regression surface will be many-D. . . in general, a hyperplane. But when there are only\r\ntwo explanatory variables the hyperplane is just an ordinary plane and we can look at it with a 3D\r\nscatterplot.\r\nOne way to do this is with the R Commander in the Rcmdr package [31]. It has a 3D scatterplot\r\noption under the Graphs menu. It is especially great because the resulting graph is dynamic; it can\r\nbe moved around with the mouse, zoomed, etc. But that particular display does not translate well\r\nto a printed book.\r\nAnother way to do it is with the scatterplot3d function in the scatterplot3d package.\r\nThe code follows, and the result is shown in Figure 12.1.2.\r\n> library(scatterplot3d)\r\n> s3d <- with(trees, scatterplot3d(Girth, Height, Volume, pch = 16,\r\n+ highlight.3d = TRUE, angle = 60))\r\n> fit <- lm(Volume ~ Girth + Height, data = trees)\r\n> s3d$plane3d(fit)\r\nLooking at the graph we see that the data points fall close to a plane in three dimensional space.\r\n(The plot looks remarkably good. In the author’s experience it is rare to see points fit so well to the\r\nplane without some additional work.)\r\n12.2 Estimation and Prediction\r\n12.2.1 Parameter estimates\r\nWe will proceed exactly like we did in Section 11.2. We know\r\n\u000f ∼ mvnorm \u0010mean = 0n×1, sigma = σ\r\n2\r\nIn×n\r\n\u0011\r\n, (12.2.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4717a3f1-982a-45c7-a1e6-3fb45fe5a28d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=10bbd0a55215b6fbb52e42cb82d27ff59b283ee6e36679f556829ea53ebce954",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "20c22f31-f8df-4621-b5c0-c13709856f8e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 305,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.2. ESTIMATION AND PREDICTION 289\r\n 8 10 12 14 16 18 20 22\r\n10 20 30 40 50 60 70 80\r\n60\r\n65\r\n70\r\n75\r\n80\r\n85\r\n90\r\nGirth\r\nHeight\r\nVolume\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●● ●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●\r\nFigure 12.1.2: 3D scatterplot with regression plane for the trees data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/20c22f31-f8df-4621-b5c0-c13709856f8e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a2c122071c2f4062691f5281fcdd74b57fc179d5731838e5a26e9c8cdaa32e06",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "94e42f94-778d-4cef-b205-1d6ae37c55cf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 306,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "290 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nwhich means that Y = Xβ + \u000f has an mvnorm \u0010mean = Xβ, sigma = σ\r\n2\r\nIn×n\r\n\u0011\r\ndistribution. There\u0002fore, the likelihood function is\r\nL(β, σ) =\r\n1\r\n2π\r\nn/2σ\r\nexp (−\r\n1\r\n2σ2\r\n(Y − Xβ)\r\nT\r\n(Y − Xβ)\r\n)\r\n. (12.2.2)\r\nTo maximize the likelihood in β, we need to minimize the quantity g(β) = (Y − Xβ)\r\nT\r\n(Y − Xβ).\r\nWe do this by differentiating g with respect to β. (It may be a good idea to brush up on the material\r\nin Appendices E.5 and E.6.) First we will rewrite g:\r\ng(β) = Y\r\nTY − YTXβ − βTXTY + βTXTXβ, (12.2.3)\r\nwhich can be further simplified to g(β) = Y\r\nTY − 2βTXTY + βTXTXβ since βTXTY is 1 × 1 and\r\nthus equal to its transpose. Now we differentiate to get\r\n∂g\r\n∂β\r\n= 0 − 2X\r\nTY + 2XTXβ, (12.2.4)\r\nsince X\r\nTX is symmetric. Setting the derivative equal to the zero vector yields the so called “normal\r\nequations”\r\nX\r\nTXβ = XTY. (12.2.5)\r\nIn the case that X\r\nTX is invertible1\r\n, we may solve the equation for β to get the maximum likelihood\r\nestimator of β which we denote by b:\r\nb =\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nTY. (12.2.6)\r\nRemark 12.2. The formula in Equation 12.2.6 is convenient for mathematical study but is inconve\u0002nient for numerical computation. Researchers have devised much more efficient algorithms for the\r\nactual calculation of the parameter estimates, and we do not explore them here.\r\nRemark 12.3. We have only found a critical value, and have not actually shown that the critical\r\nvalue is a minimum. We omit the details and refer the interested reader to [69].\r\n12.2.2 How to do it with R\r\nWe do all of the above just as we would in simple linear regression. The powerhouse is the lm\r\nfunction. Everything else is based on it. We separate explanatory variables in the model formula\r\nby a plus sign.\r\n> trees.lm <- lm(Volume ~ Girth + Height, data = trees)\r\n> trees.lm\r\nCall:\r\nlm(formula = Volume ~ Girth + Height, data = trees)\r\nCoefficients:\r\n(Intercept) Girth Height\r\n-57.9877 4.7082 0.3393\r\n1We can find solutions of the normal equations even when XTX is not of full rank, but the topic falls outside the scope\r\nof this book. The interested reader can consult an advanced text such as Rao [69].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/94e42f94-778d-4cef-b205-1d6ae37c55cf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c9865194c5bcb24c4556fd1c1e899ae54be0e41a827b75d534708b37366ae634",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 405
      },
      {
        "segments": [
          {
            "segment_id": "a8e120ea-257d-41cb-bad7-fbd7c04a2622",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 307,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.2. ESTIMATION AND PREDICTION 291\r\nWe see from the output that for the trees data our parameter estimates are b =\r\nh\r\n−58.0 4.7 0.3\r\ni\r\n,\r\nand consequently our estimate of the mean response is ˆµ given by\r\nµˆ(x1, x2) = b0 + b1 x1 + b2 x2, (12.2.7)\r\n≈ − 58.0 + 4.7x1 + 0.3x2. (12.2.8)\r\nWe could see the entire model matrix X with the model.matrix function, but in the interest of\r\nbrevity we only show the first few rows.\r\n> head(model.matrix(trees.lm))\r\n(Intercept) Girth Height\r\n1 1 8.3 70\r\n2 1 8.6 65\r\n3 1 8.8 63\r\n4 1 10.5 72\r\n5 1 10.7 81\r\n6 1 10.8 83\r\n12.2.3 Point Estimates of the Regression Surface\r\nThe parameter estimates b make it easy to find the fitted values, Yˆ . We write them individually as\r\nYˆ\r\ni\r\n, i = 1, 2, . . . , n, and recall that they are defined by\r\nYˆ\r\ni = µˆ(x1i\r\n, x2i), (12.2.9)\r\n= b0 + b1 x1i + b2 x2i, i = 1, 2, . . . , n. (12.2.10)\r\nThey are expressed more compactly by the matrix equation\r\nYˆ = Xb. (12.2.11)\r\nFrom Equation 12.2.6 we know that b =\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nTY, so we can rewrite\r\nYˆ = X\r\n\u0014\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nTY\r\n\u0015\r\n, (12.2.12)\r\n= HY, (12.2.13)\r\nwhere H = X\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nT\r\nis appropriately named the hat matrix because it “puts the hat on Y”.\r\nThe hat matrix is very important in later courses. Some facts about H are\r\n• H is a symmetric square matrix, of dimension n × n.\r\n• The diagonal entries hii satisfy 0 ≤ hii ≤ 1 (compare to Equation 11.5.2).\r\n• The trace is tr(H) = p.\r\n• H is idempotent (also known as a projection matrix) which means that H2 = H. The same is\r\ntrue of I − H.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a8e120ea-257d-41cb-bad7-fbd7c04a2622.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=86ecaffbd017b0e23dd1cdd89e127ee112551b259ef414b862ed0a8bc4d57055",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 323
      },
      {
        "segments": [
          {
            "segment_id": "05a850ac-cec5-4066-8a7b-b929d1ae0dd3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 308,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "292 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nNow let us write a column vector x0 = (x10, x20)\r\nT\r\nto denote given values of the explanatory variables\r\nGirth = x10 and Height = x20. These values may match those of the collected data, or they may\r\nbe completely new values not observed in the original data set. We may use the parameter estimates\r\nto find Yˆ(x0), which will give us\r\n1. an estimate of µ(x0), the mean value of a future observation at x0, and\r\n2. a prediction for Y(x0), the actual value of a future observation at x0.\r\nWe can represent Yˆ(x0) by the matrix equation\r\nYˆ(x0) = x\r\nT\r\n0\r\nb, (12.2.14)\r\nwhich is just a fancy way to write\r\nYˆ(x10, x20) = b0 + b1 x10 + b2 x20. (12.2.15)\r\nExample 12.4. If we wanted to predict the average volume of black cherry trees that have Girth\r\n= 15 in and are Height = 77 ft tall then we would use the estimate\r\nµˆ(15, 77) = − 58 + 4.7(15) + 0.3(77),\r\n≈35.6 ft3.\r\nWe would use the same estimate Yˆ = 35.6 to predict the measured Volume of another black cherry\r\ntree – yet to be observed – that has Girth = 15 in and is Height = 77 ft tall.\r\n12.2.4 How to do it with R\r\nThe fitted values are stored inside trees.lm and may be accessed with the fitted function. We\r\nonly show the first five fitted values.\r\n> fitted(trees.lm)[1:5]\r\n1 2 3 4 5\r\n4.837660 4.553852 4.816981 15.874115 19.869008\r\nThe syntax for general prediction does not change much from simple linear regression. The\r\ncomputations are done with the predict function as described below.\r\nThe only difference from SLR is in the way we tell R the values of the explanatory variables\r\nfor which we want predictions. In SLR we had only one independent variable but in MLR we have\r\nmany (for the trees data we have two). We will store values for the independent variables in the\r\ndata frame new, which has two columns (one for each independent variable) and three rows (we\r\nshall make predictions at three different locations).\r\n> new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69,\r\n+ 74, 87))\r\nWe can view the locations at which we will predict:\r\n> new",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/05a850ac-cec5-4066-8a7b-b929d1ae0dd3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e49fd8417a1c54679a38b07e2d6ec386a6f1f0504625578246b99c680b8851ae",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 383
      },
      {
        "segments": [
          {
            "segment_id": "e4c82566-2b80-4bb4-b545-604c88fbafc5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 309,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.2. ESTIMATION AND PREDICTION 293\r\nGirth Height\r\n1 9.1 69\r\n2 11.6 74\r\n3 12.5 87\r\nWe continue just like we would have done in SLR.\r\n> predict(trees.lm, newdata = new)\r\n1 2 3\r\n8.264937 21.731594 30.379205\r\nExample 12.5. Using the trees data,\r\n1. Report a point estimate of the mean Volume of a tree of Girth 9.1 in and Height 69 ft.\r\nThe fitted value for x1 = 9.1 and x2 = 69 is 8.3, so a point estimate would be 8.3 cubic feet.\r\n2. Report a point prediction for and a 95% prediction interval for the Volume of a hypothetical\r\ntree of Girth 12.5 in and Height 87 ft.\r\nThe fitted value for x1 = 12.5 and x2 = 87 is 30.4, so a point prediction for the Volume is\r\n30.4 cubic feet.\r\n12.2.5 Mean Square Error and Standard Error\r\nThe residuals are given by\r\nE = Y − Yˆ = Y − HY = (I − H)Y. (12.2.16)\r\nNow we can use Theorem 7.34 to see that the residuals are distributed\r\nE ∼ mvnorm(mean = 0, sigma = σ\r\n2\r\n(I − H)), (12.2.17)\r\nsince (I − H)Xβ = Xβ − Xβ = 0 and (I − H) (σ\r\n2\r\nI) (I − H)\r\nT = σ2\r\n(I − H)\r\n2 = σ2\r\n(I − H). The sum\r\nof squared errors S S E is just\r\nS S E = E\r\nTE = YT\r\n(I − H)(I − H)Y = Y\r\nT\r\n(I − H)Y. (12.2.18)\r\nRecall that in SLR we had two parameters (β0 and β1) in our regression model and we estimated\r\nσ\r\n2 with s2 = S S E/(n − 2). In MLR, we have p + 1 parameters in our regression model and we\r\nmight guess that to estimate σ\r\n2 we would use the mean square error S 2 defined by\r\nS\r\n2 =\r\nS S E\r\nn − (p + 1)\r\n. (12.2.19)\r\nThat would be a good guess. The residual standard error is S =\r\n√\r\nS\r\n2\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e4c82566-2b80-4bb4-b545-604c88fbafc5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e5c8b92a8322727902253f7fe8444845dc0a6d6f6cb54d8b493106064f816842",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 339
      },
      {
        "segments": [
          {
            "segment_id": "abe922df-eb79-4193-b05b-2eb90d58294b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 310,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "294 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\n12.2.6 How to do it with R\r\nThe residuals are also stored with trees.lm and may be accessed with the residuals function.\r\nWe only show the first five residuals.\r\n> residuals(trees.lm)[1:5]\r\n1 2 3 4 5\r\n5.4623403 5.7461484 5.3830187 0.5258848 -1.0690084\r\nThe summary function output (shown later) lists the Residual Standard Error which is\r\njust S =\r\n√\r\nS\r\n2\r\n. It is stored in the sigma component of the summary object.\r\n> treesumry <- summary(trees.lm)\r\n> treesumry$sigma\r\n[1] 3.881832\r\nFor the trees data we find s ≈ 3.882.\r\n12.2.7 Interval Estimates of the Parameters\r\nWe showed in Section 12.2.1 that b =\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nTY, which is really just a big matrix – namely\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nT – multiplied by Y. It stands to reason that the sampling distribution of b would be\r\nintimately related to the distribution of Y, which we assumed to be\r\nY ∼ mvnorm \u0010mean = Xβ, sigma = σ\r\n2\r\nI\r\n\u0011\r\n. (12.2.20)\r\nNow recall Theorem 7.34 that we said we were going to need eventually (the time is now). That\r\nproposition guarantees that\r\nb ∼ mvnorm \u0012mean = β, sigma = σ\r\n2\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\n\u0013\r\n, (12.2.21)\r\nsince\r\nIE b =\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nT\r\n(Xβ) = β, (12.2.22)\r\nand\r\nVar(b) =\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nX\r\nT\r\n(σ\r\n2\r\nI)X\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\n= σ\r\n2\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\n, (12.2.23)\r\nthe first equality following because the matrix \u0010X\r\nTX\r\n\u0011−1\r\nis symmetric.\r\nThere is a lot that we can glean from Equation 12.2.21. First, it follows that the estimator b\r\nis unbiased (see Section 9.1). Second, the variances of b0, b1, . . . , bn are exactly the diagonal\r\nelements of σ\r\n2\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\n, which is completely known except for that pesky parameter σ\r\n2\r\n. Third,\r\nwe can estimate the standard error of bi (denoted S bi) with the mean square error S (defined in\r\nthe previous section) multiplied by the corresponding diagonal element of \u0010X\r\nTX\r\n\u0011−1\r\n. Finally, given\r\nestimates of the standard errors we may construct confidence intervals for βi with an interval that\r\nlooks like\r\nbi ± tα/2(df = n − p − 1)S bi. (12.2.24)\r\nThe degrees of freedom for the Student’s t distribution2are the same as the denominator of S\r\n2\r\n.\r\n2We are taking great leaps over the mathematical details. In particular, we have yet to show that s2 has a chi-square",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/abe922df-eb79-4193-b05b-2eb90d58294b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4965d800fced90260d3f8ed2fadee48d12e51412736b1914c55b4a137f820362",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 424
      },
      {
        "segments": [
          {
            "segment_id": "ba0c2978-cb50-4a74-b0bf-f9bca73d110f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 311,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.2. ESTIMATION AND PREDICTION 295\r\n12.2.8 How to do it with R\r\nTo get confidence intervals for the parameters we need only use confint:\r\n> confint(trees.lm)\r\n2.5 % 97.5 %\r\n(Intercept) -75.68226247 -40.2930554\r\nGirth 4.16683899 5.2494820\r\nHeight 0.07264863 0.6058538\r\nFor example, using the calculations above we say that for the regression model Volume\r\n~Girth + Height we are 95% confident that the parameter β1 lies somewhere in the interval\r\n[4.2, 5.2].\r\n12.2.9 Confidence and Prediction Intervals\r\nWe saw in Section 12.2.3 how to make point estimates of the mean value of additional observations\r\nand predict values of future observations, but how good are our estimates? We need confidence and\r\nprediction intervals to gauge their accuracy, and lucky for us the formulas look similar to the ones\r\nwe saw in SLR.\r\nIn Equation 12.2.14 we wrote Yˆ(x0) = x\r\nT\r\n0\r\nb, and in Equation 12.2.21 we saw that\r\nb ∼ mvnorm \u0012mean = β, sigma = σ\r\n2\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\n\u0013\r\n, (12.2.25)\r\nThe following is therefore immediate from Theorem 7.34:\r\nYˆ(x0) ∼ mvnorm \u0012mean = x\r\nT\r\n0β, sigma = σ\r\n2\r\nx\r\nT\r\n0\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nx0\r\n\u0013\r\n. (12.2.26)\r\nIt should be no surprise that confidence intervals for the mean value of a future observation at the\r\nlocation x0 =\r\nh\r\nx10 x20 . . . xp0\r\niT\r\nare given by\r\nYˆ(x0) ± tα/2(df = n − p − 1) S\r\nq\r\nx\r\nT\r\n0\r\n\r\nXTX\r\n\u0001−1\r\nx0. (12.2.27)\r\nIntuitively, x\r\nT\r\n0\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\nx0 measures the distance of x0 from the center of the data. The degrees of\r\nfreedom in the Student’s t critical value are n−(p+1) because we need to estimate p+1 parameters.\r\nPrediction intervals for a new observation at x0 are given by\r\nYˆ(x0) ± tα/2(df = n − p − 1) S\r\nq\r\n1 + x\r\nT\r\n0\r\n\r\nXTX\r\n\u0001−1\r\nx0. (12.2.28)\r\nThe prediction intervals are wider than the confidence intervals, just as in Section 11.2.5.\r\ndistribution and we have not even come close to showing that bi and sbiare independent. But these are entirely outside the\r\nscope of the present book and the reader may rest assured that the proofs await in later classes. See C.R. Rao for more.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ba0c2978-cb50-4a74-b0bf-f9bca73d110f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=baab64351699d4bb490add22d6cd767b9b7683f654a6384d7835c96d22d486c9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 374
      },
      {
        "segments": [
          {
            "segment_id": "5d1f0926-18f1-48e5-9620-e07218bada7b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 312,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "296 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\n12.2.10 How to do it with R\r\nThe syntax is identical to that used in SLR, with the proviso that we need to specify values of the\r\nindependent variables in the data frame new as we did in Section 11.2.5 (which we repeat here for\r\nillustration).\r\n> new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69,\r\n+ 74, 87))\r\nConfidence intervals are given by\r\n> predict(trees.lm, newdata = new, interval = \"confidence\")\r\nfit lwr upr\r\n1 8.264937 5.77240 10.75747\r\n2 21.731594 20.11110 23.35208\r\n3 30.379205 26.90964 33.84877\r\nPrediction intervals are given by\r\n> predict(trees.lm, newdata = new, interval = \"prediction\")\r\nfit lwr upr\r\n1 8.264937 -0.06814444 16.59802\r\n2 21.731594 13.61657775 29.84661\r\n3 30.379205 21.70364103 39.05477\r\nAs before, the interval type is decided by the interval argument and the default confidence\r\nlevel is 95% (which can be changed with the level argument).\r\nExample 12.6. Using the trees data,\r\n1. Report a 95% confidence interval for the mean Volume of a tree of Girth 9.1 in and Height\r\n69 ft.\r\nThe 95% CI is given by [5.8, 10.8], so with 95% confidence the mean Volume lies somewhere\r\nbetween 5.8 cubic feet and 10.8 cubic feet.\r\n2. Report a 95% prediction interval for the Volume of a hypothetical tree of Girth 12.5 in and\r\nHeight 87 ft.\r\nThe 95% prediction interval is given by [26.9, 33.8], so with 95% confidence we may assert\r\nthat the hypothetical Volume of a tree of Girth 12.5 in and Height 87 ft would lie some\u0002where between 26.9 cubic feet and 33.8 feet.\r\n12.3 Model Utility and Inference\r\n12.3.1 Multiple Coefficient of Determination\r\nWe saw in Section 12.2.5 that the error sum of squares S S E can be conveniently written in MLR\r\nas\r\nS S E = Y\r\nT\r\n(I − H)Y. (12.3.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5d1f0926-18f1-48e5-9620-e07218bada7b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=218537b0d0aa50700d72a1e7fa79ebe4b70e429146877eab83746d5ffdbe005d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 303
      },
      {
        "segments": [
          {
            "segment_id": "43d7f85d-3603-4667-811d-c719f2f89d65",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 313,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.3. MODEL UTILITY AND INFERENCE 297\r\nIt turns out that there are equally convenient formulas for the total sum of squares S S TO and the\r\nregression sum of squares S S R. They are :\r\nS S TO =Y\r\nT\r\n \r\nI −\r\n1\r\nn\r\nJ\r\n!\r\nY (12.3.2)\r\nand\r\nS S R =Y\r\nT\r\n \r\nH −\r\n1\r\nn\r\nJ\r\n!\r\nY. (12.3.3)\r\n(The matrix J is defined in Appendix E.5.) Immediately from Equations 12.3.1, 12.3.2, and 12.3.3\r\nwe get the Anova Equality\r\nS S TO = S S E + S S R. (12.3.4)\r\n(See Exercise 12.1.) We define the multiple coefficient of determination by the formula\r\nR\r\n2 = 1 −\r\nS S E\r\nS S TO\r\n. (12.3.5)\r\nWe interpret R\r\n2\r\nas the proportion of total variation that is explained by the multiple regression\r\nmodel. In MLR we must be careful, however, because the value of R\r\n2\r\ncan be artificially inflated by\r\nthe addition of explanatory variables to the model, regardless of whether or not the added variables\r\nare useful with respect to prediction of the response variable. In fact, it can be proved that the\r\naddition of a single explanatory variable to a regression model will increase the value of R\r\n2\r\n, no\r\nmatter how worthless the explanatory variable is. We could model the height of the ocean tides,\r\nthen add a variable for the length of cheetah tongues on the Serengeti plain, and our R\r\n2 would\r\ninevitably increase.\r\nThis is a problem, because as the philosopher, Occam, once said: “causes should not be multi\u0002plied beyond necessity”. We address the problem by penalizing R\r\n2 when parameters are added to\r\nthe model. The result is an adjusted R2 which we denote by R\r\n2\r\n.\r\nR\r\n2\r\n=\r\n\u0012\r\nR\r\n2 −\r\np\r\nn − 1\r\n\u0013\r\n \r\nn − 1\r\nn − p − 1\r\n!\r\n. (12.3.6)\r\nIt is good practice for the statistician to weigh both R\r\n2\r\nand R\r\n2\r\nduring assessment of model utility.\r\nIn many cases their values will be very close to each other. If their values differ substantially, or\r\nif one changes dramatically when an explanatory variable is added, then (s)he should take a closer\r\nlook at the explanatory variables in the model.\r\n12.3.2 How to do it with R\r\nFor the trees data, we can get R\r\n2\r\nand R\r\n2\r\nfrom the summary output or access the values directly\r\nby name as shown (recall that we stored the summary object in treesumry).\r\n> treesumry$r.squared\r\n[1] 0.94795\r\n> treesumry$adj.r.squared\r\n[1] 0.9442322\r\nHigh values of R\r\n2\r\nand R\r\n2\r\nsuch as these indicate that the model fits very well, which agrees with\r\nwhat we saw in Figure 12.1.2.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/43d7f85d-3603-4667-811d-c719f2f89d65.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=928b8a40d37b0e6d55a7f7d9279f028c814309c219c1492e95779565e267c2b3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 454
      },
      {
        "segments": [
          {
            "segment_id": "d8524b63-a315-471d-bbc5-acbd17a08855",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 314,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "298 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\n12.3.3 Overall F-Test\r\nAnother way to assess the model’s utility is to to test the hypothesis\r\nH0 : β1 = β2 = · · · = βp = 0 versus H1 : at least one βi , 0.\r\nThe idea is that if all βi’s were zero, then the explanatory variables X1, . . . , Xp would be worthless\r\npredictors for the response variable Y. We can test the above hypothesis with the overall F statistic,\r\nwhich in MLR is defined by\r\nF =\r\nS S R/p\r\nS S E/(n − p − 1)\r\n. (12.3.7)\r\nWhen the regression assumptions hold and under H0, it can be shown that F ∼ f(df1 = p, df2 =\r\nn − p − 1). We reject H0 when F is large, that is, when the explained variation is large relative to\r\nthe unexplained variation.\r\n12.3.4 How to do it with R\r\nThe overall F statistic and its associated p-value is listed at the bottom of the summary output,\r\nor we can access it directly by name; it is stored in the fstatistic component of the summary\r\nobject.\r\n> treesumry$fstatistic\r\nvalue numdf dendf\r\n254.9723 2.0000 28.0000\r\nFor the trees data, we see that F = 254.972337410669 with a p-value < 2.2e-16. Conse\u0002quently we reject H0, that is, the data provide strong evidence that not all βi’s are zero.\r\n12.3.5 Student’s t Tests\r\nWe know that\r\nb ∼ mvnorm \u0012mean = β, sigma = σ\r\n2\r\n\u0010\r\nX\r\nTX\r\n\u0011−1\r\n\u0013\r\n(12.3.8)\r\nand we have seen how to test the hypothesis H0 : β1 = β2 = · · · = βp = 0, but let us now consider\r\nthe test\r\nH0 : βi = 0 versus H1 : βi , 0, (12.3.9)\r\nwhere βiis the coefficient for the i\r\nth independent variable. We test the hypothesis by calculating a\r\nstatistic, examining it’s null distribution, and rejecting H0 if the p-value is small. If H0 is rejected,\r\nthen we conclude that there is a significant relationship between Y and xiin the regression model\r\nY ∼ (x1, . . . , xp). This last part of the sentence is very important because the significance of the\r\nvariable xi sometimes depends on the presence of other independent variables in the model3.\r\n3\r\nIn other words, a variable might be highly significant one moment but then fail to be significant when another variable is\r\nadded to the model. When this happens it often indicates a problem with the explanatory variables, such as multicollinearity.\r\nSee Section 12.9.3.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d8524b63-a315-471d-bbc5-acbd17a08855.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=da496ed683ea55597620f7f9c97acb30080962fb016a081c32c7a76ce66bd9af",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 425
      },
      {
        "segments": [
          {
            "segment_id": "a3df5584-ef44-4d03-86a1-4329afd7212e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 315,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.4. POLYNOMIAL REGRESSION 299\r\nTo test the hypothesis we go to find the sampling distribution of bi, the estimator of the corre\u0002sponding parameter βi\r\n, when the null hypothesis is true. We saw in Section 12.2.7 that\r\nTi =\r\nbi − βi\r\nS bi\r\n(12.3.10)\r\nhas a Student’s t distribution with n − (p + 1) degrees of freedom. (Remember, we are estimating\r\np + 1 parameters.) Consequently, under the null hypothesis H0 : βi = 0 the statistic ti = bi/S bi\r\nhas\r\na t(df = n − p − 1) distribution.\r\n12.3.6 How to do it with R\r\nThe Student’s t tests for significance of the individual explanatory variables are shown in the\r\nsummary output.\r\n> treesumry\r\nCall:\r\nlm(formula = Volume ~ Girth + Height, data = trees)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-6.4065 -2.6493 -0.2876 2.2003 8.4847\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) -57.9877 8.6382 -6.713 2.75e-07 ***\r\nGirth 4.7082 0.2643 17.816 < 2e-16 ***\r\nHeight 0.3393 0.1302 2.607 0.0145 *\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 3.882 on 28 degrees of freedom\r\nMultiple R-squared: 0.948, Adjusted R-squared: 0.9442\r\nF-statistic: 255 on 2 and 28 DF, p-value: < 2.2e-16\r\nWe see from the p-values that there is a significant linear relationship between Volume and\r\nGirth and between Volume and Height in the regression model Volume ~Girth + Height.\r\nFurther, it appears that the Intercept is significant in the aforementioned model.\r\n12.4 Polynomial Regression\r\n12.4.1 Quadratic Regression Model\r\nIn each of the previous sections we assumed that µ was a linear function of the explanatory vari\u0002ables. For example, in SLR we assumed that µ(x) = β0 + β1 x, and in our previous MLR examples\r\nwe assumed µ(x1, x2) = β0+β1 x1+β2 x2. In every case the scatterplots indicated that our assumption",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a3df5584-ef44-4d03-86a1-4329afd7212e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=249a6c70281b3e65e05ad4725b607d7a5723352e237ea3ce0f34592389e72f26",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "dbec08ad-34dd-4c92-af52-8fc1aab8f7f3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 316,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "300 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\n● ●●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●● ● ●\r\n●\r\n●\r\n● ●●\r\n●\r\n●\r\n● ●\r\n●\r\n●● ●\r\n●●\r\n●\r\n8 10 12 14 16 18 20\r\n10 30 50 70\r\nGirth\r\nVolume\r\nFigure 12.4.1: Scatterplot of Volume versus Girth for the trees data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/dbec08ad-34dd-4c92-af52-8fc1aab8f7f3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71c5f40f191b64e38ef86e17bd25dba8835d22d287e60e3a0f44eb885bddd0c9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 364
      },
      {
        "segments": [
          {
            "segment_id": "9bfe6654-3729-4c7d-b98d-3db790dbbc2d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 317,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.4. POLYNOMIAL REGRESSION 301\r\nwas reasonable. Sometimes, however, plots of the data suggest that the linear model is incomplete\r\nand should be modified.\r\nFor example, let us examine a scatterplot of Volume versus Girth a little more closely. See\r\nFigure 12.4.1. There might be a slight curvature to the data; the volume curves ever so slightly\r\nupward as the girth increases. After looking at the plot we might try to capture the curvature with\r\na mean response such as\r\nµ(x1) = β0 + β1 x1 + β2 x\r\n2\r\n1\r\n. (12.4.1)\r\nThe model associated with this choice of µ is\r\nY = β0 + β1 x1 + β2 x\r\n2\r\n1 + \u000f. (12.4.2)\r\nThe regression assumptions are the same. Almost everything indeed is the same. In fact, it is still\r\ncalled a “linear regression model”, since the mean response µ is linear in the parameters β0, β1,\r\nand β2.\r\nHowever, there is one important difference. When we introduce the squared variable in the\r\nmodel we inadvertently also introduce strong dependence between the terms which can cause sig\u0002nificant numerical problems when it comes time to calculate the parameter estimates. Therefore,\r\nwe should usually rescale the independent variable to have mean zero (and even variance one if we\r\nwish) before fitting the model. That is, we replace the xi’s with xi − x (or (xi − x)/s) before fitting\r\nthe model4.\r\nHow to do it with R\r\nThere are multiple ways to fit a quadratic model to the variables Volume and Girth using R.\r\n1. One way would be to square the values for Girth and save them in a vector Girthsq. Next,\r\nfit the linear model Volume ~Girth + Girthsq.\r\n2. A second way would be to use the insulate function in R, denoted by I:\r\nVolume ~ Girth + I(Girth ^2)\r\nThe second method is shorter than the first but the end result is the same. And once we\r\ncalculate and store the fitted model (in, say, treesquad.lm) all of the previous comments\r\nregarding R apply.\r\n3. A third and “right” way to do it is with orthogonal polynomials:\r\nVolume ~ poly(Girth , degree = 2)\r\nSee ?poly and ?cars for more information. Note that we can recover the approach in 2 with\r\npoly(Girth, degree = 2, raw = TRUE).\r\n4Rescaling the data gets the job done but a better way to avoid the multicollinearity introduced by the higher order terms\r\nis with orthogonal polynomials, whose coefficients are chosen just right so that the polynomials are not correlated with each\r\nother. This is beginning to linger outside the scope of this book, however, so we will content ourselves with a brief mention\r\nand then stick with the rescaling approach in the discussion that follows. A nice example of orthogonal polynomials in\r\naction can be run with example(cars).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9bfe6654-3729-4c7d-b98d-3db790dbbc2d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7a9060b9ac42cc03df6892a3800890f98a11b0300da8bea580d80ecb3aea463f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 469
      },
      {
        "segments": [
          {
            "segment_id": "47acabb3-2538-47e7-9b9b-04e1c135dff5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 318,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "302 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nExample 12.7. We will fit the quadratic model to the trees data and display the results with\r\nsummary, being careful to rescale the data before fitting the model. We may rescale the Girth\r\nvariable to have zero mean and unit variance on-the-fly with the scale function.\r\n> treesquad.lm <- lm(Volume ~ scale(Girth) + I(scale(Girth)^2),\r\n+ data = trees)\r\n> summary(treesquad.lm)\r\nCall:\r\nlm(formula = Volume ~ scale(Girth) + I(scale(Girth)^2), data = trees)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-5.4889 -2.4293 -0.3718 2.0764 7.6447\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) 27.7452 0.8161 33.996 < 2e-16 ***\r\nscale(Girth) 14.5995 0.6773 21.557 < 2e-16 ***\r\nI(scale(Girth)^2) 2.5067 0.5729 4.376 0.000152 ***\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 3.335 on 28 degrees of freedom\r\nMultiple R-squared: 0.9616, Adjusted R-squared: 0.9588\r\nF-statistic: 350.5 on 2 and 28 DF, p-value: < 2.2e-16\r\nWe see that the F statistic indicates the overall model including Girth and Girth^2 is signif\u0002icant. Further, there is strong evidence that both Girth and Girth^2 are significantly related to\r\nVolume. We may examine a scatterplot together with the fitted quadratic function using the lines\r\nfunction, which adds a line to the plot tracing the estimated mean response.\r\n> plot(Volume ~ scale(Girth), data = trees)\r\n> lines(fitted(treesquad.lm) ~ scale(Girth), data = trees)\r\nThe plot is shown in Figure 12.4.2. Pay attention to the scale on the x-axis: it is on the scale of\r\nthe transformed Girth data and not on the original scale.\r\nRemark 12.8. When a model includes a quadratic term for an independent variable, it is customary\r\nto also include the linear term in the model. The principle is called parsimony. More generally, if\r\nthe researcher decides to include x\r\nm as a term in the model, then (s)he should also include all lower\r\norder terms x, x\r\n2\r\n, . . . ,x\r\nm−1\r\nin the model.\r\nWe do estimation/prediction the same way that we did in Section 12.2.3, except we do not need\r\na Height column in the dataframe new since the variable is not included in the quadratic model.\r\n> new <- data.frame(Girth = c(9.1, 11.6, 12.5))\r\n> predict(treesquad.lm, newdata = new, interval = \"prediction\")",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/47acabb3-2538-47e7-9b9b-04e1c135dff5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8efcb8e692c6c58fe4bbc13d817c137e6999da99aa0261057fb894a48dc0eee7",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "6a8312bd-5d34-44bf-ad18-939fecb4db4e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 319,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.4. POLYNOMIAL REGRESSION 303\r\n● ●●\r\n●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●● ● ●\r\n●\r\n●\r\n● ●●\r\n●\r\n●\r\n● ●\r\n●\r\n●● ●\r\n●●\r\n●\r\n−1 0 1 2\r\n10 30 50 70\r\nscale(Girth)\r\nVolume\r\nFigure 12.4.2: A quadratic model for the trees data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6a8312bd-5d34-44bf-ad18-939fecb4db4e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=771f9cdf1e15b51f4aeba9ddb6eea51f2b2e584dfeadcad0fa0a211f8dea2e37",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 423
      },
      {
        "segments": [
          {
            "segment_id": "567a870c-0ab7-41ce-8b3e-17166cdb503e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 320,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "304 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nfit lwr upr\r\n1 11.56982 4.347426 18.79221\r\n2 20.30615 13.299050 27.31325\r\n3 25.92290 18.972934 32.87286\r\nThe predictions and intervals are slightly different from what they were previously. Notice that\r\nit was not necessary to rescale the Girth prediction data before input to the predict function; the\r\nmodel did the rescaling for us automatically.\r\nRemark 12.9. We have mentioned on several occasions that it is important to rescale the explana\u0002tory variables for polynomial regression. Watch what happens if we ignore this advice:\r\n> summary(lm(Volume ~ Girth + I(Girth^2), data = trees))\r\nCall:\r\nlm(formula = Volume ~ Girth + I(Girth^2), data = trees)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-5.4889 -2.4293 -0.3718 2.0764 7.6447\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) 10.78627 11.22282 0.961 0.344728\r\nGirth -2.09214 1.64734 -1.270 0.214534\r\nI(Girth^2) 0.25454 0.05817 4.376 0.000152 ***\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 3.335 on 28 degrees of freedom\r\nMultiple R-squared: 0.9616, Adjusted R-squared: 0.9588\r\nF-statistic: 350.5 on 2 and 28 DF, p-value: < 2.2e-16\r\nNow nothing is significant in the model except Girth^2. We could delete the Intercept and\r\nGirth from the model, but the model would no longer be parsimonious. A novice may see the\r\noutput and be confused about how to proceed, while the seasoned statistician recognizes immedi\u0002ately that Girth and Girth^2 are highly correlated (see Section 12.9.3). The only remedy to this\r\nailment is to rescale Girth, which we should have done in the first place.\r\nIn Example 12.14 of Section 12.7 we investigate this issue further.\r\n12.5 Interaction\r\nIn our model for tree volume there have been two independent variables: Girth and Height. We\r\nmay suspect that the independent variables are related, that is, values of one variable may tend to\r\ninfluence values of the other. It may be desirable to include an additional term in our model to try\r\nand capture the dependence between the variables. Interaction terms are formed by multiplying\r\none (or more) explanatory variable(s) by another.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/567a870c-0ab7-41ce-8b3e-17166cdb503e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1c839e2288a5ffe31776e902e5022f14bfd87c1a1781326c6ff788ea377413c5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 341
      },
      {
        "segments": [
          {
            "segment_id": "76035fe9-e05c-4c9c-98d8-ae5f94215640",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 321,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.5. INTERACTION 305\r\nExample 12.10. Perhaps the Girth and Height of the tree interact to influence the its Volume;\r\nwe would like to investigate whether the model (Girth = x1 and Height = x2)\r\nY = β0 + β1 x1 + β2 x2 + \u000f (12.5.1)\r\nwould be significantly improved by the model\r\nY = β0 + β1 x1 + β2 x2 + β1:2 x1 x2 + \u000f, (12.5.2)\r\nwhere the subscript 1 : 2 denotes that β1:2 is a coefficient of an interaction term between x1 and x2.\r\nWhat does it mean? Consider the mean response µ(x1, x2) as a function of x2:\r\nµ(x2) = (β0 + β1 x1) + β2 x2. (12.5.3)\r\nThis is a linear function of x2 with slope β2. As x1 changes, the y-intercept of the mean response in\r\nx2 changes, but the slope remains the same. Therefore, the mean response in x2 is represented by a\r\ncollection of parallel lines all with common slope β2.\r\nNow think about what happens when the interaction term β1:2 x1 x2 is included. The mean re\u0002sponse in x2 now looks like\r\nµ(x2) = (β0 + β1 x1) + (β2 + β1:2 x1)x2. (12.5.4)\r\nIn this case we see that not only the y-intercept changes when x1 varies, but the slope also changes in\r\nx1. Thus, the interaction term allows the slope of the mean response in x2 to increase and decrease\r\nas x1 varies.\r\nHow to do it with R\r\nThere are several ways to introduce an interaction term into the model.\r\n1. Make a new variable prod <-Girth *Height, then include prod in the model formula\r\nVolume ~Girth + Height + prod. This method is perhaps the most transparent, but it\r\nalso reserves memory space unnecessarily.\r\n2. Once can construct an interaction term directly in R with a colon “:”. For this example, the\r\nmodel formula would look like Volume ~Girth + Height + Girth:Height.\r\nFor the trees data, we fit the model with the interaction using method two and see if it is signifi\u0002cant:\r\n> treesint.lm <- lm(Volume ~ Girth + Height + Girth:Height,\r\n+ data = trees)\r\n> summary(treesint.lm)\r\nCall:\r\nlm(formula = Volume ~ Girth + Height + Girth:Height, data = trees)\r\nResiduals:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/76035fe9-e05c-4c9c-98d8-ae5f94215640.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=12d4a89d3c6e87b2d43cd2848635c58c102e1cbb50e49d5eef6934416a773c39",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 368
      },
      {
        "segments": [
          {
            "segment_id": "85d912c3-4061-49e1-84d3-866ef7ebbbf1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 322,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "306 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nMin 1Q Median 3Q Max\r\n-6.5821 -1.0673 0.3026 1.5641 4.6649\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) 69.39632 23.83575 2.911 0.00713 **\r\nGirth -5.85585 1.92134 -3.048 0.00511 **\r\nHeight -1.29708 0.30984 -4.186 0.00027 ***\r\nGirth:Height 0.13465 0.02438 5.524 7.48e-06 ***\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 2.709 on 27 degrees of freedom\r\nMultiple R-squared: 0.9756, Adjusted R-squared: 0.9728\r\nF-statistic: 359.3 on 3 and 27 DF, p-value: < 2.2e-16\r\nWe can see from the output that the interaction term is highly significant. Further, the estimate\r\nb1:2 is positive. This means that the slope of µ(x2) is steeper for bigger values of Girth. Keep in\r\nmind: the same interpretation holds for µ(x1); that is, the slope of µ(x1) is steeper for bigger values\r\nof Height.\r\nFor the sake of completeness we calculate confidence intervals for the parameters and do pre\u0002diction as before.\r\n> confint(treesint.lm)\r\n2.5 % 97.5 %\r\n(Intercept) 20.48938699 118.3032441\r\nGirth -9.79810354 -1.9135923\r\nHeight -1.93282845 -0.6613383\r\nGirth:Height 0.08463628 0.1846725\r\n> new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69,\r\n+ 74, 87))\r\n> predict(treesint.lm, newdata = new, interval = \"prediction\")\r\nfit lwr upr\r\n1 11.15884 5.236341 17.08134\r\n2 21.07164 15.394628 26.74866\r\n3 29.78862 23.721155 35.85608\r\nRemark 12.11. There are two other ways to include interaction terms in model formulas. For\r\nexample, we could have written Girth *Height or even (Girth + Height)^2 and both would\r\nbe the same as Girth + Height + Girth:Height.\r\nThese examples can be generalized to more than two independent variables, say three, four, or even\r\nmore. We may be interested in seeing whether any pairwise interactions are significant. We do this\r\nwith a model formula that looks something like y ~ (x1 + x2 + x3 + x4)^2.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/85d912c3-4061-49e1-84d3-866ef7ebbbf1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bed87401528050c19b261868e7436e5b25041e2f88a8f480e2acfe38587d2efb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 302
      },
      {
        "segments": [
          {
            "segment_id": "e046c669-b66d-4d3f-8ecd-4b7f526392c4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 323,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.6. QUALITATIVE EXPLANATORY VARIABLES 307\r\n12.6 Qualitative Explanatory Variables\r\nWe have so far been concerned with numerical independent variables taking values in a subset of\r\nreal numbers. In this section, we extend our treatment to include the case in which one of the\r\nexplanatory variables is qualitative, that is, a factor. Qualitative variables take values in a set of\r\nlevels, which may or may not be ordered. See Section 3.1.2.\r\nNote. The trees data do not have any qualitative explanatory variables, so we will construct one\r\nfor illustrative purposes5. We will leave the Girth variable alone, but we will replace the variable\r\nHeight by a new variable Tall which indicates whether or not the cherry tree is taller than a\r\ncertain threshold (which for the sake of argument will be the sample median height of 76 ft). That\r\nis, Tall will be defined by\r\nTall =\r\n\r\n\r\n\r\nyes, if Height > 76,\r\nno, if Height ≤ 76.\r\n(12.6.1)\r\nWe can construct Tall very quickly in R with the cut function:\r\n> trees$Tall <- cut(trees$Height, breaks = c(-Inf, 76, Inf),\r\n+ labels = c(\"no\", \"yes\"))\r\n> trees$Tall[1:5]\r\n[1] no no no no yes\r\nLevels: no yes\r\nNote that Tall is automatically generated to be a factor with the labels in the correct order. See\r\n?cut for more.\r\nOnce we have Tall, we include it in the regression model just like we would any other variable.\r\nIt is handled internally in a special way. Define a “dummy variable” Tallyes that takes values\r\nTallyes =\r\n\r\n\r\n\r\n1, if Tall = yes,\r\n0, otherwise.\r\n(12.6.2)\r\nThat is, Tallyes is an indicator variable which indicates when a respective tree is tall. The model\r\nmay now be written as\r\nVolume = β0 + β1Girth + β2Tallyes + \u000f. (12.6.3)\r\nLet us take a look at what this definition does to the mean response. Trees with Tall = yes will\r\nhave the mean response\r\nµ(Girth) = (β0 + β2) + β1Girth, (12.6.4)\r\nwhile trees with Tall = no will have the mean response\r\nµ(Girth) = β0 + β1Girth. (12.6.5)\r\nIn essence, we are fitting two regression lines: one for tall trees, and one for short trees. The\r\nregression lines have the same slope but they have different y intercepts (which are exactly |β2| far\r\napart).\r\n5This procedure of replacing a continuous variable by a discrete/qualitative one is called binning, and is almost never\r\nthe right thing to do. We are in a bind at this point, however, because we have invested this chapter in the trees data and\r\nI do not want to switch mid-discussion. I am currently searching for a data set with pre-existing qualitative variables that\r\nalso conveys the same points present in the trees data, and when I find it I will update this chapter accordingly.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e046c669-b66d-4d3f-8ecd-4b7f526392c4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71b8cf553efefb6110d8279f86c5984460ee1145df3700b3fb3733d36045bea5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 466
      },
      {
        "segments": [
          {
            "segment_id": "df959c8a-2c8b-449f-b11a-23be56049578",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 324,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "308 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nHow to do it with R\r\nThe important thing is to double check that the qualitative variable in question is stored as a factor.\r\nThe way to check is with the class command. For example,\r\n> class(trees$Tall)\r\n[1] \"factor\"\r\nIf the qualitative variable is not yet stored as a factor then we may convert it to one with the\r\nfactor command. See Section 3.1.2. Other than this we perform MLR as we normally would.\r\n> treesdummy.lm <- lm(Volume ~ Girth + Tall, data = trees)\r\n> summary(treesdummy.lm)\r\nCall:\r\nlm(formula = Volume ~ Girth + Tall, data = trees)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-5.7788 -3.1710 0.4888 2.6737 10.0619\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) -34.1652 3.2438 -10.53 3.02e-11 ***\r\nGirth 4.6988 0.2652 17.72 < 2e-16 ***\r\nTallyes 4.3072 1.6380 2.63 0.0137 *\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 3.875 on 28 degrees of freedom\r\nMultiple R-squared: 0.9481, Adjusted R-squared: 0.9444\r\nF-statistic: 255.9 on 2 and 28 DF, p-value: < 2.2e-16\r\nFrom the output we see that all parameter estimates are statistically significant and we conclude\r\nthat the mean response differs for trees with Tall = yes and trees with Tall = no.\r\nRemark 12.12. We were somewhat disingenuous when we defined the dummy variable Tallyes\r\nbecause, in truth, R defines Tallyes automatically without input from the user6. Indeed, the author\r\nfit the model beforehand and wrote the discussion afterward with the knowledge of what R would\r\ndo so that the output the reader saw would match what (s)he had previously read. The way that R\r\nhandles factors internally is part of a much larger topic concerning contrasts, which falls outside\r\nthe scope of this book. The interested reader should see Neter et al [67] or Fox [28] for more.\r\nRemark 12.13. In general, if an explanatory variable foo is qualitative with n levels bar1, bar2,\r\n. . . , barn then R will by default automatically define n−1 indicator variables in the following way:\r\nfoobar2 =\r\n\r\n\r\n\r\n1, if foo = ”bar2”,\r\n0, otherwise.\r\n, . . . , foobarn =\r\n\r\n\r\n\r\n1, if foo = ”barn”,\r\n0, otherwise.\r\n6That is, R by default handles contrasts according to its internal settings which may be customized by the user for fine\r\ncontrol. Given that we will not investigate contrasts further in this book it does not serve the discussion to delve into those\r\nsettings, either. The interested reader should check ?contrasts for details.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/df959c8a-2c8b-449f-b11a-23be56049578.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=db56bd6b67687af4adcb0fd18190276339edfc726f6ff5f9105590442d2be8a6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 427
      },
      {
        "segments": [
          {
            "segment_id": "fc66ad76-a263-49ac-9b2b-195ad07b20f9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 325,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.6. QUALITATIVE EXPLANATORY VARIABLES 309\r\n8 10 12 14 16 18 20\r\n10 30 50 70\r\nGirth\r\nVolume\r\n●●\r\n●●\r\n●\r\n●\r\n●\r\n●\r\n●\r\n●● ●\r\n●●\r\n●\r\nFigure 12.6.1: A dummy variable model for the trees data\r\nThe level bar1 is represented by foobar2 = · · · = foobarn = 0. We just need to make sure that\r\nfoo is stored as a factor and R will take care of the rest.\r\nGraphing the Regression Lines\r\nWe can see a plot of the two regression lines with the following mouthful of code.\r\n> treesTall <- split(trees, trees$Tall)\r\n> treesTall[[\"yes\"]]$Fit <- predict(treesdummy.lm, treesTall[[\"yes\"]])\r\n> treesTall[[\"no\"]]$Fit <- predict(treesdummy.lm, treesTall[[\"no\"]])\r\n> plot(Volume ~ Girth, data = trees, type = \"n\")\r\n> points(Volume ~ Girth, data = treesTall[[\"yes\"]], pch = 1)\r\n> points(Volume ~ Girth, data = treesTall[[\"no\"]], pch = 2)\r\n> lines(Fit ~ Girth, data = treesTall[[\"yes\"]])\r\n> lines(Fit ~ Girth, data = treesTall[[\"no\"]])\r\nIt may look intimidating but there is reason to the madness. First we split the trees data into\r\ntwo pieces, with groups determined by the Tall variable. Next we add the Fitted values to each\r\npiece via predict. Then we set up a plot for the variables Volume versus Girth, but we do not",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/fc66ad76-a263-49ac-9b2b-195ad07b20f9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e64e88492e5321f97b4cece681d45f373ecd7c3de2f8d8b9743ea1f424e193b1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 208
      },
      {
        "segments": [
          {
            "segment_id": "f6eddef9-4b30-4b35-bb0e-5a07f6dbe3d0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 326,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "310 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nplot anything yet (type = n) because we want to use different symbols for the two groups. Next\r\nwe add points to the plot for the Tall = yes trees and use an open circle for a plot character\r\n(pch = 1), followed by points for the Tall = no trees with a triangle character (pch = 2).\r\nFinally, we add regression lines to the plot, one for each group.\r\nThere are other – shorter – ways to plot regression lines by groups, namely the scatterplot\r\nfunction in the car [30] package and the xyplot function in the lattice package. We elected to\r\nintroduce the reader to the above approach since many advanced plots in R are done in a similar,\r\nconsecutive fashion.\r\n12.7 Partial F Statistic\r\nWe saw in Section 12.3.3 how to test H0 : β0 = β1 = · · · = βp = 0 with the overall F statistic and\r\nwe saw in Section 12.3.5 how to test H0 : βi = 0 that a particular coefficient βiis zero. Sometimes,\r\nhowever, we would like to test whether a certain part of the model is significant. Consider the\r\nregression model\r\nY = β0 + β1 x1 + · · · + βjxj + βj+1 xj+1 + · · · + βp xp + \u000f, (12.7.1)\r\nwhere j ≥ 1 and p ≥ 2. Now we wish to test the hypothesis\r\nH0 : βj+1 = βj+2 = · · · = βp = 0 (12.7.2)\r\nversus the alternative\r\nH1 : at least one of βj+1, βj+2, , . . . , βp , 0. (12.7.3)\r\nThe interpretation of H0 is that none of the variables xj+1, . . . ,xp is significantly related to Y and\r\nthe interpretation of H1 is that at least one of xj+1, . . . ,xp is significantly related to Y. In essence,\r\nfor this hypothesis test there are two competing models under consideration:\r\nthe full model: y = β0 + β1 x1 + · · · + βp xp + \u000f, (12.7.4)\r\nthe reduced model: y = β0 + β1 x1 + · · · + βjxj + \u000f, (12.7.5)\r\nOf course, the full model will always explain the data better than the reduced model, but does the\r\nfull model explain the data significantly better than the reduced model? This question is exactly\r\nwhat the partial F statistic is designed to answer.\r\nWe first calculate S S Ef, the unexplained variation in the full model, and S S Er, the unexplained\r\nvariation in the reduced model. We base our test on the difference S S Er − S S Ef which measures\r\nthe reduction in unexplained variation attributable to the variables xj+1, . . . ,xp. In the full model\r\nthere are p + 1 parameters and in the reduced model there are j + 1 parameters, which gives a\r\ndifference of p − j parameters (hence degrees of freedom). The partial F statistic is\r\nF =\r\n(S S Er − S S Ef)/(p − j)\r\nS S Ef /(n − p − 1)\r\n. (12.7.6)\r\nIt can be shown when the regression assumptions hold under H0 that the partial F statistic has an\r\nf(df1 = p − j, df2 = n − p − 1) distribution. We calculate the p-value of the observed partial F\r\nstatistic and reject H0 if the p-value is small.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f6eddef9-4b30-4b35-bb0e-5a07f6dbe3d0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=859f63dcbe24313b56f0d883a3f8d3792f70564e180e09e2067e8cc10f7f27e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 569
      },
      {
        "segments": [
          {
            "segment_id": "f6eddef9-4b30-4b35-bb0e-5a07f6dbe3d0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 326,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "310 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nplot anything yet (type = n) because we want to use different symbols for the two groups. Next\r\nwe add points to the plot for the Tall = yes trees and use an open circle for a plot character\r\n(pch = 1), followed by points for the Tall = no trees with a triangle character (pch = 2).\r\nFinally, we add regression lines to the plot, one for each group.\r\nThere are other – shorter – ways to plot regression lines by groups, namely the scatterplot\r\nfunction in the car [30] package and the xyplot function in the lattice package. We elected to\r\nintroduce the reader to the above approach since many advanced plots in R are done in a similar,\r\nconsecutive fashion.\r\n12.7 Partial F Statistic\r\nWe saw in Section 12.3.3 how to test H0 : β0 = β1 = · · · = βp = 0 with the overall F statistic and\r\nwe saw in Section 12.3.5 how to test H0 : βi = 0 that a particular coefficient βiis zero. Sometimes,\r\nhowever, we would like to test whether a certain part of the model is significant. Consider the\r\nregression model\r\nY = β0 + β1 x1 + · · · + βjxj + βj+1 xj+1 + · · · + βp xp + \u000f, (12.7.1)\r\nwhere j ≥ 1 and p ≥ 2. Now we wish to test the hypothesis\r\nH0 : βj+1 = βj+2 = · · · = βp = 0 (12.7.2)\r\nversus the alternative\r\nH1 : at least one of βj+1, βj+2, , . . . , βp , 0. (12.7.3)\r\nThe interpretation of H0 is that none of the variables xj+1, . . . ,xp is significantly related to Y and\r\nthe interpretation of H1 is that at least one of xj+1, . . . ,xp is significantly related to Y. In essence,\r\nfor this hypothesis test there are two competing models under consideration:\r\nthe full model: y = β0 + β1 x1 + · · · + βp xp + \u000f, (12.7.4)\r\nthe reduced model: y = β0 + β1 x1 + · · · + βjxj + \u000f, (12.7.5)\r\nOf course, the full model will always explain the data better than the reduced model, but does the\r\nfull model explain the data significantly better than the reduced model? This question is exactly\r\nwhat the partial F statistic is designed to answer.\r\nWe first calculate S S Ef, the unexplained variation in the full model, and S S Er, the unexplained\r\nvariation in the reduced model. We base our test on the difference S S Er − S S Ef which measures\r\nthe reduction in unexplained variation attributable to the variables xj+1, . . . ,xp. In the full model\r\nthere are p + 1 parameters and in the reduced model there are j + 1 parameters, which gives a\r\ndifference of p − j parameters (hence degrees of freedom). The partial F statistic is\r\nF =\r\n(S S Er − S S Ef)/(p − j)\r\nS S Ef /(n − p − 1)\r\n. (12.7.6)\r\nIt can be shown when the regression assumptions hold under H0 that the partial F statistic has an\r\nf(df1 = p − j, df2 = n − p − 1) distribution. We calculate the p-value of the observed partial F\r\nstatistic and reject H0 if the p-value is small.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f6eddef9-4b30-4b35-bb0e-5a07f6dbe3d0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=859f63dcbe24313b56f0d883a3f8d3792f70564e180e09e2067e8cc10f7f27e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 569
      },
      {
        "segments": [
          {
            "segment_id": "c551ad6a-8714-40cc-8051-c162b233c849",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 327,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.7. PARTIAL F STATISTIC 311\r\nHow to do it with R\r\nThe key ingredient above is that the two competing models are nested in the sense that the reduced\r\nmodel is entirely contained within the complete model. The way to test whether the improvement\r\nis significant is to compute lm objects both for the complete model and the reduced model then\r\ncompare the answers with the anova function.\r\nExample 12.14. For the trees data, let us fit a polynomial regression model and for the sake of\r\nargument we will ignore our own good advice and fail to rescale the explanatory variables.\r\n> treesfull.lm <- lm(Volume ~ Girth + I(Girth^2) + Height +\r\n+ I(Height^2), data = trees)\r\n> summary(treesfull.lm)\r\nCall:\r\nlm(formula = Volume ~ Girth + I(Girth^2) + Height + I(Height^2),\r\ndata = trees)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-4.368 -1.670 -0.158 1.792 4.358\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) -0.955101 63.013630 -0.015 0.988\r\nGirth -2.796569 1.468677 -1.904 0.068 .\r\nI(Girth^2) 0.265446 0.051689 5.135 2.35e-05 ***\r\nHeight 0.119372 1.784588 0.067 0.947\r\nI(Height^2) 0.001717 0.011905 0.144 0.886\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 2.674 on 26 degrees of freedom\r\nMultiple R-squared: 0.9771, Adjusted R-squared: 0.9735\r\nF-statistic: 277 on 4 and 26 DF, p-value: < 2.2e-16\r\nIn this ill-formed model nothing is significant except Girth and Girth^2. Let us continue\r\ndown this path and suppose that we would like to try a reduced model which contains nothing but\r\nGirth and Girth^2 (not even an Intercept). Our two models are now\r\nthe full model: Y = β0 + β1 x1 + β2 x\r\n2\r\n1 + β3 x2 + β4 x\r\n2\r\n2 + \u000f,\r\nthe reduced model: Y = β1 x1 + β2 x\r\n2\r\n1 + \u000f,\r\nWe fit the reduced model with lm and store the results:\r\n> treesreduced.lm <- lm(Volume ~ -1 + Girth + I(Girth^2), data = trees)\r\nTo delete the intercept from the model we used -1 in the model formula. Next we compare the\r\ntwo models with the anova function. The convention is to list the models from smallest to largest.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c551ad6a-8714-40cc-8051-c162b233c849.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e5fca48777878ccab204497e3a31af7766d2c3cf5ac85263c19d3a1631918ce5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 360
      },
      {
        "segments": [
          {
            "segment_id": "d6cb6069-66b3-4e33-979c-639813abc0d8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 328,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "312 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\n> anova(treesreduced.lm, treesfull.lm)\r\nAnalysis of Variance Table\r\nModel 1: Volume ~ -1 + Girth + I(Girth^2)\r\nModel 2: Volume ~ Girth + I(Girth^2) + Height + I(Height^2)\r\nRes.Df RSS Df Sum of Sq F Pr(>F)\r\n1 29 321.65\r\n2 26 185.86 3 135.79 6.3319 0.002279 **\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nWe see from the output that the complete model is highly significant compared to the model\r\nthat does not incorporate Height or the Intercept. We wonder (with our tongue in our cheek) if\r\nthe Height^2 term in the full model is causing all of the trouble. We will fit an alternative reduced\r\nmodel that only deletes Height^2.\r\n> treesreduced2.lm <- lm(Volume ~ Girth + I(Girth^2) + Height,\r\n+ data = trees)\r\n> anova(treesreduced2.lm, treesfull.lm)\r\nAnalysis of Variance Table\r\nModel 1: Volume ~ Girth + I(Girth^2) + Height\r\nModel 2: Volume ~ Girth + I(Girth^2) + Height + I(Height^2)\r\nRes.Df RSS Df Sum of Sq F Pr(>F)\r\n1 27 186.01\r\n2 26 185.86 1 0.14865 0.0208 0.8865\r\nIn this case, the improvement to the reduced model that is attributable to Height^2 is not\r\nsignificant, so we can delete Height^2 from the model with a clear conscience. We notice that the\r\np-value for this latest partial F test is 0.8865, which seems to be remarkably close to the p-value we\r\nsaw for the univariate t test of Height^2 at the beginning of this example. In fact, the p-values are\r\nexactly the same. Perhaps now we gain some insight into the true meaning of the univariate tests.\r\n12.8 Residual Analysis and Diagnostic Tools\r\nWe encountered many, many diagnostic measures for simple linear regression in Sections 11.4 and\r\n11.5. All of these are valid in multiple linear regression, too, but there are some slight changes\r\nthat we need to make for the multivariate case. We list these below, and apply them to the trees\r\nexample.\r\nShapiro-Wilk, Breusch-Pagan, Durbin-Watson: unchanged from SLR, but we are now equipped\r\nto talk about the Shapiro-Wilk test statistic for the residuals. It is defined by the formula\r\nW =\r\na\r\nTE∗\r\nETE\r\n, (12.8.1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d6cb6069-66b3-4e33-979c-639813abc0d8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5e64ce560c78512a836850aeca61d9259ca1f686db745dbd088285e0b8c81c9e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 362
      },
      {
        "segments": [
          {
            "segment_id": "f29080b6-dd04-4cc5-8311-366d21396d29",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 329,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.9. ADDITIONAL TOPICS 313\r\nwhere E\r\n∗\r\nis the sorted residuals and a1×n is defined by\r\na =\r\nmTV\r\n−1\r\n√\r\nmTV−1V−1m\r\n, (12.8.2)\r\nwhere mn×1 and Vn×n are the mean and covariance matrix, respectively, of the order statistics\r\nfrom an mvnorm (mean = 0, sigma = I) distribution.\r\nLeverages: are defined to be the diagonal entries of the hat matrix H (which is why we called\r\nthem hii in Section 12.2.3). The sum of the leverages is tr(H) = p + 1. One rule of thumb\r\nconsiders a leverage extreme if it is larger than double the mean leverage value, which is\r\n2(p + 1)/n, and another rule of thumb considers leverages bigger than 0.5 to indicate high\r\nleverage, while values between 0.3 and 0.5 indicate moderate leverage.\r\nStandardized residuals: unchanged. Considered extreme if |Ri| > 2.\r\nStudentized residuals: compared to a t(df = n − p − 2) distribution.\r\nDFBET AS : The formula is generalized to\r\n(DFBET AS )j(i) =\r\nbj − bj(i)\r\nS (i)\r\n√\r\ncj j\r\n, j = 0, . . . p, i = 1, . . . , n, (12.8.3)\r\nwhere cj j is the j\r\nth diagonal entry of (XTX)−1\r\n. Values larger than one for small data sets or\r\n2/\r\n√\r\nn for large data sets should be investigated.\r\nDFF IT S : unchanged. Larger than one in absolute value is considered extreme.\r\nCook’s D: compared to an f(df1 = p + 1, df2 = n − p − 1) distribution. Observations falling\r\nhigher than the 50th percentile are extreme.\r\nNote that plugging the value p = 1 into the formulas will recover all of the ones we saw in Chapter\r\n11.\r\n12.9 Additional Topics\r\n12.9.1 Nonlinear Regression\r\nWe spent the entire chapter talking about the trees data, and all of our models looked like Volume\r\n~Girth + Height or a variant of this model. But let us think again: we know from elementary\r\nschool that the volume of a rectangle is V = lwh and the volume of a cylinder (which is closer to\r\nwhat a black cherry tree looks like) is\r\nV = πr\r\n2\r\nh or V = 4πdh, (12.9.1)\r\nwhere r and d represent the radius and diameter of the tree, respectively. With this in mind, it would\r\nseem that a more appropriate model for µ might be\r\nµ(x1, x2) = β0 x\r\nβ1\r\n1\r\nx\r\nβ2\r\n2\r\n, (12.9.2)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f29080b6-dd04-4cc5-8311-366d21396d29.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=af7a6dec2c99dbf38be914639dbaccfe2a5875c6066a369008c9a2077040c578",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 404
      },
      {
        "segments": [
          {
            "segment_id": "2e976c99-40e6-457f-bf22-93a625e7361d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 330,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "314 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nwhere β1 and β2 are parameters to adjust for the fact that a black cherry tree is not a perfect cylinder.\r\nHow can we fit this model? The model is not linear in the parameters any more, so our linear\r\nregression methods will not work. . . or will they? In the trees example we may take the logarithm\r\nof both sides of Equation 12.9.2 to get\r\nµ\r\n∗\r\n(x1, x2) = ln \u0002µ(x1, x2)\r\n\u0003\r\n= ln β0 + β1 ln x1 + β2 ln x2, (12.9.3)\r\nand this new model µ\r\n∗\r\nis linear in the parameters β\r\n∗\r\n0\r\n= ln β0, β\r\n∗\r\n1\r\n= β1 and β\r\n∗\r\n2\r\n= β2. We can use what\r\nwe have learned to fit a linear model log(Volume)~log(Girth)+ log(Height), and everything\r\nwill proceed as before, with one exception: we will need to be mindful when it comes time to make\r\npredictions because the model will have been fit on the log scale, and we will need to transform our\r\npredictions back to the original scale (by exponentiating with exp) to make sense.\r\n> treesNonlin.lm <- lm(log(Volume) ~ log(Girth) + log(Height),\r\n+ data = trees)\r\n> summary(treesNonlin.lm)\r\nCall:\r\nlm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)\r\nResiduals:\r\nMin 1Q Median 3Q Max\r\n-0.168561 -0.048488 0.002431 0.063637 0.129223\r\nCoefficients:\r\nEstimate Std. Error t value Pr(>|t|)\r\n(Intercept) -6.63162 0.79979 -8.292 5.06e-09 ***\r\nlog(Girth) 1.98265 0.07501 26.432 < 2e-16 ***\r\nlog(Height) 1.11712 0.20444 5.464 7.81e-06 ***\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 0.08139 on 28 degrees of freedom\r\nMultiple R-squared: 0.9777, Adjusted R-squared: 0.9761\r\nF-statistic: 613.2 on 2 and 28 DF, p-value: < 2.2e-16\r\nThis is our best model yet (judging by R\r\n2\r\nand R\r\n2\r\n), all of the parameters are significant, it is\r\nsimpler than the quadratic or interaction models, and it even makes theoretical sense. It rarely gets\r\nany better than that.\r\nWe may get confidence intervals for the parameters, but remember that it is usually better to\r\ntransform back to the original scale for interpretation purposes :\r\n> exp(confint(treesNonlin.lm))\r\n2.5 % 97.5 %\r\n(Intercept) 0.0002561078 0.006783093\r\nlog(Girth) 6.2276411645 8.468066317\r\nlog(Height) 2.0104387829 4.645475188",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/2e976c99-40e6-457f-bf22-93a625e7361d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4d525babcf00c8ba367e391da363210841bd27be6fb500aae680f39154780674",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 374
      },
      {
        "segments": [
          {
            "segment_id": "b33bfa3c-c488-4954-bc2b-b67839f54c54",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 331,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.9. ADDITIONAL TOPICS 315\r\n(Note that we did not update the row labels of the matrix to show that we exponentiated and\r\nso they are misleading as written.) We do predictions just as before. Remember to transform the\r\nresponse variable back to the original scale after prediction.\r\n> new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69,\r\n+ 74, 87))\r\n> exp(predict(treesNonlin.lm, newdata = new, interval = \"confidence\"))\r\nfit lwr upr\r\n1 11.90117 11.25908 12.57989\r\n2 20.82261 20.14652 21.52139\r\n3 28.93317 27.03755 30.96169\r\nThe predictions and intervals are slightly different from those calculated earlier, but they are\r\nclose. Note that we did not need to transform the Girth and Height arguments in the dataframe\r\nnew. All transformations are done for us automatically.\r\n12.9.2 Real Nonlinear Regression\r\nWe saw with the trees data that a nonlinear model might be more appropriate for the data based\r\non theoretical considerations, and we were lucky because the functional form of µ allowed us to\r\ntake logarithms to transform the nonlinear model to a linear one. The same trick will not work in\r\nother circumstances, however. We need techniques to fit general models of the form\r\nY = µ(X) + \u000f, (12.9.4)\r\nwhere µ is some crazy function that does not lend itself to linear transformations.\r\nThere are a host of methods to address problems like these which are studied in advanced\r\nregression classes. The interested reader should see Neter et al [67] or Tabachnick and Fidell [83].\r\nIt turns out that John Fox has posted an Appendix to his book [29] which discusses some of the\r\nmethods and issues associated with nonlinear regression; see\r\nhttp://cran.r-project.org/doc/contrib/Fox-Companion/appendix.html\r\nHere is an example of how it works, based on a question from R-help.\r\n> set.seed(1)\r\n> x <- seq(from = 0, to = 1000, length.out = 200)\r\n> y <- 1 + 2 * (sin((2 * pi * x/360) - 3))^2 + rnorm(200, sd = 2)\r\n> plot(x, y)\r\n> acc.nls <- nls(y ~ a + b * (sin((2 * pi * x/360) - c))^2,\r\n+ start = list(a = 0.9, b = 2.3, c = 2.9))\r\n> summary(acc.nls)\r\nFormula: y ~ a + b * (sin((2 * pi * x/360) - c))^2\r\nParameters:\r\nEstimate Std. Error t value Pr(>|t|)\r\na 0.95884 0.23097 4.151 4.92e-05 ***",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b33bfa3c-c488-4954-bc2b-b67839f54c54.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7acbdfe0dc811bb47dd6d5dfa9f3cfd3e64ef3d59b3a205410fdb6dca943858f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 379
      },
      {
        "segments": [
          {
            "segment_id": "d1d36d3c-de51-4a9c-b3f4-f136f6fddc1b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 332,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "316 CHAPTER 12. MULTIPLE LINEAR REGRESSION\r\nb 2.22868 0.37114 6.005 9.07e-09 ***\r\nc 3.04343 0.08434 36.084 < 2e-16 ***\r\n---\r\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\nResidual standard error: 1.865 on 197 degrees of freedom\r\nNumber of iterations to convergence: 3\r\nAchieved convergence tolerance: 6.554e-08\r\n12.9.3 Multicollinearity\r\nA multiple regression model exhibits multicollinearity when two or more of the explanatory vari\u0002ables are substantially correlated with each other. We can measure multicollinearity by having one\r\nof the explanatory play the role of “dependent variable” and regress it on the remaining explanatory\r\nvariables. The the R\r\n2 of the resulting model is near one, then we say that the model is multicollinear\r\nor shows multicollinearity.\r\nMulticollinearity is a problem because it causes instability in the regression model. The insta\u0002bility is a consequence of redundancy in the explanatory variables: a high R\r\n2\r\nindicates a strong\r\ndependence between the selected independent variable and the others. The redundant information\r\ninflates the variance of the parameter estimates which can cause them to be statistically insignificant\r\nwhen they would have been significant otherwise. To wit, multicollinearity is usually measured by\r\nwhat are called variance inflation factors.\r\nOnce multicollinearity has been diagnosed there are several approaches to remediate it. Here\r\nare a couple of important ones.\r\nPrincipal Components Analysis. This approach casts out two or more of the original explanatory\r\nvariables and replaces them with new variables, derived from the original ones, that are\r\nby design uncorrelated with one another. The redundancy is thus eliminated and we may\r\nproceed as usual with the new variables in hand. Principal Components Analysis is important\r\nfor other reasons, too, not just for fixing multicollinearity problems.\r\nRidge Regression. The idea of this approach is to replace the original parameter estimates with a\r\ndifferent type of parameter estimate which is more stable under multicollinearity. The esti\u0002mators are not found by ordinary least squares but rather a different optimization procedure\r\nwhich incorporates the variance inflation factor information.\r\nWe decided to omit a thorough discussion of multicollinearity because we are not equipped to\r\nhandle the mathematical details. Perhaps the topic will receive more attention in a later edition.\r\n• What to do when data are not normal\r\n◦ Bootstrap (see Chapter 13).\r\n12.9.4 Akaike’s Information Criterion\r\nAIC = −2 ln L + 2(p + 1)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d1d36d3c-de51-4a9c-b3f4-f136f6fddc1b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c172cf0bce414343f2c2599c38690e91da9d327f1dc1799e9d69e786e454d289",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "41c983bd-3048-4d34-b463-39fbe589e883",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 333,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "12.9. ADDITIONAL TOPICS 317\r\nChapter Exercises\r\nExercise 12.1. Use Equations 12.3.1, 12.3.2, and 12.3.3 to prove the Anova Equality:\r\nS S TO = S S E + S S R.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/41c983bd-3048-4d34-b463-39fbe589e883.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=303a15ff9f3b9a6ddda8fda39d81cdc9f5222220d96fd7d87db721b431738db4",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "0bdd087d-be11-4a8e-bfe1-4bb5c241ea51",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 334,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "318 CHAPTER 12. MULTIPLE LINEAR REGRESSION",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0bdd087d-be11-4a8e-bfe1-4bb5c241ea51.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f0a637918e80b69572645a830bd16f16557d5c79c42509362d6e287ba1b5e36",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 425
      },
      {
        "segments": [
          {
            "segment_id": "77949dff-9ded-4283-aa71-c3257512418a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 335,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 13\r\nResampling Methods\r\nComputers have changed the face of statistics. Their quick computational speed and flawless ac\u0002curacy, coupled with large data sets acquired by the researcher, make them indispensable for many\r\nmodern analyses. In particular, resampling methods (due in large part to Bradley Efron) have\r\ngained prominence in the modern statistician’s repertoire. We first look at a classical problem to\r\nget some insight why.\r\nI have seen Statistical Computing with R by Rizzo [71] and I recommend it to those looking for\r\na more advanced treatment with additional topics. I believe that Monte Carlo Statistical Methods\r\nby Robert and Casella [72] has a new edition that integrates R into the narrative.\r\nWhat do I want them to know?\r\n• basic philosophy of resampling and why it is important\r\n• resampling for standard errors and confidence intervals\r\n• resampling for hypothesis tests (permutation tests)\r\n13.1 Introduction\r\nClassical question Given a population of interest, how may we effectively learn some of its salient\r\nfeatures, e.g., the population’s mean? One way is through representative random sampling.\r\nGiven a random sample, we summarize the information contained therein by calculating a\r\nreasonable statistic, e.g., the sample mean. Given a value of a statistic, how do we know\r\nwhether that value is significantly different from that which was expected? We don’t; we\r\nlook at the sampling distribution of the statistic, and we try to make probabilistic assertions\r\nbased on a confidence level or other consideration. For example, we may find ourselves\r\nsaying things like, \"With 95% confidence, the true population mean is greater than zero.\"\r\nProblem Unfortunately, in most cases the sampling distribution is unknown. Thus, in the past,\r\nin efforts to say something useful, statisticians have been obligated to place some restrictive\r\nassumptions on the underlying population. For example, if we suppose that the population\r\nhas a normal distribution, then we can say that the distribution of X is normal, too, with the\r\n319",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/77949dff-9ded-4283-aa71-c3257512418a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ce0cfa1068e6eb61a7ba262be17c74066e272b9d0043a46bcad31ddb72955293",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 320
      },
      {
        "segments": [
          {
            "segment_id": "7b6582bb-853c-4924-a307-a73749f4f022",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 336,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "320 CHAPTER 13. RESAMPLING METHODS\r\nsame mean (and a smaller standard deviation). It is then easy to draw conclusions, make\r\ninferences, and go on about our business.\r\nAlternative We don’t know what the underlying population distributions is, so let us estimate it,\r\njust like we would with any other parameter. The statistic we use is the empirical CDF,\r\nthat is, the function that places mass 1/n at each of the observed data points x1, . . . , xn (see\r\nSection 5.5). As the sample size increases, we would expect the approximation to get better\r\nand better (with i.i.d. observations, it does, and there is a wonderful theorem by Glivenko and\r\nCantelli that proves it). And now that we have an (estimated) population distribution, it is\r\neasy to find the sampling distribution of any statistic we like: just sample from the empirical\r\nCDF many, many times, calculate the statistic each time, and make a histogram. Done!\r\nOf course, the number of samples needed to get a representative histogram is prohibitively\r\nlarge. . . human beings are simply too slow (and clumsy) to do this tedious procedure.\r\nFortunately, computers are very skilled at doing simple, repetitive tasks very quickly and\r\naccurately. So we employ them to give us a reasonable idea about the sampling distribution\r\nof our statistic, and we use the generated sampling distribution to guide our inferences and\r\ndraw our conclusions. If we would like to have a better approximation for the sampling\r\ndistribution (within the confines of the information contained in the original sample), we\r\nmerely tell the computer to sample more. In this (restricted) sense, we are limited only by\r\nour current computational speed and pocket book.\r\nIn short, here are some of the benefits that the advent of resampling methods has given us:\r\nFewer assumptions. We are no longer required to assume the population is normal or the sample\r\nsize is large (though, as before, the larger the sample the better).\r\nGreater accuracy. Many classical methods are based on rough upper bounds or Taylor expan\u0002sions. The bootstrap procedures can be iterated long enough to give results accurate to sev\u0002eral decimal places, often beating classical approximations.\r\nGenerality. Resampling methods are easy to understand and apply to a large class of seemingly\r\nunrelated procedures. One no longer needs to memorize long complicated formulas and\r\nalgorithms.\r\nRemark 13.1. Due to the special structure of the empirical CDF, to get an i.i.d. sample we just need\r\nto take a random sample of size n, with replacement, from the observed data x1, . . . , xn. Repeats are\r\nexpected and acceptable. Since we already sampled to get the original data, the term resampling is\r\nused to describe the procedure.\r\nGeneral bootstrap procedure. The above discussion leads us to the following general procedure\r\nto approximate the sampling distribution of a statistic S = S (x1, x2, . . . , xn) based on an observed\r\nsimple random sample x = (x1, x2, . . . , xn) of size n:\r\n1. Create many many samples x\r\n∗\r\n1\r\n, . . . , x\r\n∗\r\nM\r\n, called resamples, by sampling with replacement from\r\nthe data.\r\n2. Calculate the statistic of interest S (x\r\n∗\r\n1\r\n), . . . , S (x\r\n∗\r\nM\r\n) for each resample. The distribution of the\r\nresample statistics is called a bootstrap distribution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7b6582bb-853c-4924-a307-a73749f4f022.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0d6cca573cc9dd7640c66f2f4251f287070d736d01bd7c57b5c9e48e8e00da16",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "7b6582bb-853c-4924-a307-a73749f4f022",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 336,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "320 CHAPTER 13. RESAMPLING METHODS\r\nsame mean (and a smaller standard deviation). It is then easy to draw conclusions, make\r\ninferences, and go on about our business.\r\nAlternative We don’t know what the underlying population distributions is, so let us estimate it,\r\njust like we would with any other parameter. The statistic we use is the empirical CDF,\r\nthat is, the function that places mass 1/n at each of the observed data points x1, . . . , xn (see\r\nSection 5.5). As the sample size increases, we would expect the approximation to get better\r\nand better (with i.i.d. observations, it does, and there is a wonderful theorem by Glivenko and\r\nCantelli that proves it). And now that we have an (estimated) population distribution, it is\r\neasy to find the sampling distribution of any statistic we like: just sample from the empirical\r\nCDF many, many times, calculate the statistic each time, and make a histogram. Done!\r\nOf course, the number of samples needed to get a representative histogram is prohibitively\r\nlarge. . . human beings are simply too slow (and clumsy) to do this tedious procedure.\r\nFortunately, computers are very skilled at doing simple, repetitive tasks very quickly and\r\naccurately. So we employ them to give us a reasonable idea about the sampling distribution\r\nof our statistic, and we use the generated sampling distribution to guide our inferences and\r\ndraw our conclusions. If we would like to have a better approximation for the sampling\r\ndistribution (within the confines of the information contained in the original sample), we\r\nmerely tell the computer to sample more. In this (restricted) sense, we are limited only by\r\nour current computational speed and pocket book.\r\nIn short, here are some of the benefits that the advent of resampling methods has given us:\r\nFewer assumptions. We are no longer required to assume the population is normal or the sample\r\nsize is large (though, as before, the larger the sample the better).\r\nGreater accuracy. Many classical methods are based on rough upper bounds or Taylor expan\u0002sions. The bootstrap procedures can be iterated long enough to give results accurate to sev\u0002eral decimal places, often beating classical approximations.\r\nGenerality. Resampling methods are easy to understand and apply to a large class of seemingly\r\nunrelated procedures. One no longer needs to memorize long complicated formulas and\r\nalgorithms.\r\nRemark 13.1. Due to the special structure of the empirical CDF, to get an i.i.d. sample we just need\r\nto take a random sample of size n, with replacement, from the observed data x1, . . . , xn. Repeats are\r\nexpected and acceptable. Since we already sampled to get the original data, the term resampling is\r\nused to describe the procedure.\r\nGeneral bootstrap procedure. The above discussion leads us to the following general procedure\r\nto approximate the sampling distribution of a statistic S = S (x1, x2, . . . , xn) based on an observed\r\nsimple random sample x = (x1, x2, . . . , xn) of size n:\r\n1. Create many many samples x\r\n∗\r\n1\r\n, . . . , x\r\n∗\r\nM\r\n, called resamples, by sampling with replacement from\r\nthe data.\r\n2. Calculate the statistic of interest S (x\r\n∗\r\n1\r\n), . . . , S (x\r\n∗\r\nM\r\n) for each resample. The distribution of the\r\nresample statistics is called a bootstrap distribution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7b6582bb-853c-4924-a307-a73749f4f022.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0d6cca573cc9dd7640c66f2f4251f287070d736d01bd7c57b5c9e48e8e00da16",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "02a26461-299b-4274-91e1-c276e372389e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 337,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "13.2. BOOTSTRAP STANDARD ERRORS 321\r\n3. The bootstrap distribution gives information about the sampling distribution of the origi\u0002nal statistic S . In particular, the bootstrap distribution gives us some idea about the center,\r\nspread, and shape of the sampling distribution of S .\r\n13.2 Bootstrap Standard Errors\r\nSince the bootstrap distribution gives us information about a statistic’s sampling distribution, we\r\ncan use the bootstrap distribution to estimate properties of the statistic. We will illustrate the boot\u0002strap procedure in the special case that the statistic S is a standard error.\r\nExample 13.2. Standard error of the mean. In this example we illustrate the bootstrap by esti\u0002mating the standard error of the sample meanand we will do it in the special case that the underlying\r\npopulation is norm(mean = 3, sd = 1).\r\nOf course, we do not really need a bootstrap distribution here because from Section 8.2 we\r\nknow that X ∼ norm(mean = 3, sd = 1/\r\n√\r\nn), but we proceed anyway to investigate how the\r\nbootstrap performs when we know what the answer should be ahead of time.\r\nWe will take a random sample of size n = 25 from the population. Then we will resample the\r\ndata 1000 times to get 1000 resamples of size 25. We will calculate the sample mean of each of the\r\nresamples, and will study the data distribution of the 1000 values of x.\r\n> srs <- rnorm(25, mean = 3)\r\n> resamps <- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)\r\n> xbarstar <- sapply(resamps, mean, simplify = TRUE)\r\nA histogram of the 1000 values of x is shown in Figure 13.2.1, and was produced by the fol\u0002lowing code.\r\n> hist(xbarstar, breaks = 40, prob = TRUE)\r\n> curve(dnorm(x, 3, 0.2), add = TRUE) # overlay true normal density\r\nWe have overlain what we know to be the true sampling distribution of X, namely, a norm(mean =\r\n3, sd = 1/\r\n√\r\n25) distribution. The histogram matches the true sampling distribution pretty well with\r\nrespect to shape and spread. . . but notice how the histogram is off-center a little bit. This is not a\r\ncoincidence – in fact, it can be shown that the mean of the bootstrap distribution is exactly the mean\r\nof the original sample, that is, the value of the statistic that we originally observed. Let us calculate\r\nthe mean of the bootstrap distribution and compare it to the mean of the original sample:\r\n> mean(xbarstar)\r\n[1] 3.056430\r\n> mean(srs)\r\n[1] 3.05766\r\n> mean(xbarstar) - mean(srs)\r\n[1] -0.001229869",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/02a26461-299b-4274-91e1-c276e372389e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8dd7957fd3a67d7ff38e5a6f19249550cb1791e6ae836c4abf7fb258410955c0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 420
      },
      {
        "segments": [
          {
            "segment_id": "30ae2924-17c1-4501-b0df-7616b38ba979",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 338,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "322 CHAPTER 13. RESAMPLING METHODS\r\nHistogram of xbarstar\r\nxbarstar\r\nDensity\r\n2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8\r\n0.0 0.5 1.0 1.5 2.0\r\nFigure 13.2.1: Bootstrapping the standard error of the mean, simulated data\r\nThe original data were 25 observations generated from a norm(mean = 3, sd = 1) distribution. We next\r\nresampled to get 1000 resamples, each of size 25, and calculated the sample mean for each resample. A his\u0002togram of the 1000 values of x is shown above. Also shown (with a solid line) is the true sampling distribution\r\nof X, which is a norm(mean = 3, sd = 0.2) distribution. Note that the histogram is centered at the sample\r\nmean of the original data, while the true sampling distribution is centered at the true value of µ = 3. The shape\r\nand spread of the histogram is similar to the shape and spread of the true sampling distribution.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/30ae2924-17c1-4501-b0df-7616b38ba979.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cc48fe54c24fb10fb40ce354a036306a7ff89fcaeb560b926791b8e59e24dee7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 151
      },
      {
        "segments": [
          {
            "segment_id": "c0eb24c2-44a5-4876-91f9-f3160efcc694",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 339,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "13.2. BOOTSTRAP STANDARD ERRORS 323\r\nNotice how close the two values are. The difference between them is an estimate of how biased\r\nthe original statistic is, the so-called bootstrap estimate of bias. Since the estimate is so small we\r\nwould expect our original statistic (X) to have small bias, but this is no surprise to us because we\r\nalready knew from Section 8.1.1 that X is an unbiased estimator of the population mean.\r\nNow back to our original problem, we would like to estimate the standard error of X. Looking\r\nat the histogram, we see that the spread of the bootstrap distribution is similar to the spread of the\r\nsampling distribution. Therefore, it stands to reason that we could estimate the standard error of X\r\nwith the sample standard deviation of the resample statistics. Let us try and see.\r\n> sd(xbarstar)\r\n[1] 0.2134432\r\nWe know from theory that the true standard error is 1/\r\n√\r\n25 = 0.20. Our bootstrap estimate is\r\nnot very far from the theoretical value.\r\nRemark 13.3. What would happen if we take more resamples? Instead of 1000 resamples, we could\r\nincrease to, say, 2000, 3000, or even 4000. . . would it help? The answer is both yes and no. Keep\r\nin mind that with resampling methods there are two sources of randomness: that from the original\r\nsample, and that from the subsequent resampling procedure. An increased number of resamples\r\nwould reduce the variation due to the second part, but would do nothing to reduce the variation due\r\nto the first part. We only took an original sample of size n = 25, and resampling more and more\r\nwould never generate more information about the population than was already there. In this sense,\r\nthe statistician is limited by the information contained in the original sample.\r\nExample 13.4. Standard error of the median. We look at one where we do not know the answer\r\nahead of time. This example uses the rivers data set. Recall the stemplot on page on page 42 that\r\nwe made for these data which shows them to be markedly right-skewed, so a natural estimate of\r\ncenter would be the sample median. Unfortunately, its sampling distribution falls out of our reach.\r\nWe use the bootstrap to help us with this problem, and the modifications to the last example are\r\ntrivial.\r\n> resamps <- replicate(1000, sample(rivers, 141, TRUE), simplify = FALSE)\r\n> medstar <- sapply(resamps, median, simplify = TRUE)\r\n> sd(medstar)\r\n[1] 25.2366\r\nThe graph is shown in Figure 13.2.2, and was produced by the following code.\r\n> hist(medstar, breaks = 40, prob = TRUE)\r\n> median(rivers)\r\n[1] 425\r\n> mean(medstar)\r\n[1] 426.47\r\n> mean(medstar) - median(rivers)\r\n[1] 1.47",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c0eb24c2-44a5-4876-91f9-f3160efcc694.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=abaad0fda64f51c8ccc96dd293093f3c577b09c78831ed19e8a7054fdb2ec6e4",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "fa661ecf-e6a6-433b-a795-e12f75025ac0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 340,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "324 CHAPTER 13. RESAMPLING METHODS\r\nHistogram of medstar\r\nmedstar\r\nDensity\r\n350 400 450 500\r\n0.000 0.010 0.020 Figure 13.2.2: Bootstrapping the standard error of the median for the rivers data",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/fa661ecf-e6a6-433b-a795-e12f75025ac0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5529ea9043d43ba3bf61f8474bb3c1b1b2faa1cb4f7e5064aef69ebe398b5c6d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 476
      },
      {
        "segments": [
          {
            "segment_id": "55a99c3d-b9e3-4b60-b796-7b39b1ee6ab6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 341,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "13.2. BOOTSTRAP STANDARD ERRORS 325\r\nExample 13.5. The boot package in R. It turns out that there are many bootstrap procedures and\r\ncommands already built into base R, in the boot package. Further, inside the boot package there\r\nis even a function called boot. The basic syntax is of the form:\r\nboot(data , statistic , R)\r\nHere, data is a vector (or matrix) containing the data to be resampled, statistic is a defined\r\nfunction, of two arguments, that tells which statistic should be computed, and the parameter R\r\nspecifies how many resamples should be taken.\r\nFor the standard error of the mean (Example 13.2):\r\n> library(boot)\r\n> mean_fun <- function(x, indices) mean(x[indices])\r\n> boot(data = srs, statistic = mean_fun, R = 1000)\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\nCall:\r\nboot(data = srs, statistic = mean_fun, R = 1000)\r\nBootstrap Statistics :\r\noriginal bias std. error\r\nt1* 3.05766 -0.01117741 0.2183400\r\nFor the standard error of the median (Example 13.4):\r\n> median_fun <- function(x, indices) median(x[indices])\r\n> boot(data = rivers, statistic = median_fun, R = 1000)\r\nORDINARY NONPARAMETRIC BOOTSTRAP\r\nCall:\r\nboot(data = rivers, statistic = median_fun, R = 1000)\r\nBootstrap Statistics :\r\noriginal bias std. error\r\nt1* 425 2.003 26.87630\r\nWe notice that the output from both methods of estimating the standard errors produced similar\r\nresults. In fact, the boot procedure is to be preferred since it invisibly returns much more informa\u0002tion (which we will use later) than our naive script and it is much quicker in its computations.\r\nRemark 13.6. Some things to keep in mind about the bootstrap:\r\n• For many statistics, the bootstrap distribution closely resembles the sampling distribution\r\nwith respect to spread and shape. However, the bootstrap will not have the same center as",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/55a99c3d-b9e3-4b60-b796-7b39b1ee6ab6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=67bcfc90eb627cbfe4046b300115c6f611c57bf36b0a03e22db9d0085addc350",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 283
      },
      {
        "segments": [
          {
            "segment_id": "2230d440-9dd1-4393-8518-697f479c17b3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 342,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "326 CHAPTER 13. RESAMPLING METHODS\r\nthe true sampling distribution. While the sampling distribution is centered at the population\r\nmean (plus any bias), the bootstrap distribution is centered at the original value of the statistic\r\n(plus any bias). The boot function gives an empirical estimate of the bias of the statistic as\r\npart of its output.\r\n• We tried to estimate the standard error, but we could have (in principle) tried to estimate\r\nsomething else. Note from the previous remark, however, that it would be useless to estimate\r\nthe population mean µ using the bootstrap since the mean of the bootstrap distribution is the\r\nobserved x.\r\n• You don’t get something from nothing. We have seen that we can take a random sample\r\nfrom a population and use bootstrap methods to get a very good idea about standard errors,\r\nbias, and the like. However, one must not get lured into believing that by doing some random\r\nresampling somehow one gets more information about the parameters than that which was\r\ncontained in the original sample. Indeed, there is some uncertainty about the parameter due\r\nto the randomness of the original sample, and there is even more uncertainty introduced by\r\nresampling. One should think of the bootstrap as just another estimation method, nothing\r\nmore, nothing less.\r\n13.3 Bootstrap Confidence Intervals\r\n13.3.1 Percentile Confidence Intervals\r\nAs a first try, we want to obtain a 95% confidence interval for a parameter. Typically the statistic\r\nwe use to estimate the parameter is centered at (or at least close by) the parameter; in such cases a\r\n95% confidence interval for the parameter is nothing more than a 95% confidence interval for the\r\nstatistic. And to find a 95% confidence interval for the statistic we need only go to its sampling\r\ndistribution to find an interval that contains 95% of the area. (The most popular choice is the\r\nequal-tailed interval with 2.5% in each tail.)\r\nThis is incredibly easy to accomplish with the bootstrap. We need only to take a bunch of\r\nbootstrap resamples, order them, and choose the α/2th and (1−α)th percentiles. There is a function\r\nboot.ci in R already created to do just this. Note that in order to use the function boot.ci we\r\nmust first run the boot function and save the output in a variable, for example, data.boot. We\r\nthen plug data.boot into the function boot.ci.\r\nExample 13.7. Percentile interval for the expected value of the median. Wee will try the naive\r\napproach where we generate the resamples and calculate the percentile interval by hand.\r\n> btsamps <- replicate(2000, sample(stack.loss, 21, TRUE),\r\n+ simplify = FALSE)\r\n> thetast <- sapply(btsamps, median, simplify = TRUE)\r\n> mean(thetast)\r\n[1] 14.78\r\n> median(stack.loss)\r\n[1] 15\r\n> quantile(thetast, c(0.025, 0.975))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/2230d440-9dd1-4393-8518-697f479c17b3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9a6845d851a1891e082543fce72700cace2df6aaf51626c7a69ffd055e65393e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 452
      },
      {
        "segments": [
          {
            "segment_id": "7da48a52-c6c8-4148-a9d9-1620b290553f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 343,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "13.3. BOOTSTRAP CONFIDENCE INTERVALS 327\r\n2.5% 97.5%\r\n12 18\r\nExample 13.8. Confidence interval for expected value of the median, 2\r\nnd try. Now we will do\r\nit the right way with the boot function.\r\n> library(boot)\r\n> med_fun <- function(x, ind) median(x[ind])\r\n> med_boot <- boot(stack.loss, med_fun, R = 2000)\r\n> boot.ci(med_boot, type = c(\"perc\", \"norm\", \"bca\"))\r\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\r\nBased on 2000 bootstrap replicates\r\nCALL :\r\nboot.ci(boot.out = med_boot, type = c(\"perc\", \"norm\", \"bca\"))\r\nIntervals :\r\nLevel Normal Percentile BCa\r\n95% (11.93, 18.53 ) (12.00, 18.00 ) (11.00, 18.00 )\r\nCalculations and Intervals on Original Scale\r\n13.3.2 Student’s t intervals (“normal intervals”)\r\nThe idea is to use confidence intervals that we already know and let the bootstrap help us when we\r\nget into trouble. We know that a 100(1 − α)% confidence interval for the mean of a S RS (n) from a\r\nnormal distribution is\r\nX ± tα/2(df = n − 1)\r\nS\r\n√\r\nn\r\n, (13.3.1)\r\nwhere tα/2(df = n − 1) is the appropriate critical value from Student’s t distribution, and we re\u0002member that an estimate for the standard error of X is S/\r\n√\r\nn. Of course, the estimate for the\r\nstandard error will change when the underlying population distribution is not normal, or when we\r\nuse a statistic more complicated than X. In those situations the bootstrap will give us quite rea\u0002sonable estimates for the standard error. And as long as the sampling distribution of our statistic is\r\napproximately bell-shaped with small bias, the interval\r\nstatistic ± tα/2(df = n − 1) ∗ SE(statistic) (13.3.2)\r\nwill have approximately 100(1 − α)% confidence of containing IE(statistic).\r\nExample 13.9. We will use the t-interval method to find the bootstrap CI for the median. We\r\nhave looked at the bootstrap distribution; it appears to be symmetric and approximately mound\r\nshaped. Further, we may check that the bias is approximately 40, which on the scale of these data\r\nis practically negligible. Thus, we may consider looking at the t-intervals. Note that, since our\r\nsample is so large, instead of t-intervals we will essentially be using z-intervals.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/7da48a52-c6c8-4148-a9d9-1620b290553f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=18c8fd16703c09fbc261a4996d09dcbfd1339344dca84adaa379713c2ce86350",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 349
      },
      {
        "segments": [
          {
            "segment_id": "b3ee0432-4cd3-4c9f-a876-35862ee7de5d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 344,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "328 CHAPTER 13. RESAMPLING METHODS\r\nPlease see the handout, “Bootstrapping Confidence Intervals for the Median, 3rd try.”\r\nWe see that, considering the scale of the data, the confidence intervals compare with each other\r\nquite well.\r\nRemark 13.10. We have seen two methods for bootstrapping confidence intervals for a statistic.\r\nWhich method should we use? If the bias of the bootstrap distribution is small and if the distri\u0002bution is close to normal, then the percentile and t-intervals will closely agree. If the intervals are\r\nnoticeably different, then it should be considered evidence that the normality and bias conditions\r\nare not met. In this case, neither interval should be used.\r\n• BCa: bias-corrected and accelerated\r\n◦ transformation invariant\r\n◦ more correct and accurate\r\n◦ not monotone in coverage level?\r\n• t - intervals\r\n◦ more natural\r\n◦ numerically unstable\r\n• Can do things like transform scales, compute confidence intervals, and then transform back.\r\n• Studentized bootstrap confidence intervals where is the Studentized version of is the order\r\nstatistic of the simulation\r\n13.4 Resampling in Hypothesis Tests\r\nThe classical two-sample problem can be stated as follows: given two groups of interest, we would\r\nlike to know whether these two groups are significantly different from one another or whether the\r\ngroups are reasonably similar. The standard way to decide is to\r\n1. Go collect some information from the two groups and calculate an associated statistic, for\r\nexample, X1 − X2.\r\n2. Suppose that there is no difference in the groups, and find the distribution of the statistic in\r\n1.\r\n3. Locate the observed value of the statistic with respect to the distribution found in 2. A value\r\nin the main body of the distribution is not spectacular, it could reasonably have occurred by\r\nchance. A value in the tail of the distribution is unlikely, and hence provides evidence against\r\nthe null hypothesis that the population distributions are the same.\r\nOf course, we usually compute a p-value, defined to be the probability of the observed value of the\r\nstatistic or more extreme when the null hypothesis is true. Small p-values are evidence against the\r\nnull hypothesis. It is not immediately obvious how to use resampling methods here, so we discuss\r\nan example.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b3ee0432-4cd3-4c9f-a876-35862ee7de5d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=19156eb025b6966868efa91d206787e1de0f8bb6b393105cb4aa79a2becb755a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 367
      },
      {
        "segments": [
          {
            "segment_id": "aaab60ab-cffc-490d-afca-adca71061850",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 345,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "13.4. RESAMPLING IN HYPOTHESIS TESTS 329\r\nExample 13.11. A study concerned differing dosages of the antiretroviral drug AZT. The common\r\ndosage is 300mg daily. Higher doses cause more side affects, but are they significantly higher? We\r\nexamine for a 600mg dose. The data are as follows: We compare the scores from the two groups\r\nby computing the difference in their sample means. The 300mg data were entered in x1 and the\r\n600mg data were entered into x2. The observed difference was\r\n300 mg 284 279 289 292 287 295 285 279 306 298\r\n600 mg 298 307 297 279 291 335 299 300 306 291\r\nThe average amounts can be found:\r\n> mean(x1)\r\n[1] 289.4\r\n> mean(x2)\r\n[1] 300.3\r\nwith an observed difference of mean(x2) - mean(x1) = 10.9. As expected, the 600 mg\r\nmeasurements seem to have a higher average, and we might be interested in trying to decide if\r\nthe average amounts are significantly different. The null hypothesis should be that there is no\r\ndifference in the amounts, that is, the groups are more or less the same. If the null hypothesis\r\nwere true, then the two groups would indeed be the same, or just one big group. In that case, the\r\nobserved difference in the sample means just reflects the random assignment into the arbitrary x1\r\nand x2 categories. It is now clear how we may resample, consistent with the null hypothesis.\r\nProcedure:\r\n1. Randomly resample 10 scores from the combined scores of x1 and x2, and assign then to the\r\n“x1” group. The rest will then be in the “x2” group. Calculate the difference in (re)sampled\r\nmeans, and store that value.\r\n2. Repeat this procedure many, many times and draw a histogram of the resampled statistics,\r\ncalled the permutation distribution. Locate the observed difference 10.9 on the histogram to\r\nget the p-value. If the p-value is small, then we consider that evidence against the hypothesis\r\nthat the groups are the same.\r\nRemark 13.12. In calculating the permutation test p-value, the formula is essentially the propor\u0002tion of resample statistics that are greater than or equal to the observed value. Of course, this is\r\nmerely an estimate of the true p-value. As it turns out, an adjustment of +1 to both the numerator\r\nand denominator of the proportion improves the performance of the estimated p-value, and this\r\nadjustment is implemented in the ts.perm function.\r\n> library(coin)\r\n> oneway_test(len ~ supp, data = ToothGrowth)\r\nAsymptotic 2-Sample Permutation Test\r\ndata: len by supp (OJ, VC)\r\nZ = 1.8734, p-value = 0.06102\r\nalternative hypothesis: true mu is not equal to 0",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/aaab60ab-cffc-490d-afca-adca71061850.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=df0efd8c8f71d03c4743dda46ba7cfac8110ef7b4bcecc87eafe4abf2836c6ae",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 430
      },
      {
        "segments": [
          {
            "segment_id": "597e5d2b-3393-47f3-92a5-aa5d9abe2f0a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 346,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "330 CHAPTER 13. RESAMPLING METHODS\r\n13.4.1 Comparison with the Two Sample t test\r\nWe know from Chapter 10 to use the two-sample t-test to tell whether there is an improvement as a\r\nresult of taking the intervention class. Note that the t-test assumes normal underlying populations,\r\nwith unknown variance, and small sample n = 10. What does the t-test say? Below is the output.\r\n> t.test(len ~ supp, data = ToothGrowth, alt = \"greater\", var.equal = TRUE)\r\nTwo Sample t-test\r\ndata: len by supp\r\nt = 1.9153, df = 58, p-value = 0.03020\r\nalternative hypothesis: true difference in means is greater than 0\r\n95 percent confidence interval:\r\n0.4708204 Inf\r\nsample estimates:\r\nmean in group OJ mean in group VC\r\n20.66333 16.96333\r\nThe p-value for the t-test was 0.03, while the permutation test p-value was 0.061. Note that\r\nthere is an underlying normality assumption for the t-test, which isn’t present in the permutation\r\ntest. If the normality assumption may be questionable, then the permutation test would be more\r\nreasonable. We see what can happen when using a test in a situation where the assumptions are\r\nnot met: smaller p-values. In situations where the normality assumptions are not met, for example,\r\nsmall sample scenarios, the permutation test is to be preferred. In particular, if accuracy is very\r\nimportant then we should use the permutation test.\r\nRemark 13.13. Here are some things about permutation tests to keep in mind.\r\n• While the permutation test does not require normality of the populations (as contrasted with\r\nthe t-test), nevertheless it still requires that the two groups are exchangeable; see Section 7.5.\r\nIn particular, this means that they must be identically distributed under the null hypothesis.\r\nThey must have not only the same means, but they must also have the same spread, shape,\r\nand everything else. This assumption may or may not be true in a given example, but it will\r\nrarely cause the t-test to outperform the permutation test, because even if the sample standard\r\ndeviations are markedly different it does not mean that the population standard deviations are\r\ndifferent. In many situations the permutation test will also carry over to the t-test.\r\n• If the distribution of the groups is close to normal, then the t-test p-value and the bootstrap\r\np-value will be approximately equal. If they differ markedly, then this should be considered\r\nevidence that the normality assumptions do not hold.\r\n• The generality of the permutation test is such that one can use all kinds of statistics to com\u0002pare the two groups. One could compare the difference in variances or the difference in (just\r\nabout anything). Alternatively, one could compare the ratio of sample means, X1/X2. Of\r\ncourse, under the null hypothesis this last quantity should be near 1.\r\n• Just as with the bootstrap, the answer we get is subject to variability due to the inherent\r\nrandomness of resampling from the data. We can make the variability as small as we like by\r\ntaking sufficiently many resamples. How many? If the conclusion is very important (that is,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/597e5d2b-3393-47f3-92a5-aa5d9abe2f0a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=038af1a61958ca43a96d4376e24ab04979dbe30aba0fe3d088ffab3c204a8745",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 506
      },
      {
        "segments": [
          {
            "segment_id": "25138aa4-314d-4404-b90a-e5881d7fb5a3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 347,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "13.4. RESAMPLING IN HYPOTHESIS TESTS 331\r\nif lots of money is at stake), then take thousands. For point estimation problems typically,\r\nR = 1000 resamples, or so, is enough. In general, if the true p-value is p then the standard\r\nerror of the estimated p-value is pp(1 − p)/R. You can choose R to get whatever accuracy\r\ndesired.\r\n• Other possible testing designs:\r\n◦ Matched Pairs Designs.\r\n◦ Relationship between two variables.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/25138aa4-314d-4404-b90a-e5881d7fb5a3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=37fda86dfdd813591f4ceb55a0effe153643002c0d25910f2e73976a4df26376",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "0a3bd427-24df-4ee5-8b55-24c35731a4fd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 348,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "332 CHAPTER 13. RESAMPLING METHODS\r\nChapter Exercises",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0a3bd427-24df-4ee5-8b55-24c35731a4fd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ede19e999922072319e068495ea5036cc0de9fbf1de555801ada1a54802ff505",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "1452cd45-2b9b-4484-8f5f-3d860d9aab32",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 349,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 14\r\nCategorical Data Analysis\r\nThis chapter is still under substantial revision. At any time you can preview any released drafts\r\nwith the development version of the IPSUR package which is available from R-Forge:\r\n> install.packages(\"IPSUR\", repos = \"http://R-Forge.R-project.org\")\r\n> library(IPSUR)\r\n> read(IPSUR)\r\n333",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1452cd45-2b9b-4484-8f5f-3d860d9aab32.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71d28b8947b5dc18b07e9abed5cd6988beaf2c17f171b1e85b3301583590b8df",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "66b3c068-758f-465a-8b54-a167fbd3f6fa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 350,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "334 CHAPTER 14. CATEGORICAL DATA ANALYSIS",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/66b3c068-758f-465a-8b54-a167fbd3f6fa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ed64774ce64d859b3b4182f5ba9ebbc85e47330e29298bbfe1848b67b9a0e408",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "a6582e49-56a4-4c31-bbd5-93a8b0fc6afc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 351,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 15\r\nNonparametric Statistics\r\nThis chapter is still under substantial revision. At any time you can preview any released drafts\r\nwith the development version of the IPSUR package which is available from R-Forge:\r\n> install.packages(\"IPSUR\", repos = \"http://R-Forge.R-project.org\")\r\n> library(IPSUR)\r\n> read(IPSUR)\r\n335",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a6582e49-56a4-4c31-bbd5-93a8b0fc6afc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c82493df14e58908500b35eea1fb10684caf5f3cff042641b956858d29641991",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "e9eb671a-9945-4921-ae48-955b55090b65",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 352,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "336 CHAPTER 15. NONPARAMETRIC STATISTICS",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e9eb671a-9945-4921-ae48-955b55090b65.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=76fd30ecd36229a5f19565fea9fe12f908456b8934bd1cf2084c2abd46096d57",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "951355c5-a43a-43f6-a1b3-13eabe5621d6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 353,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Chapter 16\r\nTime Series\r\nThis chapter is still under substantial revision. At any time you can preview any released drafts\r\nwith the development version of the IPSUR package which is available from R-Forge:\r\n> install.packages(\"IPSUR\", repos = \"http://R-Forge.R-project.org\")\r\n> library(IPSUR)\r\n> read(IPSUR)\r\n337",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/951355c5-a43a-43f6-a1b3-13eabe5621d6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11bf4a87feed0f4b10b219a9308ca4fefefe7744e3a5a9699709cf76689e19b3",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "a0907dfa-555d-47d3-89b7-a73ca263a6d6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 354,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "338 CHAPTER 16. TIME SERIES",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a0907dfa-555d-47d3-89b7-a73ca263a6d6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3be2fad4ae6c12d160579049ae0d633760543b0cf5953a49077386b3482b55f9",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "00c84c08-5f4b-40fc-af27-865b2d655574",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 355,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix A\r\nR Session Information\r\nIf you ever write the R help mailing list with a question, then you should include your session\r\ninformation in the email; it makes the reader’s job easier and is requested by the Posting Guide.\r\nHere is how to do that, and below is what the output looks like.\r\n> sessionInfo()\r\nR version 2.12.2 Patched (2011-03-18 r54866)\r\nPlatform: x86_64-unknown-linux-gnu (64-bit)\r\nlocale:\r\n[1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C\r\n[3] LC_TIME=en_US.UTF-8 LC_COLLATE=C\r\n[5] LC_MONETARY=C LC_MESSAGES=en_US.UTF-8\r\n[7] LC_PAPER=en_US.UTF-8 LC_NAME=C\r\n[9] LC_ADDRESS=C LC_TELEPHONE=C\r\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C\r\nattached base packages:\r\n[1] grid stats4 splines tcltk stats graphics grDevices\r\n[8] utils datasets methods base\r\nother attached packages:\r\n[1] coin_1.0-18 modeltools_0.2-17\r\n[3] boot_1.2-43 scatterplot3d_0.3-32\r\n[5] lmtest_0.9-27 zoo_1.6-4\r\n[7] reshape_0.8.4 plyr_1.4\r\n[9] Hmisc_3.8-3 HH_2.1-32\r\n[11] leaps_2.9 multcomp_1.2-5\r\n[13] TeachingDemos_2.7 mvtnorm_0.9-96\r\n[15] distrEx_2.3 actuar_1.1-1\r\n[17] evd_2.2-4 distr_2.3.1\r\n[19] SweaveListingUtils_0.5 sfsmisc_1.0-14\r\n339",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/00c84c08-5f4b-40fc-af27-865b2d655574.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=90aed85b6580100b36eccdfdb2dc79507a4028e07c04ca43ede3544941287a9f",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "a74f43a3-a97f-440d-b620-53e1b6956c3e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 356,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "340 APPENDIX A. R SESSION INFORMATION\r\n[21] startupmsg_0.7.1 combinat_0.0-8\r\n[23] prob_0.9-2 diagram_1.5.2\r\n[25] shape_1.3.1 lattice_0.19-17\r\n[27] e1071_1.5-25 class_7.3-3\r\n[29] qcc_2.0.1 aplpack_1.2.3\r\n[31] RcmdrPlugin.IPSUR_0.1-7 Rcmdr_1.6-3\r\n[33] car_2.0-9 survival_2.36-5\r\n[35] nnet_7.3-1 MASS_7.3-11\r\nloaded via a namespace (and not attached):\r\n[1] cluster_1.13.3 tools_2.12.2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a74f43a3-a97f-440d-b620-53e1b6956c3e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=65f380fa35153714a36e7addbd5368d0a3bef05abe030902913c024e2db271e1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 399
      },
      {
        "segments": [
          {
            "segment_id": "cdeae107-b56f-4ba1-a9fe-77cf163da0d8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 357,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix B\r\nGNU Free Documentation License\r\nVersion 1.3, 3 November 2008\r\nCopyright (C) 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc.\r\nhttp://fsf.org/\r\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing\r\nit is not allowed.\r\n0. PREAMBLE\r\nThe purpose of this License is to make a manual, textbook, or other functional and useful document\r\n\"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it,\r\nwith or without modifying it, either commercially or noncommercially. Secondarily, this License\r\npreserves for the author and publisher a way to get credit for their work, while not being considered\r\nresponsible for modifications made by others.\r\nThis License is a kind of \"copyleft\", which means that derivative works of the document must\r\nthemselves be free in the same sense. It complements the GNU General Public License, which is a\r\ncopyleft license designed for free software.\r\nWe have designed this License in order to use it for manuals for free software, because free\r\nsoftware needs free documentation: a free program should come with manuals providing the same\r\nfreedoms that the software does. But this License is not limited to software manuals; it can be used\r\nfor any textual work, regardless of subject matter or whether it is published as a printed book. We\r\nrecommend this License principally for works whose purpose is instruction or reference.\r\n1. APPLICABILITY AND DEFINITIONS\r\nThis License applies to any manual or other work, in any medium, that contains a notice placed\r\nby the copyright holder saying it can be distributed under the terms of this License. Such a notice\r\n341",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/cdeae107-b56f-4ba1-a9fe-77cf163da0d8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=945474587aba5ab72ffc3f451e29d96cfcd09263381917f8b5b0a49f7f0d258f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 272
      },
      {
        "segments": [
          {
            "segment_id": "fde82099-8e5d-4afd-90b3-28e5d2ce7ff4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 358,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "342 APPENDIX B. GNU FREE DOCUMENTATION LICENSE\r\ngrants a world-wide, royalty-free license, unlimited in duration, to use that work under the condi\u0002tions stated herein. The \"Document\", below, refers to any such manual or work. Any member of\r\nthe public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or\r\ndistribute the work in a way requiring permission under copyright law.\r\nA \"Modified Version\" of the Document means any work containing the Document or a portion\r\nof it, either copied verbatim, or with modifications and/or translated into another language.\r\nA \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals\r\nexclusively with the relationship of the publishers or authors of the Document to the Document’s\r\noverall subject (or to related matters) and contains nothing that could fall directly within that overall\r\nsubject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not\r\nexplain any mathematics.) The relationship could be a matter of historical connection with the\r\nsubject or with related matters, or of legal, commercial, philosophical, ethical or political position\r\nregarding them.\r\nThe \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being\r\nthose of Invariant Sections, in the notice that says that the Document is released under this License.\r\nIf a section does not fit the above definition of Secondary then it is not allowed to be designated as\r\nInvariant. The Document may contain zero Invariant Sections. If the Document does not identify\r\nany Invariant Sections then there are none.\r\nThe \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or\r\nBack-Cover Texts, in the notice that says that the Document is released under this License. A\r\nFront-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.\r\nA \"Transparent\" copy of the Document means a machine-readable copy, represented in a for\u0002mat whose specification is available to the general public, that is suitable for revising the document\r\nstraightforwardly with generic text editors or (for images composed of pixels) generic paint pro\u0002grams or (for drawings) some widely available drawing editor, and that is suitable for input to text\r\nformatters or for automatic translation to a variety of formats suitable for input to text formatters.\r\nA copy made in an otherwise Transparent file format whose markup, or absence of markup, has\r\nbeen arranged to thwart or discourage subsequent modification by readers is not Transparent. An\r\nimage format is not Transparent if used for any substantial amount of text. A copy that is not\r\n\"Transparent\" is called \"Opaque\".\r\nExamples of suitable formats for Transparent copies include plain ASCII without markup,\r\nTexinfo input format, LATEX input format, SGML or XML using a publicly available DTD, and\r\nstandard-conforming simple HTML, PostScript or PDF designed for human modification. Exam\u0002ples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary\r\nformats that can be read and edited only by proprietary word processors, SGML or XML for which\r\nthe DTD and/or processing tools are not generally available, and the machine-generated HTML,\r\nPostScript or PDF produced by some word processors for output purposes only.\r\nThe \"Title Page\" means, for a printed book, the title page itself, plus such following pages as\r\nare needed to hold, legibly, the material this License requires to appear in the title page. For works\r\nin formats which do not have any title page as such, \"Title Page\" means the text near the most\r\nprominent appearance of the work’s title, preceding the beginning of the body of the text.\r\nThe \"publisher\" means any person or entity that distributes copies of the Document to the\r\npublic.\r\nA section \"Entitled XYZ\" means a named subunit of the Document whose title either is pre\u0002cisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language.\r\n(Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\",",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/fde82099-8e5d-4afd-90b3-28e5d2ce7ff4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c79733126592f24d25fa0b8a3e98c9e81187902bef875d17266eaf6862b87fe4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 658
      },
      {
        "segments": [
          {
            "segment_id": "fde82099-8e5d-4afd-90b3-28e5d2ce7ff4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 358,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "342 APPENDIX B. GNU FREE DOCUMENTATION LICENSE\r\ngrants a world-wide, royalty-free license, unlimited in duration, to use that work under the condi\u0002tions stated herein. The \"Document\", below, refers to any such manual or work. Any member of\r\nthe public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or\r\ndistribute the work in a way requiring permission under copyright law.\r\nA \"Modified Version\" of the Document means any work containing the Document or a portion\r\nof it, either copied verbatim, or with modifications and/or translated into another language.\r\nA \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals\r\nexclusively with the relationship of the publishers or authors of the Document to the Document’s\r\noverall subject (or to related matters) and contains nothing that could fall directly within that overall\r\nsubject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not\r\nexplain any mathematics.) The relationship could be a matter of historical connection with the\r\nsubject or with related matters, or of legal, commercial, philosophical, ethical or political position\r\nregarding them.\r\nThe \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being\r\nthose of Invariant Sections, in the notice that says that the Document is released under this License.\r\nIf a section does not fit the above definition of Secondary then it is not allowed to be designated as\r\nInvariant. The Document may contain zero Invariant Sections. If the Document does not identify\r\nany Invariant Sections then there are none.\r\nThe \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or\r\nBack-Cover Texts, in the notice that says that the Document is released under this License. A\r\nFront-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.\r\nA \"Transparent\" copy of the Document means a machine-readable copy, represented in a for\u0002mat whose specification is available to the general public, that is suitable for revising the document\r\nstraightforwardly with generic text editors or (for images composed of pixels) generic paint pro\u0002grams or (for drawings) some widely available drawing editor, and that is suitable for input to text\r\nformatters or for automatic translation to a variety of formats suitable for input to text formatters.\r\nA copy made in an otherwise Transparent file format whose markup, or absence of markup, has\r\nbeen arranged to thwart or discourage subsequent modification by readers is not Transparent. An\r\nimage format is not Transparent if used for any substantial amount of text. A copy that is not\r\n\"Transparent\" is called \"Opaque\".\r\nExamples of suitable formats for Transparent copies include plain ASCII without markup,\r\nTexinfo input format, LATEX input format, SGML or XML using a publicly available DTD, and\r\nstandard-conforming simple HTML, PostScript or PDF designed for human modification. Exam\u0002ples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary\r\nformats that can be read and edited only by proprietary word processors, SGML or XML for which\r\nthe DTD and/or processing tools are not generally available, and the machine-generated HTML,\r\nPostScript or PDF produced by some word processors for output purposes only.\r\nThe \"Title Page\" means, for a printed book, the title page itself, plus such following pages as\r\nare needed to hold, legibly, the material this License requires to appear in the title page. For works\r\nin formats which do not have any title page as such, \"Title Page\" means the text near the most\r\nprominent appearance of the work’s title, preceding the beginning of the body of the text.\r\nThe \"publisher\" means any person or entity that distributes copies of the Document to the\r\npublic.\r\nA section \"Entitled XYZ\" means a named subunit of the Document whose title either is pre\u0002cisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language.\r\n(Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\",",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/fde82099-8e5d-4afd-90b3-28e5d2ce7ff4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c79733126592f24d25fa0b8a3e98c9e81187902bef875d17266eaf6862b87fe4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 658
      },
      {
        "segments": [
          {
            "segment_id": "e45c9674-3faa-40e4-89e5-40aee0afa4b1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 359,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "343\r\n\"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you\r\nmodify the Document means that it remains a section \"Entitled XYZ\" according to this definition.\r\nThe Document may include Warranty Disclaimers next to the notice which states that this\r\nLicense applies to the Document. These Warranty Disclaimers are considered to be included by\r\nreference in this License, but only as regards disclaiming warranties: any other implication that\r\nthese Warranty Disclaimers may have is void and has no effect on the meaning of this License.\r\n2. VERBATIM COPYING\r\nYou may copy and distribute the Document in any medium, either commercially or noncommer\u0002cially, provided that this License, the copyright notices, and the license notice saying this License\r\napplies to the Document are reproduced in all copies, and that you add no other conditions whatso\u0002ever to those of this License. You may not use technical measures to obstruct or control the reading\r\nor further copying of the copies you make or distribute. However, you may accept compensation\r\nin exchange for copies. If you distribute a large enough number of copies you must also follow the\r\nconditions in section 3.\r\nYou may also lend copies, under the same conditions stated above, and you may publicly dis\u0002play copies.\r\n3. COPYING IN QUANTITY\r\nIf you publish printed copies (or copies in media that commonly have printed covers) of the Doc\u0002ument, numbering more than 100, and the Document’s license notice requires Cover Texts, you\r\nmust enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover\r\nTexts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly\r\nand legibly identify you as the publisher of these copies. The front cover must present the full title\r\nwith all words of the title equally prominent and visible. You may add other material on the covers\r\nin addition. Copying with changes limited to the covers, as long as they preserve the title of the\r\nDocument and satisfy these conditions, can be treated as verbatim copying in other respects.\r\nIf the required texts for either cover are too voluminous to fit legibly, you should put the first\r\nones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent\r\npages.\r\nIf you publish or distribute Opaque copies of the Document numbering more than 100, you must\r\neither include a machine-readable Transparent copy along with each Opaque copy, or state in or\r\nwith each Opaque copy a computer-network location from which the general network-using public\r\nhas access to download using public-standard network protocols a complete Transparent copy of the\r\nDocument, free of added material. If you use the latter option, you must take reasonably prudent\r\nsteps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent\r\ncopy will remain thus accessible at the stated location until at least one year after the last time you\r\ndistribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.\r\nIt is requested, but not required, that you contact the authors of the Document well before\r\nredistributing any large number of copies, to give them a chance to provide you with an updated\r\nversion of the Document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e45c9674-3faa-40e4-89e5-40aee0afa4b1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4cb917699dad449be105e6f0249f0cdf147a827b09b346d89a6436d41a6f2b6f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 541
      },
      {
        "segments": [
          {
            "segment_id": "e45c9674-3faa-40e4-89e5-40aee0afa4b1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 359,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "343\r\n\"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you\r\nmodify the Document means that it remains a section \"Entitled XYZ\" according to this definition.\r\nThe Document may include Warranty Disclaimers next to the notice which states that this\r\nLicense applies to the Document. These Warranty Disclaimers are considered to be included by\r\nreference in this License, but only as regards disclaiming warranties: any other implication that\r\nthese Warranty Disclaimers may have is void and has no effect on the meaning of this License.\r\n2. VERBATIM COPYING\r\nYou may copy and distribute the Document in any medium, either commercially or noncommer\u0002cially, provided that this License, the copyright notices, and the license notice saying this License\r\napplies to the Document are reproduced in all copies, and that you add no other conditions whatso\u0002ever to those of this License. You may not use technical measures to obstruct or control the reading\r\nor further copying of the copies you make or distribute. However, you may accept compensation\r\nin exchange for copies. If you distribute a large enough number of copies you must also follow the\r\nconditions in section 3.\r\nYou may also lend copies, under the same conditions stated above, and you may publicly dis\u0002play copies.\r\n3. COPYING IN QUANTITY\r\nIf you publish printed copies (or copies in media that commonly have printed covers) of the Doc\u0002ument, numbering more than 100, and the Document’s license notice requires Cover Texts, you\r\nmust enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover\r\nTexts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly\r\nand legibly identify you as the publisher of these copies. The front cover must present the full title\r\nwith all words of the title equally prominent and visible. You may add other material on the covers\r\nin addition. Copying with changes limited to the covers, as long as they preserve the title of the\r\nDocument and satisfy these conditions, can be treated as verbatim copying in other respects.\r\nIf the required texts for either cover are too voluminous to fit legibly, you should put the first\r\nones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent\r\npages.\r\nIf you publish or distribute Opaque copies of the Document numbering more than 100, you must\r\neither include a machine-readable Transparent copy along with each Opaque copy, or state in or\r\nwith each Opaque copy a computer-network location from which the general network-using public\r\nhas access to download using public-standard network protocols a complete Transparent copy of the\r\nDocument, free of added material. If you use the latter option, you must take reasonably prudent\r\nsteps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent\r\ncopy will remain thus accessible at the stated location until at least one year after the last time you\r\ndistribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.\r\nIt is requested, but not required, that you contact the authors of the Document well before\r\nredistributing any large number of copies, to give them a chance to provide you with an updated\r\nversion of the Document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/e45c9674-3faa-40e4-89e5-40aee0afa4b1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4cb917699dad449be105e6f0249f0cdf147a827b09b346d89a6436d41a6f2b6f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 541
      },
      {
        "segments": [
          {
            "segment_id": "da4af63a-3f2b-4e87-a567-53e92fc55736",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 360,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "344 APPENDIX B. GNU FREE DOCUMENTATION LICENSE\r\n4. MODIFICATIONS\r\nYou may copy and distribute a Modified Version of the Document under the conditions of sections\r\n2 and 3 above, provided that you release the Modified Version under precisely this License, with\r\nthe Modified Version filling the role of the Document, thus licensing distribution and modification\r\nof the Modified Version to whoever possesses a copy of it. In addition, you must do these things in\r\nthe Modified Version:\r\nA. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document,\r\nand from those of previous versions (which should, if there were any, be listed in the History section\r\nof the Document). You may use the same title as a previous version if the original publisher of that\r\nversion gives permission.\r\nB. List on the Title Page, as authors, one or more persons or entities responsible for authorship\r\nof the modifications in the Modified Version, together with at least five of the principal authors of\r\nthe Document (all of its principal authors, if it has fewer than five), unless they release you from\r\nthis requirement.\r\nC. State on the Title page the name of the publisher of the Modified Version, as the publisher.\r\nD. Preserve all the copyright notices of the Document.\r\nE. Add an appropriate copyright notice for your modifications adjacent to the other copyright\r\nnotices.\r\nF. Include, immediately after the copyright notices, a license notice giving the public permission\r\nto use the Modified Version under the terms of this License, in the form shown in the Addendum\r\nbelow.\r\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts\r\ngiven in the Document’s license notice.\r\nH. Include an unaltered copy of this License.\r\nI. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least\r\nthe title, year, new authors, and publisher of the Modified Version as given on the Title Page. If\r\nthere is no section Entitled \"History\" in the Document, create one stating the title, year, authors,\r\nand publisher of the Document as given on its Title Page, then add an item describing the Modified\r\nVersion as stated in the previous sentence.\r\nJ. Preserve the network location, if any, given in the Document for public access to a Transpar\u0002ent copy of the Document, and likewise the network locations given in the Document for previous\r\nversions it was based on. These may be placed in the \"History\" section. You may omit a network\r\nlocation for a work that was published at least four years before the Document itself, or if the\r\noriginal publisher of the version it refers to gives permission.\r\nK. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the\r\nsection, and preserve in the section all the substance and tone of each of the contributor acknowl\u0002edgements and/or dedications given therein.\r\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles.\r\nSection numbers or the equivalent are not considered part of the section titles.\r\nM. Delete any section Entitled \"Endorsements\". Such a section may not be included in the\r\nModified Version.\r\nN. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with\r\nany Invariant Section.\r\nO. Preserve any Warranty Disclaimers.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/da4af63a-3f2b-4e87-a567-53e92fc55736.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=21341814140a201a93f430cc5e89e1fc3c4606976e0c2d31529cf63df8e44388",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 557
      },
      {
        "segments": [
          {
            "segment_id": "da4af63a-3f2b-4e87-a567-53e92fc55736",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 360,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "344 APPENDIX B. GNU FREE DOCUMENTATION LICENSE\r\n4. MODIFICATIONS\r\nYou may copy and distribute a Modified Version of the Document under the conditions of sections\r\n2 and 3 above, provided that you release the Modified Version under precisely this License, with\r\nthe Modified Version filling the role of the Document, thus licensing distribution and modification\r\nof the Modified Version to whoever possesses a copy of it. In addition, you must do these things in\r\nthe Modified Version:\r\nA. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document,\r\nand from those of previous versions (which should, if there were any, be listed in the History section\r\nof the Document). You may use the same title as a previous version if the original publisher of that\r\nversion gives permission.\r\nB. List on the Title Page, as authors, one or more persons or entities responsible for authorship\r\nof the modifications in the Modified Version, together with at least five of the principal authors of\r\nthe Document (all of its principal authors, if it has fewer than five), unless they release you from\r\nthis requirement.\r\nC. State on the Title page the name of the publisher of the Modified Version, as the publisher.\r\nD. Preserve all the copyright notices of the Document.\r\nE. Add an appropriate copyright notice for your modifications adjacent to the other copyright\r\nnotices.\r\nF. Include, immediately after the copyright notices, a license notice giving the public permission\r\nto use the Modified Version under the terms of this License, in the form shown in the Addendum\r\nbelow.\r\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts\r\ngiven in the Document’s license notice.\r\nH. Include an unaltered copy of this License.\r\nI. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least\r\nthe title, year, new authors, and publisher of the Modified Version as given on the Title Page. If\r\nthere is no section Entitled \"History\" in the Document, create one stating the title, year, authors,\r\nand publisher of the Document as given on its Title Page, then add an item describing the Modified\r\nVersion as stated in the previous sentence.\r\nJ. Preserve the network location, if any, given in the Document for public access to a Transpar\u0002ent copy of the Document, and likewise the network locations given in the Document for previous\r\nversions it was based on. These may be placed in the \"History\" section. You may omit a network\r\nlocation for a work that was published at least four years before the Document itself, or if the\r\noriginal publisher of the version it refers to gives permission.\r\nK. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the\r\nsection, and preserve in the section all the substance and tone of each of the contributor acknowl\u0002edgements and/or dedications given therein.\r\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles.\r\nSection numbers or the equivalent are not considered part of the section titles.\r\nM. Delete any section Entitled \"Endorsements\". Such a section may not be included in the\r\nModified Version.\r\nN. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with\r\nany Invariant Section.\r\nO. Preserve any Warranty Disclaimers.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/da4af63a-3f2b-4e87-a567-53e92fc55736.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=21341814140a201a93f430cc5e89e1fc3c4606976e0c2d31529cf63df8e44388",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 557
      },
      {
        "segments": [
          {
            "segment_id": "9c160c81-4411-4d17-bdff-fa969212fadf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 361,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "345\r\nIf the Modified Version includes new front-matter sections or appendices that qualify as Sec\u0002ondary Sections and contain no material copied from the Document, you may at your option des\u0002ignate some or all of these sections as invariant. To do this, add their titles to the list of Invariant\r\nSections in the Modified Version’s license notice. These titles must be distinct from any other\r\nsection titles.\r\nYou may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements\r\nof your Modified Version by various parties–for example, statements of peer review or that the text\r\nhas been approved by an organization as the authoritative definition of a standard.\r\nYou may add a passage of up to five words as a Front-Cover Text, and a passage of up to\r\n25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version.\r\nOnly one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through\r\narrangements made by) any one entity. If the Document already includes a cover text for the same\r\ncover, previously added by you or by arrangement made by the same entity you are acting on\r\nbehalf of, you may not add another; but you may replace the old one, on explicit permission from\r\nthe previous publisher that added the old one.\r\nThe author(s) and publisher(s) of the Document do not by this License give permission to use\r\ntheir names for publicity for or to assert or imply endorsement of any Modified Version.\r\n5. COMBINING DOCUMENTS\r\nYou may combine the Document with other documents released under this License, under the terms\r\ndefined in section 4 above for modified versions, provided that you include in the combination all\r\nof the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant\r\nSections of your combined work in its license notice, and that you preserve all their Warranty\r\nDisclaimers.\r\nThe combined work need only contain one copy of this License, and multiple identical Invariant\r\nSections may be replaced with a single copy. If there are multiple Invariant Sections with the same\r\nname but different contents, make the title of each such section unique by adding at the end of\r\nit, in parentheses, the name of the original author or publisher of that section if known, or else a\r\nunique number. Make the same adjustment to the section titles in the list of Invariant Sections in\r\nthe license notice of the combined work.\r\nIn the combination, you must combine any sections Entitled \"History\" in the various original\r\ndocuments, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Ac\u0002knowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled\r\n\"Endorsements\".\r\n6. COLLECTIONS OF DOCUMENTS\r\nYou may make a collection consisting of the Document and other documents released under this Li\u0002cense, and replace the individual copies of this License in the various documents with a single copy\r\nthat is included in the collection, provided that you follow the rules of this License for verbatim\r\ncopying of each of the documents in all other respects.\r\nYou may extract a single document from such a collection, and distribute it individually under\r\nthis License, provided you insert a copy of this License into the extracted document, and follow\r\nthis License in all other respects regarding verbatim copying of that document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9c160c81-4411-4d17-bdff-fa969212fadf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=db43267d6dbbe1cc1a08f3debd99f8ca3956ed1da6c1ce50262e80716848f93e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "9c160c81-4411-4d17-bdff-fa969212fadf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 361,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "345\r\nIf the Modified Version includes new front-matter sections or appendices that qualify as Sec\u0002ondary Sections and contain no material copied from the Document, you may at your option des\u0002ignate some or all of these sections as invariant. To do this, add their titles to the list of Invariant\r\nSections in the Modified Version’s license notice. These titles must be distinct from any other\r\nsection titles.\r\nYou may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements\r\nof your Modified Version by various parties–for example, statements of peer review or that the text\r\nhas been approved by an organization as the authoritative definition of a standard.\r\nYou may add a passage of up to five words as a Front-Cover Text, and a passage of up to\r\n25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version.\r\nOnly one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through\r\narrangements made by) any one entity. If the Document already includes a cover text for the same\r\ncover, previously added by you or by arrangement made by the same entity you are acting on\r\nbehalf of, you may not add another; but you may replace the old one, on explicit permission from\r\nthe previous publisher that added the old one.\r\nThe author(s) and publisher(s) of the Document do not by this License give permission to use\r\ntheir names for publicity for or to assert or imply endorsement of any Modified Version.\r\n5. COMBINING DOCUMENTS\r\nYou may combine the Document with other documents released under this License, under the terms\r\ndefined in section 4 above for modified versions, provided that you include in the combination all\r\nof the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant\r\nSections of your combined work in its license notice, and that you preserve all their Warranty\r\nDisclaimers.\r\nThe combined work need only contain one copy of this License, and multiple identical Invariant\r\nSections may be replaced with a single copy. If there are multiple Invariant Sections with the same\r\nname but different contents, make the title of each such section unique by adding at the end of\r\nit, in parentheses, the name of the original author or publisher of that section if known, or else a\r\nunique number. Make the same adjustment to the section titles in the list of Invariant Sections in\r\nthe license notice of the combined work.\r\nIn the combination, you must combine any sections Entitled \"History\" in the various original\r\ndocuments, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Ac\u0002knowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled\r\n\"Endorsements\".\r\n6. COLLECTIONS OF DOCUMENTS\r\nYou may make a collection consisting of the Document and other documents released under this Li\u0002cense, and replace the individual copies of this License in the various documents with a single copy\r\nthat is included in the collection, provided that you follow the rules of this License for verbatim\r\ncopying of each of the documents in all other respects.\r\nYou may extract a single document from such a collection, and distribute it individually under\r\nthis License, provided you insert a copy of this License into the extracted document, and follow\r\nthis License in all other respects regarding verbatim copying of that document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9c160c81-4411-4d17-bdff-fa969212fadf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=db43267d6dbbe1cc1a08f3debd99f8ca3956ed1da6c1ce50262e80716848f93e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "c08fe03b-e804-47ff-bdc7-47e1860e21ef",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 362,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "346 APPENDIX B. GNU FREE DOCUMENTATION LICENSE\r\n7. AGGREGATION WITH INDEPENDENT WORKS\r\nA compilation of the Document or its derivatives with other separate and independent documents\r\nor works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the\r\ncopyright resulting from the compilation is not used to limit the legal rights of the compilation’s\r\nusers beyond what the individual works permit. When the Document is included in an aggregate,\r\nthis License does not apply to the other works in the aggregate which are not themselves derivative\r\nworks of the Document.\r\nIf the Cover Text requirement of section 3 is applicable to these copies of the Document, then\r\nif the Document is less than one half of the entire aggregate, the Document’s Cover Texts may be\r\nplaced on covers that bracket the Document within the aggregate, or the electronic equivalent of\r\ncovers if the Document is in electronic form. Otherwise they must appear on printed covers that\r\nbracket the whole aggregate.\r\n8. TRANSLATION\r\nTranslation is considered a kind of modification, so you may distribute translations of the Docu\u0002ment under the terms of section 4. Replacing Invariant Sections with translations requires special\r\npermission from their copyright holders, but you may include translations of some or all Invariant\r\nSections in addition to the original versions of these Invariant Sections. You may include a trans\u0002lation of this License, and all the license notices in the Document, and any Warranty Disclaimers,\r\nprovided that you also include the original English version of this License and the original versions\r\nof those notices and disclaimers. In case of a disagreement between the translation and the original\r\nversion of this License or a notice or disclaimer, the original version will prevail.\r\nIf a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the\r\nrequirement (section 4) to Preserve its Title (section 1) will typically require changing the actual\r\ntitle.\r\n9. TERMINATION\r\nYou may not copy, modify, sublicense, or distribute the Document except as expressly provided\r\nunder this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and\r\nwill automatically terminate your rights under this License.\r\nHowever, if you cease all violation of this License, then your license from a particular copyright\r\nholder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally\r\nterminates your license, and (b) permanently, if the copyright holder fails to notify you of the\r\nviolation by some reasonable means prior to 60 days after the cessation.\r\nMoreover, your license from a particular copyright holder is reinstated permanently if the copy\u0002right holder notifies you of the violation by some reasonable means, this is the first time you have\r\nreceived notice of violation of this License (for any work) from that copyright holder, and you cure\r\nthe violation prior to 30 days after your receipt of the notice.\r\nTermination of your rights under this section does not terminate the licenses of parties who\r\nhave received copies or rights from you under this License. If your rights have been terminated and\r\nnot permanently reinstated, receipt of a copy of some or all of the same material does not give you\r\nany rights to use it.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c08fe03b-e804-47ff-bdc7-47e1860e21ef.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7a13196f0b2cb53df4c7d9239345f2ee94923b843149e04fcaa541eadcad457a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 532
      },
      {
        "segments": [
          {
            "segment_id": "c08fe03b-e804-47ff-bdc7-47e1860e21ef",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 362,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "346 APPENDIX B. GNU FREE DOCUMENTATION LICENSE\r\n7. AGGREGATION WITH INDEPENDENT WORKS\r\nA compilation of the Document or its derivatives with other separate and independent documents\r\nor works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the\r\ncopyright resulting from the compilation is not used to limit the legal rights of the compilation’s\r\nusers beyond what the individual works permit. When the Document is included in an aggregate,\r\nthis License does not apply to the other works in the aggregate which are not themselves derivative\r\nworks of the Document.\r\nIf the Cover Text requirement of section 3 is applicable to these copies of the Document, then\r\nif the Document is less than one half of the entire aggregate, the Document’s Cover Texts may be\r\nplaced on covers that bracket the Document within the aggregate, or the electronic equivalent of\r\ncovers if the Document is in electronic form. Otherwise they must appear on printed covers that\r\nbracket the whole aggregate.\r\n8. TRANSLATION\r\nTranslation is considered a kind of modification, so you may distribute translations of the Docu\u0002ment under the terms of section 4. Replacing Invariant Sections with translations requires special\r\npermission from their copyright holders, but you may include translations of some or all Invariant\r\nSections in addition to the original versions of these Invariant Sections. You may include a trans\u0002lation of this License, and all the license notices in the Document, and any Warranty Disclaimers,\r\nprovided that you also include the original English version of this License and the original versions\r\nof those notices and disclaimers. In case of a disagreement between the translation and the original\r\nversion of this License or a notice or disclaimer, the original version will prevail.\r\nIf a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the\r\nrequirement (section 4) to Preserve its Title (section 1) will typically require changing the actual\r\ntitle.\r\n9. TERMINATION\r\nYou may not copy, modify, sublicense, or distribute the Document except as expressly provided\r\nunder this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and\r\nwill automatically terminate your rights under this License.\r\nHowever, if you cease all violation of this License, then your license from a particular copyright\r\nholder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally\r\nterminates your license, and (b) permanently, if the copyright holder fails to notify you of the\r\nviolation by some reasonable means prior to 60 days after the cessation.\r\nMoreover, your license from a particular copyright holder is reinstated permanently if the copy\u0002right holder notifies you of the violation by some reasonable means, this is the first time you have\r\nreceived notice of violation of this License (for any work) from that copyright holder, and you cure\r\nthe violation prior to 30 days after your receipt of the notice.\r\nTermination of your rights under this section does not terminate the licenses of parties who\r\nhave received copies or rights from you under this License. If your rights have been terminated and\r\nnot permanently reinstated, receipt of a copy of some or all of the same material does not give you\r\nany rights to use it.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c08fe03b-e804-47ff-bdc7-47e1860e21ef.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7a13196f0b2cb53df4c7d9239345f2ee94923b843149e04fcaa541eadcad457a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 532
      },
      {
        "segments": [
          {
            "segment_id": "8976594f-5b95-4c36-baa0-ffa865a9e682",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 363,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "347\r\n10. FUTURE REVISIONS OF THIS LICENSE\r\nThe Free Software Foundation may publish new, revised versions of the GNU Free Documentation\r\nLicense from time to time. Such new versions will be similar in spirit to the present version, but\r\nmay differ in detail to address new problems or concerns. See http://www.gnu.org/copyleft/.\r\nEach version of the License is given a distinguishing version number. If the Document specifies\r\nthat a particular numbered version of this License \"or any later version\" applies to it, you have the\r\noption of following the terms and conditions either of that specified version or of any later version\r\nthat has been published (not as a draft) by the Free Software Foundation. If the Document does\r\nnot specify a version number of this License, you may choose any version ever published (not as a\r\ndraft) by the Free Software Foundation. If the Document specifies that a proxy can decide which\r\nfuture versions of this License can be used, that proxy’s public statement of acceptance of a version\r\npermanently authorizes you to choose that version for the Document.\r\n11. RELICENSING\r\n\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server\r\nthat publishes copyrightable works and also provides prominent facilities for anybody to edit those\r\nworks. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiau\u0002thor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus\r\npublished on the MMC site.\r\n\"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by\r\nCreative Commons Corporation, a not-for-profit corporation with a principal place of business in\r\nSan Francisco, California, as well as future copyleft versions of that license published by that same\r\norganization.\r\n\"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another\r\nDocument.\r\nAn MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works\r\nthat were first published under this License somewhere other than this MMC, and subsequently\r\nincorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2)\r\nwere thus incorporated prior to November 1, 2008.\r\nThe operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA\r\non the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing.\r\nADDENDUM: How to use this License for your documents\r\nTo use this License in a document you have written, include a copy of the License in the document\r\nand put the following copyright and license notices just after the title page:\r\nCopyright (c) YEAR YOUR NAME. Permission is granted to copy, distribute and/or\r\nmodify this document under the terms of the GNU Free Documentation License, Ver\u0002sion 1.3 or any later version published by the Free Software Foundation; with no Invari\u0002ant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is\r\nincluded in the section entitled \"GNU Free Documentation License\".",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/8976594f-5b95-4c36-baa0-ffa865a9e682.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d369831728025e9e579359fca06a363c42a2fbaadd6971d381e4ca743a671cfb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 499
      },
      {
        "segments": [
          {
            "segment_id": "3e0e5ac0-dd68-4c31-a546-c43063cd0a7e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 364,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "348 APPENDIX B. GNU FREE DOCUMENTATION LICENSE\r\nIf you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the \"with...Texts.\"\r\nline with this:\r\nwith the Invariant Sections being LIST THEIR TITLES, with the Front-Cover Texts\r\nbeing LIST, and with the Back-Cover Texts being LIST.\r\nIf you have Invariant Sections without Cover Texts, or some other combination of the three, merge\r\nthose two alternatives to suit the situation.\r\nIf your document contains nontrivial examples of program code, we recommend releasing these\r\nexamples in parallel under your choice of free software license, such as the GNU General Public\r\nLicense, to permit their use in free software.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3e0e5ac0-dd68-4c31-a546-c43063cd0a7e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7756d5939b2bfa4e4668be8c387b6e9c4b79fa375ece6a2b7ff8bf8d394a56fb",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "6ca3bb3e-3815-4f1f-bd42-bf23e0898a2f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 365,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix C\r\nHistory\r\nTitle: Introduction to Probability and Statistics Using R\r\nYear: 2010\r\nAuthors: G. Jay Kerns\r\nPublisher: G. Jay Kerns\r\n349",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6ca3bb3e-3815-4f1f-bd42-bf23e0898a2f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4acd678eaf5ff1d66cbfa8ce137142cb7b1e2abc3c4e69fecb7ce3c427f34277",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "a7a1a94e-9452-4622-b611-f32608729891",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 366,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "350 APPENDIX C. HISTORY",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a7a1a94e-9452-4622-b611-f32608729891.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=47a21205b949c2b8f55bcd395c8ba68b0b7610428792760b9eedd2e302ab1b27",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "f59ef1ff-38c3-42a9-b61f-59f5d1ccd249",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 367,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix D\r\nData\r\nThis appendix is a reference of sorts regarding some of the data structures a statistician is likely to\r\nencounter. We discuss their salient features and idiosyncrasies.\r\nD.1 Data Structures\r\nD.1.1 Vectors\r\nSee the “Vectors and Assignment” section of An Introduction to R. A vector is an ordered sequence\r\nof elements, such as numbers, characters, or logical values, and there may be NA’s present. We\r\nusually make vectors with the assignment operator <-.\r\n> x <- c(3, 5, 9)\r\nVectors are atomic in the sense that if you try to mix and match elements of different modes\r\nthen all elements will be coerced to the most convenient common mode.\r\n> y <- c(3, \"5\", TRUE)\r\nIn the example all elements were coerced to character mode. We can test whether a given object\r\nis a vector with is.vector and can coerce an object (if possible) to a vector with as.vector.\r\nD.1.2 Matrices and Arrays\r\nSee the “Arrays and Matrices” section of An Introduction to R. Loosely speaking, a matrix is a\r\nvector that has been reshaped into rectangular form, and an array is a multidimensional matrix.\r\nStrictly speaking, it is the other way around: an array is a data vector with a dimension attribute\r\n(dim), and a matrix is the special case of an array with only two dimensions. We can construct a\r\nmatrix with the matrix function.\r\n> matrix(letters[1:6], nrow = 2, ncol = 3)\r\n[,1] [,2] [,3]\r\n[1,] \"a\" \"c\" \"e\"\r\n[2,] \"b\" \"d\" \"f\"\r\n351",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/f59ef1ff-38c3-42a9-b61f-59f5d1ccd249.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ccc3e114ce0134f070f741cc4fc382956e6da0a2e4df218af08ecd67ceb46b64",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 379
      },
      {
        "segments": [
          {
            "segment_id": "71f42f37-ece2-46cc-974c-f61f88a18079",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 368,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "352 APPENDIX D. DATA\r\nNotice the order of the matrix entries, which shows how the matrix is populated by default. We\r\ncan change this with the byrow argument:\r\n> matrix(letters[1:6], nrow = 2, ncol = 3, byrow = TRUE)\r\n[,1] [,2] [,3]\r\n[1,] \"a\" \"b\" \"c\"\r\n[2,] \"d\" \"e\" \"f\"\r\nWe can test whether a given object is a matrix with is.matrix and can coerce an object (if\r\npossible) to a matrix with as.matrix. As a final example watch what happens when we mix and\r\nmatch types in the first argument:\r\n> matrix(c(1, \"2\", NA, FALSE), nrow = 2, ncol = 3)\r\n[,1] [,2] [,3]\r\n[1,] \"1\" NA \"1\"\r\n[2,] \"2\" \"FALSE\" \"2\"\r\nNotice how all of the entries were coerced to character for the final result (except NA). Also\r\nnotice how the four values were recycled to fill up the six entries of the matrix.\r\nThe standard arithmetic operations work element-wise with matrices.\r\n> A <- matrix(1:6, 2, 3)\r\n> B <- matrix(2:7, 2, 3)\r\n> A + B\r\n[,1] [,2] [,3]\r\n[1,] 3 7 11\r\n[2,] 5 9 13\r\n> A * B\r\n[,1] [,2] [,3]\r\n[1,] 2 12 30\r\n[2,] 6 20 42\r\nIf you want the standard definition of matrix multiplication then use the %*% function. If we\r\nwere to try A %*%B we would get an error because the dimensions do not match correctly, but for\r\nfun, we could transpose B to get conformable matrices. The transpose function t only works for\r\nmatrices (and data frames).\r\n> try(A * B) # an error\r\n[,1] [,2] [,3]\r\n[1,] 2 12 30\r\n[2,] 6 20 42\r\n> A %*% t(B) # this is alright\r\n[,1] [,2]\r\n[1,] 44 53\r\n[2,] 56 68",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/71f42f37-ece2-46cc-974c-f61f88a18079.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ca66185b0aa7f6387969240d922bd9218b1ec96f49d22090f2541930b9b6cd89",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 285
      },
      {
        "segments": [
          {
            "segment_id": "0453039f-1d3a-4698-8676-dd258fc8d41c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 369,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "D.1. DATA STRUCTURES 353\r\nTo get the ordinary matrix inverse use the solve function:\r\n> solve(A %*% t(B)) # input matrix must be square\r\n[,1] [,2]\r\n[1,] 2.833333 -2.208333\r\n[2,] -2.333333 1.833333\r\nArrays more general than matrices, and some functions (like transpose) do not work for the\r\nmore general array. Here is what an array looks like:\r\n> array(LETTERS[1:24], dim = c(3,4,2))\r\n, , 1\r\n[,1] [,2] [,3] [,4]\r\n[1,] \"A\" \"D\" \"G\" \"J\"\r\n[2,] \"B\" \"E\" \"H\" \"K\"\r\n[3,] \"C\" \"F\" \"I\" \"L\"\r\n, , 2\r\n[,1] [,2] [,3] [,4]\r\n[1,] \"M\" \"P\" \"S\" \"V\"\r\n[2,] \"N\" \"Q\" \"T\" \"W\"\r\n[3,] \"O\" \"R\" \"U\" \"X\"\r\nWe can test with is.array and may coerce with as.array.\r\nD.1.3 Data Frames\r\nA data frame is a rectangular array of information with a special status in R. It is used as the\r\nfundamental data structure by many of the modeling functions. It is like a matrix in that all of the\r\ncolumns must be the same length, but it is more general than a matrix in that columns are allowed\r\nto have different modes.\r\n> x <- c(1.3, 5.2, 6)\r\n> y <- letters[1:3]\r\n> z <- c(TRUE, FALSE, TRUE)\r\n> A <- data.frame(x, y, z)\r\n> A\r\nx y z\r\n1 1.3 a TRUE\r\n2 5.2 b FALSE\r\n3 6.0 c TRUE\r\nNotice the names on the columns of A. We can change those with the names function.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0453039f-1d3a-4698-8676-dd258fc8d41c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=38750f24a9408196ddae56ff1e1f86e5f5cbee966f2c908ba4aa2764b2420288",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 236
      },
      {
        "segments": [
          {
            "segment_id": "c1c9055a-323b-4917-9ccb-3466d35d6573",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 370,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "354 APPENDIX D. DATA\r\n> names(A) <- c(\"Fred\", \"Mary\", \"Sue\")\r\n> A\r\nFred Mary Sue\r\n1 1.3 a TRUE\r\n2 5.2 b FALSE\r\n3 6.0 c TRUE\r\nBasic command is data.frame. You can test with is.data.frame and you can coerce with\r\nas.data.frame.\r\nD.1.4 Lists\r\nA list is more general than a data frame.\r\nD.1.5 Tables\r\nThe word “table” has a special meaning in R. More precisely, a contingency table is an object of\r\nclass “table” which is an array\r\nSuppose you have a contingency table and would like to do descriptive or inferential statistics\r\non it. The default form of the table is usually inconvenient to use unless we are working with a\r\nfunction specially tailored for tables. Here is how to transform your data to a more manageable\r\nform, namely, the raw data used to make the table.\r\nFirst, we coerce the table to a data frame with :\r\n> A <- as.data.frame(Titanic)\r\n> head(A)\r\nClass Sex Age Survived Freq\r\n1 1st Male Child No 0\r\n2 2nd Male Child No 0\r\n3 3rd Male Child No 35\r\n4 Crew Male Child No 0\r\n5 1st Female Child No 0\r\n6 2nd Female Child No 0\r\nNote that there are as many preliminary columns of A as there are dimensions to the table. The\r\nrows of A contain every possible combination of levels from each of the dimensions. There is also\r\na Freq column, which shows how many observations there were at that particular combination of\r\nlevels.\r\nThe form of A is often sufficient for our purposes, but more often we need to do more work: we\r\nwould usually like to repeat each row of A exactly the number of times shown in the Freq column.\r\nThe reshape package [89] has the function untable designed for that very purpose:\r\n> library(reshape)\r\n> B <- with(A, untable(A, Freq))\r\n> head(B)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c1c9055a-323b-4917-9ccb-3466d35d6573.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=960e46219d50904a29f792d004f88f0eef15b634bd516c1b1237c3d86902131f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 310
      },
      {
        "segments": [
          {
            "segment_id": "6fa477b1-f8f1-4bc0-ab25-01fb3a8873b7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 371,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "D.1. DATA STRUCTURES 355\r\nClass Sex Age Survived Freq\r\n3 3rd Male Child No 35\r\n3.1 3rd Male Child No 35\r\n3.2 3rd Male Child No 35\r\n3.3 3rd Male Child No 35\r\n3.4 3rd Male Child No 35\r\n3.5 3rd Male Child No 35\r\nNow, this is more like it. Note that we slipped in a call to the with function, which was done\r\nto make the call to untable more pretty; we could just as easily have done\r\nuntable(TitanicDF , A$Freq)\r\nThe only fly in the ointment is the lingering Freq column which has repeated values that do\r\nnot have any meaning any more. We could just ignore it, but it would be better to get rid of the\r\nmeaningless column so that it does not cause trouble later. While we are at it, we could clean up\r\nthe rownames, too.\r\n> C <- B[, -5]\r\n> rownames(C) <- 1:dim(C)[1]\r\n> head(C)\r\nClass Sex Age Survived\r\n1 3rd Male Child No\r\n2 3rd Male Child No\r\n3 3rd Male Child No\r\n4 3rd Male Child No\r\n5 3rd Male Child No\r\n6 3rd Male Child No\r\nD.1.6 More about Tables\r\nSuppose you want to make a table that looks like this:\r\nThere are at least two ways to do it.\r\n• Using a matrix:\r\n> tab <- matrix(1:6, nrow = 2, ncol = 3)\r\n> rownames(tab) <- c(\"first\", \"second\")\r\n> colnames(tab) <- c(\"A\", \"B\", \"C\")\r\n> tab\r\nA B C\r\nfirst 1 3 5\r\nsecond 2 4 6\r\n◦ note that the columns are filled in consecutively by default. If you want to fill the data\r\nin by rows then do byrow = TRUE in the matrix command.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6fa477b1-f8f1-4bc0-ab25-01fb3a8873b7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c7c9443ab222285707b732f352eb6b2cc7603e950d6717fdcd6e8820ee549dde",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 280
      },
      {
        "segments": [
          {
            "segment_id": "b4cdd135-6dc9-41e1-b8d3-64d16765a29a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 372,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "356 APPENDIX D. DATA\r\n◦ the object is a matrix\r\n• Using a dataframe\r\n> p <- c(\"milk\", \"tea\")\r\n> g <- c(\"milk\", \"tea\")\r\n> catgs <- expand.grid(poured = p, guessed = g)\r\n> cnts <- c(3, 1, 1, 3)\r\n> D <- cbind(catgs, count = cnts)\r\n> xtabs(count ~ poured + guessed, data = D)\r\nguessed\r\npoured milk tea\r\nmilk 3 1\r\ntea 1 3\r\n◦ again, the data are filled in column-wise.\r\n◦ the object is a dataframe\r\n◦ if you want to store it as a table then do A <- xtabs(count ~ poured + guessed, data =\r\nD)\r\nD.2 Importing Data\r\nStatistics is the study of data, so the statistician’s first step is usually to obtain data from somewhere\r\nor another and read them into R. In this section we describe some of the most common sources of\r\ndata and how to get data from those sources into a running R session.\r\nFor more information please refer to the R Data Import/Export Manual, [68] and An Introduc\u0002tion to R, [85].\r\nD.2.1 Data in Packages\r\nThere are many data sets stored in the datasets package of base R. To see a list of them all issue\r\nthe command data(package = \"datasets\"). The output is omitted here because the list is so\r\nlong. The names of the data sets are listed in the left column. Any data set in that list is already on\r\nthe search path by default, which means that a user can use it immediately without any additional\r\nwork.\r\nThere are many other data sets available in the thousands of contributed packages. To see\r\nthe data sets available in those packages that are currently loaded into memory issue the single\r\ncommand data(). If you would like to see all of the data sets that are available in all packages\r\nthat are installed on your computer (but not necessarily loaded), issue the command\r\ndata(package = .packages (all. available = TRUE))\r\nTo load the data set foo in the contributed package bar issue the commands library(bar)\r\nfollowed by data(foo), or just the single command\r\ndata(foo , package = \"bar\")",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b4cdd135-6dc9-41e1-b8d3-64d16765a29a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=95f31b54ce135e9835bf1703efadc1a9cf2f053174934008fb4b4021f6fdcdec",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 352
      },
      {
        "segments": [
          {
            "segment_id": "bc8c8118-e82f-4bce-96e6-b518e4dc7a2e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 373,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "D.3. CREATING NEW DATA SETS 357\r\nD.2.2 Text Files\r\nMany sources of data are simple text files. The entries in the file are separated by delimeters such\r\nas TABS (tab-delimeted), commas (comma separated values, or .csv, for short) or even just white\r\nspace (no special name). A lot of data on the Internet are stored with text files, and even if they are\r\nnot, a person can copy-paste information from a web page to a text file, save it on the computer,\r\nand read it into R.\r\nD.2.3 Other Software Files\r\nOften the data set of interest is stored in some other, proprietary, format by third-party software\r\nsuch as Minitab, SAS, or SPSS. The foreign package supports import/conversion from many of\r\nthese formats. Please note, however, that data sets from other software sometimes have properties\r\nwith no direct analogue in R. In those cases the conversion process may lose some information\r\nwhich will need to be reentered manually from within R. See the Data Import/Export Manual.\r\nAs an example, suppose the data are stored in the SPSS file foo.sav which the user has copied\r\nto the working directory; it can be imported with the commands\r\n> library(foreign)\r\n> read.spss(\"foo.sav\")\r\nSee ?read.spss for the available options to customize the file import. Note that the R Com\u0002mander will import many of the common file types with a menu driven interface.\r\nD.2.4 Importing a Data Frame\r\nThe basic command is read.table.\r\nD.3 Creating New Data Sets\r\nUsing c\r\nUsing scan\r\nUsing the R Commander.\r\nD.4 Editing Data\r\nD.4.1 Editing Data Values\r\nD.4.2 Inserting Rows and Columns\r\nD.4.3 Deleting Rows and Columns\r\nD.4.4 Sorting Data\r\nWe can sort a vector with the sort function. Normally we have a data frame of several columns\r\n(variables) and many, many rows (observations). The goal is to shuffle the rows so that they are\r\nordered by the values of one or more columns. This is done with the order function.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/bc8c8118-e82f-4bce-96e6-b518e4dc7a2e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4b201c9e5f66fc8b773d76028e7d6bbbec1944adc871871e75419480a19899b2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 322
      },
      {
        "segments": [
          {
            "segment_id": "76ac37a2-f0fd-4e32-b5a2-e6c5d7b2a6d9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 374,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "358 APPENDIX D. DATA\r\nFor example, we may sort all of the rows of the Puromycin data (in ascending order) by the\r\nvariable conc with the following:\r\n> Tmp <- Puromycin[order(Puromycin$conc), ]\r\n> head(Tmp)\r\nconc rate state\r\n1 0.02 76 treated\r\n2 0.02 47 treated\r\n13 0.02 67 untreated\r\n14 0.02 51 untreated\r\n3 0.06 97 treated\r\n4 0.06 107 treated\r\nWe can accomplish the same thing with the command\r\n> with(Puromycin, Puromycin[order(conc), ])\r\nWe can sort by more than one variable. To sort first by state and next by conc do\r\n> with(Puromycin, Puromycin[order(state, conc), ])\r\nIf we would like to sort a numeric variable in descending order then we put a minus sign in\r\nfront of it.\r\n> Tmp <- with(Puromycin, Puromycin[order(-conc), ])\r\n> head(Tmp)\r\nconc rate state\r\n11 1.10 207 treated\r\n12 1.10 200 treated\r\n23 1.10 160 untreated\r\n9 0.56 191 treated\r\n10 0.56 201 treated\r\n21 0.56 144 untreated\r\nIf we would like to sort by a character (or factor) in decreasing order then we can use the xtfrm\r\nfunction which produces a numeric vector in the same order as the character vector.\r\n> Tmp <- with(Puromycin, Puromycin[order(-xtfrm(state)), ])\r\n> head(Tmp)\r\nconc rate state\r\n13 0.02 67 untreated\r\n14 0.02 51 untreated\r\n15 0.06 84 untreated\r\n16 0.06 86 untreated\r\n17 0.11 98 untreated\r\n18 0.11 115 untreated",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/76ac37a2-f0fd-4e32-b5a2-e6c5d7b2a6d9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=27c098971e08f9163c04d4f1b6c08a6d730ab36be16f5abe680d94d7e2aeb40c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "b17f2e54-6469-4e7d-903d-a71737343283",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 375,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "D.5. EXPORTING DATA 359\r\nD.5 Exporting Data\r\nThe basic function is write.table. The MASS package also has a write.matrix function.\r\nD.6 Reshaping Data\r\n• Aggregation\r\n• Convert Tables to data frames and back\r\nrbind, cbind\r\nab[order(ab[,1]),]\r\ncomplete.cases\r\naggregate\r\nstack",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b17f2e54-6469-4e7d-903d-a71737343283.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9275e9b367f451867975a82b058c1b1d24de552b50dd24140304aa8a9feec67c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "41dc11ea-ac92-46eb-b927-9823067b25fb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 376,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "360 APPENDIX D. DATA",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/41dc11ea-ac92-46eb-b927-9823067b25fb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b6d584044c16ac9c82cf62da770a2a78389d53e8932166d561a8545384da16ba",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 266
      },
      {
        "segments": [
          {
            "segment_id": "72130954-89cf-49a6-8892-7e24f6fb44fd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 377,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix E\r\nMathematical Machinery\r\nThis appendix houses many of the standard definitions and theorems that are used at some point\r\nduring the narrative. It is targeted for someone reading the book who forgets the precise definition\r\nof something and would like a quick reminder of an exact statement. No proofs are given, and\r\nthe interested reader should consult a good text on Calculus (say, Stewart [80] or Apostol [4, 5]),\r\nLinear Algebra (say, Strang [82] and Magnus [62]), Real Analysis (say, Folland [27], or Carothers\r\n[12]), or Measure Theory (Billingsley [8], Ash [6], Resnick [70]) for details.\r\nE.1 Set Algebra\r\nWe denote sets by capital letters, A, B, C, etc. The letter S is reserved for the sample space, also\r\nknown as the universe or universal set, the set which contains all possible elements. The symbol ∅\r\nrepresents the empty set, the set with no elements.\r\nSet Union, Intersection, and Difference\r\nGiven subsets A and B, we may manipulate them in an algebraic fashion. To this end, we have three\r\nset operations at our disposal: union, intersection, and difference. Below is a table summarizing\r\nthe pertinent information about these operations.\r\nIdentities and Properties\r\n1. A ∪ ∅ = A, A ∩ ∅ = ∅\r\nName Denoted Defined by elements R syntax\r\nUnion A ∪ B in A or B or both union(A, B)\r\nIntersection A ∩ B in both A and B intersect(A, B)\r\nDifference A\\B in A but not in B setdiff(A, B)\r\nComplement A\r\nc\r\nin S but not in A setdiff(S, A)\r\nTable E.1: Set operations\r\n361",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/72130954-89cf-49a6-8892-7e24f6fb44fd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=97e821cea9495b97e57f89c6a1399c5e949430a5b0edcef882ada8df37817d58",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 260
      },
      {
        "segments": [
          {
            "segment_id": "a97169d3-d5a8-4a6f-9928-be100e7c769e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 378,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "362 APPENDIX E. MATHEMATICAL MACHINERY\r\n2. A ∪ S = S, A ∩ S = A\r\n3. A ∪ A\r\nc = S , A ∩ Ac = ∅\r\n4. (A\r\nc\r\n)\r\nc = A\r\n5. The Commutative Property:\r\nA ∪ B = B ∪ A, A ∩ B = B ∩ A (E.1.1)\r\n6. The Associative Property:\r\n(A ∪ B) ∪ C = A ∪ (B ∪ C), (A ∩ B) ∩ C = A ∩ (B ∩ C) (E.1.2)\r\n7. The Distributive Property:\r\nA ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ B), A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ B) (E.1.3)\r\n8. DeMorgan’s Laws\r\n(A ∪ B)\r\nc = Ac ∩ Bc\r\nand (A ∩ B)\r\nc = Ac ∪ Bc\r\n, (E.1.4)\r\nor more generally,\r\n\r\n\r\n[\r\nα\r\nAα\r\n\r\n\r\nc\r\n=\r\n\\\r\nα\r\nA\r\nc\r\nα\r\n, and\r\n\r\n\r\n\\\r\nα\r\nAα\r\n\r\n\r\nc\r\n=\r\n[\r\nα\r\nA\r\nc\r\nα\r\n(E.1.5)\r\nE.2 Differential and Integral Calculus\r\nA function f of one variable is said to be one-to-one if no two distinct x values are mapped to the\r\nsame y = f(x) value. To show that a function is one-to-one we can either use the horizontal line\r\ntest or we may start with the equation f(x1) = f(x2) and use algebra to show that it implies x1 = x2.\r\nLimits and Continuity\r\nDefinition E.1. Let f be a function defined on some open interval that contains the number a,\r\nexcept possibly at a itself. Then we say the limit of f(x) as x approaches a is L, and we write\r\nlim\r\nx→a\r\nf(x) = L, (E.2.1)\r\nif for every \u000f > 0 there exists a number δ > 0 such that 0 < |x − a| < δ implies | f(x) − L| < \u000f.\r\nDefinition E.2. A function f is continuous at a number a if\r\nlim\r\nx→a\r\nf(x) = f(a). (E.2.2)\r\nThe function f is right-continuous at the number a if limx→a\r\n+ f(x) = f(a), and left-continuous at a\r\nif limx→a\r\n− f(x) = f(a). Finally, the function f is continuous on an interval I if it is continuous at\r\nevery number in the interval.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a97169d3-d5a8-4a6f-9928-be100e7c769e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b70956e4abbe978eaf6c16497572883668a84f5bbecc6b2066437bab7e5b4787",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 375
      },
      {
        "segments": [
          {
            "segment_id": "208b7987-a365-4119-ae32-e3f181a7c0ef",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 379,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "E.2. DIFFERENTIAL AND INTEGRAL CALCULUS 363\r\nDifferentiation\r\nDefinition E.3. The derivative of a function f at a number a, denoted by f\r\n0\r\n(a), is\r\nf\r\n0\r\n(a) = lim\r\nh→0\r\nf(a + h) − f(a)\r\nh\r\n, (E.2.3)\r\nprovided this limit exists.\r\nA function is differentiable at a if f\r\n0\r\n(a) exists. It is differentiable on an open interval (a, b) if it\r\nis differentiable at every number in the interval.\r\nDifferentiation Rules\r\nIn the table that follows, f and g are differentiable functions and c is a constant.\r\nd\r\ndx\r\nc = 0\r\nd\r\ndx\r\nx\r\nn = nxn−1\r\n(c f)\r\n0 = c f 0\r\n(f ± g)\r\n0 = f0 ± g0\r\n(f g)\r\n0 = f0g + f g0\r\n\u0010\r\nf\r\ng\r\n\u00110\r\n=\r\nf\r\n0g−f g0\r\ng\r\n2\r\nTable E.2: Differentiation rules\r\nTheorem E.4. Chain Rule: If f and g are both differentiable and F = f ◦ g is the composite\r\nfunction defined by F(x) = f[g(x)], then F is differentiable and F0(x) = f\r\n0\r\n[g(x)] · g\r\n0\r\n(x).\r\nUseful Derivatives\r\nd\r\ndx\r\ne\r\nx = ex d\r\ndx\r\nln x = x\r\n−1 d\r\ndx\r\nsin x = cos x\r\nd\r\ndx\r\ncos x = − sin x\r\nd\r\ndx\r\ntan x = sec2x\r\nd\r\ndx\r\ntan−1x = (1 + x\r\n2\r\n)\r\n−1\r\nTable E.3: Some derivatives\r\nOptimization\r\nDefinition E.5. A critical number of the function f is a value x\r\n∗\r\nfor which f\r\n0\r\n(x\r\n∗\r\n) = 0 or for which\r\nf\r\n0\r\n(x\r\n∗\r\n) does not exist.\r\nTheorem E.6. First Derivative Test. If f is differentiable and if x∗is a critical number of f and if\r\nf\r\n0\r\n(x) ≥ 0 for x ≤ x\r\n∗ and f 0\r\n(x) ≤ 0 for x ≥ x\r\n∗\r\n, then x∗is a local maximum of f . If f 0(x) ≤ 0 for\r\nx ≤ x\r\n∗ and f 0\r\n(x) ≥ 0 for x ≥ x\r\n∗\r\n, then x∗is a local minimum of f .\r\nTheorem E.7. Second Derivative Test. If f is twice differentiable and if x∗is a critical number of\r\nf , then x∗is a local maximum of f if f 00(x\r\n∗\r\n) < 0 and x∗is a local minimum of f if f 00(x\r\n∗\r\n) > 0.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/208b7987-a365-4119-ae32-e3f181a7c0ef.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=65875d0b445327b2bd35e342c5ecba622b3553e8f21e0fbe9726bb38b64eba2c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 390
      },
      {
        "segments": [
          {
            "segment_id": "57c93637-c743-42e5-af36-f82f8ec0d6b9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 380,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "364 APPENDIX E. MATHEMATICAL MACHINERY\r\nIntegration\r\nAs it turns out, there are all sorts of things called “integrals”, each defined in its own idiosyncratic\r\nway. There are Riemann integrals, Lebesgue integrals, variants of these called Stieltjes integrals,\r\nDaniell integrals, Ito integrals, and the list continues. Given that this is an introductory book, we\r\nwill use the Riemannian integral with the caveat that the Riemann integral is not the integral that\r\nwill be used in more advanced study.\r\nDefinition E.8. Let f be defined on [a, b], a closed interval of the real line. For each n, divide\r\n[a, b] into subintervals [xi, xi+1], i = 0, 1, . . . , n − 1, of length ∆xi = (b − a)/n where x0 = a and\r\nxn = b, and let x\r\n∗\r\ni\r\nbe any points chosen from the respective subintervals. Then the definite integral\r\nof f from a to b is defined by\r\nZ b\r\na\r\nf(x) dx = lim\r\nn→∞\r\nXn−1\r\ni=0\r\nf(x\r\n∗\r\ni\r\n) ∆xi, (E.2.4)\r\nprovided the limit exists, and in that case, we say that f is integrable from a to b.\r\nTheorem E.9. The Fundamental Theorem of Calculus. Suppose f is continuous on [a, b]. Then\r\n1. the function g defined by g(x) =\r\nR x\r\na\r\nf(t) dt, a ≤ x ≤ b, is continuous on [a, b] and differen\u0002tiable on (a, b) with g0\r\n(x) = f(x).\r\n2. R b\r\na\r\nf(x) dx = F(b)−F(a), where F is any antiderivative of f , that is, any function F satisfying\r\nF\r\n0 = f .\r\nChange of Variables\r\nTheorem E.10. If g is a differentiable function whose range is the interval [a, b] and if both f and\r\ng\r\n0 are continuous on the range of u = g(x), then\r\nZ g(b)\r\ng(a)\r\nf(u) du =\r\nZ b\r\na\r\nf[g(x)] g\r\n0\r\n(x) dx. (E.2.5)\r\nUseful Integrals\r\nR\r\nx\r\nn dx = xn+1\r\n/(n + 1), n , −1\r\nR\r\ne\r\nx dx = ex\r\nR\r\nx\r\n−1 dx = ln |x|\r\nR\r\ntan x dx = ln |sec x|\r\nR\r\na\r\nx dx = ax\r\n/ ln a\r\nR\r\n(x\r\n2 + 1)−1 dx = tan−1\r\nx\r\nTable E.4: Some integrals (constants of integration omitted)\r\nIntegration by Parts\r\nZ\r\nu dv = uv −\r\nZ\r\nv du (E.2.6)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/57c93637-c743-42e5-af36-f82f8ec0d6b9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=605447e739e7fea68c739c99935f09facbce2342fb8d65588d0b1e43a0c05469",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 388
      },
      {
        "segments": [
          {
            "segment_id": "9cce95db-735c-48c3-8da2-cfa8f461b904",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 381,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "E.3. SEQUENCES AND SERIES 365\r\nTheorem E.11. L’Hôpital’s Rule. Suppose f and g are differentiable and g0(x) , 0 near a, except\r\npossibly at a. Suppose that the limit\r\nlim\r\nx→a\r\nf(x)\r\ng(x)\r\n(E.2.7)\r\nis an indeterminate form of type 0\r\n0\r\nor ∞/∞. Then\r\nlim\r\nx→a\r\nf(x)\r\ng(x)\r\n= lim\r\nx→a\r\nf\r\n0\r\n(x)\r\ng\r\n0\r\n(x)\r\n, (E.2.8)\r\nprovided the limit on the right-hand side exists or is infinite.\r\nImproper Integrals\r\nIf R t\r\na\r\nf(x)dx exists for every number t ≥ a, then we define\r\nZ ∞\r\na\r\nf(x) dx = lim\r\nt→∞ Z t\r\na\r\nf(x) dx, (E.2.9)\r\nprovided this limit exists as a finite number, and in that case we say that R ∞\r\na\r\nf(x) dx is convergent.\r\nOtherwise, we say that the improper integral is divergent.\r\nIf R b\r\nt\r\nf(x) dx exists for every number t ≤ b, then we define\r\nZ b\r\n−∞\r\nf(x) dx = lim\r\nt→−∞ Z b\r\nt\r\nf(x) dx, (E.2.10)\r\nprovided this limit exists as a finite number, and in that case we say that R b\r\n−∞\r\nf(x) dx is convergent.\r\nOtherwise, we say that the improper integral is divergent.\r\nIf both R ∞\r\na\r\nf(x) dx and R a\r\n−∞\r\nf(x) dx are convergent, then we define\r\nZ ∞\r\n−∞\r\nf(x) dx =\r\nZ a\r\n−∞\r\nf(x) dx +\r\nZ ∞\r\na\r\nf(x)dx, (E.2.11)\r\nand we say that R ∞\r\n−∞\r\nf(x) dx is convergent. Otherwise, we say that the improper integral is divergent.\r\nE.3 Sequences and Series\r\nA sequence is an ordered list of numbers, a1, a2, a3, . . . , an = (ak)\r\nn\r\nk=1\r\n. A sequence may be finite or\r\ninfinite. In the latter case we write a1, a2, a3, . . .= (ak)\r\n∞\r\nk=1\r\n. We say that the infinite sequence (ak)\r\n∞\r\nk=1\r\nconverges to the finite limit L, and we write\r\nlim\r\nk→∞\r\nak = L, (E.3.1)\r\nif for every \u000f > 0 there exists an integer N ≥ 1 such that |ak − L| < \u000f for all k ≥ N. We say that the\r\ninfinite sequence (ak)\r\n∞\r\nk=1\r\ndiverges to +∞ (or -∞) if for every M ≥ 0 there exists an integer N ≥ 1\r\nsuch that ak ≥ M for all k ≥ N (or ak ≤ −M for all k ≥ N).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/9cce95db-735c-48c3-8da2-cfa8f461b904.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a029dfca155208ad7b121c94aeec19788a2970eafc8c26a6661574b0a146a353",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 392
      },
      {
        "segments": [
          {
            "segment_id": "1a909bc7-fa97-4eb7-ba1b-10d337249b87",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 382,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "366 APPENDIX E. MATHEMATICAL MACHINERY\r\nFinite Series\r\nXn\r\nk=1\r\nk = 1 + 2 + · · · + n =\r\nn(n + 1)\r\n2\r\n(E.3.2)\r\nXn\r\nk=1\r\nk\r\n2 = 12 + 22 + · · · + n2 =\r\nn(n + 1)(2n + 3)\r\n6\r\n(E.3.3)\r\nThe Binomial Series\r\nXn\r\nk=0\r\n \r\nn\r\nk\r\n!\r\na\r\nn−k\r\nb\r\nk = (a + b)n\r\n(E.3.4)\r\nInfinite Series\r\nGiven an infinite sequence of numbers a1, a2, a3, . . .= (ak)\r\n∞\r\nk=1\r\n, let sn denote the partial sum of the\r\nfirst n terms:\r\nsn =\r\nXn\r\nk=1\r\nak = a1 + a2 + · · · + an. (E.3.5)\r\nIf the sequence (sn)\r\n∞\r\nn=1\r\nconverges to a finite number S then we say that the infinite series P\r\nk ak is\r\nconvergent and write\r\nX∞\r\nk=1\r\nak = S. (E.3.6)\r\nOtherwise we say the infinite series is divergent.\r\nRules for Series\r\nLet (ak)\r\n∞\r\nk=1\r\nand (bk)\r\n∞\r\nk=1\r\nbe infinite sequences and let c be a constant.\r\nX∞\r\nk=1\r\ncak = c\r\nX∞\r\nk=1\r\nak (E.3.7)\r\nX∞\r\nk=1\r\n(ak ± bk) =\r\nX∞\r\nk=1\r\nak ±\r\nX∞\r\nk=1\r\nbk (E.3.8)\r\nIn both of the above the series on the left is convergent if the series on the right is (are) convergent.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/1a909bc7-fa97-4eb7-ba1b-10d337249b87.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9f758568ee53c0d25fefe730d5c3bbcf4f1a395744e2a6376293ba6c9d6ff445",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "3e5fa449-5f70-41fe-8f77-2984881ade27",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 383,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "E.3. SEQUENCES AND SERIES 367\r\nThe Geometric Series\r\nX∞\r\nk=0\r\nx\r\nk =\r\n1\r\n1 − x\r\n, |x| < 1. (E.3.9)\r\nThe Exponential Series\r\nX∞\r\nk=0\r\nx\r\nk\r\nk!\r\n= e\r\nx\r\n, −∞ < x < ∞. (E.3.10)\r\nOther Series\r\nX∞\r\nk=0\r\n \r\nm + k − 1\r\nm − 1\r\n!\r\nx\r\nk =\r\n1\r\n(1 − x)\r\nm\r\n, |x| < 1. (E.3.11)\r\n−\r\nX∞\r\nk=1\r\nx\r\nn\r\nn\r\n= ln(1 − x), |x| < 1. (E.3.12)\r\nX∞\r\nk=0\r\n \r\nn\r\nk\r\n!\r\nx\r\nk = (1 + x)n\r\n, |x| < 1.\r\nTaylor Series\r\nIf the function f has a power series representation at the point a with radius of convergence R > 0,\r\nthat is, if\r\nf(x) =\r\nX∞\r\nk=0\r\nck(x − a)\r\nk\r\n, |x − a| < R, (E.3.14)\r\nfor some constants (ck)\r\n∞\r\nk=0\r\n, then ck must be\r\nck =\r\nf\r\n(k)\r\n(a)\r\nk!\r\n, k = 0, 1, 2, . . . (E.3.15)\r\nFurthermore, the function f is differentiable on the open interval (a − R, a + R) with\r\nf\r\n0\r\n(x) =\r\nX∞\r\nk=1\r\nkck(x − a)\r\nk−1\r\n, |x − a| < R, (E.3.16)\r\nZ\r\nf(x) dx = C +\r\nX∞\r\nk=0\r\nck\r\n(x − a)\r\nk+1\r\nk + 1\r\n, |x − a| < R, (E.3.17)\r\nin which case both of the above series have radius of convergence R.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3e5fa449-5f70-41fe-8f77-2984881ade27.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=84d3225bdd709964d2e06014d8ffd59c7fe96485b785b913e6059f102355d1f7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 446
      },
      {
        "segments": [
          {
            "segment_id": "ed50dc5b-e51f-4b94-aad1-5c0f053d5a75",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 384,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "368 APPENDIX E. MATHEMATICAL MACHINERY\r\nE.4 The Gamma Function\r\nThe Gamma function Γ will be defined in this book according to the formula\r\nΓ(α) =\r\nZ ∞\r\n0\r\nx\r\nα−1\r\ne\r\n−x\r\ndx, for α > 0. (E.4.1)\r\nFact E.12. Properties of the Gamma Function:\r\n• Γ(α) = (α − 1)Γ(α − 1) for any α > 1, and so Γ(n) = (n − 1)! for any positive integer n.\r\n• Γ(1/2) =\r\n√\r\nπ.\r\nE.5 Linear Algebra\r\nMatrices\r\nA matrix is an ordered array of numbers or expressions; typically we write A =\r\n\u0010\r\nai j\u0011\r\nor A =\r\nh\r\nai ji\r\n.\r\nIf A has m rows and n columns then we write\r\nAm×n =\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n. (E.5.1)\r\nThe identity matrix In×n is an n × n matrix with zeros everywhere except for 1’s along the main\r\ndiagonal:\r\nIn×n =\r\n\r\n\r\n1 0 · · · 0\r\n0 1 · · · 0\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 · · · 1\r\n\r\n\r\n. (E.5.2)\r\nand the matrix with ones everywhere is denoted Jn×n:\r\nJn×n =\r\n\r\n\r\n1 1 · · · 1\r\n1 1 · · · 1\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n1 1 · · · 1\r\n\r\n\r\n. (E.5.3)\r\nA vector is a matrix with one of the dimensions equal to one, such as Am×1 (a column vector)\r\nor A1×n (a row vector). The zero vector 0n×1 is an n × 1 matrix of zeros:\r\n0n×1 =\r\nh\r\n0 0 · · · 0\r\niT\r\n. (E.5.4)\r\nThe transpose of a matrix A =\r\n\u0010\r\nai j\u0011\r\nis the matrix A\r\nT =\r\n\u0010\r\naji\u0011\r\n, which is just like A except the\r\nrows are columns and the columns are rows. The matrix A is said to be symmetric if A\r\nT = A. Note\r\nthat (AB)\r\nT = B\r\nTAT\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ed50dc5b-e51f-4b94-aad1-5c0f053d5a75.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c6a738be9248a7ec47dd373e6a88c9f0f4fb14e3e998e80d916c873671667062",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 361
      },
      {
        "segments": [
          {
            "segment_id": "a9714f1f-5d97-43f8-b54a-c8d183a0b25a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 385,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "E.6. MULTIVARIABLE CALCULUS 369\r\nThe trace of a square matrix A is the sum of its diagonal elements: tr(A) =\r\nP\r\ni aii.\r\nThe inverse of a square matrix An×n (when it exists) is the unique matrix denoted A\r\n−1 which sat\u0002isfies AA−1 = A\r\n−1A = In×n. If A−1\r\nexists then we say A is invertible, or alternatively nonsingular.\r\nNote that \u0010A\r\nT\r\n\u0011−1\r\n=\r\n\u0010\r\nA\r\n−1\r\n\u0011T\r\n.\r\nFact E.13. The inverse of the 2 × 2 matrix\r\nA =\r\n\"\r\na b\r\nc d#\r\nis A\r\n−1 =\r\n1\r\nad − bc \"\r\nd −b\r\n−c a #\r\n, (E.5.5)\r\nprovided ad − bc , 0.\r\nDeterminants\r\nDefinition E.14. The determinant of a square matrix An×n is denoted det(A) or |A| and is defined\r\nrecursively by\r\ndet(A) =\r\nXn\r\ni=1\r\n(−1)i+jai j det(Mi j), (E.5.6)\r\nwhere Mi j is the submatrix formed by deleting the i\r\nth row and jth column of A. We may choose\r\nany fixed 1 ≤ j ≤ n we wish to compute the determinant; the final result is independent of the j\r\nchosen.\r\nFact E.15. The determinant of the 2 × 2 matrix\r\nA =\r\n\"\r\na b\r\nc d#\r\nis |A| = ad − bc. (E.5.7)\r\nFact E.16. A square matrix A is nonsingular if and only if det(A) , 0.\r\nPositive (Semi)Definite\r\nIf the matrix A satisfies x\r\nTAx ≥ 0 for all vectors x , 0, then we say that A is positive semidefinite.\r\nIf strict inequality holds for all x , 0, then A is positive definite. The connection to statistics is that\r\ncovariance matrices (see Chapter 7) are always positive semidefinite, and many of them are even\r\npositive definite.\r\nE.6 Multivariable Calculus\r\nPartial Derivatives\r\nIf f is a function of two variables, its first-order partial derivatives are defined by\r\n∂ f\r\n∂x\r\n=\r\n∂\r\n∂x\r\nf(x, y) = lim\r\nh→0\r\nf(x + h, y) − f(x, y)\r\nh\r\n(E.6.1)\r\nand\r\n∂ f\r\n∂y\r\n=\r\n∂\r\n∂y\r\nf(x, y) = lim\r\nh→0\r\nf(x, y + h) − f(x, y)\r\nh\r\n, (E.6.2)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a9714f1f-5d97-43f8-b54a-c8d183a0b25a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c3418f8c36d110111e7ada5508cc5cecb28f0bdb21d7cc78a6d5a33b8d3e09a0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 345
      },
      {
        "segments": [
          {
            "segment_id": "4827e743-a77f-42d2-af8f-96e2280b6025",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 386,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "370 APPENDIX E. MATHEMATICAL MACHINERY\r\nprovided these limits exist. The second-order partial derivatives of f are defined by\r\n∂\r\n2\r\nf\r\n∂x\r\n2\r\n=\r\n∂\r\n∂x\r\n \r\n∂ f\r\n∂x\r\n!\r\n,\r\n∂\r\n2\r\nf\r\n∂y\r\n2\r\n=\r\n∂\r\n∂y\r\n \r\n∂ f\r\n∂y\r\n!\r\n,\r\n∂\r\n2\r\nf\r\n∂x∂y\r\n=\r\n∂\r\n∂x\r\n \r\n∂ f\r\n∂y\r\n!\r\n,\r\n∂\r\n2\r\nf\r\n∂y∂x\r\n=\r\n∂\r\n∂y\r\n \r\n∂ f\r\n∂x\r\n!\r\n. (E.6.3)\r\nIn many cases (and for all cases in this book) it is true that\r\n∂\r\n2\r\nf\r\n∂x∂y\r\n=\r\n∂\r\n2\r\nf\r\n∂y∂x\r\n. (E.6.4)\r\nOptimization\r\nAn function f of two variables has a local maximum at (a, b) if f(x, y) ≥ f(a, b) for all points (x, y)\r\nnear (a, b), that is, for all points in an open disk centered at (a, b). The number f(a, b) is then called\r\na local maximum value of f . The function f has a local minimum if the same thing happens with\r\nthe inequality reversed.\r\nSuppose the point (a, b) is a critical point of f , that is, suppose (a, b) satisfies\r\n∂ f\r\n∂x\r\n(a, b) =\r\n∂ f\r\n∂y\r\n(a, b) = 0. (E.6.5)\r\nFurther suppose ∂\r\n2\r\nf\r\n∂x\r\n2 and ∂\r\n2\r\nf\r\n∂y\r\n2 are continuous near (a, b). Let the Hessian matrix H (not to be con\u0002fused with the hat matrix H of Chapter 12) be defined by\r\nH =\r\n\r\n\r\n∂\r\n2\r\nf\r\n∂x\r\n2\r\n∂\r\n2\r\nf\r\n∂x∂y\r\n∂\r\n2\r\nf\r\n∂y∂x\r\n∂\r\n2\r\nf\r\n∂y\r\n2\r\n\r\n\r\n. (E.6.6)\r\nWe use the following rules to decide whether (a, b) is an extremum (that is, a local minimum or\r\nlocal maximum) of f .\r\n• If det(H) > 0 and ∂\r\n2\r\nf\r\n∂x\r\n2 (a, b) > 0, then (a, b) is a local minimum of f .\r\n• If det(H) > 0 and ∂\r\n2\r\nf\r\n∂x\r\n2 (a, b) < 0, then (a, b) is a local maximum of f .\r\n• If det(H) < 0, then (a, b) is a saddle point of f and so is not an extremum of f .\r\n• If det(H) = 0, then we do not know the status of (a, b); it might be an extremum or it might\r\nnot be.\r\nDouble and Multiple Integrals\r\nLet f be defined on a rectangle R = [a, b] × [c, d], and for each m and n divide [a, b] (respectively\r\n[c, d]) into subintervals [xj, xj+1], i = 0, 1, . . . , m−1 (respectively [yi, yi+1]) of length ∆xj = (b−a)/m\r\n(respectively ∆yi = (d − c)/n) where x0 = a and xm = b (and y0 = c and yn = d ), and let x\r\n∗\r\nj\r\n(y\r\n∗\r\ni\r\n)\r\nbe any points chosen from their respective subintervals. Then the double integral of f over the\r\nrectangle R is\r\n\"\r\nR\r\nf(x, y) dA =\r\nZ\r\nd\r\nc\r\nZ\r\nb\r\na\r\nf(x, y) dxdy = lim\r\nm,n→∞\r\nXn\r\ni=1\r\nXm\r\nj=1\r\nf(x\r\n∗\r\nj\r\n, y\r\n∗\r\ni\r\n)∆xj∆yi, (E.6.7)\r\nprovided this limit exists. Multiple integrals are defined in the same way just with more letters and\r\nsums.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4827e743-a77f-42d2-af8f-96e2280b6025.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c396e55b4e44f6fe0c2bc0026813d21d05aeb89240a370df2c664475e8491f48",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 534
      },
      {
        "segments": [
          {
            "segment_id": "4827e743-a77f-42d2-af8f-96e2280b6025",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 386,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "370 APPENDIX E. MATHEMATICAL MACHINERY\r\nprovided these limits exist. The second-order partial derivatives of f are defined by\r\n∂\r\n2\r\nf\r\n∂x\r\n2\r\n=\r\n∂\r\n∂x\r\n \r\n∂ f\r\n∂x\r\n!\r\n,\r\n∂\r\n2\r\nf\r\n∂y\r\n2\r\n=\r\n∂\r\n∂y\r\n \r\n∂ f\r\n∂y\r\n!\r\n,\r\n∂\r\n2\r\nf\r\n∂x∂y\r\n=\r\n∂\r\n∂x\r\n \r\n∂ f\r\n∂y\r\n!\r\n,\r\n∂\r\n2\r\nf\r\n∂y∂x\r\n=\r\n∂\r\n∂y\r\n \r\n∂ f\r\n∂x\r\n!\r\n. (E.6.3)\r\nIn many cases (and for all cases in this book) it is true that\r\n∂\r\n2\r\nf\r\n∂x∂y\r\n=\r\n∂\r\n2\r\nf\r\n∂y∂x\r\n. (E.6.4)\r\nOptimization\r\nAn function f of two variables has a local maximum at (a, b) if f(x, y) ≥ f(a, b) for all points (x, y)\r\nnear (a, b), that is, for all points in an open disk centered at (a, b). The number f(a, b) is then called\r\na local maximum value of f . The function f has a local minimum if the same thing happens with\r\nthe inequality reversed.\r\nSuppose the point (a, b) is a critical point of f , that is, suppose (a, b) satisfies\r\n∂ f\r\n∂x\r\n(a, b) =\r\n∂ f\r\n∂y\r\n(a, b) = 0. (E.6.5)\r\nFurther suppose ∂\r\n2\r\nf\r\n∂x\r\n2 and ∂\r\n2\r\nf\r\n∂y\r\n2 are continuous near (a, b). Let the Hessian matrix H (not to be con\u0002fused with the hat matrix H of Chapter 12) be defined by\r\nH =\r\n\r\n\r\n∂\r\n2\r\nf\r\n∂x\r\n2\r\n∂\r\n2\r\nf\r\n∂x∂y\r\n∂\r\n2\r\nf\r\n∂y∂x\r\n∂\r\n2\r\nf\r\n∂y\r\n2\r\n\r\n\r\n. (E.6.6)\r\nWe use the following rules to decide whether (a, b) is an extremum (that is, a local minimum or\r\nlocal maximum) of f .\r\n• If det(H) > 0 and ∂\r\n2\r\nf\r\n∂x\r\n2 (a, b) > 0, then (a, b) is a local minimum of f .\r\n• If det(H) > 0 and ∂\r\n2\r\nf\r\n∂x\r\n2 (a, b) < 0, then (a, b) is a local maximum of f .\r\n• If det(H) < 0, then (a, b) is a saddle point of f and so is not an extremum of f .\r\n• If det(H) = 0, then we do not know the status of (a, b); it might be an extremum or it might\r\nnot be.\r\nDouble and Multiple Integrals\r\nLet f be defined on a rectangle R = [a, b] × [c, d], and for each m and n divide [a, b] (respectively\r\n[c, d]) into subintervals [xj, xj+1], i = 0, 1, . . . , m−1 (respectively [yi, yi+1]) of length ∆xj = (b−a)/m\r\n(respectively ∆yi = (d − c)/n) where x0 = a and xm = b (and y0 = c and yn = d ), and let x\r\n∗\r\nj\r\n(y\r\n∗\r\ni\r\n)\r\nbe any points chosen from their respective subintervals. Then the double integral of f over the\r\nrectangle R is\r\n\"\r\nR\r\nf(x, y) dA =\r\nZ\r\nd\r\nc\r\nZ\r\nb\r\na\r\nf(x, y) dxdy = lim\r\nm,n→∞\r\nXn\r\ni=1\r\nXm\r\nj=1\r\nf(x\r\n∗\r\nj\r\n, y\r\n∗\r\ni\r\n)∆xj∆yi, (E.6.7)\r\nprovided this limit exists. Multiple integrals are defined in the same way just with more letters and\r\nsums.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4827e743-a77f-42d2-af8f-96e2280b6025.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c396e55b4e44f6fe0c2bc0026813d21d05aeb89240a370df2c664475e8491f48",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 534
      },
      {
        "segments": [
          {
            "segment_id": "6a036235-34ca-4b0e-a918-bab7189d37ad",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 387,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "E.6. MULTIVARIABLE CALCULUS 371\r\nBivariate and Multivariate Change of Variables\r\nSuppose we have a transformation1 T that maps points (u, v) in a set A to points (x, y) in a set B. We\r\ntypically write x = x(u, v) and y = y(u, v), and we assume that x and y have continuous first-order\r\npartial derivatives. We say that T is one-to-one if no two distinct (u, v) pairs get mapped to the\r\nsame (x, y) pair; in this book, all of our multivariate transformations T are one-to-one.\r\nThe Jacobian (pronounced “yah-KOH-bee-uhn”) of T is denoted by ∂(x, y)/∂(u, v) and is de\u0002fined by the determinant of the following matrix of partial derivatives:\r\n∂(x, y)\r\n∂(u, v)\r\n=\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n∂x\r\n∂u\r\n∂x\r\n∂v\r\n∂y\r\n∂u\r\n∂y\r\n∂v\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n=\r\n∂x\r\n∂u\r\n∂y\r\n∂v\r\n−\r\n∂x\r\n∂v\r\n∂y\r\n∂u\r\n. (E.6.8)\r\nIf the function f is continuous on A and if the Jacobian of T is nonzero except perhaps on the\r\nboundary of A, then\r\n\"\r\nB\r\nf(x, y) dx dy =\r\n\"\r\nA\r\nf\r\n\u0002\r\nx(u, v), y(u, v)\r\n\u0003\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n∂(x, y)\r\n∂(u, v)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\ndu dv. (E.6.9)\r\nA multivariate change of variables is defined in an analogous way: the one-to-one transformation\r\nT maps points (u1, u2, . . . , un) to points (x1, x2, . . . , xn), the Jacobian is the determinant of the n × n\r\nmatrix of first-order partial derivatives of T (lined up in the natural manner), and instead of a double\r\nintegral we have a multiple integral over multidimensional sets A and B.\r\n1For our purposes T is in fact the inverse of a one-to-one transformation that we are initially given. We usually start\r\nwith functions that map (x, y) 7−→ (u, v), and one of our first tasks is to solve for the inverse transformation that maps\r\n(u, v) 7−→ (x, y). It is this inverse transformation which we are calling T.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/6a036235-34ca-4b0e-a918-bab7189d37ad.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=395e19268815163a924bae4f0320f773198ced33cbda23390a099812f8d861e4",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "b79273fe-4b1f-42b5-933f-a3dcbfcbdd58",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 388,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "372 APPENDIX E. MATHEMATICAL MACHINERY",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/b79273fe-4b1f-42b5-933f-a3dcbfcbdd58.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f2083091fb16c05e27ea438bb9b2497488baff1f61a112680351130d093a8cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 324
      },
      {
        "segments": [
          {
            "segment_id": "66175511-dd90-47f7-ad87-8325287418c2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 389,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix F\r\nWriting Reports with R\r\nPerhaps the most important part of a statistician’s job once the analysis is complete is to commu\u0002nicate the results to others. This is usually done with some type of report that is delivered to the\r\nclient, manager, or administrator. Other situations that call for reports include term papers, final\r\nprojects, thesis work, etc. This chapter is designed to pass along some tips about writing reports\r\nonce the work is completed with R.\r\nF.1 What to Write\r\nIt is possible to summarize this entire appendix with only one sentence: the statistician’s goal is\r\nto communicate with others. To this end, there are some general guidelines that I give to students\r\nwhich are based on an outline originally written and shared with me by Dr. G. Andy Chang.\r\nBasic Outline for a Statistical Report\r\n1. Executive Summary (a one page description of the study and conclusion)\r\n2. Introduction\r\n(a) What is the question, and why is it important?\r\n(b) Is the study observational or experimental?\r\n(c) What are the hypotheses of interest to the researcher?\r\n(d) What are the types of analyses employed? (one sample t-test, paired-sample t-test,\r\nANOVA, chi-square test, regression, . . . )\r\n3. Data Collection\r\n(a) Describe how the data were collected in detail.\r\n(b) Identify all variable types: quantitative, qualitative, ordered or nominal (with levels),\r\ndiscrete, continuous.\r\n(c) Discuss any limitations of the data collection procedure. Look carefully for any sources\r\nof bias.\r\n373",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/66175511-dd90-47f7-ad87-8325287418c2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4b6b2dd90e1822935c837e686a5a5f67020655beb13d21ed3bd61c417b9e0d0a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 243
      },
      {
        "segments": [
          {
            "segment_id": "3c4c0032-c1f3-4ee9-a682-24af42c4071c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 390,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "374 APPENDIX F. WRITING REPORTS WITH R\r\n4. Summary Information\r\n(a) Give numeric summaries of all variables of interest.\r\ni. Discrete: (relative) frequencies, contingency tables, odds ratios, etc.\r\nii. Continuous: measures of center, spread, shape.\r\n(b) Give visual summaries of all variables of interest.\r\ni. Side-by-side boxplots, scatterplots, histograms, etc.\r\n(c) Discuss any unusual features of the data (outliers, clusters, granularity, etc.)\r\n(d) Report any missing data and identify any potential problems or bias.\r\n5. Analysis\r\n(a) State any hypotheses employed, and check the assumptions.\r\n(b) Report test statistics, p-values, and confidence intervals.\r\n(c) Interpret the results in the context of the study.\r\n(d) Attach (labeled) tables and/or graphs and make reference to them in the report as\r\nneeded.\r\n6. Conclusion\r\n(a) Summarize the results of the study. What did you learn?\r\n(b) Discuss any limitations of the study or inferences.\r\n(c) Discuss avenues of future research suggested by the study.\r\nF.2 How to Write It with R\r\nOnce the decision has been made what to write, the next task is to typeset the information to be\r\nshared. To do this the author will need to select software to use to write the documents. There are\r\nmany options available, and choosing one over another is sometimes a matter of taste. But not all\r\nsoftware were created equal, and R plays better with some applications than it does with others.\r\nIn short, R does great with LATEX and there are many resources available to make writing a\r\ndocument with R and LATEX easier. But LATEX is not for the beginner, and there are other word\r\nprocessors which may be acceptable depending on the circumstances.\r\nF.2.1 Microsoftr Word\r\nIt is a fact of life that Microsoftr Windows is currently the most prevalent desktop operating system\r\non the planet. Those who own Windows also typically own some version of Microsoft Office, thus\r\nMicrosoft Word is the default word processor for many, many people.\r\nThe standard way to write an R report with Microsoftr Word is to generate material with R and\r\nthen copy-paste the material at selected places in a Word document. An advantage to this approach\r\nis that Word is nicely designed to make it easy to copy-and-paste from RGui to the Word document.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/3c4c0032-c1f3-4ee9-a682-24af42c4071c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3723fe4c4d8c9d23f059efd769049272450061e35195fbbb6aafe4a79f3b28bf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 372
      },
      {
        "segments": [
          {
            "segment_id": "caf3c4e9-9ec8-4d50-9430-3c945bda893a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 391,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "F.2. HOW TO WRITE IT WITH R 375\r\nA disadvantage to this approach is that the R input/output needs to be edited manually by the\r\nauthor to make it readable for others. Another disadvantage is that the approach does not work on\r\nall operating systems (not on Linux, in particular). Yet another disadvantage is that Microsoftr\r\nWord is proprietary, and as a result, R does not communicate with Microsoftr Word as well as it\r\ndoes with other software as we shall soon see.\r\nNevertheless, if you are going to write a report with Word there are some steps that you can\r\ntake to make the report more amenable to the reader.\r\n1. Copy and paste graphs into the document. You can do this by right clicking on the graph and\r\nselecting Copy as bitmap, or Copy as metafile, or one of the other options. Then move the\r\ncursor to the document where you want the picture, right-click, and select Paste.\r\n2. Resize (most) pictures so that they take up no more than 1/2 page. You may want to put\r\ngraphs side by side; do this by inserting a table and placing the graphs inside the cells.\r\n3. Copy selected R input and output to the Word document. All code should be separated from\r\nthe rest of the writing, except when specifically mentioning a function or object in a sentence.\r\n4. The font of R input/output should be Courier New, or some other monowidth font (not Times\r\nNew Roman or Calibri); the default font size of 12 is usually too big for R code and should\r\nbe reduced to, for example, 10pt.\r\nIt is also possible to communicate with R through OpenOffice.org, which can export to the propri\u0002etary (.doc) format.\r\nF.2.2 OpenOffice.org and odfWeave\r\nOpenOffice.org (OO.o) is an open source desktop productivity suite which mirrors Microsoftr Of\u0002fice. It is especially nice because it works on all operating systems. OO.o can read most document\r\nformats, and in particular, it will read .doc files. The standard OO.o file extension for documents\r\nis .odt, which stands for “open document text”.\r\nThe odfWeave package [55] provides a way to generate an .odt file with R input and output\r\ncode formatted correctly and inserted in the correct places, without any additional work. In this\r\nway, one does not need to worry about all of the trouble of typesetting R output. Another advantage\r\nof odfWeave is that it allows you to generate the report dynamically; if the data underlying the\r\nreport change or are updated, then a few clicks (or commands) will generate a brand new report.\r\nOne disadvantage is that the source .odt file is not easy to read, because it is difficult to visually\r\ndistinguish the noweb parts (where the R code is) from the non-noweb parts. This can be fixed by\r\nmanually changing the font of the noweb sections to, for instance, Courier font, size 10pt. But it\r\nis extra work. It would be nice if a program would discriminate between the two different sections\r\nand automatically typeset the respective parts in their correct fonts. This is one of the advantages\r\nto LYX.\r\nAnother advantage of OO.o is that even after you have generated the outfile, it is fully editable\r\njust like any other .odt document. If there are errors or formatting problems, they can be fixed at\r\nany time.\r\nHere are the basic steps to typeset a statistical report with OO.o.\r\n1. Write your report as an .odt document in OO.o just as you would any other document. Call\r\nthis document infile.odt, and make sure that it is saved in your working directory.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/caf3c4e9-9ec8-4d50-9430-3c945bda893a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0b0fb64b61c9f450cf915c2eb7f18812485fbe0f5731a99a0833939558cd300e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 600
      },
      {
        "segments": [
          {
            "segment_id": "caf3c4e9-9ec8-4d50-9430-3c945bda893a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 391,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "F.2. HOW TO WRITE IT WITH R 375\r\nA disadvantage to this approach is that the R input/output needs to be edited manually by the\r\nauthor to make it readable for others. Another disadvantage is that the approach does not work on\r\nall operating systems (not on Linux, in particular). Yet another disadvantage is that Microsoftr\r\nWord is proprietary, and as a result, R does not communicate with Microsoftr Word as well as it\r\ndoes with other software as we shall soon see.\r\nNevertheless, if you are going to write a report with Word there are some steps that you can\r\ntake to make the report more amenable to the reader.\r\n1. Copy and paste graphs into the document. You can do this by right clicking on the graph and\r\nselecting Copy as bitmap, or Copy as metafile, or one of the other options. Then move the\r\ncursor to the document where you want the picture, right-click, and select Paste.\r\n2. Resize (most) pictures so that they take up no more than 1/2 page. You may want to put\r\ngraphs side by side; do this by inserting a table and placing the graphs inside the cells.\r\n3. Copy selected R input and output to the Word document. All code should be separated from\r\nthe rest of the writing, except when specifically mentioning a function or object in a sentence.\r\n4. The font of R input/output should be Courier New, or some other monowidth font (not Times\r\nNew Roman or Calibri); the default font size of 12 is usually too big for R code and should\r\nbe reduced to, for example, 10pt.\r\nIt is also possible to communicate with R through OpenOffice.org, which can export to the propri\u0002etary (.doc) format.\r\nF.2.2 OpenOffice.org and odfWeave\r\nOpenOffice.org (OO.o) is an open source desktop productivity suite which mirrors Microsoftr Of\u0002fice. It is especially nice because it works on all operating systems. OO.o can read most document\r\nformats, and in particular, it will read .doc files. The standard OO.o file extension for documents\r\nis .odt, which stands for “open document text”.\r\nThe odfWeave package [55] provides a way to generate an .odt file with R input and output\r\ncode formatted correctly and inserted in the correct places, without any additional work. In this\r\nway, one does not need to worry about all of the trouble of typesetting R output. Another advantage\r\nof odfWeave is that it allows you to generate the report dynamically; if the data underlying the\r\nreport change or are updated, then a few clicks (or commands) will generate a brand new report.\r\nOne disadvantage is that the source .odt file is not easy to read, because it is difficult to visually\r\ndistinguish the noweb parts (where the R code is) from the non-noweb parts. This can be fixed by\r\nmanually changing the font of the noweb sections to, for instance, Courier font, size 10pt. But it\r\nis extra work. It would be nice if a program would discriminate between the two different sections\r\nand automatically typeset the respective parts in their correct fonts. This is one of the advantages\r\nto LYX.\r\nAnother advantage of OO.o is that even after you have generated the outfile, it is fully editable\r\njust like any other .odt document. If there are errors or formatting problems, they can be fixed at\r\nany time.\r\nHere are the basic steps to typeset a statistical report with OO.o.\r\n1. Write your report as an .odt document in OO.o just as you would any other document. Call\r\nthis document infile.odt, and make sure that it is saved in your working directory.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/caf3c4e9-9ec8-4d50-9430-3c945bda893a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0b0fb64b61c9f450cf915c2eb7f18812485fbe0f5731a99a0833939558cd300e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 600
      },
      {
        "segments": [
          {
            "segment_id": "05964938-e9c4-4a14-9080-9e93d363a07c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 392,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "376 APPENDIX F. WRITING REPORTS WITH R\r\n2. At the places you would like to insert R code in the document, write the code chunks in the\r\nfollowing format:\r\n<<>>=\r\nx <- rnorm(10)\r\nmean(x)\r\n@\r\nor write whatever code you want between the symbols <<>>= and @.\r\n3. Open R and type the following:\r\n> library(odfWeave)\r\n> odfWeave(file = \"infile.odt\", dest = \"outfile.odt\")\r\n4. The compiled (.odt) file, complete with all of the R output automatically inserted in the\r\ncorrect places, will now be the file outfile.odt located in the working directory. Open\r\noutfile.odt, examine it, modify it, and repeat if desired.\r\nThere are all sorts of extra things that can be done. For example, the R commands can be suppressed\r\nwith the tag <<echo = FALSE>>=, and the R output may be hidden with <<results = hide>>=.\r\nSee the odfWeave package documentation for details.\r\nF.2.3 Sweave and LATEX\r\nThis approach is nice because it works for all operating systems. One can quite literally typeset\r\nanything with LATEX. All of this power comes at a price, however. The writer must learn the LATEX\r\nlanguage which is a nontrivial enterprise. Even given the language, if there is a single syntax error,\r\nor a single delimeter missing in the entire document, then the whole thing breaks.\r\nLATEX can do anything, but it is relatively difficult to learn and very grumpy about syntax errors\r\nand delimiter matching. there are however programs useful for formatting LATEX.\r\nA disadvantage is that you cannot see the mathematical formulas until you run the whole file\r\nwith LATEX.\r\nA disadvantage is that figures and tables are relatively difficult.\r\nThere are programs to make the process easier AUCTEX\r\ndev.copy2eps, also dev.copy2pdf\r\nhttp://www.stat.uni-muenchen.de/~leisch/Sweave/\r\nF.2.4 Sweave and LYX\r\nThis approach is nice because it works for all operating systems. It gives you everything from the\r\nlast section and makes it easier to use LATEX. That being said, it is better to know LATEX already\r\nwhen migrating to LYX, because you understand all of the machinery going on under the hood.\r\nProgram Listings and the R language\r\nThis book was written with LYX.\r\nhttp://gregor.gorjanc.googlepages.com/lyx-sweave",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/05964938-e9c4-4a14-9080-9e93d363a07c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0517356cde7a4867899113736c236e8a89fca5d4bdac935705023f34e35963d1",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "20bacdc0-d512-443c-aac5-643ebb84bb1c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 393,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "F.3. FORMATTING TABLES 377\r\nF.3 Formatting Tables\r\nThe prettyR package\r\nthe Hmisc package\r\n> library(Hmisc)\r\n> summary(cbind(Sepal.Length, Sepal.Width) ~ Species, data = iris)\r\ncbind(Sepal.Length, Sepal.Width) N=150\r\n+-------+----------+---+------------+-----------+\r\n| | |N |Sepal.Length|Sepal.Width|\r\n+-------+----------+---+------------+-----------+\r\n|Species|setosa | 50|5.006000 |3.428000 |\r\n| |versicolor| 50|5.936000 |2.770000 |\r\n| |virginica | 50|6.588000 |2.974000 |\r\n+-------+----------+---+------------+-----------+\r\n|Overall| |150|5.843333 |3.057333 |\r\n+-------+----------+---+------------+-----------+\r\nThere is a method argument to summary, which is set to method = \"response\" by default.\r\nThere are two other methods for summarizing data: reverse and cross. See ?summary.formula or\r\nthe following document from Frank Harrell for more details http://biostat.mc.vanderbilt.\r\nedu/twiki/bin/view/Main/StatReport.\r\nF.4 Other Formats\r\nHTML and prettyR\r\nR2HTML",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/20bacdc0-d512-443c-aac5-643ebb84bb1c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=42d14dd3f13189d01dec0bad2f9a815a009671b55596718d869f82bdf07c9d52",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "14452f46-b32a-4118-b64b-028d4a1c645d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 394,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "378 APPENDIX F. WRITING REPORTS WITH R",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/14452f46-b32a-4118-b64b-028d4a1c645d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=334b2a86b3bc6b3eded2b3c9a91a44593f9e07744570d22565d8b688c8c341b3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 461
      },
      {
        "segments": [
          {
            "segment_id": "c9c19146-b4b0-4ec7-8179-a0913e173d28",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 395,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix G\r\nInstructions for Instructors\r\nWARNING: this appendix is not applicable until the exercises have been written.\r\nProbably this book could more accurately be described as software. The reason is that the\r\ndocument is one big random variable, one observation realized out of millions. It is electronically\r\ndistributed under the GNU FDL, and “free” in both senses: speech and beer.\r\nThere are four components to IPSUR: the Document, the Program used to generate it, the R\r\npackage that holds the Program, and the Ancillaries that accompany it.\r\nThe majority of the data and exercises have been designed to be randomly generated. Different\r\nrealizations of this book will have different graphs and exercises throughout. The advantage of this\r\napproach is that a teacher, say, can generate a unique version to be used in his/her class. Students\r\ncan do the exercises and the teacher will have the answers to all of the problems in their own,\r\nunique solutions manual. Students may download a different solutions manual online somewhere\r\nelse, but none of the answers will match the teacher’s copy.\r\nThen next semester, the teacher can generate a new book and the problems will be more or less\r\nidentical, except the numbers will be changed. This means that students from different sections of\r\nthe same class will not be able to copy from one another quite so easily. The same will be true for\r\nsimilar classes at different institutions. Indeed, as long as the instructor protects his/her key used\r\nto generate the book, it will be difficult for students to crack the code. And if they are industrious\r\nenough at this level to find a way to (a) download and decipher my version’s source code, (b)\r\nhack the teacher’s password somehow, and (c) generate the teacher’s book with all of the answers,\r\nthen they probably should be testing out of an “Introduction to Probability and Statistics” course,\r\nanyway.\r\nThe book that you are reading was created with a random seed which was set at the beginning.\r\nThe original seed is 42. You can choose your own seed, and generate a new book with brand new\r\ndata for the text and exercises, complete with updated manuals. A method I recommend for finding\r\na seed is to look down at your watch at this very moment and record the 6 digit hour, minute, and\r\nsecond (say, 9:52:59am): choose that for a seed1. This method already provides for over 43,000\r\nbooks, without taking military time into account. An alternative would be to go to R and type\r\n> options(digits = 16)\r\n> runif(1)\r\n1\r\nIn fact, this is essentially the method used by R to select an initial random seed (see ?set.seed). However, the instructor\r\nshould set the seed manually so that the book can be regenerated at a later time, if necessary.\r\n379",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c9c19146-b4b0-4ec7-8179-a0913e173d28.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=242ccca7b36f63c941025f23c67a31bac1569d672124fdd5b3b6faffdcaabfe4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 468
      },
      {
        "segments": [
          {
            "segment_id": "24ebbcd9-20e8-405b-9e19-685cbcd3e571",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 396,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "380 APPENDIX G. INSTRUCTIONS FOR INSTRUCTORS\r\n[1] 0.2170129411388189\r\nNow choose 2170129411388188 as your secret seed. . . write it down in a safe place and do not\r\nshare it with anyone. Next generate the book with your seed using LYX-Sweave or Sweave-LATEX.\r\nYou may wish to also generate Student and Instructor Solution Manuals. Guidance regarding this\r\nis given below in the How to Use This Document section.\r\nG.1 Generating This Document\r\nYou will need three (3) things to generate this document for yourself, in addition to a current\r\nR distribution which at the time of this writing is R version 2.12.2 Patched (2011-03-18\r\nr54866):\r\n1. a LATEX distribution,\r\n2. Sweave (which comes with R automatically), and\r\n3. LYX (optional, but recommended).\r\nWe will discuss each of these in turn.\r\nLATEX: The distribution used by the present author was TEX Live (http://www.tug.org/texlive/).\r\nThere are plenty of other perfectly suitable LATEX distributions depending on your operating\r\nsystem, one such alternative being MikTEX (http://miktex.org/) for Microsoft Win\u0002dows.\r\nSweave: If you have R installed, then the required Sweave files are already on your system. . .\r\nsomewhere. The only problems that you may have are likely associated with making sure\r\nthat your LATEX distribution knows where to find the Sweave.sty file. See the Sweave\r\nHomepage (http://www.statistik.lmu.de/~leisch/Sweave/) for guidance on how\r\nto get it working on your particular operating system.\r\nLYX: Strictly speaking, LYX is not needed to generate this document. But this document was\r\nwritten stem to stern with LYX, taking full advantage of all of the bells and whistles that\r\nLYX has to offer over plain LATEX editors. And it’s free. See the LYX homepage (http:\r\n//www.lyx.org/) for additional information.\r\nIf you decide to give LYX a try, then you will need to complete some extra steps to coordinate\r\nSweave and LYX with each other. Luckily, Gregor Gorjanc has a website and an R News\r\narticle [36] to help you do exactly that. See the LYX-Sweave homepage (http://gregor.\r\ngorjanc.googlepages.com/lyx-sweave) for details.\r\nAn attempt was made to not be extravagant with fonts or packages so that a person would not need\r\nthe entire CTAN (or CRAN) installed on their personal computer to generate the book. Nevertheless,\r\nthere are a few extra packages required. These packages are listed in the preamble of IPSUR.Rnw,\r\nIPSUR.tex, and IPSUR.lyx.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/24ebbcd9-20e8-405b-9e19-685cbcd3e571.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=61ebab9e5a2f65ec8fc90cc122438512b26254f01f781d10089cb4ab55f7fb8d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 381
      },
      {
        "segments": [
          {
            "segment_id": "5acc4a34-2eac-4fe9-a1b4-171b8e2b8f47",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 397,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "G.2. HOW TO USE THIS DOCUMENT 381\r\nG.2 How to Use This Document\r\nThe easiest way to use this document is to install the IPSUR package from CRAN and be all done.\r\nThis way would be acceptable if there is another, primary, text being used for the course and IPSUR\r\nis only meant to play a supplementary role.\r\nIf you plan for IPSUR to serve as the primary text for your course, then it would be wise to\r\ngenerate your own version of the document. You will need the source code for the Program which\r\ncan be downloaded from CRAN or the IPSUR website. Once the source is obtained there are four (4)\r\nbasic steps to generating your own copy.\r\n1. Randomly select a secret “seed” of integers and replace my seed of 42 with your own seed.\r\n2. Make sure that the maintext branch is turned ON and also make sure that both the solutions\r\nbranch and the answers branch are turned OFF. Use LYX or your LATEX editor with Sweave\r\nto generate your unique PDF copy of the book and distribute this copy to your students. (See\r\nthe LYX User’s Guide to learn more about branches; the ones referenced above can be found\r\nunder Document . Settings . Branches.)\r\n3. Turn the maintext branch2 OFF and the solutions branch ON. Generate a “Student Solu\u0002tions Manual” which has complete solutions to selected exercises and distribute the PDF to\r\nthe students.\r\n4. Leave the solutions branch ON and also turn the answers branch ON and generate an\r\n“Instructor Solutions and Answers Manual” with full solutions to some of the exercises and\r\njust answers to the remaining exercises. Do NOT distribute this to the students – unless of\r\ncourse you want them to have the answers to all of the problems.\r\nTo make it easier for those people who do not want to use LYX (or for whatever reason cannot get it\r\nworking), I have included three (3) Sweave files corresponding to the main text, student solutions,\r\nand instructor answers, that are included in the IPSUR source package in the /tex subdirectory. In\r\nprinciple it is possible to change the seed and generate the three parts separately with only Sweave\r\nand LATEX. This method is not recommended by me, but is perhaps desirable for some people.\r\nGenerating Quizzes and Exams\r\n• you can copy paste selected exercises from the text, put them together, and you have a\r\nquiz. Since the numbers are randomly generated you do not need to worry about different\r\nsemesters. And you will have answer keys already for all of your QUIZZES and EXAMS,\r\ntoo.\r\nG.3 Ancillary Materials\r\nIn addition to the main text, student manual, and instructor manual, there is IPSUR.R which is all\r\nof the parsed R code used in the document.\r\n2You can leave the maintext branch ON when generating the solutions manuals, but (1) all of the page numbers will be\r\ndifferent, and (2) the typeset solutions will generate and take up a lot of space between exercises.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/5acc4a34-2eac-4fe9-a1b4-171b8e2b8f47.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=26e8f560119c797122bca66ecf31c4dbbe3c193d5631faf1469b68359affa694",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 504
      },
      {
        "segments": [
          {
            "segment_id": "656dae49-21e5-4c86-82a3-509f03e00fe1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 398,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "382 APPENDIX G. INSTRUCTIONS FOR INSTRUCTORS\r\nG.4 Modifying This Document\r\nSince this document is released under the GNU-FDL, you are free to modify this document however\r\nyou wish (in accordance with the license – see Appendix B). The immediate benefit of this is that\r\nyou can generate the book, with brand new problem sets, and distribute it to your students simply\r\nas a PDF (in an email, for instance). As long as you distribute less than 100 such Opaque copies,\r\nyou are not even required by the GNU-FDL to share your Transparent copy (the source code with\r\nthe secret key) that you used to generate them. Next semester, choose a new key and generate a\r\nnew copy to be distributed to the new class.\r\nBut more generally, if you are not keen on the way I explained (or failed to explain)\r\nsomething, then you are free to rewrite it. If you would like to cover more (or less)\r\nmaterial, then you are free to add (or delete) whatever Chapters/Sections/Paragraphs\r\nthat you wish. And since you have the source code, you do not need to retype the\r\nwheel.\r\nSome individuals will argue that the nature of a statistics textbook like this one, many of the exer\u0002cises being randomly generated by design, does a disservice to the students because the exercises\r\ndo not use real-world data. That is a valid criticism. . . but in my case the benefits outweighed\r\nthe detriments and I moved forward to incorporate static data sets whenever it was feasible and\r\neffective. Frankly, and most humbly, the only response I have for those individuals is: “Please refer\r\nto the preceding paragraph.”",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/656dae49-21e5-4c86-82a3-509f03e00fe1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6dc5c4a01a89a37a34ce685d153a1b537c5fa85624bcf04ae98a4f8daf6c5a5c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 274
      },
      {
        "segments": [
          {
            "segment_id": "ba96c27c-43a1-46a8-9b5d-360489ddfb25",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 399,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Appendix H\r\nRcmdrTestDrive Story\r\nThe goal of RcmdrTestDrive was to have a data set sufficiently rich in the types of data represented\r\nsuch that a person could load it into the R Commander and be able to explore all of Rcmdr’s menu\r\noptions at once. I decided early-on that an efficient way to do this would be to generate the data\r\nset randomly, and later add to the list of variables as more Rcmdr menu options became available.\r\nGenerating the data was easy, but generating a story that related all of the respective variables\r\nproved to be less so.\r\nIn the Summer of 2006 I gave a version of the raw data and variable names to my STAT 3743\r\nProbability and Statistics class and invited each of them to write a short story linking all of the\r\nvariables together in a coherent narrative. No further direction was given.\r\nThe most colorful of those I received was written by Jeffery Cornfield, submitted July 12, 2006,\r\nand is included below with his permission. It was edited slightly by the present author and updated\r\nto respond dynamically to the random generation of RcmdrTestDrive; otherwise, the story has\r\nbeen unchanged.\r\nCase File: ALU-179 “Murder Madness in Toon Town”\r\n***WARNING***\r\n***This file is not for the faint of heart, dear reader, because it is filled with horrible\r\nimages that will haunt your nightmares. If you are weak of stomach, have irritable\r\nbowel syndrome, or are simply paranoid, DO NOT READ FURTHER! Otherwise,\r\nread at your own risk.***\r\nOne fine sunny day, Police Chief R. Runner called up the forensics department at Acme-Looney\r\nUniversity. There had been 166 murders in the past 7 days, approximately one murder every hour,\r\nof many of the local Human workers, shop keepers, and residents of Toon Town. These alarming\r\nrates threatened to destroy the fragile balance of Toon and Human camaraderie that had developed\r\nin Toon Town.\r\nProfessor Twee T. Bird, a world-renowned forensics specialist and a Czechoslovakian native,\r\nreceived the call. “Professor, we need your expertise in this field to identify the pattern of the killer\r\n383",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ba96c27c-43a1-46a8-9b5d-360489ddfb25.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1e5c11d5643335e4fbba9b7c5a0b9741836231165d1629978b37df72f8bd67b3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 348
      },
      {
        "segments": [
          {
            "segment_id": "4e7b859b-2dd5-4841-82f4-42cfc11e0b0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 400,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "384 APPENDIX H. RCMDRTESTDRIVE STORY\r\nor killers,” Chief Runner exclaimed. “We need to establish a link between these people to stop this\r\nmassacre.”\r\n“Yes, Chief Runner, please give me the details of the case,” Professor Bird declared with a\r\nheavy native accent, (though, for the sake of the case file, reader, I have decided to leave out the\r\naccent due to the fact that it would obviously drive you – if you will forgive the pun – looney!)\r\n“All prints are wiped clean and there are no identifiable marks on the bodies of the victims. All\r\nwe are able to come up with is the possibility that perhaps there is some kind of alternative method\r\nof which we are unaware. We have sent a secure e-mail with a listing of all of the victims’ races,\r\ngenders, locations of the bodies, and the sequential order in which they were killed. We have also\r\nincluded other information that might be helpful,” said Chief Runner.\r\n“Thank you very much. Perhaps I will contact my colleague in the Statistics Department here,\r\nDr. Elmer Fudd-Einstein,” exclaimed Professor Bird. “He might be able to identify a pattern of\r\nattack with mathematics and statistics.”\r\n“Good luck trying to find him, Professor. Last I heard, he had a bottle of scotch and was in\r\nthe Hundred Acre Woods hunting rabbits,” Chief Runner declared in a manner that questioned the\r\nbeloved doctor’s credibility.\r\n“Perhaps I will take a drive to find him. The fresh air will do me good.”\r\n***I will skip ahead, dear reader, for much occurred during this time. Needless to\r\nsay, after a fierce battle with a mountain cat that the Toon-ology Department tagged\r\nearlier in the year as “Sylvester,” Professor Bird found Dr. Fudd-Einstein and brought\r\nhim back, with much bribery of alcohol and the promise of the future slaying of those\r\n“wascally wabbits” (it would help to explain that Dr. Fudd-Einstein had a speech im\u0002pediment which was only worsened during the consumption of alcohol.)***\r\nOnce our two heroes returned to the beautiful Acme-Looney University, and once Dr. Fudd-Einstein\r\nbecame sober and coherent, they set off to examine the case and begin solving these mysterious\r\nmurders.\r\n“First off,” Dr. Fudd-Einstein explained, “these people all worked at the University at some\r\npoint or another. Also, there also seems to be a trend in the fact that they all had a salary between\r\n$12 and $21 when they retired.”\r\n“That’s not really a lot to live off of,” explained Professor Bird.\r\n“Yes, but you forget that the Looney Currency System works differently than the rest of the\r\nAmerican Currency System. One Looney is equivalent to Ten American Dollars. Also, these\r\nfaculty members are the ones who faced a cut in their salary, as denoted by ‘reduction’. Some of\r\nthem dropped quite substantially when the University had to fix that little faux pas in the Chemistry\r\nDepartment. You remember: when Dr. D. Duck tried to create that ‘Everlasting Elixir?’ As a\r\nresult, these faculty left the university. Speaking of which, when is his memorial service?” inquired\r\nDr. Fudd-Einstein.\r\n“This coming Monday. But if there were all of these killings, how in the world could one person\r\ndo it? It just doesn’t seem to be possible; stay up 7 days straight and be able to kill all of these\r\npeople and have the energy to continue on,” Professor Bird exclaimed, doubting the guilt of only\r\none person.\r\n“Perhaps then, it was a group of people, perhaps there was more than one killer placed through\u0002out Toon Town to commit these crimes. If I feed in these variables, along with any others that\r\nmight have a pattern, the Acme Computer will give us an accurate reading of suspects, with a\r\nscant probability of error. As you know, the Acme Computer was developed entirely in house",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4e7b859b-2dd5-4841-82f4-42cfc11e0b0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=45b66efad0055b6f8401f068a461ee05598276dd584cfe5c9ade08fbc02e7b17",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 634
      },
      {
        "segments": [
          {
            "segment_id": "4e7b859b-2dd5-4841-82f4-42cfc11e0b0d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 400,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "384 APPENDIX H. RCMDRTESTDRIVE STORY\r\nor killers,” Chief Runner exclaimed. “We need to establish a link between these people to stop this\r\nmassacre.”\r\n“Yes, Chief Runner, please give me the details of the case,” Professor Bird declared with a\r\nheavy native accent, (though, for the sake of the case file, reader, I have decided to leave out the\r\naccent due to the fact that it would obviously drive you – if you will forgive the pun – looney!)\r\n“All prints are wiped clean and there are no identifiable marks on the bodies of the victims. All\r\nwe are able to come up with is the possibility that perhaps there is some kind of alternative method\r\nof which we are unaware. We have sent a secure e-mail with a listing of all of the victims’ races,\r\ngenders, locations of the bodies, and the sequential order in which they were killed. We have also\r\nincluded other information that might be helpful,” said Chief Runner.\r\n“Thank you very much. Perhaps I will contact my colleague in the Statistics Department here,\r\nDr. Elmer Fudd-Einstein,” exclaimed Professor Bird. “He might be able to identify a pattern of\r\nattack with mathematics and statistics.”\r\n“Good luck trying to find him, Professor. Last I heard, he had a bottle of scotch and was in\r\nthe Hundred Acre Woods hunting rabbits,” Chief Runner declared in a manner that questioned the\r\nbeloved doctor’s credibility.\r\n“Perhaps I will take a drive to find him. The fresh air will do me good.”\r\n***I will skip ahead, dear reader, for much occurred during this time. Needless to\r\nsay, after a fierce battle with a mountain cat that the Toon-ology Department tagged\r\nearlier in the year as “Sylvester,” Professor Bird found Dr. Fudd-Einstein and brought\r\nhim back, with much bribery of alcohol and the promise of the future slaying of those\r\n“wascally wabbits” (it would help to explain that Dr. Fudd-Einstein had a speech im\u0002pediment which was only worsened during the consumption of alcohol.)***\r\nOnce our two heroes returned to the beautiful Acme-Looney University, and once Dr. Fudd-Einstein\r\nbecame sober and coherent, they set off to examine the case and begin solving these mysterious\r\nmurders.\r\n“First off,” Dr. Fudd-Einstein explained, “these people all worked at the University at some\r\npoint or another. Also, there also seems to be a trend in the fact that they all had a salary between\r\n$12 and $21 when they retired.”\r\n“That’s not really a lot to live off of,” explained Professor Bird.\r\n“Yes, but you forget that the Looney Currency System works differently than the rest of the\r\nAmerican Currency System. One Looney is equivalent to Ten American Dollars. Also, these\r\nfaculty members are the ones who faced a cut in their salary, as denoted by ‘reduction’. Some of\r\nthem dropped quite substantially when the University had to fix that little faux pas in the Chemistry\r\nDepartment. You remember: when Dr. D. Duck tried to create that ‘Everlasting Elixir?’ As a\r\nresult, these faculty left the university. Speaking of which, when is his memorial service?” inquired\r\nDr. Fudd-Einstein.\r\n“This coming Monday. But if there were all of these killings, how in the world could one person\r\ndo it? It just doesn’t seem to be possible; stay up 7 days straight and be able to kill all of these\r\npeople and have the energy to continue on,” Professor Bird exclaimed, doubting the guilt of only\r\none person.\r\n“Perhaps then, it was a group of people, perhaps there was more than one killer placed through\u0002out Toon Town to commit these crimes. If I feed in these variables, along with any others that\r\nmight have a pattern, the Acme Computer will give us an accurate reading of suspects, with a\r\nscant probability of error. As you know, the Acme Computer was developed entirely in house",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/4e7b859b-2dd5-4841-82f4-42cfc11e0b0d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=45b66efad0055b6f8401f068a461ee05598276dd584cfe5c9ade08fbc02e7b17",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 634
      },
      {
        "segments": [
          {
            "segment_id": "65e2460f-66e5-4bc4-887a-542cf1f8f2fe",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 401,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "385\r\nhere at Acme-Looney University,” Dr. Fudd-Einstein said as he began feeding the numbers into the\r\nmassive server.\r\n“Hey, look at this,” Professor Bird exclaimed, “What’s with this before/after information?”\r\n“Scroll down; it shows it as a note from the coroner’s office. Apparently Toon Town Coroner\r\nMarvin – that strange fellow from Mars, Pennsylvania – feels, in his opinion, that given the fact\r\nthat the cadavers were either smokers or non-smokers, and given their personal health, and family\r\nmedical history, that this was their life expectancy before contact with cigarettes or second-hand\r\nsmoke and after,” Dr. Fudd-Einstein declared matter-of-factly.\r\n“Well, would race or gender have something to do with it, Elmer?” inquired Professor Bird.\r\n“Maybe, but I would bet my money on somebody was trying to quiet these faculty before they\r\nmade a big ruckus about the secret money-laundering of Old Man Acme. You know, most people\r\nthink that is how the University receives most of its funds, through the mob families out of Chicago.\r\nAnd I would be willing to bet that these faculty figured out the connection and were ready to tell\r\nthe Looney Police.” Dr. Fudd-Einstein spoke lower, fearing that somebody would overhear their\r\nconversation.\r\nDr. Fudd-Einstein then pressed Enter on the keyboard and waited for the results. The massive\r\ncomputer roared to life. . . and when I say roared, I mean it literally roared. All the hidden bells,\r\nwhistles, and alarm clocks in its secret compartments came out and created such a loud racket that\r\nclasses across the university had to come to a stand-still until it finished computing.\r\nOnce it was completed, the computer listed 4 names:\r\n***********************SUSPECTS********************************\r\nYosemite Sam (“Looney” Insane Asylum)\r\nWile E. Coyote (deceased)\r\nFoghorn Leghorn (whereabouts unknown)\r\nGranny (1313 Mockingbird Lane, Toon Town USA)\r\nDr. Fudd-Einstein and Professor Bird looked on in silence. They could not believe their eyes. The\r\ngreatest computer on the Gulf of Mexico seaboard just released the most obscure results imagin\u0002able.\r\n“There seems to be a mistake. Perhaps something is off,” Professor Bird asked, still unable to\r\nbelieve the results.\r\n“Not possible; the Acme Computer takes into account every kind of connection available. It\r\nconsiders affiliations to groups, and affiliations those groups have to other groups. It checks the\r\nFBI, CIA, British intelligence, NAACP, AARP, NSA, JAG, TWA, EPA, FDA, USWA, R, MAPLE,\r\nSPSS, SAS, and Ben & Jerry’s files to identify possible links, creating the most powerful computer\r\nin the world. . . with a tweak of Toon fanaticism,” Dr. Fudd-Einstein proclaimed, being a proud\r\nco-founder of the Acme Computer Technology.\r\n“Wait a minute, Ben & Jerry? What would eating ice cream have to do with anything?” Profes\u0002sor Bird inquired.\r\n“It is in the works now, but a few of my fellow statistician colleagues are trying to find a\r\nmathematical model to link the type of ice cream consumed to the type of person they might\r\nbecome. Assassins always ate vanilla with chocolate sprinkles, a little known fact they would tell\r\nyou about Oswald and Booth,” Dr. Fudd-Einstein declared.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/65e2460f-66e5-4bc4-887a-542cf1f8f2fe.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=07144184f34698a067281bda79503084d92bdc53d99de2f2f71b2e8868cade0b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 504
      },
      {
        "segments": [
          {
            "segment_id": "0ca62988-3c3e-4d16-bb9a-d14ce107ab24",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 402,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "386 APPENDIX H. RCMDRTESTDRIVE STORY\r\n“I’ve heard about this. My forensics graduate students are trying to identify car thieves with ei\u0002ther rocky road or mint chocolate chip. . . so far, the pattern is showing a clear trend with chocolate\r\nchip,” Professor Bird declared.\r\n“Well, what do we know about these suspects, Twee?” Dr. Fudd-Einstein asked.\r\n“Yosemite Sam was locked up after trying to rob that bank in the West Borough. Apparently\r\nhis guns were switched and he was sent the Acme Kids Joke Gun and they blew up in his face. The\r\ncontainers of peroxide they contained turned all of his facial hair red. Some little child is running\r\naround Toon Town with a pair of .38’s to this day.\r\n“Wile E. Coyote was that psychopath working for the Yahtzee - the fanatics who believed that\r\nToons were superior to Humans. He strapped sticks of Acme Dynamite to his chest to be a martyr\r\nfor the cause, but before he got to the middle of Toon Town, this defective TNT blew him up. Not\r\na single other person – Toon or Human – was even close.\r\n“Foghorn Leghorn is the most infamous Dog Kidnapper of all times. He goes to the homes\r\nof prominent Dog citizens and holds one of their relatives for ransom. If they refuse to pay, he\r\nsends them to the pound. Either way, they’re sure stuck in the dog house,” Professor Bird laughed.\r\nDr. Fudd-Einstein didn’t seem amused, so Professor Bird continued.\r\n“Granny is the most beloved alumnus of Acme-Looney University. She was in the first graduat\u0002ing class and gives graciously each year to the university. Without her continued financial support,\r\nwe wouldn’t have the jobs we do. She worked as a parking attendant at the University lots. . . wait\r\na minute, take a look at this,” Professor Bird said as he scrolled down in the police information.\r\n“Granny’s signature is on each of these faculty members’ parking tickets. Kind of odd, consider\u0002ing the Chief-of-Parking signed each personally. The deceased had from as few as 1 ticket to as\r\nmany as 18. All tickets were unpaid.\r\n“And look at this, Granny married Old Man Acme after graduation. He was a resident of\r\nChicago and rumored to be a consigliere to one of the most prominent crime families in Chicago,\r\nthe Chuck Jones/Warner Crime Family,” Professor Bird read from the screen as a cold feeling of\r\nterror rose from the pit of his stomach.\r\n“Say, don’t you live at her house? Wow, you’re living under the same roof as one of the greatest\r\ncriminals/murderers of all time!” Dr. Fudd-Einstein said in awe and sarcasm.\r\n“I would never have suspected her, but I guess it makes sense. She is older, so she doesn’t need\r\nnear the amount of sleep as a younger person. She has access to all of the vehicles so she can copy\r\nlicense plate numbers and follow them to their houses. She has the finances to pay for this kind\r\nof massive campaign on behalf of the Mob, and she hates anyone that even remotely smells like\r\nsmoke,” Professor Bird explained, wishing to have his hit of nicotine at this time.\r\n“Well, I guess there is nothing left to do but to call Police Chief Runner and have him arrest\r\nher,” Dr. Fudd-Einstein explained as he began dialing. “What I can’t understand is how in the world\r\nthe Police Chief sent me all of this information and somehow seemed to screw it up.”\r\n“What do you mean?” inquired Professor Bird.\r\n“Well, look here. The data file from the Chief’s email shows 168 murders, but there have only\r\nbeen 166. This doesn’t make any sense. I’ll have to straighten it out. Hey, wait a minute. Look at\r\nthis, Person #167 and Person #168 seem to match our stats. But how can that be?”\r\nIt was at this moment that our two heroes were shot from behind and fell over the computer,\r\ndead. The killer hit Delete on the computer and walked out slowly (considering they had arthritis)\r\nand cackling loudly in the now quiet computer lab.\r\nAnd so, I guess my question to you the reader is, did Granny murder 168 people, or did the\r\nmurderer slip through the cracks of justice? You be the statistician and come to your own conclu-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0ca62988-3c3e-4d16-bb9a-d14ce107ab24.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=13662842606b06d3770296a0a72734d68476d419dc2d6fe4593c6f950c44300c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 718
      },
      {
        "segments": [
          {
            "segment_id": "0ca62988-3c3e-4d16-bb9a-d14ce107ab24",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 402,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "386 APPENDIX H. RCMDRTESTDRIVE STORY\r\n“I’ve heard about this. My forensics graduate students are trying to identify car thieves with ei\u0002ther rocky road or mint chocolate chip. . . so far, the pattern is showing a clear trend with chocolate\r\nchip,” Professor Bird declared.\r\n“Well, what do we know about these suspects, Twee?” Dr. Fudd-Einstein asked.\r\n“Yosemite Sam was locked up after trying to rob that bank in the West Borough. Apparently\r\nhis guns were switched and he was sent the Acme Kids Joke Gun and they blew up in his face. The\r\ncontainers of peroxide they contained turned all of his facial hair red. Some little child is running\r\naround Toon Town with a pair of .38’s to this day.\r\n“Wile E. Coyote was that psychopath working for the Yahtzee - the fanatics who believed that\r\nToons were superior to Humans. He strapped sticks of Acme Dynamite to his chest to be a martyr\r\nfor the cause, but before he got to the middle of Toon Town, this defective TNT blew him up. Not\r\na single other person – Toon or Human – was even close.\r\n“Foghorn Leghorn is the most infamous Dog Kidnapper of all times. He goes to the homes\r\nof prominent Dog citizens and holds one of their relatives for ransom. If they refuse to pay, he\r\nsends them to the pound. Either way, they’re sure stuck in the dog house,” Professor Bird laughed.\r\nDr. Fudd-Einstein didn’t seem amused, so Professor Bird continued.\r\n“Granny is the most beloved alumnus of Acme-Looney University. She was in the first graduat\u0002ing class and gives graciously each year to the university. Without her continued financial support,\r\nwe wouldn’t have the jobs we do. She worked as a parking attendant at the University lots. . . wait\r\na minute, take a look at this,” Professor Bird said as he scrolled down in the police information.\r\n“Granny’s signature is on each of these faculty members’ parking tickets. Kind of odd, consider\u0002ing the Chief-of-Parking signed each personally. The deceased had from as few as 1 ticket to as\r\nmany as 18. All tickets were unpaid.\r\n“And look at this, Granny married Old Man Acme after graduation. He was a resident of\r\nChicago and rumored to be a consigliere to one of the most prominent crime families in Chicago,\r\nthe Chuck Jones/Warner Crime Family,” Professor Bird read from the screen as a cold feeling of\r\nterror rose from the pit of his stomach.\r\n“Say, don’t you live at her house? Wow, you’re living under the same roof as one of the greatest\r\ncriminals/murderers of all time!” Dr. Fudd-Einstein said in awe and sarcasm.\r\n“I would never have suspected her, but I guess it makes sense. She is older, so she doesn’t need\r\nnear the amount of sleep as a younger person. She has access to all of the vehicles so she can copy\r\nlicense plate numbers and follow them to their houses. She has the finances to pay for this kind\r\nof massive campaign on behalf of the Mob, and she hates anyone that even remotely smells like\r\nsmoke,” Professor Bird explained, wishing to have his hit of nicotine at this time.\r\n“Well, I guess there is nothing left to do but to call Police Chief Runner and have him arrest\r\nher,” Dr. Fudd-Einstein explained as he began dialing. “What I can’t understand is how in the world\r\nthe Police Chief sent me all of this information and somehow seemed to screw it up.”\r\n“What do you mean?” inquired Professor Bird.\r\n“Well, look here. The data file from the Chief’s email shows 168 murders, but there have only\r\nbeen 166. This doesn’t make any sense. I’ll have to straighten it out. Hey, wait a minute. Look at\r\nthis, Person #167 and Person #168 seem to match our stats. But how can that be?”\r\nIt was at this moment that our two heroes were shot from behind and fell over the computer,\r\ndead. The killer hit Delete on the computer and walked out slowly (considering they had arthritis)\r\nand cackling loudly in the now quiet computer lab.\r\nAnd so, I guess my question to you the reader is, did Granny murder 168 people, or did the\r\nmurderer slip through the cracks of justice? You be the statistician and come to your own conclu-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0ca62988-3c3e-4d16-bb9a-d14ce107ab24.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=13662842606b06d3770296a0a72734d68476d419dc2d6fe4593c6f950c44300c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 718
      },
      {
        "segments": [
          {
            "segment_id": "cf481d9a-f0e4-454d-9f93-3dbd8d1d5a34",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 403,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "387\r\nsion.\r\nDetective Pyork E. Pig\r\n***End File***",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/cf481d9a-f0e4-454d-9f93-3dbd8d1d5a34.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7521de02acc5805c133ee90ae4b7b297091190fd350f7f72527927c81b7223b2",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "90544c55-f2cc-4042-ba0f-5f3a91637310",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 404,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "388 APPENDIX H. RCMDRTESTDRIVE STORY",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/90544c55-f2cc-4042-ba0f-5f3a91637310.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ad2605f7fffc3fd0b006eb912175bcaad915c92cfd6c36c954c0bd2bd741d3c1",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "a6619f76-b73a-47f1-a9a2-6a8f47e0ab74",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 405,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Bibliography\r\n[1] Daniel Adler and Duncan Murdoch. rgl: 3D visualization device system (OpenGL), 2009. R\r\npackage version 0.87. Available from: http://CRAN.R-project.org/package=rgl. 3\r\n[2] A. Agresti and B. A. Coull. Approximate is better than \"exact\" for interval estimation of\r\nbinomial proportions. The American Statistician, 52:119–126, 1998.\r\n[3] Alan Agresti. Categorical Data Analysis. Wiley, 2002. 223\r\n[4] Tom M. Apostol. Calculus, volume II. Wiley, second edition, 1967. 361\r\n[5] Tom M. Apostol. Calculus, volume I. Wiley, second edition, 1967. 361\r\n[6] Robert B. Ash and Catherine Doleans-Dade. Probability & Measure Theory. Harcourt Aca\u0002demic Press, 2000. 361\r\n[7] Peter J. Bickel and Kjell A. Doksum. Mathematical Statistics, volume I. Prentice Hall, 2001.\r\n234\r\n[8] Patrick Billingsley. Probability and Measure. Wiley Interscience, 1995. 123, 361\r\n[9] Ben Bolker. emdbook: Ecological models and data (book support), 2009. R package version\r\n1.2. Available from: http://CRAN.R-project.org/package=emdbook. 180\r\n[10] Bruce L. Bowerman, Richard O’Connell, and Anne Koehler. Forecasting, Time Series, and\r\nRegression: An Applied Approach. South-Western College Pub, 2004.\r\n[11] P. J. Brockwell and R. A. Davis. Time Series and Forecasting Methods. Springer, second\r\nedition, 1991. 24\r\n[12] Neal L. Carothers. Real Analysis. Cambridge University Press, 2000. 361\r\n[13] George Casella and Roger L. Berger. Statistical Inference. Duxbury Press, 2002. viii, 177,\r\n193, 211\r\n[14] Scott Chasalow. combinat: combinatorics utilities, 2009. R package version 0.0-7. Available\r\nfrom: http://CRAN.R-project.org/package=combinat. 69\r\n[15] Erhan Cinlar. Introduction to Stochastic Processes. Prentice Hall, 1975.\r\n[16] William S. Cleveland. The Elements of Graphing Data. Hobart Press, 1994.\r\n389",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/a6619f76-b73a-47f1-a9a2-6a8f47e0ab74.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=23fcd33df7643a10e5f0685aa3b10070d2bd949d78a49221dcbd8e3103d98dfc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 257
      },
      {
        "segments": [
          {
            "segment_id": "ae04e70a-f9cb-4def-91d0-dc392c15bba1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 406,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "390 BIBLIOGRAPHY\r\n[17] Fortran code by Alan Genz and R code by Adelchi Azzalini. mnormt: The multivariate\r\nnormal and t distributions, 2009. R package version 1.3-3. Available from: http://CRAN.\r\nR-project.org/package=mnormt. 180\r\n[18] R core members, Saikat DebRoy, Roger Bivand, and others: see COPYRIGHTS file in the\r\nsources. foreign: Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, dBase, ..., 2010.\r\nR package version 0.8-39. Available from: http://CRAN.R-project.org/package=\r\nforeign. 4\r\n[19] Peter Dalgaard. Introductory Statistics with R. Springer, 2008. Available from: http:\r\n//staff.pubhealth.ku.dk/~pd/ISwR.html. viii\r\n[20] A. C. Davison and D. V. Hinkley. Bootstrap Methods and Their Applications. Cambridge\r\nUniversity Press, 1997.\r\n[21] Thomas J. DiCiccio and Bradley Efron. Bootstrap confidence intervals. Statistical Science,\r\n11:189–228, 1996.\r\n[22] Evgenia Dimitriadou, Kurt Hornik, Friedrich Leisch, David Meyer, and Andreas Weingessel.\r\ne1071: Misc Functions of the Department of Statistics (e1071), TU Wien, 2009. R package\r\nversion 1.5-22. Available from: http://CRAN.R-project.org/package=e1071. 40\r\n[23] Richard Durrett. Probability: Theory and Examples. Duxbury Press, 1996.\r\n[24] Rick Durrett. Essentials of Stochastic Processes. Springer, 1999.\r\n[25] Christophe Dutang, Vincent Goulet, and Mathieu Pigeon. actuar: An r package for actuarial\r\nscience. Journal of Statistical Software, 2008. to appear. 162\r\n[26] Brian Everitt. An R and S-Plus Companion to Multivariate Analysis. Springer, 2007.\r\n[27] Gerald B. Folland. Real Analysis: Modern Techniques and Their Applications. Wiley, 1999.\r\n361\r\n[28] John Fox. Applied Regression Analysis, Linear Models, and Related Methods. Sage, 1997.\r\n308\r\n[29] John Fox. An R and S Plus Companion to Applied Regression. Sage, 2002. 315\r\n[30] John Fox. car: Companion to Applied Regression, 2009. R package version 1.2-16. Available\r\nfrom: http://CRAN.R-project.org/package=car. 310\r\n[31] John Fox, with contributions from Liviu Andronic, Michael Ash, Theophilius Boye, Stefano\r\nCalza, Andy Chang, Philippe Grosjean, Richard Heiberger, G. Jay Kerns, Renaud Lancelot,\r\nMatthieu Lesnoff, Uwe Ligges, Samir Messad, Martin Maechler, Robert Muenchen, Duncan\r\nMurdoch, Erich Neuwirth, Dan Putler, Brian Ripley, Miroslav Ristic, and Peter Wolf. Rcmdr:\r\nR Commander, 2009. R package version 1.5-4. Available from: http://CRAN.R-project.\r\norg/package=Rcmdr. 288\r\n[32] Michael Friendly. Visualizing Categorical Data. SAS Publishing, 2000.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/ae04e70a-f9cb-4def-91d0-dc392c15bba1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c11a15b94e86ea46db62924938fe0567e1a53ec8823c421f6ed2f804ce8c9ea4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 332
      },
      {
        "segments": [
          {
            "segment_id": "d61f6a32-e6ea-4af0-ae0f-a496864fe740",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 407,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "BIBLIOGRAPHY 391\r\n[33] Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian Data Analysis.\r\nCRC Press, 2004. 175\r\n[34] Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and\r\nTorsten Hothorn. mvtnorm: Multivariate Normal and t Distributions, 2009. R package ver\u0002sion 0.9-8. Available from: http://CRAN.R-project.org/package=mvtnorm. 180\r\n[35] Rob Goedman, Gabor Grothendieck, Søren Højsgaard, and Ayal Pinkus. Ryacas: R interface\r\nto the yacas computer algebra system, 2008. R package version 0.2-9. Available from:\r\nhttp://ryacas.googlecode.com. 174\r\n[36] Gregor Gorjanc. Using sweave with lyx. R News, 1:2–9, 2008. 380\r\n[37] Charles M. Grinstead and J. Laurie Snell. Introduction to Probability. American Mathemati\u0002cal Society, 1997. Available from: http://www.dartmouth.edu/~chance/. viii\r\n[38] Bettina Grün and Achim Zeileis. Automatic generation of exams in R. Journal of Statistical\r\nSoftware, 29(10):1–14, 2009. Available from: http://www.jstatsoft.org/v29/i10/.\r\nviii\r\n[39] Frank E Harrell, Jr and with contributions from many other users. Hmisc: Harrell Miscel\u0002laneous, 2009. R package version 3.7-0. Available from: http://CRAN.R-project.org/\r\npackage=Hmisc.\r\n[40] Richard M. Heiberger. HH: Statistical Analysis and Data Display: Heiberger and Hol\u0002land, 2009. R package version 2.1-32. Available from: http://CRAN.R-project.org/\r\npackage=HH. 262\r\n[41] Richard M. Heiberger and Burt Holland. Statistical Analysis and Data Display: An Inter\u0002mediate Course with Examples in S-Plus, R, and SAS. Springer, 2004. Available from:\r\nhttp://astro.temple.edu/~rmh/HH/.\r\n[42] Richard M. Heiberger and Erich Neuwirth. R Through Excel: A Spreadsheet In\u0002terface for Statistics, Data Analysis, and Graphics. Springer, 2009. Available\r\nfrom: http://www.springer.com/statistics/computanional+statistics/book/\r\n978-1-4419-0051-7.\r\n[43] Robert V. Hogg, Joseph W. McKean, and Allen T. Craig. Introduction to Mathematical Statis\u0002tics. Pearson Prentice Hall, 2005. 193\r\n[44] Robert V. Hogg and Elliot A. Tanis. Probability and Statistical Inference. Pearson Prentice\r\nHall, 2006. viii\r\n[45] Torsten Hothorn and Kurt Hornik. exactRankTests: Exact Distributions for Rank and Permu\u0002tation Tests, 2006. R package version 0.8-18.\r\n[46] Torsten Hothorn, Kurt Hornik, Mark A. van-de Wiel, and Achim Zeileis. Implementing a\r\nclass of permutation tests: The coin package. Journal of Statistical Software, 28:1–23, 2008.\r\n[47] Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Continuous Univariate Distribu\u0002tions, volume 1. Wiley, second edition, 1994. 143",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/d61f6a32-e6ea-4af0-ae0f-a496864fe740.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63691c2ab6203df11e424114fe5c44c93cce951c66d8ba5c5118af88bc4244ad",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 335
      },
      {
        "segments": [
          {
            "segment_id": "c2a61f39-54e1-4da2-b9b6-54b61fb204a8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 408,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "392 BIBLIOGRAPHY\r\n[48] Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Continuous Univariate Distribu\u0002tions, volume 2. Wiley, second edition, 1995. 143\r\n[49] Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Discrete Multivariate Distributions.\r\nWiley, 1997. 165\r\n[50] Norman L. Johnson, Samuel Kotz, and Adrienne W. Kemp. Univariate Discrete Distributions.\r\nWiley, second edition, 1993. 111\r\n[51] Roger W. Johnson. How many fish are in the pond? Available from: http://www.rsscse.\r\norg.uk/ts/gtb/johnson3.pdf.\r\n[52] G. Jay Kerns. prob: Elementary Probability on Finite Sample Spaces, 2009. R package\r\nversion 0.9-2. Available from: http://CRAN.R-project.org/package=prob. 68\r\n[53] G. Jay Kerns, with contributions by Theophilius Boye, Tyler Drombosky, and adapted from\r\nthe work of John Fox et al. RcmdrPlugin.IPSUR: An IPSUR Plugin for the R Commander,\r\n2009. R package version 0.1-6. Available from: http://CRAN.R-project.org/package=\r\nRcmdrPlugin.IPSUR.\r\n[54] Samuel Kotz, N. Balakrishnan, and Norman L. Johnson. Continuous Multivariate Distribu\u0002tions, volume 1: Models and Applications. Wiley, second edition, 2000. 165, 179\r\n[55] Max Kuhn and Steve Weaston. odfWeave: Sweave processing of Open Document Format\r\n(ODF) files, 2009. R package version 0.7.10. 375\r\n[56] Michael Lavine. Introduction to Statistical Thought. Lavine, Michael, 2009. Available from:\r\nhttp://www.math.umass.edu/~lavine/Book/book.html. viii\r\n[57] Peter M. Lee. Bayesian Statistics: An Introduction. Wiley, 1997. 175\r\n[58] E. L. Lehmann. Testing Statistical Hypotheses. Springer-Verlag, 1986. viii\r\n[59] E. L. Lehmann and George Casella. Theory of Point Estimation. Springer, 1998. viii\r\n[60] Uwe Ligges. Accessing the sources. R News, 6:43–45, 2006. 10, 12\r\n[61] Uwe Ligges and Martin Mächler. Scatterplot3d - an r package for visualizing multi\u0002variate data. Journal of Statistical Software, 8(11):1–20, 2003. Available from: http:\r\n//www.jstatsoft.org. 188\r\n[62] Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statis\u0002tics and Econometrics. Wiley, 1999. 361\r\n[63] John Maindonald and John Braun. Data Analysis and Graphics Using R. Cambridge Univer\u0002sity Press, 2003.\r\n[64] John Maindonald and W. John Braun. DAAG: Data Analysis And Graphics data and func\u0002tions, 2009. R package version 1.01. Available from: http://CRAN.R-project.org/\r\npackage=DAAG.\r\n[65] Ben Mezrich. Bringing Down the House: The Inside Story of Six M.I.T. Students Who Took\r\nVegas for Millions. Free Press, 2003. 80",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c2a61f39-54e1-4da2-b9b6-54b61fb204a8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5e29ec02e19b182206a9cbd607b47b4ee0f1a040d3588c783a79cf8a464443b7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 340
      },
      {
        "segments": [
          {
            "segment_id": "c82000bd-93c9-4d00-8a9a-d5c35757af28",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 409,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "BIBLIOGRAPHY 393\r\n[66] Jeff Miller. Earliest known uses of some of the words of mathematics. Available from:\r\nhttp://jeff560.tripod.com/mathword.html. 21\r\n[67] John Neter, Michael H. Kutner, Christopher J. Nachtsheim, and William Wasserman. Applied\r\nLinear Regression Models. McGraw Hill, third edition, 1996. 222, 285, 308, 315\r\n[68] R Development Core Team. R: A Language and Environment for Statistical Computing. R\r\nFoundation for Statistical Computing, Vienna, Austria, 2009. ISBN 3-900051-07-0. Available\r\nfrom: http://www.R-project.org. 356\r\n[69] C. Radhakrishna Rao and Helge Toutenburg. Linear Models: Least Squares and Alternatives.\r\nSpringer, 1999. 285, 290\r\n[70] Sidney I. Resnick. A Probability Path. Birkhauser, 1999. 177, 361\r\n[71] Maria L. Rizzo. Statistical Computing with R. Chapman & Hall/CRC, 2008. 319\r\n[72] Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer, 2004.\r\n319\r\n[73] Kenneth A. Ross. Elementary Calculus: The Theory of Calculus. Springer, 1980.\r\n[74] P. Ruckdeschel, M. Kohl, T. Stabla, and F. Camphausen. S4 classes for distributions. R News,\r\n6(2):2–6, May 2006. Available from: http://www.uni-bayreuth.de/departments/\r\nmath/org/mathe7/DISTR/distr.pdf. 114, 118, 148, 197, 214\r\n[75] Deepayan Sarkar. lattice: Lattice Graphics, 2009. R package version 0.17-26. Available\r\nfrom: http://CRAN.R-project.org/package=lattice. 286\r\n[76] F. E. Satterthwaite. An approximate distribution of estimates of variance components. Bio\u0002metrics Bulletin, 2:110–114, 1946. 222\r\n[77] Luca Scrucca. qcc: an r package for quality control charting and statistical process control. R\r\nNews, 4/1:11–17, 2004. Available from: http://CRAN.R-project.org/doc/Rnews/. 28\r\n[78] Robert J. Serfling. Approximation Theorems of Mathematical Statistics. Wiley, 1980.\r\n[79] Greg Snow. TeachingDemos: Demonstrations for teaching and learning, 2009. R package\r\nversion 2.5. Available from: http://CRAN.R-project.org/package=TeachingDemos.\r\n197\r\n[80] James Stewart. Calculus. Thomson Brooks/Cole, 2008. 361\r\n[81] Stephen M. Stigler. The History of Statistics: The Measurement of Uncertainty before 1900.\r\nHarvard University Press, 1986.\r\n[82] Gilbert Strang. Linear Algebra and Its Applications. Harcourt, 1988. 361\r\n[83] Barbara G. Tabachnick and Linda S. Fidell. Using Multivariate Statistics. Allyn and Bacon,\r\n2006. 40, 315",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/c82000bd-93c9-4d00-8a9a-d5c35757af28.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9796ba4f915377c73b9e46d7c5c441421ce78655bd6022fc4372e40af0834220",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 304
      },
      {
        "segments": [
          {
            "segment_id": "274b3099-2e38-40c0-b860-0d7e87ded459",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 410,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "394 BIBLIOGRAPHY\r\n[84] W. N. Venables and B. D. Ripley. Modern Applied Statistics with S. Springer, New York,\r\nfourth edition, 2002. ISBN 0-387-95457-0. Available from: http://www.stats.ox.ac.\r\nuk/pub/MASS4. 225\r\n[85] William N. Venables and David M. Smith. An Introduction to R, 2010. Available from:\r\nhttp://www.r-project.org/Manuals. 8, 11, 356\r\n[86] John Verzani. UsingR: Data sets for the text \"Using R for Introductory Statistics\". R package\r\nversion 0.1-12. Available from: http://www.math.csi.cuny.edu/UsingR. 20\r\n[87] John Verzani. Using R for Introductory Statistics. CRC Press, 2005. Available from: http:\r\n//www.math.csi.cuny.edu/UsingR/. viii\r\n[88] B. L. Welch. The generalization of \"student’s\" problem when several different population\r\nvariances are involved. Biometrika, 34:28–35, 1947. 222\r\n[89] Hadley Wickham. Reshaping data with the reshape package. Journal of Statistical Software,\r\n21(12), 2007. Available from: http://www.jstatsoft.org/v21/i12/paper. 354\r\n[90] Hadley Wickham. ggplot2: elegant graphics for data analysis. Springer New York, 2009.\r\nAvailable from: http://had.co.nz/ggplot2/book. viii\r\n[91] Graham Williams. rattle: A graphical user interface for data mining in R using GTK, 2009.\r\nR package version 2.5.12. Available from: http://CRAN.R-project.org/package=\r\nrattle. 6\r\n[92] Peter Wolf and Uni Bielefeld. aplpack: Another Plot PACKage: stem.leaf, bagplot, faces,\r\nspin3R, and some slider functions, 2009. R package version 1.2.2. Available from: http:\r\n//CRAN.R-project.org/package=aplpack. 24\r\n[93] Achim Zeileis and Torsten Hothorn. Diagnostic checking in regression relationships. R News,\r\n2(3):7–10, 2002. Available from: http://CRAN.R-project.org/doc/Rnews/. 272",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/274b3099-2e38-40c0-b860-0d7e87ded459.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=dd5f65ecd1aaf445d803be0e7efc967ac9f310db3b515c49c14d0af8d5104f9d",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "0215ce19-8c55-4d35-a3b9-464b84cc5968",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 411,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "Index\r\n#, 7\r\n.Internal, 12\r\n.Primitive, 12\r\n.RData, 15\r\n.Rprofile, 15\r\n?, 12\r\n??, 13\r\n[], 9\r\nActive data set, 27\r\napropos, 13\r\nas.complex, 8\r\nbarplot, 28\r\nboot, 325\r\nboot.ci, 326\r\nc, 9\r\ncex.names, 28\r\ncomplex, 8\r\nconfint, 295\r\nCRAN, 14\r\nData sets\r\ncars, 250\r\ndiscoveries, 19\r\nLakeHuron, 24\r\nprecip, 18, 21\r\nrivers, 18, 323\r\nstate.abb, 25\r\nstate.division, 28\r\nstate.name, 25\r\nstate.region, 27\r\ntrees, 286\r\nUKDriverDeaths, 24\r\nDeducer, 6\r\ndepths, 24\r\ndigits, 7\r\ndot plot, see{strip chart}19\r\nDOTplot, 20\r\ndouble, 8\r\ndump, 14\r\necdf, 126\r\nEmacs, 5\r\nEmpirical distribution, 126\r\nESS, 5\r\nevent, 73\r\nexample, 13\r\nexp, 7\r\nfitted values, 291\r\nhat matrix, 291\r\nhelp, 12\r\nhelp.search, 13\r\nhelp.start, 12\r\nhist, 21\r\nHistogram, 20\r\nhistory, 14\r\ninstall.packages, 4\r\nintersect, 10\r\nJGR, 6\r\nLETTERS, 9\r\nletters, 9\r\nlibrary, 4\r\nlikelihood function, 253, 290\r\nlm, 290\r\nls, 14\r\nmaximum likelihood, 253, 290\r\nmodel\r\nmultiple linear regression, 286\r\nmodel matrix, 286\r\n395",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/0215ce19-8c55-4d35-a3b9-464b84cc5968.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=66d585bfc96da5a4155fecb79fb6d6c2217f1a63f9beca413cf9334d44fa08f2",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "71ea2af8-a251-4885-9cd0-1f515b3525c4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 535.68,
              "height": 697.68
            },
            "page_number": 412,
            "page_width": 535.68,
            "page_height": 697.68,
            "content": "396 INDEX\r\nmodel.matrix, 291\r\nmutually exclusive, 73\r\nNA,8\r\nnames, 18\r\nNaN,8\r\nnominal data, 25\r\nnormal equations, 290\r\nobjects, 14\r\noptions,7\r\nordinal data, 25\r\npar, 28\r\npareto.chart, 28\r\npie, 32\r\nplot, 24\r\nPoisson process, 135\r\nPoor Man’s GUI,6\r\npower.examp, 243\r\npredict, 261\r\nprop.table, 27\r\nR Commander,6\r\nR Editor,5\r\nR Graph Gallery, 14 R Graphical Manual, 14\r\nR packages\r\nUsingR, 20\r\nR packages\r\naplpack, 24\r\ndistr, 147\r\nqcc, 28\r\nRcmdrPlugin.IPSUR, 28\r\nR-Forge, 14 R-Wiki, 14\r\nRattle,6\r\nregression assumptions, 250\r\nregression line, 250\r\nremove, 14\r\nreplicate, 243\r\nresponse vector, 286\r\nrev, 11\r\nRprofile.site, 15\r\nRSiteSearch, 13\r\nRWinEdt,5\r\nsample, 127\r\nsample space, 67\r\nscan,9\r\nSciviews-K,5\r\nseq,9\r\nsessionInfo, 14\r\nsigma.test, 239\r\nsqrt,7\r\nstem.leaf, 24\r\nstr, 18, 27\r\nstrip chart, 19\r\nstripchart, 19\r\nt.test, 237\r\nTheR-Project, 14\r\nTinn\u0002R,5\r\ntypeof,8\r\nurnsamples, 70\r\nUseMethod, 11\r\nwilcox.test, 11\r\nz.test, 237",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/202e8752-2474-4b02-9a37-c25701197a8b/images/71ea2af8-a251-4885-9cd0-1f515b3525c4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041953Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=008702229731cc9614664f5692e848ab1f472f3dcd62c0628fc8d52918f9231a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 493
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "\"Introduction to Probability and Statistics with R\"\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "G. Jay Kerns\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "```json\n{\"date_published\": \"March 24, 2011\"}\n```"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "```json\n{\"location\": \"Northeast\\nSouth\\nNorth Central\\nWest\"}\n```"
        }
      ]
    }
  }
}