{
  "file_name": "Managing the Google T1-5 Relational Database (10.1.1.456.9390).pdf",
  "task_id": "398c1873-43a3-4bca-a35b-ac4fe8947b13",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "e945c0f9-3b0d-4807-966a-5ff8d7e41fa0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Managing the Google Web 1T 5-gram with\r\nRelational Database\r\nYan Chi LAM\r\nFaculty of the Graduate School of Global Studies\r\nTokyo University of Foreign Studies\r\nTokyo, Japan\r\nABSTRACT\r\nOn Sep 19 2006, Google released Web 1T 5-gram, an\r\nn-gram corpus generated from a source of approxi\u0002mately 1 trillion words. It provides a valuable refer\u0002ence of English usage since there is no other compa\u0002rable corpus of this data size. However, it has not\r\nbeen widely used in language education due to the\r\ndifficulty in managing the huge data size. In this pa\u0002per, a practical approach of using relational database\r\nto store, index and search the corpus is described\r\nand implemented with commodity hardware. Basic\r\nsearch queries are also designed for performance test\u0002ing. Sample performance results are recorded which\r\nshow acceptable data processing and search response\r\ntimes. It is shown that the 5-gram corpus can be man\u0002aged using relational database and commodity hard\u0002ware. Further search queries can be designed and im\u0002plemented to make better use of the corpus in lan\u0002guage education.\r\nKeywords: Google Web 1T, 5-gram, N-gram, Mysql,\r\nCorpus, Relational Database, Language education\r\n1 INTRODUCTION\r\nThe use of corpora in language education has been\r\nwidely discussed in publication such as Rethinking lan\u0002guage pedagogy from a corpus perspective [1]. As men\u0002tioned in one of the paper in [1] by Aston [2], the use\r\nof corpora in teaching languages take into account the\r\nfrequencies and characteristics of language usage by\r\nnative speakers which are ignored by traditional syl\u0002labus and teaching materials. In that sense, the big\u0002ger the corpus size, the better the representativeness\r\nof the language usage. Recent technology has already\r\nallowed researchers to harness the resources of corpora\r\nwith notable sizes such as The British National Cor\u0002pus (BNC) [3] containing 100 million words and The\r\nCorpus of Contemporary American English (COCA)\r\n[5] containing more than 385 million words.\r\nOn Sep 19 2006, Google released an English corpus,\r\nWeb 1T 5-gram Version 1 [6]. It contains English\r\nword n-grams and their observed frequency counts.\r\nThe length of the n-grams ranges from unigrams (sin\u0002gle words) to five-grams. The n-gram counts were\r\ngenerated from approximately 1 trillion word tokens\r\nof text from publicly accessible Web pages. Its data\r\nsize is about 10000 times bigger than BNC and about\r\n2500 times bigger than COCA. It provides a unique\r\nreference of global English language usage since there\r\nis no other comparable corpus of this data size. Here\r\nis an overview of its data sizes:\r\nNumber of tokens: 1,024,908,267,229\r\nNumber of sentences: 95,119,665,584\r\nNumber of unigrams: 13,588,391\r\nNumber of bigrams: 314,843,401\r\nNumber of trigrams: 977,069,902\r\nNumber of fourgrams: 1,313,818,354\r\nNumber of fivegrams: 1,176,470,663\r\nPhysically the data are distributed in 6 DVDs, as\r\ngzip’ed text files. Each gzip’ed text files contains\r\nexactly 1,000,000 grams or less and their frequency\r\ncounts except for the unigram file which contains all\r\nof the unigrams. All the raw data amount to around\r\n25GB in gzip’ed format.\r\nIn this paper it will be described in details how the\r\nGoogle 5-gram corpus can be stored and organized us\u0002ing relational database (RDB) with common commod\u0002ity machine hardware. Two kinds of search queries are\r\nimplemented to demonstrate the feasibility in running\r\nsearches on top of RDB. Results and performance will\r\nbe discussed.\r\n2 RELATED WORK\r\nThere are a few researches related to managing and\r\nextracting data from the Google N-gram corpus that\r\nare found for references. Their main purposes of using\r\nthe corpus are for NLP tasks. Here is a summary of",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/e945c0f9-3b0d-4807-966a-5ff8d7e41fa0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=afe016ca2c86b00ef8ac8e10565d26feb54f85636fec13dc3f796345284500aa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 574
      },
      {
        "segments": [
          {
            "segment_id": "e945c0f9-3b0d-4807-966a-5ff8d7e41fa0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Managing the Google Web 1T 5-gram with\r\nRelational Database\r\nYan Chi LAM\r\nFaculty of the Graduate School of Global Studies\r\nTokyo University of Foreign Studies\r\nTokyo, Japan\r\nABSTRACT\r\nOn Sep 19 2006, Google released Web 1T 5-gram, an\r\nn-gram corpus generated from a source of approxi\u0002mately 1 trillion words. It provides a valuable refer\u0002ence of English usage since there is no other compa\u0002rable corpus of this data size. However, it has not\r\nbeen widely used in language education due to the\r\ndifficulty in managing the huge data size. In this pa\u0002per, a practical approach of using relational database\r\nto store, index and search the corpus is described\r\nand implemented with commodity hardware. Basic\r\nsearch queries are also designed for performance test\u0002ing. Sample performance results are recorded which\r\nshow acceptable data processing and search response\r\ntimes. It is shown that the 5-gram corpus can be man\u0002aged using relational database and commodity hard\u0002ware. Further search queries can be designed and im\u0002plemented to make better use of the corpus in lan\u0002guage education.\r\nKeywords: Google Web 1T, 5-gram, N-gram, Mysql,\r\nCorpus, Relational Database, Language education\r\n1 INTRODUCTION\r\nThe use of corpora in language education has been\r\nwidely discussed in publication such as Rethinking lan\u0002guage pedagogy from a corpus perspective [1]. As men\u0002tioned in one of the paper in [1] by Aston [2], the use\r\nof corpora in teaching languages take into account the\r\nfrequencies and characteristics of language usage by\r\nnative speakers which are ignored by traditional syl\u0002labus and teaching materials. In that sense, the big\u0002ger the corpus size, the better the representativeness\r\nof the language usage. Recent technology has already\r\nallowed researchers to harness the resources of corpora\r\nwith notable sizes such as The British National Cor\u0002pus (BNC) [3] containing 100 million words and The\r\nCorpus of Contemporary American English (COCA)\r\n[5] containing more than 385 million words.\r\nOn Sep 19 2006, Google released an English corpus,\r\nWeb 1T 5-gram Version 1 [6]. It contains English\r\nword n-grams and their observed frequency counts.\r\nThe length of the n-grams ranges from unigrams (sin\u0002gle words) to five-grams. The n-gram counts were\r\ngenerated from approximately 1 trillion word tokens\r\nof text from publicly accessible Web pages. Its data\r\nsize is about 10000 times bigger than BNC and about\r\n2500 times bigger than COCA. It provides a unique\r\nreference of global English language usage since there\r\nis no other comparable corpus of this data size. Here\r\nis an overview of its data sizes:\r\nNumber of tokens: 1,024,908,267,229\r\nNumber of sentences: 95,119,665,584\r\nNumber of unigrams: 13,588,391\r\nNumber of bigrams: 314,843,401\r\nNumber of trigrams: 977,069,902\r\nNumber of fourgrams: 1,313,818,354\r\nNumber of fivegrams: 1,176,470,663\r\nPhysically the data are distributed in 6 DVDs, as\r\ngzip’ed text files. Each gzip’ed text files contains\r\nexactly 1,000,000 grams or less and their frequency\r\ncounts except for the unigram file which contains all\r\nof the unigrams. All the raw data amount to around\r\n25GB in gzip’ed format.\r\nIn this paper it will be described in details how the\r\nGoogle 5-gram corpus can be stored and organized us\u0002ing relational database (RDB) with common commod\u0002ity machine hardware. Two kinds of search queries are\r\nimplemented to demonstrate the feasibility in running\r\nsearches on top of RDB. Results and performance will\r\nbe discussed.\r\n2 RELATED WORK\r\nThere are a few researches related to managing and\r\nextracting data from the Google N-gram corpus that\r\nare found for references. Their main purposes of using\r\nthe corpus are for NLP tasks. Here is a summary of",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/e945c0f9-3b0d-4807-966a-5ff8d7e41fa0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=afe016ca2c86b00ef8ac8e10565d26feb54f85636fec13dc3f796345284500aa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 574
      },
      {
        "segments": [
          {
            "segment_id": "1e84f537-3f76-4440-94a4-d02616d9575f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "their approaches in handling the corpus:\r\nResearch Strategies\r\nHawker etc.\r\n[7]\r\n- hash-based strategy that pre\u0002process queries and/or data\r\n- reducing the resolution of the\r\ndata to give only approximate\r\nfrequency counts and sometimes\r\nfalse positive counts\r\n- data compressing\r\nIslam etc. [8] - only 5-gram data are processed\r\n- reducing the size of the data\r\nset by deletion and substitution\r\nof grams\r\n- sorting data into different files\r\nbased on query word as indexing\r\nstrategies\r\nSekine [9] - customized trie indexing\r\n- index all of the 5-grams using a\r\nindex file 277GB of size\r\nNLP tasks involve numerous statistical queries on the\r\ndata. It may justify the approaches of designing com\u0002plex indexing methods and softwares, and sacrificing\r\nthe accuracies of the data as they aim to return query\r\nresults within a faction of a second.\r\nHowever, for usage such as language education, such\r\napproaches can be redundant as time factor is not as\r\nessential and priority should be put in the ease of set\u0002ting up the system, the flexibility of designing queries,\r\nand the ability to browse accurate data. Under such\r\nconditions, it justifies more to use existing RDB soft\u0002wares in handling the corpus for language education as\r\nthey have readily available internal storage and index\u0002ing functionalities that can be leveraged. This paper\r\nwill explore the practicability and feasibility of such\r\nmeans.\r\n3 PROPOSED APPROACH\r\nThis section will propose in abstract terms how the\r\ncorpus can be processed and organized into a RDB\r\nand afterwards be indexed by it.\r\n3.1 Data Modeling\r\nIn order to efficiently store and index all the n-grams\r\ndata into a RDB, each of the unique English words\r\nin the corpus is given a numeric word id since storage\r\nand indexing of integers require less space and execute\r\nfaster compared with strings data type. The following\r\nrelational data models are proposed:\r\nUnigrams Table\r\nField Name Data Type Description\r\nword id integer A unique id rang\u0002ing from 1 to\r\n1,024,908,267,229\r\nidentifying the\r\nEnglish word\r\nword string The English word\r\nfrequency integer The frequency\r\ncount of the En\u0002glish word\r\n*All columns are to be indexed by the RDB\r\nBigrams, Trigrams, 4-grams, 5-grams Tables\r\nField Name Data Type Description\r\ngram id integer A unique id identi\u0002fying the gram in\u0002stance\r\nword1 id integer The corresponding\r\nword id of the first\r\nword in the gram\r\naccording to the\r\nUnigrams table\r\n... ... ...\r\nword(n) id integer The corresponding\r\nword id of the nth\r\n(up to 5) word in\r\nthe gram according\r\nto the Unigrams ta\u0002ble\r\nfrequency integer The frequency\r\ncount of the gram\r\n*All columns are to be indexed by the RDB\r\nThe assignment of word ids should be done when cre\u0002ating the Unigrams table. Considering the large scale\r\nof the data, the following problems may arise if each\r\nsets of the two to five grams is stored into one single\r\ntable:\r\n• The actual file used by the RDB software to store\r\nthe table may exceed the maximum file size of the\r\nunderlying operating system\r\n• The number of entries in a set of grams (e.g. 4-\r\ngrams has 1,313,818,354 entries) may exceed the\r\nlimit of the maximum number of rows in a single\r\ntable of the RDB software\r\n• If the index size of a single table is too big, the\r\nindex may not load or effectively load into the\r\nRAM, affecting search speed\r\nThus, each of the two to five grams tables is split\r\ninto smaller tables to avoid the mentioned problems.\r\nThe optimal way to split the tables depend largely",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/1e84f537-3f76-4440-94a4-d02616d9575f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d4f1b3030314b9a02f4d9635c3352502afcd3d9b24b8cdbb1517fba8168af240",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 584
      },
      {
        "segments": [
          {
            "segment_id": "1e84f537-3f76-4440-94a4-d02616d9575f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "their approaches in handling the corpus:\r\nResearch Strategies\r\nHawker etc.\r\n[7]\r\n- hash-based strategy that pre\u0002process queries and/or data\r\n- reducing the resolution of the\r\ndata to give only approximate\r\nfrequency counts and sometimes\r\nfalse positive counts\r\n- data compressing\r\nIslam etc. [8] - only 5-gram data are processed\r\n- reducing the size of the data\r\nset by deletion and substitution\r\nof grams\r\n- sorting data into different files\r\nbased on query word as indexing\r\nstrategies\r\nSekine [9] - customized trie indexing\r\n- index all of the 5-grams using a\r\nindex file 277GB of size\r\nNLP tasks involve numerous statistical queries on the\r\ndata. It may justify the approaches of designing com\u0002plex indexing methods and softwares, and sacrificing\r\nthe accuracies of the data as they aim to return query\r\nresults within a faction of a second.\r\nHowever, for usage such as language education, such\r\napproaches can be redundant as time factor is not as\r\nessential and priority should be put in the ease of set\u0002ting up the system, the flexibility of designing queries,\r\nand the ability to browse accurate data. Under such\r\nconditions, it justifies more to use existing RDB soft\u0002wares in handling the corpus for language education as\r\nthey have readily available internal storage and index\u0002ing functionalities that can be leveraged. This paper\r\nwill explore the practicability and feasibility of such\r\nmeans.\r\n3 PROPOSED APPROACH\r\nThis section will propose in abstract terms how the\r\ncorpus can be processed and organized into a RDB\r\nand afterwards be indexed by it.\r\n3.1 Data Modeling\r\nIn order to efficiently store and index all the n-grams\r\ndata into a RDB, each of the unique English words\r\nin the corpus is given a numeric word id since storage\r\nand indexing of integers require less space and execute\r\nfaster compared with strings data type. The following\r\nrelational data models are proposed:\r\nUnigrams Table\r\nField Name Data Type Description\r\nword id integer A unique id rang\u0002ing from 1 to\r\n1,024,908,267,229\r\nidentifying the\r\nEnglish word\r\nword string The English word\r\nfrequency integer The frequency\r\ncount of the En\u0002glish word\r\n*All columns are to be indexed by the RDB\r\nBigrams, Trigrams, 4-grams, 5-grams Tables\r\nField Name Data Type Description\r\ngram id integer A unique id identi\u0002fying the gram in\u0002stance\r\nword1 id integer The corresponding\r\nword id of the first\r\nword in the gram\r\naccording to the\r\nUnigrams table\r\n... ... ...\r\nword(n) id integer The corresponding\r\nword id of the nth\r\n(up to 5) word in\r\nthe gram according\r\nto the Unigrams ta\u0002ble\r\nfrequency integer The frequency\r\ncount of the gram\r\n*All columns are to be indexed by the RDB\r\nThe assignment of word ids should be done when cre\u0002ating the Unigrams table. Considering the large scale\r\nof the data, the following problems may arise if each\r\nsets of the two to five grams is stored into one single\r\ntable:\r\n• The actual file used by the RDB software to store\r\nthe table may exceed the maximum file size of the\r\nunderlying operating system\r\n• The number of entries in a set of grams (e.g. 4-\r\ngrams has 1,313,818,354 entries) may exceed the\r\nlimit of the maximum number of rows in a single\r\ntable of the RDB software\r\n• If the index size of a single table is too big, the\r\nindex may not load or effectively load into the\r\nRAM, affecting search speed\r\nThus, each of the two to five grams tables is split\r\ninto smaller tables to avoid the mentioned problems.\r\nThe optimal way to split the tables depend largely",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/1e84f537-3f76-4440-94a4-d02616d9575f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d4f1b3030314b9a02f4d9635c3352502afcd3d9b24b8cdbb1517fba8168af240",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 584
      },
      {
        "segments": [
          {
            "segment_id": "5c328fa4-4d25-4b24-9a0f-bca3e0c59937",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "on the architectures of the hardware, the operating\r\nsystem and the RDB software. Since the aim of this\r\npaper is to explore the feasibility of using RDB to han\u0002dle the data rather than how to use RDB to handle\r\nthe data optimally, a naive splitting method is pro\u0002posed here. Each sets of the two to five grams is split\r\ninto the same number of tables as the number of raw\r\ntext files containing the whole set. E.g. The set of\r\n4-grams come in a total of 132 text files so the set of\r\n4-grams will be split into 132 tables accordingly with\r\neach table holding the data of one of the text files.\r\n3.2 Search Queries\r\nTwo kinds of queries are proposed here to serve the\r\npurpose of demonstrating the feasibility of searching\r\nthe corpus processed into the proposed data models.\r\n1. Exact Query\r\nTwo to five words or the special wildcard character *\r\nare to be input. The number of words and wildcards\r\ntogether are taken as the grams to be searched. All\r\nmatching instances are returned sorted in descending\r\nfrequency order. E.g. If ”Apple *” is the input, all\r\nbigrams will be searched and all instances with the\r\nfirst word matching ”Apple” (case sensitive) and the\r\nsecond word matching anything (wildcard) will be re\u0002turned sorted in descending frequency order.\r\n2. Keyword Query\r\nTwo to five words and the number of grams to search\r\nare to be input. Then any instances in the specified\r\ngrams to be searched containing all of the keywords\r\nare returned in descending frequency order. E.g. If\r\n”apple tree” is the query and the search is specified to\r\n5-grams, then all matching instances of 5-grams con\u0002taining both the word ”apple” and ”tree” (case sensi\u0002tive) will be returned in descending frequency order.\r\nMoreover, another optional wildcard * can be used in\r\nbetween words. E.g. If ”apple * tree” is the query and\r\nthe search is specified to 5-grams, then all matching\r\ninstances of 5-grams containg ”apple” as the first and\r\n”tree” as the last word with be returned.\r\nThese two queries are for demonstrating possible us\u0002ages of the data and are not designed for any specific\r\npurposes. Many other possible queries can be further\r\ndesigned and implemented to extract data from the\r\n5-gram corpus for specific purposes in language edu\u0002cation but they are out of the scope of this paper.\r\n4 IMPLEMENTATION\r\n4.1 System Setup\r\nHardware and OS\r\nIn this research two machines are used. Their specifi\u0002cations are as follow:\r\nDevelopment\r\nMachine\r\nServer Machine\r\nCPU Intel Core(TM)2\r\nDuo CPU E8400\r\n3.00GHz\r\nIntel Xeon\r\nQuad-Core\r\nE5506 2.13GHz\r\nMemory 4GB 8GB\r\nHarddisk 200GB 1TB\r\nOS Ubuntu 9.10 64-\r\nbit Server\r\nUbuntu 9.10 64-\r\nbit Server\r\nThe development machine’s specification is common\r\nto most desktop machines. It is used for developing\r\nthe scripts and codes before deployment and for com\u0002parison of speed with the server machine. The server\r\nmachine is for final deployment and physically holds\r\nthe database that contains all the data in the 5-gram\r\ncorpus.\r\nRDB and Programming Language\r\nMysql [10] is a free, open source, popular, easy to set\r\nup, and stable RDB software. Mysql version 5.0 is\r\nused in this research. Python [11] is an expressive in\u0002terpreted programming language which provides good\r\nbalance between coding time and execution speed.\r\nPython version 2.6 is used in this research.\r\n4.2 Data Processing\r\nFirst, the Unigrams table is created according to the\r\ndata model described, assigning a word id to each of\r\nthe English words. Then, algorithm 1 is used for read\u0002ing each n-gram raw text files and inputting them into\r\nMysql.\r\nThe mapping of the English words to their word ids\r\nand the insertion of data into the Mysql table are the\r\nheaviest tasks in this process. The mapping is done\r\nusing an on memory cache of Python data structure\r\ndictionary to make it fast. The cache holding the map\u0002pings of all words implemented by Python dictionary\r\ntakes up about 1.7GB of RAM.\r\nIt is essential that the insertion into Mysql table\r\nis done in a batch to minimize the overhead of each\r\ninsertion calls to Mysql. The INSERT statement in\r\nMysql supports multiple rows insert in one SQL com\u0002mand. Batch size of 10000 table rows is used in this\r\nimplementation.\r\nIndexes are to be created after all the insertion of\r\none file instead of during insertion or it will slow the\r\nprocess down. Locking the table during insertion gives",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/5c328fa4-4d25-4b24-9a0f-bca3e0c59937.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e9564eb6382c74a125e7cc0161c04f12d1e909104729e2821adbd18d1ec7cdd4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 733
      },
      {
        "segments": [
          {
            "segment_id": "5c328fa4-4d25-4b24-9a0f-bca3e0c59937",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "on the architectures of the hardware, the operating\r\nsystem and the RDB software. Since the aim of this\r\npaper is to explore the feasibility of using RDB to han\u0002dle the data rather than how to use RDB to handle\r\nthe data optimally, a naive splitting method is pro\u0002posed here. Each sets of the two to five grams is split\r\ninto the same number of tables as the number of raw\r\ntext files containing the whole set. E.g. The set of\r\n4-grams come in a total of 132 text files so the set of\r\n4-grams will be split into 132 tables accordingly with\r\neach table holding the data of one of the text files.\r\n3.2 Search Queries\r\nTwo kinds of queries are proposed here to serve the\r\npurpose of demonstrating the feasibility of searching\r\nthe corpus processed into the proposed data models.\r\n1. Exact Query\r\nTwo to five words or the special wildcard character *\r\nare to be input. The number of words and wildcards\r\ntogether are taken as the grams to be searched. All\r\nmatching instances are returned sorted in descending\r\nfrequency order. E.g. If ”Apple *” is the input, all\r\nbigrams will be searched and all instances with the\r\nfirst word matching ”Apple” (case sensitive) and the\r\nsecond word matching anything (wildcard) will be re\u0002turned sorted in descending frequency order.\r\n2. Keyword Query\r\nTwo to five words and the number of grams to search\r\nare to be input. Then any instances in the specified\r\ngrams to be searched containing all of the keywords\r\nare returned in descending frequency order. E.g. If\r\n”apple tree” is the query and the search is specified to\r\n5-grams, then all matching instances of 5-grams con\u0002taining both the word ”apple” and ”tree” (case sensi\u0002tive) will be returned in descending frequency order.\r\nMoreover, another optional wildcard * can be used in\r\nbetween words. E.g. If ”apple * tree” is the query and\r\nthe search is specified to 5-grams, then all matching\r\ninstances of 5-grams containg ”apple” as the first and\r\n”tree” as the last word with be returned.\r\nThese two queries are for demonstrating possible us\u0002ages of the data and are not designed for any specific\r\npurposes. Many other possible queries can be further\r\ndesigned and implemented to extract data from the\r\n5-gram corpus for specific purposes in language edu\u0002cation but they are out of the scope of this paper.\r\n4 IMPLEMENTATION\r\n4.1 System Setup\r\nHardware and OS\r\nIn this research two machines are used. Their specifi\u0002cations are as follow:\r\nDevelopment\r\nMachine\r\nServer Machine\r\nCPU Intel Core(TM)2\r\nDuo CPU E8400\r\n3.00GHz\r\nIntel Xeon\r\nQuad-Core\r\nE5506 2.13GHz\r\nMemory 4GB 8GB\r\nHarddisk 200GB 1TB\r\nOS Ubuntu 9.10 64-\r\nbit Server\r\nUbuntu 9.10 64-\r\nbit Server\r\nThe development machine’s specification is common\r\nto most desktop machines. It is used for developing\r\nthe scripts and codes before deployment and for com\u0002parison of speed with the server machine. The server\r\nmachine is for final deployment and physically holds\r\nthe database that contains all the data in the 5-gram\r\ncorpus.\r\nRDB and Programming Language\r\nMysql [10] is a free, open source, popular, easy to set\r\nup, and stable RDB software. Mysql version 5.0 is\r\nused in this research. Python [11] is an expressive in\u0002terpreted programming language which provides good\r\nbalance between coding time and execution speed.\r\nPython version 2.6 is used in this research.\r\n4.2 Data Processing\r\nFirst, the Unigrams table is created according to the\r\ndata model described, assigning a word id to each of\r\nthe English words. Then, algorithm 1 is used for read\u0002ing each n-gram raw text files and inputting them into\r\nMysql.\r\nThe mapping of the English words to their word ids\r\nand the insertion of data into the Mysql table are the\r\nheaviest tasks in this process. The mapping is done\r\nusing an on memory cache of Python data structure\r\ndictionary to make it fast. The cache holding the map\u0002pings of all words implemented by Python dictionary\r\ntakes up about 1.7GB of RAM.\r\nIt is essential that the insertion into Mysql table\r\nis done in a batch to minimize the overhead of each\r\ninsertion calls to Mysql. The INSERT statement in\r\nMysql supports multiple rows insert in one SQL com\u0002mand. Batch size of 10000 table rows is used in this\r\nimplementation.\r\nIndexes are to be created after all the insertion of\r\none file instead of during insertion or it will slow the\r\nprocess down. Locking the table during insertion gives",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/5c328fa4-4d25-4b24-9a0f-bca3e0c59937.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e9564eb6382c74a125e7cc0161c04f12d1e909104729e2821adbd18d1ec7cdd4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 733
      },
      {
        "segments": [
          {
            "segment_id": "a27990c9-6d39-4917-9caf-c38f0eb2f9c2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "a better performance.\r\nAlgorithm 1 Processing a two to five grams text file\r\ninto a table in Mysql\r\nRequire: Unigram file, one of the n-gram files, Mysql\r\nconnection\r\n1: Create an empty Python dictionary data structure\r\ncache\r\n2: i ← 0\r\n3: for each English word in the unigram file do\r\n4: cahce[word] ← i {Assigning an ID to the word.\r\nSame assignment is used in creating the Uni\u0002grams table.}\r\n5: i ← i + 1\r\n6: end for\r\n7: Create a Mysql table to hold the data according\r\nto the data model\r\n8: Lock the table for faster insertion\r\n9: while Lines can be read from the n-gram file do\r\n10: batch ← Create an empty data structure (e.g.\r\narray or list) for temporary storage\r\n11: lines ← Read as many as 10000 lines from the\r\nfile\r\n12: for each line in lines do\r\n13: Split line to get individual words in the gram\r\nand its corresponding frequency\r\n14: Use the cache dictionary to get the word ids\r\nfor each words in the gram\r\n15: Save all the word ids and the frequency count\r\ninto batch\r\n16: end for\r\n17: Insert all data in batch in a single batch into the\r\nMysql table\r\n18: end while\r\n19: Unlock the Mysql table\r\n20: Create index on each columns in the Mysql table\r\nAfter processing all the raw n-gram text files, the\r\nMysql database contains the following tables:\r\nNo. of\r\nTables\r\nPhysical Size\r\nUnigram 1 1.3GB (data: 463MB, index:\r\n878MB)\r\nBigram 32 19.7GB (Each tables - data:\r\n201MB, index: 430MB)\r\nTrigram 98 73.3GB(Each tables - data:\r\n239MB, index: 527MB)\r\n4-gram 132 116.1GB (Each tables - data:\r\n277MB, index: 624MB)\r\n5-gram 118 119.4GB (Each tables - data:\r\n315MB, index: 721MB)\r\nTotal 381 329.8GB\r\n4.3 Search Queries\r\nAlgorithm 2 and 3 describe how the exact and keyword\r\nsearches are implemented respectively.\r\nAlgorithm 2 Exact Search\r\n1: query ← Get user input\r\n2: Parse query to get individual words and wildcards\r\n3: n ← the total number of words and wildcards\r\n4: Query the Unigrams table to get the word ids for\r\nall the words in the query\r\n5: table stacks ← Create an empty data structure\r\n(e.g. array or list) for holding temporary Mysql\r\ntable data\r\n6: for each n-gram tables do\r\n7: Execute an SQL query to return only the first\r\ninstance in descending frequency order match\u0002ing all the word ids in the right word positions\r\n8: if result are returned then\r\n9: Append the result, frequency count, row off\u0002set (which is 1 now) and table name in\r\ntable stacks\r\n10: end if\r\n11: end for\r\n12: Sort table stacks with descending frequency count\r\n13: cache ← Create an empty Python dictionary to\r\ncache word ids mappings\r\n14: result set ← Create an empty data structure to\r\nstore results (grams and frequency sets)\r\n15: while table stacks is not empty do\r\n16: top table ← Pop the top table (highest fre\u0002quency count), its cached offset and cached re\u0002sult from table stacks\r\n17: Replace the word ids in the cached result in\r\ntop table with actual words using cache, if the\r\nmappings are not found in cache, query the un\u0002igram table and cached them in cached for later\r\nuse\r\n18: Append the gram and frequency in top table to\r\nresult set\r\n19: Try to fetch a new row from top table with the\r\nsame matching condition\r\n20: If fetched then, append the result, frequency\r\ncount, row offset and table name in table stacks\r\nand sort table stacks by descending frequency\r\ncount\r\n21: end while\r\n22: return result set",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/a27990c9-6d39-4917-9caf-c38f0eb2f9c2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cf290e8a10629f518ae6f646a98eb3a1dd823d9e93a5ff115e8845707538d8a7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 595
      },
      {
        "segments": [
          {
            "segment_id": "a27990c9-6d39-4917-9caf-c38f0eb2f9c2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "a better performance.\r\nAlgorithm 1 Processing a two to five grams text file\r\ninto a table in Mysql\r\nRequire: Unigram file, one of the n-gram files, Mysql\r\nconnection\r\n1: Create an empty Python dictionary data structure\r\ncache\r\n2: i ← 0\r\n3: for each English word in the unigram file do\r\n4: cahce[word] ← i {Assigning an ID to the word.\r\nSame assignment is used in creating the Uni\u0002grams table.}\r\n5: i ← i + 1\r\n6: end for\r\n7: Create a Mysql table to hold the data according\r\nto the data model\r\n8: Lock the table for faster insertion\r\n9: while Lines can be read from the n-gram file do\r\n10: batch ← Create an empty data structure (e.g.\r\narray or list) for temporary storage\r\n11: lines ← Read as many as 10000 lines from the\r\nfile\r\n12: for each line in lines do\r\n13: Split line to get individual words in the gram\r\nand its corresponding frequency\r\n14: Use the cache dictionary to get the word ids\r\nfor each words in the gram\r\n15: Save all the word ids and the frequency count\r\ninto batch\r\n16: end for\r\n17: Insert all data in batch in a single batch into the\r\nMysql table\r\n18: end while\r\n19: Unlock the Mysql table\r\n20: Create index on each columns in the Mysql table\r\nAfter processing all the raw n-gram text files, the\r\nMysql database contains the following tables:\r\nNo. of\r\nTables\r\nPhysical Size\r\nUnigram 1 1.3GB (data: 463MB, index:\r\n878MB)\r\nBigram 32 19.7GB (Each tables - data:\r\n201MB, index: 430MB)\r\nTrigram 98 73.3GB(Each tables - data:\r\n239MB, index: 527MB)\r\n4-gram 132 116.1GB (Each tables - data:\r\n277MB, index: 624MB)\r\n5-gram 118 119.4GB (Each tables - data:\r\n315MB, index: 721MB)\r\nTotal 381 329.8GB\r\n4.3 Search Queries\r\nAlgorithm 2 and 3 describe how the exact and keyword\r\nsearches are implemented respectively.\r\nAlgorithm 2 Exact Search\r\n1: query ← Get user input\r\n2: Parse query to get individual words and wildcards\r\n3: n ← the total number of words and wildcards\r\n4: Query the Unigrams table to get the word ids for\r\nall the words in the query\r\n5: table stacks ← Create an empty data structure\r\n(e.g. array or list) for holding temporary Mysql\r\ntable data\r\n6: for each n-gram tables do\r\n7: Execute an SQL query to return only the first\r\ninstance in descending frequency order match\u0002ing all the word ids in the right word positions\r\n8: if result are returned then\r\n9: Append the result, frequency count, row off\u0002set (which is 1 now) and table name in\r\ntable stacks\r\n10: end if\r\n11: end for\r\n12: Sort table stacks with descending frequency count\r\n13: cache ← Create an empty Python dictionary to\r\ncache word ids mappings\r\n14: result set ← Create an empty data structure to\r\nstore results (grams and frequency sets)\r\n15: while table stacks is not empty do\r\n16: top table ← Pop the top table (highest fre\u0002quency count), its cached offset and cached re\u0002sult from table stacks\r\n17: Replace the word ids in the cached result in\r\ntop table with actual words using cache, if the\r\nmappings are not found in cache, query the un\u0002igram table and cached them in cached for later\r\nuse\r\n18: Append the gram and frequency in top table to\r\nresult set\r\n19: Try to fetch a new row from top table with the\r\nsame matching condition\r\n20: If fetched then, append the result, frequency\r\ncount, row offset and table name in table stacks\r\nand sort table stacks by descending frequency\r\ncount\r\n21: end while\r\n22: return result set",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/a27990c9-6d39-4917-9caf-c38f0eb2f9c2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cf290e8a10629f518ae6f646a98eb3a1dd823d9e93a5ff115e8845707538d8a7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 595
      },
      {
        "segments": [
          {
            "segment_id": "581d771b-918e-409b-9523-e7e31c33fdc0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Algorithm 3 Keyword Search\r\n1: query, ngrams ← Get user input for keywords and\r\ngrams to search\r\n2: Parse query to get individual words and get their\r\ncorresponding word ids by querying the Unigrams\r\ntable\r\n3: table stacks ← Create an empty data structure\r\n(e.g. array or list) for holding temporary Mysql\r\ntable data\r\n4: for each n-gram tables of ngrams do\r\n5: Execute an SQL query to return only the first\r\ninstances in descending frequency order match\u0002ing all the word ids in any word positions or\r\npositions that match with the wildcard criteria\r\n6: if result are returned then\r\n7: Append the result, frequency count, row off\u0002set (which is 1 now) and table name in\r\ntable stacks\r\n8: end if\r\n9: end for\r\n10: Sort table stacks with descending frequency count\r\n11: Follow step 12 to 22 described in the Exact Search\r\nalgorithm\r\n5 PERFORMANCE\r\n5.1 Data Processing\r\nDevelopment Machine\r\nIt takes around 150 seconds to insert and index a bi\u0002gram text file into a Mysql table while it takes around\r\n230 seconds for a 5-gram text file. Trigram and 4-gram\r\nfiles take more time than bigram but less time than 5-\r\ngram. Let us generously assume that the time to pro\u0002cess one text file (there are totally 381) is 4 minutes,\r\nit would take 1524 minutes, 25.4 hours, only a little\r\nbit over a day to process the whole Google 5-gram\r\ncorpus into Mysql and index them, with a commonly\r\navailable desktop machine specification.\r\nServer Machine\r\nIt takes around 210 seconds to insert and index a bi\u0002gram text file into a Mysql table while it takes around\r\n340 seconds for a 5-gram text file. Trigram and 4-gram\r\nfiles take more time than bigram but less time than 5-\r\ngram. The process takes longer in the server machine\r\nthan the development machine probably due to the\r\nbigger overhead in utilizing a bigger RAM size and a\r\nbigger harddisk size. However, the performance can be\r\nlargely compensated by running multiple processes in\r\nparallel to process several text files at the same time.\r\nIn this research, up to four processes are running in\r\nparallel processing 4 different text files at the same\r\ntime. Again, for easy calculations, let us generously\r\nassume that three parallel processes are run and the\r\ntime to process one text file (there are totally 381) is\r\n6 minutes, thus, the average time to process one file\r\nbecomes 2 minutes. It would then take 762 minutes,\r\n12.7 hours, only a little bit over half a day to process\r\nthe whole Google 5-gram corpus into Mysql and index\r\nthem.\r\n5.2 Search Queries\r\nExact Search\r\nThe following table gives some examples of execution\r\ntimes of the wildcard search implemented. All of the\r\nquery return within one minute which is very accept\u0002able in querying a corpus of this data size. Second\r\nruns are much faster due to the caching mechanism of\r\nMysql.\r\nQuery Time taken to return the\r\nfirst 100 results (in sec\u0002onds)\r\n1st run 2nd run\r\n”banana *” 0.4 0.1\r\n”* banana” 11.9 0.5\r\n”cake * * * *” 4 0.2\r\n”* * cake * *” 33.6 1.6\r\n”* * * * cake” 47.3 1.7\r\n”day dream * *” 3.6 0.2\r\n”day * * dream” 3.7 0.2\r\n”* day dream *” 31.7 0.5\r\n”* * day dream” 54.1 0.5\r\nKeyword Search\r\nThe following table gives some examples of execu\u0002tion times of the keyword search implemented. Some\r\nqueries take up to 6-7 minutes to return. Second\r\nruns are much faster due to the caching mechanism\r\nof Mysql. The search is now running in a sequential\r\nmanner, querying Mysql tables one by one and does\r\nnot take any advantage of the possibility of distributed\r\ncomputing. The way how the data models are pro\u0002posed, the data can actually be stored across several\r\nservers in the same network running Mysql. By run\u0002ning the part from line 6-11 described in algorithm 3\r\nin parallel across for example n machines, the speed\r\nwould be shortened by close to n times theoretically.\r\nQuery N-gram Time taken to\r\nreturn the first\r\n100 results (in\r\nseconds)\r\n1st run 2nd run\r\n”love” 2 45.2 0.5\r\n”love” 3 209.3 1.7\r\n”love” 4 371.6 2.3\r\n”love” 5 394.8 2\r\n”book library” 3 191.1 1\r\n”book library” 4 314.4 1.5\r\n”book library” 5 321.6 1.5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/581d771b-918e-409b-9523-e7e31c33fdc0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=13930b94edbfc6f44230c9f5a410da001f43340fb6f6b77ec5dd64c37ee77f14",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 712
      },
      {
        "segments": [
          {
            "segment_id": "581d771b-918e-409b-9523-e7e31c33fdc0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Algorithm 3 Keyword Search\r\n1: query, ngrams ← Get user input for keywords and\r\ngrams to search\r\n2: Parse query to get individual words and get their\r\ncorresponding word ids by querying the Unigrams\r\ntable\r\n3: table stacks ← Create an empty data structure\r\n(e.g. array or list) for holding temporary Mysql\r\ntable data\r\n4: for each n-gram tables of ngrams do\r\n5: Execute an SQL query to return only the first\r\ninstances in descending frequency order match\u0002ing all the word ids in any word positions or\r\npositions that match with the wildcard criteria\r\n6: if result are returned then\r\n7: Append the result, frequency count, row off\u0002set (which is 1 now) and table name in\r\ntable stacks\r\n8: end if\r\n9: end for\r\n10: Sort table stacks with descending frequency count\r\n11: Follow step 12 to 22 described in the Exact Search\r\nalgorithm\r\n5 PERFORMANCE\r\n5.1 Data Processing\r\nDevelopment Machine\r\nIt takes around 150 seconds to insert and index a bi\u0002gram text file into a Mysql table while it takes around\r\n230 seconds for a 5-gram text file. Trigram and 4-gram\r\nfiles take more time than bigram but less time than 5-\r\ngram. Let us generously assume that the time to pro\u0002cess one text file (there are totally 381) is 4 minutes,\r\nit would take 1524 minutes, 25.4 hours, only a little\r\nbit over a day to process the whole Google 5-gram\r\ncorpus into Mysql and index them, with a commonly\r\navailable desktop machine specification.\r\nServer Machine\r\nIt takes around 210 seconds to insert and index a bi\u0002gram text file into a Mysql table while it takes around\r\n340 seconds for a 5-gram text file. Trigram and 4-gram\r\nfiles take more time than bigram but less time than 5-\r\ngram. The process takes longer in the server machine\r\nthan the development machine probably due to the\r\nbigger overhead in utilizing a bigger RAM size and a\r\nbigger harddisk size. However, the performance can be\r\nlargely compensated by running multiple processes in\r\nparallel to process several text files at the same time.\r\nIn this research, up to four processes are running in\r\nparallel processing 4 different text files at the same\r\ntime. Again, for easy calculations, let us generously\r\nassume that three parallel processes are run and the\r\ntime to process one text file (there are totally 381) is\r\n6 minutes, thus, the average time to process one file\r\nbecomes 2 minutes. It would then take 762 minutes,\r\n12.7 hours, only a little bit over half a day to process\r\nthe whole Google 5-gram corpus into Mysql and index\r\nthem.\r\n5.2 Search Queries\r\nExact Search\r\nThe following table gives some examples of execution\r\ntimes of the wildcard search implemented. All of the\r\nquery return within one minute which is very accept\u0002able in querying a corpus of this data size. Second\r\nruns are much faster due to the caching mechanism of\r\nMysql.\r\nQuery Time taken to return the\r\nfirst 100 results (in sec\u0002onds)\r\n1st run 2nd run\r\n”banana *” 0.4 0.1\r\n”* banana” 11.9 0.5\r\n”cake * * * *” 4 0.2\r\n”* * cake * *” 33.6 1.6\r\n”* * * * cake” 47.3 1.7\r\n”day dream * *” 3.6 0.2\r\n”day * * dream” 3.7 0.2\r\n”* day dream *” 31.7 0.5\r\n”* * day dream” 54.1 0.5\r\nKeyword Search\r\nThe following table gives some examples of execu\u0002tion times of the keyword search implemented. Some\r\nqueries take up to 6-7 minutes to return. Second\r\nruns are much faster due to the caching mechanism\r\nof Mysql. The search is now running in a sequential\r\nmanner, querying Mysql tables one by one and does\r\nnot take any advantage of the possibility of distributed\r\ncomputing. The way how the data models are pro\u0002posed, the data can actually be stored across several\r\nservers in the same network running Mysql. By run\u0002ning the part from line 6-11 described in algorithm 3\r\nin parallel across for example n machines, the speed\r\nwould be shortened by close to n times theoretically.\r\nQuery N-gram Time taken to\r\nreturn the first\r\n100 results (in\r\nseconds)\r\n1st run 2nd run\r\n”love” 2 45.2 0.5\r\n”love” 3 209.3 1.7\r\n”love” 4 371.6 2.3\r\n”love” 5 394.8 2\r\n”book library” 3 191.1 1\r\n”book library” 4 314.4 1.5\r\n”book library” 5 321.6 1.5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/581d771b-918e-409b-9523-e7e31c33fdc0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=13930b94edbfc6f44230c9f5a410da001f43340fb6f6b77ec5dd64c37ee77f14",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 712
      },
      {
        "segments": [
          {
            "segment_id": "cd9ca3c9-d472-40c8-b25c-660ee063df6f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "6 FUTURE WORK\r\nThe Google 5-gram corpus can serve as a valuable re\u0002source in language education. It is shown and docu\u0002mented in this paper how the Google English 5-gram\r\ncorpus can be handled by using commodity machines\r\nleveraging the power of readily available relational\r\ndatabase softwares. Furthermore, search queries are\r\nalso implemented on top of the proposed data mod\u0002els to demonstrate the feasibility of designing useful\r\nsearches. With this knowledge, the Google 5-gram\r\ncorpus can now be set up easily and be examined,\r\nbrowsed and considered for use in language education.\r\nCurrently, web interface has been setup to allow\r\nteachers and students on campus to use the imple\u0002mented search functions. Figure 1 shows a sample\r\nscreenshot of the web interface. After more testing\r\nand usage data collection, more meaningful searches\r\ntailored to language education can be developed and\r\nsearch performance can be optimized according to ac\u0002tual needs.\r\nFinally, as Google has also released n-grams corpora\r\nin Japanese and other European languages, the same\r\nway of handling data can be extended to those corpora\r\nand thus can benefit language education research in\r\nthose languages.\r\nFigure 1: Sample web interface scrrenshot\r\nReferences\r\n[1] Burnard, L., & McEnery, T. (Eds.). (2000). Re\u0002thinking language pedagogy from a corpus perspec\u0002tive: Papers from the Third International Confer\u0002ence on Teaching and Language Corpora. Frank\u0002furt: Peter Lang.\r\n[2] Aston, Guy (2000): Corpora and language teach\u0002ing. In: Burnard, Lou & McEnery, Tony (eds),\r\n7-17.\r\n[3] The British National Corpus, version 3 (BNC\r\nXML Edition). 2007. Distributed by Oxford Uni\u0002versity Computing Services on behalf of the BNC\r\nConsortium. URL: http://www.natcorp.ox.ac.uk/\r\n[4] What makes an Oxford Dictionary? AskOx\u0002ford.com. Oxford University Press. URL:\r\nhttp://www.askoxford.com/oec/mainpage/\r\nRetrieved 13 Feb, 2010.\r\n[5] Davies, Mark (2009), The 385+ Million Word\r\nCorpus of Contemporary American English (1990-\r\npresent). International Journal of Corpus Linguis\u0002tics.\r\n[6] Thorsten Brants, Alex Franz. 2006. Web 1T 5-\r\ngram Version 1. Linguistic Data Consortium,\r\nPhiladelphia.\r\n[7] Tobias Hawker, Mary Gardiner and Andrew Ben\u0002netts (2007). Practical Queries of a Massive n\u0002gram Database. Proceedings of the Australasian\r\nLanguage Technology Workshop 2007. Melbourne,\r\nAustralia, 10th–11th September, 2007, pages\r\n40–48.\r\n[8] Aminul Islam, Diana Inkpen. Managing the Google\r\nWeb 1T 5-gram Data Set. Proceedings of the IEEE\r\nInternational Conference on Natural Language\r\nProcessing and Knowledge Engineering (IEEE\r\nNLP-KE’09). Dalian, China, September, 2009.\r\n[9] Sekine, Satoshi (2008). A Linguistic Knowledge\r\nDiscovery Tool: Very Large Ngram Database\r\nSearch with Arbitrary Wildcards. Proceedings of\r\nColing 2008: Companion volume: Demonstra\u0002tions. Coling 2008 Organizing Committee. Manch\u0002ester, UK. Pages 181-184\r\n[10] http://www.mysql.com/\r\n[11] http://www.python.org/",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/398c1873-43a3-4bca-a35b-ac4fe8947b13/images/cd9ca3c9-d472-40c8-b25c-660ee063df6f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041945Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b7b1b04860518a1055ae01488655db5e417f8a7572dfcfcdb0cddf351f485862",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 411
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "Managing the Google Web 1T 5-gram with Relational Database\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Yan Chi LAM\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "Sep 19 2006\n"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "Tokyo, Japan\n"
        }
      ]
    }
  }
}