{
  "file_name": "Lecture Notes on Linear Algebra (2015).pdf",
  "task_id": "34c0d054-82c1-453b-98db-1af675e31e57",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "ac765811-4cd4-4b5e-a165-1e7c1a3f3e62",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "Lecture Notes on Linear Algebra\r\nA K Lal S Pati\r\nFebruary 10, 2015",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ac765811-4cd4-4b5e-a165-1e7c1a3f3e62.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=20ed19e6e8cef12413c701e5d67b01d4dfa512902c40194082c4e67404707190",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "9a70bc93-5a71-448e-86a1-93409bf790e3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "2",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9a70bc93-5a71-448e-86a1-93409bf790e3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=88bbcb790daf9d10d6e502b531203df0baa0b3f147dbb5a1d67eabf1336919d0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 14
      },
      {
        "segments": [
          {
            "segment_id": "41502694-a264-4601-8431-f3cc06f35830",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Contents\r\n1 Introduction to Matrices 5\r\n1.1 Definition of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\r\n1.1.1 Special Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\r\n1.2 Operations on Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\r\n1.2.1 Multiplication of Matrices . . . . . . . . . . . . . . . . . . . . . . . . 8\r\n1.2.2 Inverse of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\r\n1.3 Some More Special Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\r\n1.3.1 Submatrix of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n1.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\r\n2 System of Linear Equations 23\r\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\r\n2.1.1 A Solution Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\r\n2.1.2 Gauss Elimination Method . . . . . . . . . . . . . . . . . . . . . . . 28\r\n2.1.3 Gauss-Jordan Elimination . . . . . . . . . . . . . . . . . . . . . . . . 34\r\n2.2 Elementary Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\r\n2.3 Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\r\n2.4 Existence of Solution of Ax = b . . . . . . . . . . . . . . . . . . . . . . . . . 47\r\n2.5 Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\n2.5.1 Adjoint of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\r\n2.5.2 Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\r\n2.6 Miscellaneous Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\r\n2.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\r\n3 Finite Dimensional Vector Spaces 61\r\n3.1 Finite Dimensional Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . 61\r\n3.1.1 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\r\n3.1.2 Linear Span . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\r\n3.2 Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\r\n3.3 Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\r\n3.3.1 Dimension of a Finite Dimensional Vector Space . . . . . . . . . . . 78\r\n3.3.2 Application to the study of C\r\nn\r\n. . . . . . . . . . . . . . . . . . . . . 81\r\n3.4 Ordered Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/41502694-a264-4601-8431-f3cc06f35830.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0d92b0206601d948b7426cb660a601be305627c057b80d0c448ab8a1069f6dca",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 971
      },
      {
        "segments": [
          {
            "segment_id": "41502694-a264-4601-8431-f3cc06f35830",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "Contents\r\n1 Introduction to Matrices 5\r\n1.1 Definition of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\r\n1.1.1 Special Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\r\n1.2 Operations on Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\r\n1.2.1 Multiplication of Matrices . . . . . . . . . . . . . . . . . . . . . . . . 8\r\n1.2.2 Inverse of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\r\n1.3 Some More Special Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\r\n1.3.1 Submatrix of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 16\r\n1.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\r\n2 System of Linear Equations 23\r\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\r\n2.1.1 A Solution Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\r\n2.1.2 Gauss Elimination Method . . . . . . . . . . . . . . . . . . . . . . . 28\r\n2.1.3 Gauss-Jordan Elimination . . . . . . . . . . . . . . . . . . . . . . . . 34\r\n2.2 Elementary Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\r\n2.3 Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\r\n2.4 Existence of Solution of Ax = b . . . . . . . . . . . . . . . . . . . . . . . . . 47\r\n2.5 Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\n2.5.1 Adjoint of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\r\n2.5.2 Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\r\n2.6 Miscellaneous Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\r\n2.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\r\n3 Finite Dimensional Vector Spaces 61\r\n3.1 Finite Dimensional Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . 61\r\n3.1.1 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\r\n3.1.2 Linear Span . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\r\n3.2 Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\r\n3.3 Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\r\n3.3.1 Dimension of a Finite Dimensional Vector Space . . . . . . . . . . . 78\r\n3.3.2 Application to the study of C\r\nn\r\n. . . . . . . . . . . . . . . . . . . . . 81\r\n3.4 Ordered Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\r\n3",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/41502694-a264-4601-8431-f3cc06f35830.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0d92b0206601d948b7426cb660a601be305627c057b80d0c448ab8a1069f6dca",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 971
      },
      {
        "segments": [
          {
            "segment_id": "1d6ed85c-92fd-47d2-88b8-734de8e35ea6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "4 CONTENTS\r\n3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\r\n4 Linear Transformations 95\r\n4.1 Definitions and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . 95\r\n4.2 Matrix of a linear transformation . . . . . . . . . . . . . . . . . . . . . . . . 99\r\n4.3 Rank-Nullity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\r\n4.4 Similarity of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\r\n4.5 Change of Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r\n4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\r\n5 Inner Product Spaces 113\r\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\r\n5.2 Definition and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . 113\r\n5.2.1 Basic Results on Orthogonal Vectors . . . . . . . . . . . . . . . . . . 121\r\n5.3 Gram-Schmidt Orthogonalization Process . . . . . . . . . . . . . . . . . . . 123\r\n5.4 Orthogonal Projections and Applications . . . . . . . . . . . . . . . . . . . . 130\r\n5.4.1 Matrix of the Orthogonal Projection . . . . . . . . . . . . . . . . . . 135\r\n5.5 QR Decomposition∗\r\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\r\n5.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\r\n6 Eigenvalues, Eigenvectors and Diagonalization 141\r\n6.1 Introduction and Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\r\n6.2 Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\r\n6.3 Diagonalizable Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\r\n6.4 Sylvester’s Law of Inertia and Applications . . . . . . . . . . . . . . . . . . 156\r\n7 Appendix 163\r\n7.1 Permutation/Symmetric Groups . . . . . . . . . . . . . . . . . . . . . . . . 163\r\n7.2 Properties of Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\r\n7.3 Dimension of M + N . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\r\nIndex 174",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1d6ed85c-92fd-47d2-88b8-734de8e35ea6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c975f0381f8a509e9d076610dc8aa7050bf104ade409e3f0c3640c72dca3b624",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 743
      },
      {
        "segments": [
          {
            "segment_id": "1d6ed85c-92fd-47d2-88b8-734de8e35ea6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "4 CONTENTS\r\n3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\r\n4 Linear Transformations 95\r\n4.1 Definitions and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . 95\r\n4.2 Matrix of a linear transformation . . . . . . . . . . . . . . . . . . . . . . . . 99\r\n4.3 Rank-Nullity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\r\n4.4 Similarity of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\r\n4.5 Change of Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r\n4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\r\n5 Inner Product Spaces 113\r\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\r\n5.2 Definition and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . 113\r\n5.2.1 Basic Results on Orthogonal Vectors . . . . . . . . . . . . . . . . . . 121\r\n5.3 Gram-Schmidt Orthogonalization Process . . . . . . . . . . . . . . . . . . . 123\r\n5.4 Orthogonal Projections and Applications . . . . . . . . . . . . . . . . . . . . 130\r\n5.4.1 Matrix of the Orthogonal Projection . . . . . . . . . . . . . . . . . . 135\r\n5.5 QR Decomposition∗\r\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\r\n5.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\r\n6 Eigenvalues, Eigenvectors and Diagonalization 141\r\n6.1 Introduction and Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\r\n6.2 Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\r\n6.3 Diagonalizable Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\r\n6.4 Sylvester’s Law of Inertia and Applications . . . . . . . . . . . . . . . . . . 156\r\n7 Appendix 163\r\n7.1 Permutation/Symmetric Groups . . . . . . . . . . . . . . . . . . . . . . . . 163\r\n7.2 Properties of Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\r\n7.3 Dimension of M + N . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\r\nIndex 174",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1d6ed85c-92fd-47d2-88b8-734de8e35ea6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c975f0381f8a509e9d076610dc8aa7050bf104ade409e3f0c3640c72dca3b624",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 743
      },
      {
        "segments": [
          {
            "segment_id": "7c7aeef2-23a3-4350-a05e-82616919a08a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 1\r\nIntroduction to Matrices\r\n1.1 Definition of a Matrix\r\nDefinition 1.1.1 (Matrix). A rectangular array of numbers is called a matrix.\r\nThe horizontal arrays of a matrix are called its rows and the vertical arrays are called\r\nits columns. A matrix is said to have the order m × n if it has m rows and n columns.\r\nAn m × n matrix A can be represented in either of the following forms:\r\nA =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n\r\n\r\n\r\n\r\nor A =\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n,\r\nwhere aij is the entry at the intersection of the i\r\nth row and jth column. In a more concise\r\nmanner, we also write Am×n = [aij ] or A = [aij ]m×n or A = [aij ]. We shall mostly\r\nbe concerned with matrices having real numbers, denoted R, as entries. For example, if\r\nA =\r\n\"\r\n1 3 7\r\n4 5 6#\r\nthen a11 = 1, a12 = 3, a13 = 7, a21 = 4, a22 = 5, and a23 = 6.\r\nA matrix having only one column is called a column vector; and a matrix with\r\nonly one row is called a row vector. Whenever a vector is used, it should\r\nbe understood from the context whether it is a row vector or a column\r\nvector. Also, all the vectors will be represented by bold letters.\r\nDefinition 1.1.2 (Equality of two Matrices). Two matrices A = [aij ] and B = [bij ] having\r\nthe same order m × n are equal if aij = bij for each i = 1, 2, . . . , m and j = 1, 2, . . . , n.\r\nIn other words, two matrices are said to be equal if they have the same order and their\r\ncorresponding entries are equal.\r\nExample 1.1.3. The linear system of equations 2x + 3y = 5 and 3x + 2y = 5 can be\r\nidentified with the matrix \"\r\n2 3 : 5\r\n3 2 : 5#\r\n. Note that x and y are indeterminate and we can\r\nthink of x being associated with the first column and y being associated with the second\r\ncolumn.\r\n5",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/7c7aeef2-23a3-4350-a05e-82616919a08a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8006c2923993924392f4ab44f8e41c9e1ae653e55a00488c43efa1271a68ae57",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 430
      },
      {
        "segments": [
          {
            "segment_id": "7b921359-ec93-4199-b90d-d77479286c42",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "6 CHAPTER 1. INTRODUCTION TO MATRICES\r\n1.1.1 Special Matrices\r\nDefinition 1.1.4. 1. A matrix in which each entry is zero is called a zero-matrix, de\u0002noted by 0. For example,\r\n02×2 =\r\n\"\r\n0 0\r\n0 0#\r\nand 02×3 =\r\n\"\r\n0 0 0\r\n0 0 0#\r\n.\r\n2. A matrix that has the same number of rows as the number of columns, is called a\r\nsquare matrix. A square matrix is said to have order n if it is an n × n matrix.\r\n3. The entries a11, a22, . . . , ann of an n×n square matrix A = [aij ] are called the diagonal\r\nentries (the principal diagonal) of A.\r\n4. A square matrix A = [aij ] is said to be a diagonal matrix if aij = 0 for i 6= j. In\r\nother words, the non-zero entries appear only on the principal diagonal. For example,\r\nthe zero matrix 0n and \"\r\n4 0\r\n0 1#\r\nare a few diagonal matrices.\r\nA diagonal matrix D of order n with the diagonal entries d1, d2, . . . , dn is denoted by\r\nD = diag(d1, . . . , dn). If di = d for all i = 1, 2, . . . , n then the diagonal matrix D is\r\ncalled a scalar matrix.\r\n5. A scalar matrix A of order n is called an identity matrix if d = 1. This matrix is\r\ndenoted by In.\r\nFor example, I2 =\r\n\"\r\n1 0\r\n0 1#\r\nand I3 =\r\n\r\n\r\n\r\n1 0 0\r\n0 1 0\r\n0 0 1\r\n\r\n\r\n . The subscript n is suppressed in\r\ncase the order is clear from the context or if no confusion arises.\r\n6. A square matrix A = [aij ] is said to be an upper triangular matrix if aij = 0 for\r\ni > j.\r\nA square matrix A = [aij ] is said to be a lower triangular matrix if aij = 0 for\r\ni < j.\r\nA square matrix A is said to be triangular if it is an upper or a lower triangular\r\nmatrix.\r\nFor example,\r\n\r\n\r\n\r\n0 1 4\r\n0 3 −1\r\n0 0 −2\r\n\r\n\r\n is upper triangular,\r\n\r\n\r\n\r\n0 0 0\r\n1 0 0\r\n0 1 1\r\n\r\n\r\n is lower triangular.\r\nExercise 1.1.5. Are the following matrices upper triangular, lower triangular or both?\r\n1.\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\n0 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/7b921359-ec93-4199-b90d-d77479286c42.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7fc2e1607bcbb104716be43540901e7b88a929ba30235abc8228551af8fc3211",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 448
      },
      {
        "segments": [
          {
            "segment_id": "6019b916-d62b-4573-9a64-7a8f15001036",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "1.2. OPERATIONS ON MATRICES 7\r\n2. The square matrices 0 and I or order n.\r\n3. The matrix diag(1, −1, 0, 1).\r\n1.2 Operations on Matrices\r\nDefinition 1.2.1 (Transpose of a Matrix). The transpose of an m × n matrix A = [aij ] is\r\ndefined as the n × m matrix B = [bij ], with bij = aji for 1 ≤ i ≤ m and 1 ≤ j ≤ n. The\r\ntranspose of A is denoted by At.\r\nThat is, if A =\r\n\"\r\n1 4 5\r\n0 1 2#\r\nthen At =\r\n\r\n\r\n\r\n1 0\r\n4 1\r\n5 2\r\n\r\n\r\n . Thus, the transpose of a row vector is a\r\ncolumn vector and vice-versa.\r\nTheorem 1.2.2. For any matrix A, (At)\r\nt = A.\r\nProof. Let A = [aij ], At = [bij ] and (At\r\n)\r\nt = [cij ]. Then, the definition of transpose gives\r\ncij = bji = aij for all i, j\r\nand the result follows.\r\nDefinition 1.2.3 (Addition of Matrices). let A = [aij ] and B = [bij ] be two m×n matrices.\r\nThen the sum A + B is defined to be the matrix C = [cij ] with cij = aij + bij .\r\nNote that, we define the sum of two matrices only when the order of the two matrices\r\nare same.\r\nDefinition 1.2.4 (Multiplying a Scalar to a Matrix). Let A = [aij ] be an m × n matrix.\r\nThen for any element k ∈ R, we define kA = [kaij ].\r\nFor example, if A =\r\n\"\r\n1 4 5\r\n0 1 2#\r\nand k = 5, then 5A =\r\n\"\r\n5 20 25\r\n0 5 10#\r\n.\r\nTheorem 1.2.5. Let A, B and C be matrices of order m × n, and let k, ℓ ∈ R. Then\r\n1. A + B = B + A (commutativity).\r\n2. (A + B) + C = A + (B + C) (associativity).\r\n3. k(ℓA) = (kℓ)A.\r\n4. (k + ℓ)A = kA + ℓA.\r\nProof. Part 1.\r\nLet A = [aij ] and B = [bij ]. Then\r\nA + B = [aij ] + [bij ] = [aij + bij ] = [bij + aij ] = [bij ] + [aij ] = B + A\r\nas real numbers commute.\r\nThe reader is required to prove the other parts as all the results follow from the prop\u0002erties of real numbers.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6019b916-d62b-4573-9a64-7a8f15001036.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=afcd64ad5799c723011637f9fcd7e84e1a92416e14ac62c4f17e578376102a76",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 409
      },
      {
        "segments": [
          {
            "segment_id": "cda1765e-5e8f-48a0-a1e6-c8d83e4c5e22",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "8 CHAPTER 1. INTRODUCTION TO MATRICES\r\nDefinition 1.2.6 (Additive Inverse). Let A be an m × n matrix.\r\n1. Then there exists a matrix B with A + B = 0. This matrix B is called the additive\r\ninverse of A, and is denoted by −A = (−1)A.\r\n2. Also, for the matrix 0m×n, A + 0 = 0 + A = A. Hence, the matrix 0m×n is called the\r\nadditive identity.\r\nExercise 1.2.7. 1. Find a 3 × 3 non-zero matrix A satisfying A = At.\r\n2. Find a 3 × 3 non-zero matrix A such that At = −A.\r\n3. Find the 3 × 3 matrix A = [aij ] satisfying aij = 1 if i 6= j and 2 otherwise.\r\n4. Find the 3 × 3 matrix A = [aij ] satisfying aij = 1 if |i − j| ≤ 1 and 0 otherwise.\r\n5. Find the 4 × 4 matrix A = [aij ] satisfying aij = i + j.\r\n6. Find the 4 × 4 matrix A = [aij ] satisfying aij = 2i+j\r\n.\r\n7. Suppose A + B = A. Then show that B = 0.\r\n8. Suppose A + B = 0. Then show that B = (−1)A = [−aij ].\r\n9. Let A =\r\n\r\n\r\n\r\n1 −1\r\n2 3\r\n0 1\r\n\r\n\r\n\r\nand B =\r\n\"\r\n2 3 −1\r\n1 1 2 #\r\n. Compute A + Bt and B + At\r\n.\r\n1.2.1 Multiplication of Matrices\r\nDefinition 1.2.8 (Matrix Multiplication / Product). Let A = [aij ] be an m × n matrix\r\nand B = [bij ] be an n × r matrix. The product AB is a matrix C = [cij ] of order m × r,\r\nwith\r\ncij =\r\nXn\r\nk=1\r\naikbkj = ai1b1j + ai2b2j + · · · + ainbnj .\r\nThat is, if Am×n =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n· · · · · · · · · · · ·\r\n· · · · · · · · · · · ·\r\nai1 ai2 · · · ain\r\n· · · · · · · · · · · ·\r\n· · · · · · · · · · · ·\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nand Bn×r =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\n.\r\n. b1j\r\n.\r\n.\r\n.\r\n.\r\n.\r\n. b2j\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n. bmj\r\n.\r\n.\r\n.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nthen\r\nAB = [(AB)ij ]m×r and (AB)ij = ai1b1j + ai2b2j + · · · + ainbnj .\r\nObserve that the product AB is defined if and only if\r\nthe number of columns of A = the number of rows of B.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/cda1765e-5e8f-48a0-a1e6-c8d83e4c5e22.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=355051d183e99763bf8cf7f57a03357c681177e1773feeb439d35eeffff5a3f8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 474
      },
      {
        "segments": [
          {
            "segment_id": "04620019-1973-40f4-8f72-23f83fcb2b47",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "1.2. OPERATIONS ON MATRICES 9\r\nFor example, if A =\r\n\"\r\na b c\r\nd e f#\r\nand B =\r\n\r\n\r\n\r\nα β γ δ\r\nx y z t\r\nu v w s\r\n\r\n\r\n\r\nthen\r\nAB =\r\n\"\r\naα + bx + cu aβ + by + cv aγ + bz + cw aδ + bt + cs\r\ndα + ex + f u dβ + ey + fv dγ + ez + fw dδ + et + fs#\r\n. (1.2.1)\r\nObserve that in Equation (1.2.1), the first row of AB can be re-written as\r\na ·\r\nh\r\nα β γ δi\r\n+ b ·\r\nh\r\nx y z ti\r\n+ c ·\r\nh\r\nu v w si\r\n.\r\nThat is, if Rowi(B) denotes the i-th row of B for 1 ≤ i ≤ 3, then the matrix product AB\r\ncan be re-written as\r\nAB =\r\n\"\r\na · Row1(B) + b · Row2(B) + c · Row3(B)\r\nd · Row1(B) + e · Row2(B) + f · Row3(B)\r\n#\r\n. (1.2.2)\r\nSimilarly, observe that if Colj (A) denotes the j-th column of A for 1 ≤ j ≤ 3, then the\r\nmatrix product AB can be re-written as\r\nAB =\r\nh\r\nCol1(A) · α + Col2(A) · x + Col3(A) · u,\r\nCol1(A) · β + Col2(A) · y + Col3(A) · v,\r\nCol1(A) · γ + Col2(A) · z + Col3(A) · w\r\nCol1(A) · δ + Col2(A) · t + Col3(A) · s] . (1.2.3)\r\nRemark 1.2.9. Observe the following:\r\n1. In this example, while AB is defined, the product BA is not defined.\r\nHowever, for square matrices A and B of the same order, both the product AB and\r\nBA are defined.\r\n2. The product AB corresponds to operating on the rows of the matrix B (see Equa\u0002tion (1.2.2)). This is row method for calculating the matrix product.\r\n3. The product AB also corresponds to operating on the columns of the matrix A (see\r\nEquation (1.2.3)). This is column method for calculating the matrix product.\r\n4. Let A = [aij ] and B = [bij ] be two matrices. Suppose a1, a2, . . . , an are the rows\r\nof A and b1, b2, . . . , bp are the columns of B. If the product AB is defined, then\r\ncheck that\r\nAB = [Ab1, Ab2, . . . , Abp] =\r\n\r\n\r\n\r\n\r\n\r\n\r\na1B\r\na2B\r\n.\r\n.\r\n.\r\nanB\r\n\r\n\r\n\r\n\r\n\r\n\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/04620019-1973-40f4-8f72-23f83fcb2b47.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d558de4dd0036fc0831b778a262493dc1f22a031e988d2f0676f6671f867810a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 421
      },
      {
        "segments": [
          {
            "segment_id": "aab6d827-dfbb-439c-a59c-b51bdeee0783",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "10 CHAPTER 1. INTRODUCTION TO MATRICES\r\nExample 1.2.10. Let A =\r\n\r\n\r\n\r\n1 2 0\r\n1 0 1\r\n0 −1 1\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n1 0 −1\r\n0 0 1\r\n0 −1 1\r\n\r\n\r\n. Use the row/column\r\nmethod of matrix multiplication to\r\n1. find the second row of the matrix AB.\r\nSolution: Observe that the second row of AB is obtained by multiplying the second\r\nrow of A with B. Hence, the second row of AB is\r\n1 · [1, 0, −1] + 0 · [0, 0, 1] + 1 · [0, −1, 1] = [1, −1, 0].\r\n2. find the third column of the matrix AB.\r\nSolution: Observe that the third column of AB is obtained by multiplying A with\r\nthe third column of B. Hence, the third column of AB is\r\n−1 ·\r\n\r\n\r\n\r\n1\r\n1\r\n0\r\n\r\n\r\n + 1 ·\r\n\r\n\r\n\r\n2\r\n0\r\n−1\r\n\r\n\r\n + 1 ·\r\n\r\n\r\n\r\n0\r\n1\r\n1\r\n\r\n\r\n =\r\n\r\n\r\n\r\n1\r\n0\r\n0\r\n\r\n\r\n .\r\nDefinition 1.2.11 (Commutativity of Matrix Product). Two square matrices A and B\r\nare said to commute if AB = BA.\r\nRemark 1.2.12. Note that if A is a square matrix of order n and if B is a scalar matrix of\r\norder n then AB = BA. In general, the matrix product is not commutative. For example,\r\nconsider A =\r\n\"\r\n1 1\r\n0 0#\r\nand B =\r\n\"\r\n1 0\r\n1 0#\r\n. Then check that the matrix product\r\nAB =\r\n\"\r\n2 0\r\n0 0#\r\n6=\r\n\"\r\n1 1\r\n1 1#\r\n= BA.\r\nTheorem 1.2.13. Suppose that the matrices A, B and C are so chosen that the matrix\r\nmultiplications are defined.\r\n1. Then (AB)C = A(BC). That is, the matrix multiplication is associative.\r\n2. For any k ∈ R, (kA)B = k(AB) = A(kB).\r\n3. Then A(B + C) = AB + AC. That is, multiplication distributes over addition.\r\n4. If A is an n × n matrix then AIn = InA = A.\r\n5. For any square matrix A of order n and D = diag(d1, d2, . . . , dn), we have\r\n• the first row of DA is d1 times the first row of A;\r\n• for 1 ≤ i ≤ n, the i\r\nth row of DA is di times the i\r\nth row of A.\r\nA similar statement holds for the columns of A when A is multiplied on the right by\r\nD.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/aab6d827-dfbb-439c-a59c-b51bdeee0783.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4dc3c1f89966b8092c331cb61227e3fc8f5bb67435e21c6d6073ed0be2a47f7b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 431
      },
      {
        "segments": [
          {
            "segment_id": "32d6d911-3def-41f2-ac3d-76c379f246ce",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 11,
            "page_width": 612,
            "page_height": 792,
            "content": "1.2. OPERATIONS ON MATRICES 11\r\nProof. Part 1. Let A = [aij ]m×n, B = [bij ]n×p and C = [cij ]p×q. Then\r\n(BC)kj =\r\nX\r\np\r\nℓ=1\r\nbkℓcℓj and (AB)iℓ =\r\nXn\r\nk=1\r\naikbkℓ.\r\nTherefore,\r\n\r\nA(BC)\r\n\u0001\r\nij =\r\nXn\r\nk=1\r\naikBC\u0001\r\nkj =\r\nXn\r\nk=1\r\naikX\r\np\r\nℓ=1\r\nbkℓcℓj\u0001\r\n=\r\nXn\r\nk=1\r\nX\r\np\r\nℓ=1\r\naikbkℓcℓj\u0001=\r\nXn\r\nk=1\r\nX\r\np\r\nℓ=1\r\n\r\naikbkℓ\u0001cℓj\r\n=\r\nX\r\np\r\nℓ=1\r\nXn\r\nk=1\r\naikbkℓ\u0001cℓj =\r\nX\r\nt\r\nℓ=1\r\n\r\nAB\u0001\r\niℓcℓj\r\n=\r\n\r\n(AB)C\r\n\u0001\r\nij .\r\nPart 5. For all j = 1, 2, . . . , n, we have\r\n(DA)ij =\r\nXn\r\nk=1\r\ndikakj = diaij\r\nas dik = 0 whenever i 6= k. Hence, the required result follows.\r\nThe reader is required to prove the other parts.\r\nExercise 1.2.14. 1. Find a 2 × 2 non-zero matrix A satisfying A2 = 0.\r\n2. Find a 2 × 2 non-zero matrix A satisfying A2 = A and A 6= I2.\r\n3. Find 2 × 2 non-zero matrices A, B and C satisfying AB = AC but B 6= C. That is,\r\nthe cancelation law doesn’t hold.\r\n4. Let A =\r\n\r\n\r\n\r\n0 1 0\r\n0 0 1\r\n1 0 0\r\n\r\n\r\n . Compute A + 3A2 − A3 and aA3 + bA + cA2\r\n.\r\n5. Let A and B be two matrices. If the matrix addition A + B is defined, then prove\r\nthat (A + B)\r\nt = At + Bt\r\n. Also, if the matrix product AB is defined then prove that\r\n(AB)\r\nt = BtAt\r\n.\r\n6. Let A = [a1, a2, . . . , an] and Bt = [b1, b2, . . . , bn]. Then check that order of AB is\r\n1 × 1, whereas BA has order n × n. Determine the matrix products AB and BA.\r\n7. Let A and B be two matrices such that the matrix product AB is defined.\r\n(a) If the first row of A consists entirely of zeros, prove that the first row of AB\r\nalso consists entirely of zeros.\r\n(b) If the first column of B consists entirely of zeros, prove that the first column of\r\nAB also consists entirely of zeros.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/32d6d911-3def-41f2-ac3d-76c379f246ce.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eb237e72eb0bc39635dda2c40f2052e3eec9ddd5eb7b489c13e1cb0b025ca94a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 369
      },
      {
        "segments": [
          {
            "segment_id": "3c431d6d-50dc-4c79-b6e4-5c7c4654d6c0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 12,
            "page_width": 612,
            "page_height": 792,
            "content": "12 CHAPTER 1. INTRODUCTION TO MATRICES\r\n(c) If A has two identical rows then the corresponding rows of AB are also identical.\r\n(d) If B has two identical columns then the corresponding columns of AB are also\r\nidentical.\r\n8. Let A =\r\n\r\n\r\n\r\n1 1 −2\r\n1 −2 1\r\n0 1 1\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n1 0\r\n0 1\r\n−1 1\r\n\r\n\r\n. Use the row/column method of matrix\r\nmultiplication to compute the\r\n(a) first row of the matrix AB.\r\n(b) third row of the matrix AB.\r\n(c) first column of the matrix AB.\r\n(d) second column of the matrix AB.\r\n(e) first column of BtAt\r\n.\r\n(f) third column of BtAt.\r\n(g) first row of BtAt.\r\n(h) second row of BtAt.\r\n9. Let A and B be the matrices given in Exercise 1.2.14.8. Compute A − At\r\n, (3AB)\r\nt −\r\n4BtA and 3A − 2At\r\n.\r\n10. Let n be a positive integer. Compute Anfor the following matrices:\r\n\"\r\n1 1\r\n0 1#\r\n,\r\n\r\n\r\n\r\n1 1 1\r\n0 1 1\r\n0 0 1\r\n\r\n\r\n,\r\n\r\n\r\n\r\n1 1 1\r\n1 1 1\r\n1 1 1\r\n\r\n\r\n .\r\nCan you guess a formula for An and prove it by induction?\r\n11. Construct the matrices A and B satisfying the following statements.\r\n(a) The matrix product AB is defined but BA is not defined.\r\n(b) The matrix products AB and BA are defined but they have different orders.\r\n(c) The matrix products AB and BA are defined and they have the same order but\r\nAB 6= BA.\r\n12. Let A be a 3 × 3 matrix satisfying A\r\n\r\n\r\n\r\na\r\nb\r\nc\r\n\r\n\r\n =\r\n\r\n\r\n\r\na + b\r\nb − c\r\n0\r\n\r\n\r\n. Determine the matrix A.\r\n13. Let A be a 2 × 2 matrix satisfying A\r\n\"\r\na\r\nb\r\n#\r\n=\r\n\"\r\na · b\r\na\r\n#\r\n. Can you construct the matrix A\r\nsatisfying the above? Why!",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/3c431d6d-50dc-4c79-b6e4-5c7c4654d6c0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=71aac00d53dcaf0056a4309c48f4511c58f6b9016131bd7e4e90b423d7f1ca0c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 345
      },
      {
        "segments": [
          {
            "segment_id": "508b0898-cec6-4a65-8b51-6ff82f3ea3d7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 13,
            "page_width": 612,
            "page_height": 792,
            "content": "1.2. OPERATIONS ON MATRICES 13\r\n1.2.2 Inverse of a Matrix\r\nDefinition 1.2.15 (Inverse of a Matrix). Let A be a square matrix of order n.\r\n1. A square matrix B is said to be a left inverse of A if BA = In.\r\n2. A square matrix C is called a right inverse of A, if AC = In.\r\n3. A matrix A is said to be invertible (or is said to have an inverse) if there exists\r\na matrix B such that AB = BA = In.\r\nLemma 1.2.16. Let A be an n × n matrix. Suppose that there exist n × n matrices B and\r\nC such that AB = In and CA = In, then B = C.\r\nProof. Note that\r\nC = CIn = C(AB) = (CA)B = InB = B.\r\nRemark 1.2.17. 1. From the above lemma, we observe that if a matrix A is invertible,\r\nthen the inverse is unique.\r\n2. As the inverse of a matrix A is unique, we denote it by A−1. That is, AA−1 =\r\nA−1A = I.\r\nExample 1.2.18. 1. Let A =\r\n\"\r\na b\r\nc d#\r\n.\r\n(a) If ad − bc 6= 0. Then verify that A−1 = 1\r\nad−bc \"\r\nd −b\r\n−c a #\r\n.\r\n(b) If ad−bc = 0 then prove that either [a b] = α[c d] for some α ∈ R or [a c] = β[b d]\r\nfor some β ∈ R. Hence, prove that A is not invertible.\r\n(c) In particular, the inverse of \"\r\n2 3\r\n4 7#\r\nequals 1\r\n2\r\n\"\r\n7 −3\r\n−4 2 #\r\n. Also, the matrices\r\n\"\r\n1 2\r\n0 0#\r\n,\r\n\"\r\n1 0\r\n4 0#\r\nand \"\r\n4 2\r\n6 3#\r\ndo not have inverses.\r\n2. Let A =\r\n\r\n\r\n\r\n1 2 3\r\n2 3 4\r\n3 4 6\r\n\r\n\r\n . Then A−1 =\r\n\r\n\r\n\r\n−2 0 1\r\n0 3 −2\r\n1 −2 1\r\n\r\n\r\n .\r\nTheorem 1.2.19. Let A and B be two matrices with inverses A−1 and B−1\r\n, respectively.\r\nThen\r\n1. (A−1\r\n)\r\n−1 = A.\r\n2. (AB)\r\n−1 = B−1A−1\r\n.\r\n3. (At)\r\n−1 = (A−1\r\n)\r\nt\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/508b0898-cec6-4a65-8b51-6ff82f3ea3d7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f3b32736dfffd4d4d5fcf4f6810f725341e54a747ccff73e07ec524e419e3177",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 370
      },
      {
        "segments": [
          {
            "segment_id": "baa3d5ce-ddaa-4f52-a72e-05f362a68cf5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 14,
            "page_width": 612,
            "page_height": 792,
            "content": "14 CHAPTER 1. INTRODUCTION TO MATRICES\r\nProof. Proof of Part 1.\r\nBy definition AA−1 = A−1A = I. Hence, if we denote A−1 by B, then we get AB = BA = I.\r\nThus, the definition, implies B−1 = A, or equivalently (A−1)\r\n−1 = A.\r\nProof of Part 2.\r\nVerify that (AB)(B−1A−1) = I = (B−1A−1)(AB).\r\nProof of Part 3.\r\nWe know AA−1 = A−1A = I. Taking transpose, we get\r\n(AA−1\r\n)\r\nt = (A−1A)t = It ⇐⇒ (A−1\r\n)\r\ntAt = At\r\n(A\r\n−1\r\n)\r\nt = I.\r\nHence, by definition (At)\r\n−1 = (A−1\r\n)\r\nt\r\n.\r\nWe will again come back to the study of invertible matrices in Sections 2.2 and 2.5.\r\nExercise 1.2.20. 1. Let A be an invertible matrix and let r be a positive integer. Prove\r\nthat (A−1\r\n)\r\nr = A−r\r\n.\r\n2. Find the inverse of \"\r\n− cos(θ) sin(θ)\r\nsin(θ) cos(θ)\r\n#\r\nand \"\r\ncos(θ) sin(θ)\r\n− sin(θ) cos(θ)\r\n#\r\n.\r\n3. Let A1, A2, . . . , Ar be invertible matrices. Prove that the product A1A2 · · · Ar is also\r\nan invertible matrix.\r\n4. Let x\r\nt = [1, 2, 3] and yt = [2, −1, 4]. Prove that xyt\r\nis not invertible even though x\r\nty\r\nis invertible.\r\n5. Let A be an n × n invertible matrix. Then prove that\r\n(a) A cannot have a row or column consisting entirely of zeros.\r\n(b) any two rows of A cannot be equal.\r\n(c) any two columns of A cannot be equal.\r\n(d) the third row of A cannot be equal to the sum of the first two rows, whenever\r\nn ≥ 3.\r\n(e) the third column of A cannot be equal to the first column minus the second\r\ncolumn, whenever n ≥ 3.\r\n6. Suppose A is a 2 × 2 matrix satisfying (I + 3A)\r\n−1 =\r\n\"\r\n1 2\r\n2 1#\r\n. Determine the matrix\r\nA.\r\n7. Let A be a 3×3 matrix such that (I −A)\r\n−1 =\r\n\r\n\r\n\r\n−2 0 1\r\n0 3 −2\r\n1 −2 1\r\n\r\n\r\n . Determine the matrix\r\nA [Hint: See Example 1.2.18.2 and Theorem 1.2.19.1].\r\n8. Let A be a square matrix satisfying A3 + A − 2I = 0. Prove that A−1 =\r\n1\r\n2\r\n\r\nA2 + I\r\n\u0001\r\n.\r\n9. Let A = [aij ] be an invertible matrix and let p be a nonzero real number. Then\r\ndetermine the inverse of the matrix B = [p\r\ni−jaij ].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/baa3d5ce-ddaa-4f52-a72e-05f362a68cf5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=776b5857ed39795a0118e6e8e0bc5b42c1afe9f64cdd0ebfc8fd5f58ebb64be1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 420
      },
      {
        "segments": [
          {
            "segment_id": "ab12e2f4-5906-4b0b-8017-899aae79c02e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 15,
            "page_width": 612,
            "page_height": 792,
            "content": "1.3. SOME MORE SPECIAL MATRICES 15\r\n1.3 Some More Special Matrices\r\nDefinition 1.3.1. 1. A matrix A over R is called symmetric if At = A and skew\u0002symmetric if At = −A.\r\n2. A matrix A is said to be orthogonal if AAt = AtA = I.\r\nExample 1.3.2. 1. Let A =\r\n\r\n\r\n\r\n1 2 3\r\n2 4 −1\r\n3 −1 4\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n0 1 2\r\n−1 0 −3\r\n−2 3 0\r\n\r\n\r\n. Then A is a\r\nsymmetric matrix and B is a skew-symmetric matrix.\r\n2. Let A =\r\n\r\n\r\n\r\n√\r\n1\r\n3\r\n√\r\n1\r\n3\r\n√\r\n1\r\n3\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n0\r\n√\r\n1\r\n6\r\n√\r\n1\r\n6\r\n− √\r\n2\r\n6\r\n\r\n\r\n . Then A is an orthogonal matrix.\r\n3. Let A = [aij ] be an n × n matrix with aij equal to 1 if i − j = 1 and 0, otherwise.\r\nThen An = 0 and Aℓ 6= 0 for 1 ≤ ℓ ≤ n − 1. The matrices A for which a positive\r\ninteger k exists such that Ak = 0 are called nilpotent matrices. The least positive\r\ninteger k for which Ak = 0 is called the order of nilpotency.\r\n4. Let A =\r\n\"\r\n1\r\n2\r\n1\r\n2\r\n1\r\n2\r\n1\r\n2\r\n#\r\n. Then A2 = A. The matrices that satisfy the condition that A2 = A\r\nare called idempotent matrices.\r\nExercise 1.3.3. 1. Let A be a real square matrix. Then S =\r\n1\r\n2\r\n(A + At) is symmetric,\r\nT = 1\r\n2\r\n(A − At) is skew-symmetric, and A = S + T.\r\n2. Show that the product of two lower triangular matrices is a lower triangular matrix.\r\nA similar statement holds for upper triangular matrices.\r\n3. Let A and B be symmetric matrices. Show that AB is symmetric if and only if\r\nAB = BA.\r\n4. Show that the diagonal entries of a skew-symmetric matrix are zero.\r\n5. Let A, B be skew-symmetric matrices with AB = BA. Is the matrix AB symmetric\r\nor skew-symmetric?\r\n6. Let A be a symmetric matrix of order n with A2 = 0. Is it necessarily true that\r\nA = 0?\r\n7. Let A be a nilpotent matrix. Prove that there exists a matrix B such that B(I +A) =\r\nI = (I + A)B [ Hint: If Ak = 0 then look at I − A + A2 − · · · + (−1)k−1Ak−1].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ab12e2f4-5906-4b0b-8017-899aae79c02e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=259a1d8fda619621b94d2f9e7be8b8830bbd650d6f9d337717b318b8b55aba20",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 426
      },
      {
        "segments": [
          {
            "segment_id": "663e3fb9-4ae0-462b-a6d8-b073dd189c65",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 16,
            "page_width": 612,
            "page_height": 792,
            "content": "16 CHAPTER 1. INTRODUCTION TO MATRICES\r\n1.3.1 Submatrix of a Matrix\r\nDefinition 1.3.4. A matrix obtained by deleting some of the rows and/or columns of a\r\nmatrix is said to be a submatrix of the given matrix.\r\nFor example, if A =\r\n\"\r\n1 4 5\r\n0 1 2#\r\n, a few submatrices of A are\r\n[1], [2],\r\n\"\r\n1\r\n0\r\n#\r\n, [1 5],\r\n\"\r\n1 5\r\n0 2#\r\n, A.\r\nBut the matrices \"\r\n1 4\r\n1 0#\r\nand \"\r\n1 4\r\n0 2#\r\nare not submatrices of A. (The reader is advised\r\nto give reasons.)\r\nLet A be an n × m matrix and B be an m × p matrix. Suppose r < m. Then, we can\r\ndecompose the matrices A and B as A = [P Q] and B =\r\n\"\r\nH\r\nK\r\n#\r\n; where P has order n × r\r\nand H has order r × p. That is, the matrices P and Q are submatrices of A and P consists\r\nof the first r columns of A and Q consists of the last m − r columns of A. Similarly, H\r\nand K are submatrices of B and H consists of the first r rows of B and K consists of the\r\nlast m − r rows of B. We now prove the following important theorem.\r\nTheorem 1.3.5. Let A = [aij ] = [P Q] and B = [bij ] = \"\r\nH\r\nK\r\n#\r\nbe defined as above. Then\r\nAB = P H + QK.\r\nProof. First note that the matrices P H and QK are each of order n × p. The matrix\r\nproducts P H and QK are valid as the order of the matrices P, H, Q and K are respectively,\r\nn × r, r × p, n × (m − r) and (m − r) × p. Let P = [Pij ], Q = [Qij ], H = [Hij ], and\r\nK = [kij ]. Then, for 1 ≤ i ≤ n and 1 ≤ j ≤ p, we have\r\n(AB)ij =\r\nXm\r\nk=1\r\naikbkj =\r\nXr\r\nk=1\r\naikbkj +\r\nXm\r\nk=r+1\r\naikbkj\r\n=\r\nXr\r\nk=1\r\nPikHkj +\r\nXm\r\nk=r+1\r\nQikKkj\r\n= (P H)ij + (QK)ij = (P H + QK)ij .\r\nRemark 1.3.6. Theorem 1.3.5 is very useful due to the following reasons:\r\n1. The order of the matrices P, Q, H and K are smaller than that of A or B.\r\n2. It may be possible to block the matrix in such a way that a few blocks are either\r\nidentity matrices or zero matrices. In this case, it may be easy to handle the matrix\r\nproduct using the block form.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/663e3fb9-4ae0-462b-a6d8-b073dd189c65.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8c50e3576dbf2a81e3d40d1a732fd94457c46d5e0d4188f5dd7c0c1cddbe314a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 447
      },
      {
        "segments": [
          {
            "segment_id": "b7d63e87-837f-421e-b112-b737bcbe3ac8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 17,
            "page_width": 612,
            "page_height": 792,
            "content": "1.3. SOME MORE SPECIAL MATRICES 17\r\n3. Or when we want to prove results using induction, then we may assume the result for\r\nr × r submatrices and then look for (r + 1) × (r + 1) submatrices, etc.\r\nFor example, if A =\r\n\"\r\n1 2 0\r\n2 5 0#\r\nand B =\r\n\r\n\r\n\r\na b\r\nc d\r\ne f\r\n\r\n\r\n , Then\r\nAB =\r\n\"\r\n1 2\r\n2 5# \"a bc d#\r\n+\r\n\"\r\n0\r\n0\r\n#\r\n[e f] = \"\r\na + 2c b + 2d\r\n2a + 5c 2b + 5d\r\n#\r\n.\r\nIf A =\r\n\r\n\r\n\r\n0 −1 2\r\n3 1 4\r\n−2 5 −3\r\n\r\n\r\n , then A can be decomposed as follows:\r\nA =\r\n\r\n\r\n\r\n0 −1 2\r\n3 1 4\r\n−2 5 −3\r\n\r\n\r\n , or A =\r\n\r\n\r\n\r\n0 −1 2\r\n3 1 4\r\n−2 5 −3\r\n\r\n\r\n , or\r\nA =\r\n\r\n\r\n\r\n0 −1 2\r\n3 1 4\r\n−2 5 −3\r\n\r\n\r\n\r\nand so on.\r\nSuppose A =\r\nm1 m2\r\nn1\r\nn2\r\n\"\r\nP Q\r\nR S#\r\nand B =\r\ns1 s2\r\nr1\r\nr2\r\n\"\r\nE F\r\nG H#\r\n. Then the matrices P, Q, R, S\r\nand E, F, G, H, are called the blocks of the matrices A and B, respectively.\r\nEven if A+B is defined, the orders of P and E may not be same and hence, we may\r\nnot be able to add A and B in the block form. But, if A + B and P + E is defined then\r\nA + B =\r\n\"\r\nP + E Q + F\r\nR + G S + H\r\n#\r\n.\r\nSimilarly, if the product AB is defined, the product P E need not be defined. Therefore,\r\nwe can talk of matrix product AB as block product of matrices, if both the products AB\r\nand P E are defined. And in this case, we have AB =\r\n\"\r\nP E + QG P F + QH\r\nRE + SG RF + SH #\r\n.\r\nThat is, once a partition of A is fixed, the partition of B has to be properly\r\nchosen for purposes of block addition or multiplication.\r\nExercise 1.3.7. 1. Complete the proofs of Theorems 1.2.5 and 1.2.13.\r\n2. Let A =\r\n\r\n\r\n\r\n1/2 0 0\r\n0 1 0\r\n0 0 1\r\n\r\n\r\n , B =\r\n\r\n\r\n\r\n1 0 0\r\n−2 1 0\r\n−3 0 1\r\n\r\n\r\n\r\nand C =\r\n\r\n\r\n\r\n2 2 2 6\r\n2 1 2 5\r\n3 3 4 10\r\n\r\n\r\n. Compute\r\n(a) the first row of AC,\r\n(b) the first row of B(AC),",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b7d63e87-837f-421e-b112-b737bcbe3ac8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3799d5d3cf174c78c279deb30f6f6442e447bffc82c50a0fc7dc8d77e4843933",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 466
      },
      {
        "segments": [
          {
            "segment_id": "c46a0382-5f35-429d-84e1-ac8a5defb6f6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 18,
            "page_width": 612,
            "page_height": 792,
            "content": "18 CHAPTER 1. INTRODUCTION TO MATRICES\r\n(c) the second row of B(AC), and\r\n(d) the third row of B(AC).\r\n(e) Let x\r\nt = [1, 1, 1, −1]. Compute the matrix product Cx.\r\n3. Let x =\r\n\"\r\nx1\r\nx2\r\n#\r\nand y =\r\n\"\r\ny1\r\ny2\r\n#\r\n. Determine the 2 × 2 matrix\r\n(a) A such that the y = Ax gives rise to counter-clockwise rotation through an angle\r\nα.\r\n(b) B such that y = Bx gives rise to the reflection along the line y = (tan γ)x.\r\nNow, let C and D be two 2× 2 matrices such that y = Cx gives rise to counter\u0002clockwise rotation through an angle β and y = Dx gives rise to the reflection\r\nalong the line y = (tan δ) x, respectively. Then prove that\r\n(c) y = (AC)x or y = (CA)x give rise to counter-clockwise rotation through an\r\nangle α + β.\r\n(d) y = (BD)x or y = (DB)x give rise to rotations. Which angles do they repre\u0002sent?\r\n(e) What can you say about y = (AB)x or y = (BA)x ?\r\n4. Let A =\r\n\"\r\n1 0\r\n0 −1\r\n#\r\n, B =\r\n\"\r\ncos α − sin α\r\nsin α cos α\r\n#\r\nand C =\r\n\"\r\ncos θ − sin θ\r\nsin θ cos θ\r\n#\r\n. If x =\r\n\"\r\nx1\r\nx2\r\n#\r\nand y =\r\n\"\r\ny1\r\ny2\r\n#\r\nthen geometrically interpret the following:\r\n(a) y = Ax, y = Bx and y = Cx.\r\n(b) y = (BC)x, y = (CB)x, y = (BA)x and y = (AB)x.\r\n5. Consider the two coordinate transformations\r\nx1 = a11y1 + a12y2\r\nx2 = a21y1 + a22y2\r\nand y1 = b11z1 + b12z2\r\ny2 = b21z1 + b22z2\r\n.\r\n(a) Compose the two transformations to express x1, x2 in terms of z1, z2.\r\n(b) If x\r\nt = [x1, x2], yt = [y1, y2] and zt = [z1, z2] then find matrices A, B and C\r\nsuch that x = Ay, y = Bz and x = Cz.\r\n(c) Is C = AB?\r\n6. Let A be an n × n matrix. Then trace of A, denoted tr(A), is defined as\r\ntr(A) = a11 + a22 + · · · ann.\r\n(a) Let A =\r\n\"\r\n3 2\r\n2 2#\r\nand B =\r\n\"\r\n4 −3\r\n−5 1 #\r\n. Compute tr(A) and tr(B).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c46a0382-5f35-429d-84e1-ac8a5defb6f6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d43d472d78a1e6533fc74b7091cdb94d650ad675638e9d358fe4d7ed228c7f3e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 402
      },
      {
        "segments": [
          {
            "segment_id": "38abef19-18c7-4481-8078-f5205fc22356",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 19,
            "page_width": 612,
            "page_height": 792,
            "content": "1.3. SOME MORE SPECIAL MATRICES 19\r\n(b) Then for two square matrices, A and B of the same order, prove that\r\ni. tr (A + B) = tr (A) + tr (B).\r\nii. tr (AB) = tr (BA).\r\n(c) Prove that there do not exist matrices A and B such that AB − BA = cIn for\r\nany c 6= 0.\r\n7. Let A and B be two m × n matrices with real entries. Then prove that\r\n(a) Ax = 0 for all n × 1 vector x with real entries implies A = 0, the zero matrix.\r\n(b) Ax = Bx for all n × 1 vector x with real entries implies A = B.\r\n8. Let A be an n × n matrix such that AB = BA for all n × n matrices B. Show that\r\nA = αI for some α ∈ R.\r\n9. Let A =\r\n\"\r\n1 2 3\r\n2 1 1#\r\n.\r\n(a) Find a matrix B such that AB = I2.\r\n(b) What can you say about the number of such matrices? Give reasons for your\r\nanswer.\r\n(c) Does there exist a matrix C such that CA = I3? Give reasons for your answer.\r\n10. Let A =\r\n\r\n\r\n\r\n\r\n\r\n1 0 0 1\r\n0 1 1 1\r\n0 1 1 0\r\n0 1 0 1\r\n\r\n\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n\r\n\r\n1 2 2 1\r\n1 1 2 1\r\n1 1 1 1\r\n−1 1 −1 1\r\n\r\n\r\n\r\n\r\n\r\n. Compute the matrix product\r\nAB using the block matrix multiplication.\r\n11. Let A =\r\n\"\r\nP Q\r\nR S#\r\n. If P, Q, R and S are symmetric, is the matrix A symmetric? If A\r\nis symmetric, is it necessary that the matrices P, Q, R and S are symmetric?\r\n12. Let A be an (n + 1) × (n + 1) matrix and let A =\r\n\"\r\nA11 A12\r\nA21 c\r\n#\r\n, where A11 is an n × n\r\ninvertible matrix and c is a real number.\r\n(a) If p = c − A21A\r\n−1\r\n11 A12 is non-zero, prove that\r\nB =\r\n\"\r\nA\r\n−1\r\n11 0\r\n0 0#\r\n+\r\n1\r\np\r\n\"\r\nA\r\n−1\r\n11 A12\r\n−1\r\n#\r\nh\r\nA21A\r\n−1\r\n11 −1\r\ni\r\nis the inverse of A.\r\n(b) Find the inverse of the matrices\r\n\r\n\r\n\r\n0 −1 2\r\n1 1 4\r\n−2 1 1\r\n\r\n\r\n\r\nand\r\n\r\n\r\n\r\n0 −1 2\r\n3 1 4\r\n−2 5 −3\r\n\r\n\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/38abef19-18c7-4481-8078-f5205fc22356.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3e43f06753242d8ed2f9f1c90854a6ec65c8542cf31ee9ddf7f9c1f26a4f5b6d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 434
      },
      {
        "segments": [
          {
            "segment_id": "399051c1-5e63-4011-8ef8-4f4bba90f62e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 20,
            "page_width": 612,
            "page_height": 792,
            "content": "20 CHAPTER 1. INTRODUCTION TO MATRICES\r\n13. Let x be an n × 1 matrix satisfying x\r\ntx = 1.\r\n(a) Define A = In − 2xxt. Prove that A is symmetric and A2 = I. The matrix A\r\nis commonly known as the Householder matrix.\r\n(b) Let α 6= 1 be a real number and define A = In−αxxt\r\n. Prove that A is symmetric\r\nand invertible [Hint: the inverse is also of the form In + βxxt\r\nfor some value of\r\nβ].\r\n14. Let A be an n × n invertible matrix and let x and y be two n × 1 matrices. Also,\r\nlet β be a real number such that α = 1 + βy\r\ntA−1x 6= 0. Then prove the famous\r\nShermon-Morrison formula\r\n(A + βxyt)\r\n−1 = A−1 −\r\nβ\r\nα\r\nA\r\n−1xytA−1\r\n.\r\nThis formula gives the information about the inverse when an invertible matrix is\r\nmodified by a rank one matrix.\r\n15. Let J be an n × n matrix having each entry 1.\r\n(a) Prove that J\r\n2 = nJ.\r\n(b) Let α1, α2, β1, β2 ∈ R. Prove that there exist α3, β3 ∈ R such that\r\n(α1In + β1J) · (α2In + β2J) = α3In + β3J.\r\n(c) Let α, β ∈ R with α 6= 0 and α + nβ 6= 0 and define A = αIn + βJ. Prove that\r\nA is invertible.\r\n16. Let A be an upper triangular matrix. If A∗A = AA∗then prove that A is a diagonal\r\nmatrix. The same holds for lower triangular matrix.\r\n1.4 Summary\r\nIn this chapter, we started with the definition of a matrix and came across lots of examples.\r\nIn particular, the following examples were important:\r\n1. The zero matrix of size m × n, denoted 0m×n or 0.\r\n2. The identity matrix of size n × n, denoted In or I.\r\n3. Triangular matrices\r\n4. Hermitian/Symmetric matrices\r\n5. Skew-Hermitian/skew-symmetric matrices\r\n6. Unitary/Orthogonal matrices\r\nWe also learnt product of two matrices. Even though it seemed complicated, it basically\r\ntells the following:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/399051c1-5e63-4011-8ef8-4f4bba90f62e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c831f7aff4d532488018d8d4f16e87b52dfa8af49d6b275e578d06a1660a24ae",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "c9a996c4-80aa-4042-ba37-b39d6118334d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 21,
            "page_width": 612,
            "page_height": 792,
            "content": "1.4. SUMMARY 21\r\n1. Multiplying by a matrix on the left to a matrix A is same as row operations.\r\n2. Multiplying by a matrix on the right to a matrix A is same as column operations.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c9a996c4-80aa-4042-ba37-b39d6118334d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=47c3e280e78febb2af29cda66170becb081dcbc6388b36eba7e5eb72e09845dd",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "bbb0b90d-77fe-4518-ac2a-475bad7e6967",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 22,
            "page_width": 612,
            "page_height": 792,
            "content": "22 CHAPTER 1. INTRODUCTION TO MATRICES",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/bbb0b90d-77fe-4518-ac2a-475bad7e6967.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=64c9a81e443258ca37de09d854ede30eeec2e288496ab1f8df7f84a60f0b4de4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 386
      },
      {
        "segments": [
          {
            "segment_id": "02fe0672-82a6-4229-925c-a034d5bdc778",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 23,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 2\r\nSystem of Linear Equations\r\n2.1 Introduction\r\nLet us look at some examples of linear systems.\r\n1. Suppose a, b ∈ R. Consider the system ax = b.\r\n(a) If a 6= 0 then the system has a unique solution x =\r\nb\r\na\r\n.\r\n(b) If a = 0 and\r\ni. b 6= 0 then the system has no solution.\r\nii. b = 0 then the system has infinite number of solutions, namely all\r\nx ∈ R.\r\n2. Consider a system with 2 equations in 2 unknowns. The equation ax + by = c\r\nrepresents a line in R\r\n2\r\nif either a 6= 0 or b 6= 0. Thus the solution set of the system\r\na1x + b1y = c1, a2x + b2y = c2\r\nis given by the points of intersection of the two lines. The different cases are illustrated\r\nby examples (see Figure 1).\r\n(a) Unique Solution\r\nx + 2y = 1 and x + 3y = 1. The unique solution is (x, y)\r\nt = (1, 0)t\r\n.\r\nObserve that in this case, a1b2 − a2b1 6= 0.\r\n(b) Infinite Number of Solutions\r\nx + 2y = 1 and 2x + 4y = 2. The solution set is (x, y)\r\nt = (1 − 2y, y)t =\r\n(1, 0)t + y(−2, 1)t with y arbitrary as both the equations represent the same\r\nline. Observe the following:\r\ni. Here, a1b2 − a2b1 = 0, a1c2 − a2c1 = 0 and b1c2 − b2c1 = 0.\r\nii. The vector (1, 0)tcorresponds to the solution x = 1, y = 0 of the given\r\nsystem whereas the vector (−2, 1)tcorresponds to the solution x = −2, y = 1\r\nof the system x + 2y = 0, 2x + 4y = 0.\r\n23",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/02fe0672-82a6-4229-925c-a034d5bdc778.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb2400ed3fc22e4c1ee33e320a644e5ad3e366d14e013df77a4796e23467184f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 296
      },
      {
        "segments": [
          {
            "segment_id": "03613d7a-b120-452b-8c35-a6dd3e50c3dc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 24,
            "page_width": 612,
            "page_height": 792,
            "content": "24 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n(c) No Solution\r\nx + 2y = 1 and 2x + 4y = 3. The equations represent a pair of parallel lines and\r\nhence there is no point of intersection. Observe that in this case, a1b2−a2b1 = 0\r\nbut a1c2 − a2c1 6= 0.\r\nℓ1\r\nℓ2\r\nNo Solution\r\nPair of Parallel lines\r\nℓ1 and ℓ2\r\nInfinite Number of Solutions\r\nCoincident Lines\r\nℓ1\r\nP ℓ2\r\nUnique Solution: Intersecting Lines\r\nP: Point of Intersection\r\nFigure 1 : Examples in 2 dimension.\r\n3. As a last example, consider 3 equations in 3 unknowns.\r\nA linear equation ax+by+cz = d represent a plane in R\r\n3 provided (a, b, c) 6= (0, 0, 0).\r\nHere, we have to look at the points of intersection of the three given planes.\r\n(a) Unique Solution\r\nConsider the system x + y + z = 3, x + 4y + 2z = 7 and 4x + 10y − z = 13. The\r\nunique solution to this system is (x, y, z)\r\nt = (1, 1, 1)t\r\n; i.e. the three planes\r\nintersect at a point.\r\n(b) Infinite Number of Solutions\r\nConsider the system x + y + z = 3, x + 2y + 2z = 5 and 3x + 4y + 4z = 11. The\r\nsolution set is (x, y, z)\r\nt = (1, 2−z, z)t = (1, 2, 0)t +z(0, −1, 1)t\r\n, with z arbitrary.\r\nObserve the following:\r\ni. Here, the three planes intersect in a line.\r\nii. The vector (1, 2, 0)t\r\ncorresponds to the solution x = 1, y = 2 and z = 0 of\r\nthe linear system x+y+z = 3, x+ 2y+ 2z = 5 and 3x+ 4y+ 4z = 11. Also,\r\nthe vector (0, −1, 1)tcorresponds to the solution x = 0, y = −1 and z = 1\r\nof the linear system x + y + z = 0, x + 2y + 2z = 0 and 3x + 4y + 4z = 0.\r\n(c) No Solution\r\nThe system x + y + z = 3, x + 2y + 2z = 5 and 3x + 4y + 4z = 13 has no\r\nsolution. In this case, we get three parallel lines as intersections of the above\r\nplanes, namely\r\ni. a line passing through (1, 2, 0) with direction ratios (0, −1, 1),\r\nii. a line passing through (3, 1, 0) with direction ratios (0, −1, 1), and\r\niii. a line passing through (−1, 4, 0) with direction ratios (0, −1, 1).\r\nThe readers are advised to supply the proof.\r\nDefinition 2.1.1 (Linear System). A system of m linear equations in n unknowns x1, x2, . . . , xn\r\nis a set of equations of the form",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/03613d7a-b120-452b-8c35-a6dd3e50c3dc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=39a54fb0c9525be78d749e91be89ba148d06e461556080dcb5ff60e4d529a12f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 456
      },
      {
        "segments": [
          {
            "segment_id": "e15758b1-603f-4c5d-8f72-85ede0248ac7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 25,
            "page_width": 612,
            "page_height": 792,
            "content": "2.1. INTRODUCTION 25\r\na11x1 + a12x2 + · · · + a1nxn = b1\r\na21x1 + a22x2 + · · · + a2nxn = b2\r\n.\r\n.\r\n.\r\n.\r\n.\r\n. (2.1.1)\r\nam1x1 + am2x2 + · · · + amnxn = bm\r\nwhere for 1 ≤ i ≤ n, and 1 ≤ j ≤ m; aij , bi ∈ R. Linear System (2.1.1) is called homoge\u0002neous if b1 = 0 = b2 = · · · = bm and non-homogeneous otherwise.\r\nWe rewrite the above equations in the form Ax = b, where\r\nA =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n\r\n\r\n\r\n\r\n, x =\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\n.\r\n.\r\n.\r\nxn\r\n\r\n\r\n\r\n\r\n\r\n\r\n, and b =\r\n\r\n\r\n\r\n\r\n\r\n\r\nb1\r\nb2\r\n.\r\n.\r\n.\r\nbm\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe matrix A is called the coefficient matrix and the block matrix [A b] , is called\r\nthe augmented matrix of the linear system (2.1.1).\r\nRemark 2.1.2. 1. The first column of the augmented matrix corresponds to the coeffi\u0002cients of the variable x1.\r\n2. In general, the j\r\nth column of the augmented matrix corresponds to the coefficients of\r\nthe variable xj , for j = 1, 2, . . . , n.\r\n3. The (n + 1)th column of the augmented matrix consists of the vector b.\r\n4. The i\r\nth row of the augmented matrix represents the ith equation for i = 1, 2, . . . , m.\r\nThat is, for i = 1, 2, . . . , m and j = 1, 2, . . . , n, the entry aij of the coefficient matrix\r\nA corresponds to the i\r\nth linear equation and the jth variable xj .\r\nDefinition 2.1.3. For a system of linear equations Ax = b, the system Ax = 0 is called\r\nthe associated homogeneous system.\r\nDefinition 2.1.4 (Solution of a Linear System). A solution of Ax = b is a column vector\r\ny with entries y1, y2, . . . , yn such that the linear system (2.1.1) is satisfied by substituting\r\nyiin place of xi. The collection of all solutions is called the solution set of the system.\r\nThat is, if y\r\nt = [y1, y2, . . . , yn] is a solution of the linear system Ax = b then Ay = b\r\nholds. For example, from Example 3.3a, we see that the vector y\r\nt = [1, 1, 1] is a solution\r\nof the system Ax = b, where A =\r\n\r\n\r\n\r\n1 1 1\r\n1 4 2\r\n4 10 −1\r\n\r\n\r\n, x\r\nt = [x, y, z] and bt = [3, 7, 13].\r\nWe now state a theorem about the solution set of a homogeneous system. The readers\r\nare advised to supply the proof.\r\nTheorem 2.1.5. Consider the homogeneous linear system Ax = 0. Then",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/e15758b1-603f-4c5d-8f72-85ede0248ac7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=744d2b9201c846041cafb758a55da7b8e95775d75c6727ffb8fbc00e54ebeddd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "e15758b1-603f-4c5d-8f72-85ede0248ac7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 25,
            "page_width": 612,
            "page_height": 792,
            "content": "2.1. INTRODUCTION 25\r\na11x1 + a12x2 + · · · + a1nxn = b1\r\na21x1 + a22x2 + · · · + a2nxn = b2\r\n.\r\n.\r\n.\r\n.\r\n.\r\n. (2.1.1)\r\nam1x1 + am2x2 + · · · + amnxn = bm\r\nwhere for 1 ≤ i ≤ n, and 1 ≤ j ≤ m; aij , bi ∈ R. Linear System (2.1.1) is called homoge\u0002neous if b1 = 0 = b2 = · · · = bm and non-homogeneous otherwise.\r\nWe rewrite the above equations in the form Ax = b, where\r\nA =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n\r\n\r\n\r\n\r\n, x =\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\n.\r\n.\r\n.\r\nxn\r\n\r\n\r\n\r\n\r\n\r\n\r\n, and b =\r\n\r\n\r\n\r\n\r\n\r\n\r\nb1\r\nb2\r\n.\r\n.\r\n.\r\nbm\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe matrix A is called the coefficient matrix and the block matrix [A b] , is called\r\nthe augmented matrix of the linear system (2.1.1).\r\nRemark 2.1.2. 1. The first column of the augmented matrix corresponds to the coeffi\u0002cients of the variable x1.\r\n2. In general, the j\r\nth column of the augmented matrix corresponds to the coefficients of\r\nthe variable xj , for j = 1, 2, . . . , n.\r\n3. The (n + 1)th column of the augmented matrix consists of the vector b.\r\n4. The i\r\nth row of the augmented matrix represents the ith equation for i = 1, 2, . . . , m.\r\nThat is, for i = 1, 2, . . . , m and j = 1, 2, . . . , n, the entry aij of the coefficient matrix\r\nA corresponds to the i\r\nth linear equation and the jth variable xj .\r\nDefinition 2.1.3. For a system of linear equations Ax = b, the system Ax = 0 is called\r\nthe associated homogeneous system.\r\nDefinition 2.1.4 (Solution of a Linear System). A solution of Ax = b is a column vector\r\ny with entries y1, y2, . . . , yn such that the linear system (2.1.1) is satisfied by substituting\r\nyiin place of xi. The collection of all solutions is called the solution set of the system.\r\nThat is, if y\r\nt = [y1, y2, . . . , yn] is a solution of the linear system Ax = b then Ay = b\r\nholds. For example, from Example 3.3a, we see that the vector y\r\nt = [1, 1, 1] is a solution\r\nof the system Ax = b, where A =\r\n\r\n\r\n\r\n1 1 1\r\n1 4 2\r\n4 10 −1\r\n\r\n\r\n, x\r\nt = [x, y, z] and bt = [3, 7, 13].\r\nWe now state a theorem about the solution set of a homogeneous system. The readers\r\nare advised to supply the proof.\r\nTheorem 2.1.5. Consider the homogeneous linear system Ax = 0. Then",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/e15758b1-603f-4c5d-8f72-85ede0248ac7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=744d2b9201c846041cafb758a55da7b8e95775d75c6727ffb8fbc00e54ebeddd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "2de47398-5ad0-441a-a7cb-8f18e7739ef7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 26,
            "page_width": 612,
            "page_height": 792,
            "content": "26 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n1. The zero vector, 0 = (0, . . . , 0)t\r\n, is always a solution, called the trivial solution.\r\n2. Suppose x1, x2 are two solutions of Ax = 0. Then k1x1 + k2x2 is also a solution of\r\nAx = 0 for any k1, k2 ∈ R.\r\nRemark 2.1.6. 1. A non-zero solution of Ax = 0 is called a non-trivial solution.\r\n2. If Ax = 0 has a non-trivial solution, say y 6= 0 then z = cy for every c ∈ R is also\r\na solution. Thus, the existence of a non-trivial solution of Ax = 0 is equivalent to\r\nhaving an infinite number of solutions for the system Ax = 0.\r\n3. If u, v are two distinct solutions of Ax = b then one has the following:\r\n(a) u − v is a solution of the system Ax = 0.\r\n(b) Define xh = u − v. Then xh is a solution of the homogeneous system Ax = 0.\r\n(c) That is, any two solutions of Ax = b differ by a solution of the associated\r\nhomogeneous system Ax = 0.\r\n(d) Or equivalently, the set of solutions of the system Ax = b is of the form, {x0 +\r\nxh}; where x0 is a particular solution of Ax = b and xh is a solution of the\r\nassociated homogeneous system Ax = 0.\r\n2.1.1 A Solution Method\r\nExample 2.1.7. Solve the linear system y + z = 2, 2x + 3z = 5, x + y + z = 3.\r\nSolution: In this case, the augmented matrix is\r\n\r\n\r\n\r\n0 1 1 2\r\n2 0 3 5\r\n1 1 1 3\r\n\r\n\r\n\r\nand the solution method\r\nproceeds along the following steps.\r\n1. Interchange 1st and 2nd equation.\r\n2x + 3z = 5\r\ny + z = 2\r\nx + y + z = 3\r\n\r\n\r\n\r\n2 0 3 5\r\n0 1 1 2\r\n1 1 1 3\r\n\r\n\r\n .\r\n2. Replace 1st equation by 1st equation times 1\r\n2\r\n.\r\nx +\r\n3\r\n2\r\nz =\r\n5\r\n2\r\ny + z = 2\r\nx + y + z = 3\r\n\r\n\r\n\r\n1 0 3\r\n2\r\n5\r\n2\r\n0 1 1 2\r\n1 1 1 3\r\n\r\n\r\n .\r\n3. Replace 3rd equation by 3rd equation minus the 1st equation.\r\nx +\r\n3\r\n2\r\nz =\r\n5\r\n2\r\ny + z = 2\r\ny −\r\n1\r\n2\r\nz =\r\n1\r\n2\r\n\r\n\r\n\r\n1 0 3\r\n2\r\n5\r\n2\r\n0 1 1 2\r\n0 1 −\r\n1\r\n2\r\n1\r\n2\r\n\r\n\r\n .",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/2de47398-5ad0-441a-a7cb-8f18e7739ef7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0c816540f72687e22a5d7d745ffebc1848b072a1abeb61d642c44b0369b6f50c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 450
      },
      {
        "segments": [
          {
            "segment_id": "6241490b-9f83-4bf6-933c-237c4c631fb4",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 27,
            "page_width": 612,
            "page_height": 792,
            "content": "2.1. INTRODUCTION 27\r\n4. Replace 3rd equation by 3rd equation minus the 2nd equation.\r\nx +\r\n3\r\n2\r\nz =\r\n5\r\n2\r\ny + z = 2\r\n−\r\n3\r\n2\r\nz = −\r\n3\r\n2\r\n\r\n\r\n\r\n1 0 3\r\n2\r\n5\r\n2\r\n0 1 1 2\r\n0 0 −\r\n3\r\n2 −\r\n3\r\n2\r\n\r\n\r\n.\r\n5. Replace 3rd equation by 3rd equation times −2\r\n3\r\n.\r\nx +\r\n3\r\n2\r\nz =\r\n5\r\n2\r\ny + z = 2\r\nz = 1\r\n\r\n\r\n\r\n1 0 3\r\n2\r\n5\r\n2\r\n0 1 1 2\r\n0 0 1 1\r\n\r\n\r\n .\r\nThe last equation gives z = 1. Using this, the second equation gives y = 1. Finally,\r\nthe first equation gives x = 1. Hence the solution set is {(x, y, z)\r\nt\r\n: (x, y, z) = (1, 1, 1)}, a\r\nunique solution.\r\nIn Example 2.1.7, observe that certain operations on equations (rows of the augmented\r\nmatrix) helped us in getting a system in Item 5, which was easily solvable. We use this\r\nidea to define elementary row operations and equivalence of two linear systems.\r\nDefinition 2.1.8 (Elementary Row Operations). Let A be an m × n matrix. Then the\r\nelementary row operations are defined as follows:\r\n1. Rij : Interchange of the i\r\nth and the jth row of A.\r\n2. For c 6= 0, Rk(c): Multiply the k\r\nth row of A by c.\r\n3. For c 6= 0, Rij (c): Replace the j\r\nth row of A by the jth row of A plus c times the ith\r\nrow of A.\r\nDefinition 2.1.9 (Equivalent Linear Systems). Let [A b] and [C d] be augmented ma\u0002trices of two linear systems. Then the two linear systems are said to be equivalent if [C d]\r\ncan be obtained from [A b] by application of a finite number of elementary row operations.\r\nDefinition 2.1.10 (Row Equivalent Matrices). Two matrices are said to be row-equivalent\r\nif one can be obtained from the other by a finite number of elementary row operations.\r\nThus, note that linear systems at each step in Example 2.1.7 are equivalent to each\r\nother. We also prove the following result that relates elementary row operations with the\r\nsolution set of a linear system.\r\nLemma 2.1.11. Let Cx = d be the linear system obtained from Ax = b by application of\r\na single elementary row operation. Then Ax = b and Cx = d have the same solution set.\r\nProof. We prove the result for the elementary row operation Rjk(c) with c 6= 0. The reader\r\nis advised to prove the result for other elementary operations.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6241490b-9f83-4bf6-933c-237c4c631fb4.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=458ca383bf306f7273906046e733fa09c639eedd61440378415d1c2493631c4c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 445
      },
      {
        "segments": [
          {
            "segment_id": "7aafa0d1-1192-4656-aaa0-2d95e3ea170a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 28,
            "page_width": 612,
            "page_height": 792,
            "content": "28 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nIn this case, the systems Ax = b and Cx = d vary only in the k\r\nth equation. Let\r\n(α1, α2, . . . , αn) be a solution of the linear system Ax = b. Then substituting for αi\r\n’s in\r\nplace of xi’s in the k\r\nth and jth equations, we get\r\nak1α1 + ak2α2 + · · · aknαn = bk, and aj1α1 + aj2α2 + · · · ajnαn = bj .\r\nTherefore,\r\n(ak1 + caj1)α1 + (ak2 + caj2)α2 + · · · + (akn + cajn)αn = bk + cbj . (2.1.2)\r\nBut then the k\r\nth equation of the linear system Cx = d is\r\n(ak1 + caj1)x1 + (ak2 + caj2)x2 + · · · + (akn + cajn)xn = bk + cbj . (2.1.3)\r\nTherefore, using Equation (2.1.2), (α1, α2, . . . , αn) is also a solution for k\r\nth Equation\r\n(2.1.3).\r\nUse a similar argument to show that if (β1, β2, . . . , βn) is a solution of the linear system\r\nCx = d then it is also a solution of the linear system Ax = b. Hence, the required result\r\nfollows.\r\nThe readers are advised to use Lemma 2.1.11 as an induction step to prove the main\r\nresult of this subsection which is stated next.\r\nTheorem 2.1.12. Two equivalent linear systems have the same solution set.\r\n2.1.2 Gauss Elimination Method\r\nWe first define the Gauss elimination method and give a few examples to understand the\r\nmethod.\r\nDefinition 2.1.13 (Forward/Gauss Elimination Method). The Gaussian elimination method\r\nis a procedure for solving a linear system Ax = b (consisting of m equations in n unknowns)\r\nby bringing the augmented matrix\r\n[A b] =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1m · · · a1n b1\r\na21 a22 · · · a2m · · · a2n b2\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amm · · · amn bm\r\n\r\n\r\n\r\n\r\n\r\n\r\nto an upper triangular form\r\n\r\n\r\n\r\n\r\n\r\n\r\nc11 c12 · · · c1m · · · c1n d1\r\n0 c22 · · · c2m · · · c2n d2\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 · · · cmm · · · cmn dm\r\n\r\n\r\n\r\n\r\n\r\n\r\nby application of elementary row operations. This elimination process is also called the\r\nforward elimination method.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/7aafa0d1-1192-4656-aaa0-2d95e3ea170a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6d4057e7ca47e4c84022c71eae3394c4b55581c6ad65a8f679693719a6b79b2d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 441
      },
      {
        "segments": [
          {
            "segment_id": "b8855721-2f43-4533-88a6-25b3ff39dcd9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 29,
            "page_width": 612,
            "page_height": 792,
            "content": "2.1. INTRODUCTION 29\r\nWe have already seen an example before defining the notion of row equivalence. We\r\ngive two more examples to illustrate the Gauss elimination method.\r\nExample 2.1.14. Solve the following linear system by Gauss elimination method.\r\nx + y + z = 3, x + 2y + 2z = 5, 3x + 4y + 4z = 11\r\nSolution: Let A =\r\n\r\n\r\n\r\n1 1 1\r\n1 2 2\r\n3 4 4\r\n\r\n\r\n\r\nand b =\r\n\r\n\r\n\r\n3\r\n5\r\n11\r\n\r\n\r\n\r\n. The Gauss Elimination method starts\r\nwith the augmented matrix [A b] and proceeds as follows:\r\n1. Replace 2nd equation by 2nd equation minus the 1st equation.\r\nx + y + z = 3\r\ny + z = 2\r\n3x + 4y + 4z = 11\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n3 4 4 11\r\n\r\n\r\n .\r\n2. Replace 3rd equation by 3rd equation minus 3 times 1st equation.\r\nx + y + z = 3\r\ny + z = 2\r\ny + z = 2\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n0 1 1 2\r\n\r\n\r\n.\r\n3. Replace 3rd equation by 3rd equation minus the 2nd equation.\r\nx + y + z = 3\r\ny + z = 2\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n0 0 0 0\r\n\r\n\r\n.\r\nThus, the solution set is {(x, y, z)\r\nt\r\n: (x, y, z) = (1, 2 − z, z)} or equivalently {(x, y, z)\r\nt\r\n:\r\n(x, y, z) = (1, 2, 0)+z(0, −1, 1)}, with z arbitrary. In other words, the system has infinite\r\nnumber of solutions. Observe that the vector y\r\nt = (1, 2, 0) satisfies Ay = b and the\r\nvector z\r\nt = (0, −1, 1) is a solution of the homogeneous system Ax = 0.\r\nExample 2.1.15. Solve the following linear system by Gauss elimination method.\r\nx + y + z = 3, x + 2y + 2z = 5, 3x + 4y + 4z = 12\r\nSolution: Let A =\r\n\r\n\r\n\r\n1 1 1\r\n1 2 2\r\n3 4 4\r\n\r\n\r\n\r\nand b =\r\n\r\n\r\n\r\n3\r\n5\r\n12\r\n\r\n\r\n\r\n. The Gauss Elimination method starts\r\nwith the augmented matrix [A b] and proceeds as follows:\r\n1. Replace 2nd equation by 2nd equation minus the 1st equation.\r\nx + y + z = 3\r\ny + z = 2\r\n3x + 4y + 4z = 12\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n3 4 4 12\r\n\r\n\r\n .",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b8855721-2f43-4533-88a6-25b3ff39dcd9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=266fc59d14a57bcde28f639f3cc04e593d46769cbb082345825ecf40e65c572b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 452
      },
      {
        "segments": [
          {
            "segment_id": "04d9127e-e158-40f0-a9e1-2cb8f6a356bf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 30,
            "page_width": 612,
            "page_height": 792,
            "content": "30 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n2. Replace 3rd equation by 3rd equation minus 3 times 1st equation.\r\nx + y + z = 3\r\ny + z = 2\r\ny + z = 3\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n0 1 1 3\r\n\r\n\r\n .\r\n3. Replace 3rd equation by 3rd equation minus the 2nd equation.\r\nx + y + z = 3\r\ny + z = 2\r\n0 = 1\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n0 0 0 1\r\n\r\n\r\n .\r\nThe third equation in the last step is\r\n0x + 0y + 0z = 1.\r\nThis can never hold for any value of x, y, z. Hence, the system has no solution.\r\nRemark 2.1.16. Note that to solve a linear system Ax = b, one needs to apply only the\r\nrow operations to the augmented matrix [A b].\r\nDefinition 2.1.17 (Row Echelon Form of a Matrix). A matrix C is said to be in the row\r\nechelon form if\r\n1. the rows consisting entirely of zeros appears after the non-zero rows,\r\n2. the first non-zero entry in a non-zero row is 1. This term is called the leading term\r\nor a leading 1. The column containing this term is called the leading column.\r\n3. In any two successive non-zero rows, the leading 1 in the lower row occurs farther to\r\nthe right than the leading 1 in the higher row.\r\nExample 2.1.18. The matrices\r\n\r\n\r\n\r\n0 1 4 2\r\n0 0 1 1\r\n0 0 0 0\r\n\r\n\r\n\r\nand\r\n\r\n\r\n\r\n1 1 0 2 3\r\n0 0 0 1 4\r\n0 0 0 0 1\r\n\r\n\r\n\r\nare in\r\nrow-echelon form. Whereas, the matrices\r\n\r\n\r\n\r\n0 1 4 2\r\n0 0 0 0\r\n0 0 1 1\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n1 1 0 2 3\r\n0 0 0 1 4\r\n0 0 0 0 2\r\n\r\n\r\n\r\nand\r\n\r\n\r\n\r\n1 1 0 2 3\r\n0 0 0 0 1\r\n0 0 0 1 4\r\n\r\n\r\n\r\nare not in row-echelon form.\r\nDefinition 2.1.19 (Basic, Free Variables). Let Ax = b be a linear system consisting of\r\nm equations in n unknowns. Suppose the application of Gauss elimination method to the\r\naugmented matrix [A b] yields the matrix [C d].\r\n1. Then the variables corresponding to the leading columns (in the first n columns of\r\n[C d]) are called the basic variables.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/04d9127e-e158-40f0-a9e1-2cb8f6a356bf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d088fe06f0c926546c45b084c1f289d22bcae91a7ddfe387cbecd3d90e6c1f91",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 428
      },
      {
        "segments": [
          {
            "segment_id": "ab092c6e-9f17-4565-809e-7961ac0e4a6d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 31,
            "page_width": 612,
            "page_height": 792,
            "content": "2.1. INTRODUCTION 31\r\n2. The variables which are not basic are called free variables.\r\nThe free variables are called so as they can be assigned arbitrary values. Also, the basic\r\nvariables can be written in terms of the free variables and hence the value of basic variables\r\nin the solution set depend on the values of the free variables.\r\nRemark 2.1.20. Observe the following:\r\n1. In Example 2.1.14, the solution set was given by\r\n(x, y, z) = (1, 2 − z, z) = (1, 2, 0) + z(0, −1, 1), with z arbitrary.\r\nThat is, we had x, y as two basic variables and z as a free variable.\r\n2. Example 2.1.15 didn’t have any solution because the row-echelon form of the aug\u0002mented matrix had a row of the form [0, 0, 0, 1].\r\n3. Suppose the application of row operations to [A b] yields the matrix [C d] which\r\nis in row echelon form. If [C d] has r non-zero rows then [C d] will consist of r\r\nleading terms or r leading columns. Therefore, the linear system Ax = b will\r\nhave r basic variables and n − r free variables.\r\nBefore proceeding further, we have the following definition.\r\nDefinition 2.1.21 (Consistent, Inconsistent). A linear system is called consistent if it\r\nadmits a solution and is called inconsistent if it admits no solution.\r\nWe are now ready to prove conditions under which the linear system Ax = b is consis\u0002tent or inconsistent.\r\nTheorem 2.1.22. Consider the linear system Ax = b, where A is an m × n matrix\r\nand x\r\nt = (x1, x2, . . . , xn). If one obtains [C d] as the row-echelon form of [A b] with\r\nd\r\nt = (d1, d2, . . . , dm) then\r\n1. Ax = b is inconsistent (has no solution) if [C d] has a row of the form [0\r\nt 1], where\r\n0\r\nt = (0, . . . , 0).\r\n2. Ax = b is consistent (has a solution) if [C d] has no row of the form [0\r\nt 1].\r\nFurthermore,\r\n(a) if the number of variables equals the number of leading terms then Ax = b has\r\na unique solution.\r\n(b) if the number of variables is strictly greater than the number of leading terms\r\nthen Ax = b has infinite number of solutions.\r\nProof. Part 1: The linear equation corresponding to the row [0\r\nt 1] equals\r\n0x1 + 0x2 + · · · + 0xn = 1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ab092c6e-9f17-4565-809e-7961ac0e4a6d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f863540036d56b4a7ddeee37da2f845c795cc8a830f4aa1036de11b63dcc6152",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 419
      },
      {
        "segments": [
          {
            "segment_id": "c77af145-241d-4d2d-ad54-b907fa684cd2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 32,
            "page_width": 612,
            "page_height": 792,
            "content": "32 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nObviously, this equation has no solution and hence the system Cx = d has no solution.\r\nThus, by Theorem 2.1.12, Ax = b has no solution. That is, Ax = b is inconsistent.\r\nPart 2: Suppose [C d] has r non-zero rows. As [C d] is in row echelon form there\r\nexist positive integers 1 ≤ i1 < i2 < . . . < ir ≤ n such that entries cℓiℓfor 1 ≤ ℓ ≤ r\r\nare leading terms. This in turn implies that the variables xij, for 1 ≤ j ≤ r are the basic\r\nvariables and the remaining n − r variables, say xt1\r\n, xt2, . . . , xtn−r, are free variables. So\r\nfor each ℓ, 1 ≤ ℓ ≤ r, one obtains xiℓ +\r\nP\r\nk>iℓ\r\ncℓkxk = dℓ (k > iℓin the summation as [C d]\r\nis a matrix in the row reduced echelon form). Or equivalently,\r\nxiℓ = dℓ −\r\nXr\r\nj=ℓ+1\r\ncℓijxij −\r\nnX−r\r\ns=1\r\ncℓts xtsfor 1 ≤ l ≤ r.\r\nHence, a solution of the system Cx = d is given by\r\nxts = 0 for s = 1, . . . , n − r and xir = dr, xir−1 = dr−1 − dr, . . . , xi1 = d1 −\r\nXr\r\nj=2\r\ncℓijdj .\r\nThus, by Theorem 2.1.12 the system Ax = b is consistent. In case of Part 2a, there are no\r\nfree variables and hence the unique solution is given by\r\nxn = dn, xn−1 = dn−1 − dn, . . . , x1 = d1 −\r\nXn\r\nj=2\r\ncℓijdj .\r\nIn case of Part 2b, there is at least one free variable and hence Ax = b has infinite number\r\nof solutions. Thus, the proof of the theorem is complete.\r\nWe omit the proof of the next result as it directly follows from Theorem 2.1.22.\r\nCorollary 2.1.23. Consider the homogeneous system Ax = 0. Then\r\n1. Ax = 0 is always consistent as 0 is a solution.\r\n2. If m < n then n−m > 0 and there will be at least n−m free variables. Thus Ax = 0\r\nhas infinite number of solutions. Or equivalently, Ax = 0 has a non-trivial solution.\r\nWe end this subsection with some applications related to geometry.\r\nExample 2.1.24. 1. Determine the equation of the line/circle that passes through the\r\npoints (−1, 4),(0, 1) and (1, 4).\r\nSolution: The general equation of a line/circle in 2-dimensional plane is given by\r\na(x\r\n2 + y2\r\n) + bx + cy + d = 0, where a, b, c and d are the unknowns. Since this curve\r\npasses through the given points, we have\r\na((−1)2 + 42) + (−1)b + 4c + d = = 0\r\na((0)2 + 12) + (0)b + 1c + d = = 0\r\na((1)2 + 42) + (1)b + 4c + d = = 0.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c77af145-241d-4d2d-ad54-b907fa684cd2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5fe8cf6ed2acf2a83fc4e6bc06fc0b816e98c22d9ca55648765d6968aa9d1cdd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 487
      },
      {
        "segments": [
          {
            "segment_id": "a5b9c68b-86cf-48e5-b14d-b3c70b9cc325",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 33,
            "page_width": 612,
            "page_height": 792,
            "content": "2.1. INTRODUCTION 33\r\nSolving this system, we get (a, b, c, d) = ( 3\r\n13 d, 0, −\r\n16\r\n13 d, d). Hence, taking d = 13, the\r\nequation of the required circle is\r\n3(x\r\n2 + y2\r\n) − 16y + 13 = 0.\r\n2. Determine the equation of the plane that contains the points (1, 1, 1),(1, 3, 2) and\r\n(2, −1, 2).\r\nSolution: The general equation of a plane in 3-dimensional space is given by ax +\r\nby +cz +d = 0, where a, b, c and d are the unknowns. Since this plane passes through\r\nthe given points, we have\r\na + b + c + d = = 0\r\na + 3b + 2c + d = = 0\r\n2a − b + 2c + d = = 0.\r\nSolving this system, we get (a, b, c, d) = (−\r\n4\r\n3\r\nd, −\r\nd\r\n3\r\n, −\r\n2\r\n3\r\nd, d). Hence, taking d = 3, the\r\nequation of the required plane is −4x − y + 2z + 3 = 0.\r\n3. Let A =\r\n\r\n\r\n\r\n2 3 4\r\n0 −1 0\r\n0 −3 4\r\n\r\n\r\n.\r\n(a) Find a non-zero x\r\nt ∈ R3\r\nsuch that Ax = 2x.\r\n(b) Does there exist a non-zero vector y\r\nt ∈ R3\r\nsuch that Ay = 4y?\r\nSolution of Part 3a: Solving for Ax = 2x is same as solving for (A − 2I)x = 0.\r\nThis leads to the augmented matrix\r\n\r\n\r\n\r\n0 3 4 0\r\n0 −3 0 0\r\n0 4 2 0\r\n\r\n\r\n . Check that a non-zero solution\r\nis given by x\r\nt = (1, 0, 0).\r\nSolution of Part 3b: Solving for Ay = 4y is same as solving for (A − 4I)y = 0.\r\nThis leads to the augmented matrix\r\n\r\n\r\n\r\n−2 3 4 0\r\n0 −5 0 0\r\n0 −3 0 0\r\n\r\n\r\n . Check that a non-zero solution\r\nis given by y\r\nt = (2, 0, 1).\r\nExercise 2.1.25. 1. Determine the equation of the curve y = ax2 + bx + c that passes\r\nthrough the points (−1, 4),(0, 1) and (1, 4).\r\n2. Solve the following linear system.\r\n(a) x + y + z + w = 0, x − y + z + w = 0 and −x + y + 3z + 3w = 0.\r\n(b) x + 2y = 1, x + y + z = 4 and 3y + 2z = 1.\r\n(c) x + y + z = 3, x + y − z = 1 and x + y + 7z = 6.\r\n(d) x + y + z = 3, x + y − z = 1 and x + y + 4z = 6.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a5b9c68b-86cf-48e5-b14d-b3c70b9cc325.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c3622cf8e362f80693472c4e50309a34428de069cbdaa10526a203240a46ced5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 470
      },
      {
        "segments": [
          {
            "segment_id": "792df338-78e2-4616-a669-1f4903727f8c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 34,
            "page_width": 612,
            "page_height": 792,
            "content": "34 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n(e) x + y + z = 3, x + y − z = 1, x + y + 4z = 6 and x + y − 4z = −1.\r\n3. For what values of c and k, the following systems have i) no solution, ii) a unique\r\nsolution and iii) infinite number of solutions.\r\n(a) x + y + z = 3, x + 2y + cz = 4, 2x + 3y + 2cz = k.\r\n(b) x + y + z = 3, x + y + 2cz = 7, x + 2y + 3cz = k.\r\n(c) x + y + 2z = 3, x + 2y + cz = 5, x + 2y + 4z = k.\r\n(d) kx + y + z = 1, x + ky + z = 1, x + y + kz = 1.\r\n(e) x + 2y − z = 1, 2x + 3y + kz = 3, x + ky + 3z = 2.\r\n(f) x − 2y = 1, x − y + kz = 1, ky + 4z = 6.\r\n4. For what values of a, does the following systems have i) no solution, ii) a unique\r\nsolution and iii) infinite number of solutions.\r\n(a) x + 2y + 3z = 4, 2x + 5y + 5z = 6, 2x + (a\r\n2 − 6)z = a + 20.\r\n(b) x + y + z = 3, 2x + 5y + 4z = a, 3x + (a\r\n2 − 8)z = 12.\r\n5. Find the condition(s) on x, y, z so that the system of linear equations given below (in\r\nthe unknowns a, b and c) is consistent?\r\n(a) a + 2b − 3c = x, 2a + 6b − 11c = y, a − 2b + 7c = z\r\n(b) a + b + 5c = x, a + 3c = y, 2a − b + 4c = z\r\n(c) a + 2b + 3c = x, 2a + 4b + 6c = y, 3a + 6b + 9c = z\r\n6. Let A be an n×n matrix. If the system A2x = 0 has a non trivial solution then show\r\nthat Ax = 0 also has a non trivial solution.\r\n7. Prove that we need to have 5 set of distinct points to specify a general conic in 2-\r\ndimensional plane.\r\n8. Let u\r\nt = (1, 1, −2) and vt = (−1, 2, 3). Find condition on x, y and z such that the\r\nsystem cu\r\nt + dvt = (x, y, z) in the unknowns c and d is consistent.\r\n2.1.3 Gauss-Jordan Elimination\r\nThe Gauss-Jordan method consists of first applying the Gauss Elimination method to get\r\nthe row-echelon form of the matrix [A b] and then further applying the row operations\r\nas follows. For example, consider Example 2.1.7. We start with Step 5 and apply row\r\noperations once again. But this time, we start with the 3rd row.\r\nI. Replace 2nd equation by 2nd equation minus the 3rd equation.\r\nx +\r\n3\r\n2\r\nz =\r\n5\r\n2\r\ny = 2\r\nz = 1\r\n\r\n\r\n\r\n1 0 3\r\n2\r\n5\r\n2\r\n0 1 0 1\r\n0 0 1 1\r\n\r\n\r\n .",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/792df338-78e2-4616-a669-1f4903727f8c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6a6cec8a2ab1c66e44289dd38476500c9c52f429ae525d6b0c6d960381f158e7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 548
      },
      {
        "segments": [
          {
            "segment_id": "792df338-78e2-4616-a669-1f4903727f8c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 34,
            "page_width": 612,
            "page_height": 792,
            "content": "34 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n(e) x + y + z = 3, x + y − z = 1, x + y + 4z = 6 and x + y − 4z = −1.\r\n3. For what values of c and k, the following systems have i) no solution, ii) a unique\r\nsolution and iii) infinite number of solutions.\r\n(a) x + y + z = 3, x + 2y + cz = 4, 2x + 3y + 2cz = k.\r\n(b) x + y + z = 3, x + y + 2cz = 7, x + 2y + 3cz = k.\r\n(c) x + y + 2z = 3, x + 2y + cz = 5, x + 2y + 4z = k.\r\n(d) kx + y + z = 1, x + ky + z = 1, x + y + kz = 1.\r\n(e) x + 2y − z = 1, 2x + 3y + kz = 3, x + ky + 3z = 2.\r\n(f) x − 2y = 1, x − y + kz = 1, ky + 4z = 6.\r\n4. For what values of a, does the following systems have i) no solution, ii) a unique\r\nsolution and iii) infinite number of solutions.\r\n(a) x + 2y + 3z = 4, 2x + 5y + 5z = 6, 2x + (a\r\n2 − 6)z = a + 20.\r\n(b) x + y + z = 3, 2x + 5y + 4z = a, 3x + (a\r\n2 − 8)z = 12.\r\n5. Find the condition(s) on x, y, z so that the system of linear equations given below (in\r\nthe unknowns a, b and c) is consistent?\r\n(a) a + 2b − 3c = x, 2a + 6b − 11c = y, a − 2b + 7c = z\r\n(b) a + b + 5c = x, a + 3c = y, 2a − b + 4c = z\r\n(c) a + 2b + 3c = x, 2a + 4b + 6c = y, 3a + 6b + 9c = z\r\n6. Let A be an n×n matrix. If the system A2x = 0 has a non trivial solution then show\r\nthat Ax = 0 also has a non trivial solution.\r\n7. Prove that we need to have 5 set of distinct points to specify a general conic in 2-\r\ndimensional plane.\r\n8. Let u\r\nt = (1, 1, −2) and vt = (−1, 2, 3). Find condition on x, y and z such that the\r\nsystem cu\r\nt + dvt = (x, y, z) in the unknowns c and d is consistent.\r\n2.1.3 Gauss-Jordan Elimination\r\nThe Gauss-Jordan method consists of first applying the Gauss Elimination method to get\r\nthe row-echelon form of the matrix [A b] and then further applying the row operations\r\nas follows. For example, consider Example 2.1.7. We start with Step 5 and apply row\r\noperations once again. But this time, we start with the 3rd row.\r\nI. Replace 2nd equation by 2nd equation minus the 3rd equation.\r\nx +\r\n3\r\n2\r\nz =\r\n5\r\n2\r\ny = 2\r\nz = 1\r\n\r\n\r\n\r\n1 0 3\r\n2\r\n5\r\n2\r\n0 1 0 1\r\n0 0 1 1\r\n\r\n\r\n .",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/792df338-78e2-4616-a669-1f4903727f8c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6a6cec8a2ab1c66e44289dd38476500c9c52f429ae525d6b0c6d960381f158e7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 548
      },
      {
        "segments": [
          {
            "segment_id": "6dce9854-c9e6-4020-879d-d0d54cc35e2c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 35,
            "page_width": 612,
            "page_height": 792,
            "content": "2.1. INTRODUCTION 35\r\nII. Replace 1st equation by 1st equation minus 3\r\n2\r\ntimes 3rd equation.\r\nx = 1\r\ny = 1\r\nz = 1\r\n\r\n\r\n\r\n1 0 0 1\r\n0 1 0 1\r\n0 0 1 1\r\n\r\n\r\n.\r\nIII. Thus, the solution set equals {(x, y, z)\r\nt\r\n: (x, y, z) = (1, 1, 1)}.\r\nDefinition 2.1.26 (Row-Reduced Echelon Form). A matrix C is said to be in the row\u0002reduced echelon form or reduced row echelon form if\r\n1. C is already in the row echelon form;\r\n2. the leading column containing the leading 1 has every other entry zero.\r\nA matrix which is in the row-reduced echelon form is also called a row-reduced echelon\r\nmatrix.\r\nExample 2.1.27. Let A =\r\n\r\n\r\n\r\n0 1 4 2\r\n0 0 1 1\r\n0 0 0 0\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n1 1 0 2 3\r\n0 0 0 1 4\r\n0 0 0 0 1\r\n\r\n\r\n\r\n. Then A\r\nand B are in row echelon form. If C and D are the row-reduced echelon forms of A and\r\nB, respectively then C =\r\n\r\n\r\n\r\n0 1 0 −2\r\n0 0 1 1\r\n0 0 0 0\r\n\r\n\r\n\r\nand D =\r\n\r\n\r\n\r\n1 1 0 0 0\r\n0 0 0 1 0\r\n0 0 0 0 1\r\n\r\n\r\n .\r\nDefinition 2.1.28 (Back Substitution/Gauss-Jordan Method). The procedure to get The\r\nrow-reduced echelon matrix from the row-echelon matrix is called the back substitution.\r\nThe elimination process applied to obtain the row-reduced echelon form of the augmented\r\nmatrix is called the Gauss-Jordan elimination method.\r\nThat is, the Gauss-Jordan elimination method consists of both the forward elimination\r\nand the backward substitution.\r\nRemark 2.1.29. Note that the row reduction involves only row operations and proceeds\r\nfrom left to right. Hence, if A is a matrix consisting of first s columns of a matrix C,\r\nthen the row-reduced form of A will consist of the first s columns of the row-reduced form\r\nof C.\r\nThe proof of the following theorem is beyond the scope of this book and is omitted.\r\nTheorem 2.1.30. The row-reduced echelon form of a matrix is unique.\r\nRemark 2.1.31. Consider the linear system Ax = b. Then Theorem 2.1.30 implies the\r\nfollowing:\r\n1. The application of the Gauss Elimination method to the augmented matrix may yield\r\ndifferent matrices even though it leads to the same solution set.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6dce9854-c9e6-4020-879d-d0d54cc35e2c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=566cdc6c0b47acb78446b6d4f49a0f23f1313ebe6ada9bccac1695c066b3cf99",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 415
      },
      {
        "segments": [
          {
            "segment_id": "9463aadd-b56e-4a1a-a155-0802057ec2e7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 36,
            "page_width": 612,
            "page_height": 792,
            "content": "36 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n2. The application of the Gauss-Jordan method to the augmented matrix yields the same\r\nmatrix and also the same solution set even though we may have used different sequence\r\nof row operations.\r\nExample 2.1.32. Consider Ax = b, where A is a 3 × 3 matrix. Let [C d] be the row\u0002reduced echelon form of [A b]. Also, assume that the first column of A has a non-zero\r\nentry. Then the possible choices for the matrix [C d] with respective solution sets are given\r\nbelow:\r\n1.\r\n\r\n\r\n\r\n1 0 0 d1\r\n0 1 0 d2\r\n0 0 1 d3\r\n\r\n\r\n. Ax = b has a unique solution, (x, y, z) = (d1, d2, d3).\r\n2.\r\n\r\n\r\n\r\n1 0 α d1\r\n0 1 β d2\r\n0 0 0 1\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n1 α 0 d1\r\n0 0 1 d2\r\n0 0 0 1\r\n\r\n\r\n\r\nor\r\n\r\n\r\n\r\n1 α β d1\r\n0 0 0 1\r\n0 0 0 0\r\n\r\n\r\n. Ax = b has no solution for\r\nany choice of α, β.\r\n3.\r\n\r\n\r\n\r\n1 0 α d1\r\n0 1 β d2\r\n0 0 0 0\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n1 α 0 d1\r\n0 0 1 d2\r\n0 0 0 0\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n1 α β d1\r\n0 0 0 0\r\n0 0 0 0\r\n\r\n\r\n. Ax = b has Infinite number\r\nof solutions for every choice of α, β.\r\nExercise 2.1.33. 1. Let Ax = b be a linear system in 2 unknowns. What are the\r\npossible choices for the row-reduced echelon form of the augmented matrix [A b]?\r\n2. Find the row-reduced echelon form of the following matrices:\r\n\r\n\r\n\r\n0 0 1\r\n1 0 3\r\n3 0 7\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n0 1 1 3\r\n0 0 1 3\r\n1 1 0 0\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n0 −1 1\r\n−2 0 3\r\n−5 1 0\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n\r\n\r\n−1 −1 −2 3\r\n3 3 −3 −3\r\n1 1 2 2\r\n−1 −1 2 −2\r\n\r\n\r\n\r\n\r\n\r\n.\r\n3. Find all the solutions of the following system of equations using Gauss-Jordan method.\r\nNo other method will be accepted.\r\nx + y – 2 u + v = 2\r\nz + u + 2 v = 3\r\nv + w = 3\r\nv + 2 w = 5\r\n2.2 Elementary Matrices\r\nIn the previous section, we solved a system of linear equations with the help of either the\r\nGauss Elimination method or the Gauss-Jordan method. These methods required us to\r\nmake row operations on the augmented matrix. Also, we know that (see Section 1.2.1 )",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9463aadd-b56e-4a1a-a155-0802057ec2e7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c4e00a591c4a263d5887425862194d216fa8fe465c0fe4238c3131b8621c8822",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 479
      },
      {
        "segments": [
          {
            "segment_id": "5515d804-2487-42f4-a479-573ca00a5161",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 37,
            "page_width": 612,
            "page_height": 792,
            "content": "2.2. ELEMENTARY MATRICES 37\r\nthe row-operations correspond to multiplying a matrix on the left. So, in this section, we\r\ntry to understand the matrices which helped us in performing the row-operations and also\r\nuse this understanding to get some important results in the theory of square matrices.\r\nDefinition 2.2.1. A square matrix E of order n is called an elementary matrix if it\r\nis obtained by applying exactly one row operation to the identity matrix, In.\r\nRemark 2.2.2. Fix a positive integer n. Then the elementary matrices of order n are of\r\nthree types and are as follows:\r\n1. Eij corresponds to the interchange of the i\r\nth and the jth row of In.\r\n2. For c 6= 0, Ek(c) is obtained by multiplying the k\r\nth row of In by c.\r\n3. For c 6= 0, Eij (c) is obtained by replacing the j\r\nth row of In by the jth row of In plus\r\nc times the i\r\nth row of In.\r\nExample 2.2.3. 1. In particular, for n = 3 and a real number c 6= 0, one has\r\nE23 =\r\n\r\n\r\n\r\n1 0 0\r\n0 0 1\r\n0 1 0\r\n\r\n\r\n , E1(c) =\r\n\r\n\r\n\r\nc 0 0\r\n0 1 0\r\n0 0 1\r\n\r\n\r\n , and E32(c) =\r\n\r\n\r\n\r\n1 0 0\r\n0 1 c\r\n0 0 1\r\n\r\n\r\n .\r\n2. Let A =\r\n\r\n\r\n\r\n1 2 3 0\r\n2 0 3 4\r\n3 4 5 6\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n1 2 3 0\r\n3 4 5 6\r\n2 0 3 4\r\n\r\n\r\n . Then B is obtained from A by the\r\ninterchange of 2\r\nnd and 3rd row. Verify that\r\nE23A =\r\n\r\n\r\n\r\n1 0 0\r\n0 0 1\r\n0 1 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 2 3 0\r\n2 0 3 4\r\n3 4 5 6\r\n\r\n\r\n =\r\n\r\n\r\n\r\n1 2 3 0\r\n3 4 5 6\r\n2 0 3 4\r\n\r\n\r\n = B.\r\n3. Let A =\r\n\r\n\r\n\r\n0 1 1 2\r\n2 0 3 5\r\n1 1 1 3\r\n\r\n\r\n . Then B =\r\n\r\n\r\n\r\n1 0 0 1\r\n0 1 0 1\r\n0 0 1 1\r\n\r\n\r\n is the row-reduced echelon form of\r\nA. The readers are advised to verify that\r\nB = E32(−1) · E21(−1) · E3(1/3) · E23(2) · E23 · E12(−2) · E13 · A.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5515d804-2487-42f4-a479-573ca00a5161.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0a429089460b4d745c30a74f70c5980c0ff41181c154317922bf4e0542f0d7b9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 429
      },
      {
        "segments": [
          {
            "segment_id": "9bc998cc-f66f-4977-8722-01e538bebb08",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 38,
            "page_width": 612,
            "page_height": 792,
            "content": "38 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nOr equivalently, check that\r\nE13A = A1 =\r\n\r\n\r\n\r\n1 1 1 3\r\n2 0 3 5\r\n0 1 1 2\r\n\r\n\r\n, E12(−2)A1 = A2 =\r\n\r\n\r\n\r\n1 1 1 3\r\n0 −2 1 −1\r\n0 1 1 2\r\n\r\n\r\n ,\r\nE23A2 = A3 =\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n0 −2 1 −1\r\n\r\n\r\n , E23(2)A3 = A4 =\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n0 0 3 3\r\n\r\n\r\n,\r\nE3(1/3)A4 = A5 =\r\n\r\n\r\n\r\n1 1 1 3\r\n0 1 1 2\r\n0 0 1 1\r\n\r\n\r\n, E21(−1)A5 = A6 =\r\n\r\n\r\n\r\n1 0 0 1\r\n0 1 1 2\r\n0 0 1 1\r\n\r\n\r\n ,\r\nE32(−1)A6 = B =\r\n\r\n\r\n\r\n1 0 0 1\r\n0 1 0 1\r\n0 0 1 1\r\n\r\n\r\n.\r\nRemark 2.2.4. Observe the following:\r\n1. The inverse of the elementary matrix Eij is the matrix Eij itself. That is, EijEij =\r\nI = EijEij .\r\n2. Let c 6= 0. Then the inverse of the elementary matrix Ek(c) is the matrix Ek(1/c).\r\nThat is, Ek(c)Ek(1/c) = I = Ek(1/c)Ek(c).\r\n3. Let c 6= 0. Then the inverse of the elementary matrix Eij (c) is the matrix Eij (−c).\r\nThat is, Eij (c)Eij (−c) = I = Eij (−c)Eij (c).\r\nThat is, all the elementary matrices are invertible and the inverses are also elemen\u0002tary matrices.\r\n4. Suppose the row-reduced echelon form of the augmented matrix [A b] is the matrix\r\n[C d]. As row operations correspond to multiplying on the left with elementary\r\nmatrices, we can find elementary matrices, say E1, E2, . . . , Ek, such that\r\nEk · Ek−1 · · · E2 · E1 · [A b] = [C d].\r\nThat is, the Gauss-Jordan method (or Gauss Elimination method) is equivalent to\r\nmultiplying by a finite number of elementary matrices on the left to [A b].\r\nWe are now ready to prove a equivalent statements in the study of invertible matrices.\r\nTheorem 2.2.5. Let A be a square matrix of order n. Then the following statements are\r\nequivalent.\r\n1. A is invertible.\r\n2. The homogeneous system Ax = 0 has only the trivial solution.\r\n3. The row-reduced echelon form of A is In.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9bc998cc-f66f-4977-8722-01e538bebb08.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6e33d88d822b0daa75014a96c6c77740019a93b68f6fcc27d61c6d680ad851d8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 405
      },
      {
        "segments": [
          {
            "segment_id": "34a90b2d-9728-488b-96e1-b41d013dfec3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 39,
            "page_width": 612,
            "page_height": 792,
            "content": "2.2. ELEMENTARY MATRICES 39\r\n4. A is a product of elementary matrices.\r\nProof. 1 =⇒ 2\r\nAs A is invertible, we have A−1A = In = AA−1. Let x0 be a solution of the homoge\u0002neous system Ax = 0. Then, Ax0 = 0 and Thus, we see that 0 is the only solution of the\r\nhomogeneous system Ax = 0.\r\n2 =⇒ 3\r\nLet x\r\nt = [x1, x2, . . . , xn]. As 0 is the only solution of the linear system Ax = 0, the\r\nfinal equations are x1 = 0, x2 = 0, . . . , xn = 0. These equations can be rewritten as\r\n1 · x1 + 0 · x2 + 0 · x3 + · · · + 0 · xn = 0\r\n0 · x1 + 1 · x2 + 0 · x3 + · · · + 0 · xn = 0\r\n0 · x1 + 0 · x2 + 1 · x3 + · · · + 0 · xn = 0\r\n.\r\n.\r\n. =\r\n.\r\n.\r\n.\r\n0 · x1 + 0 · x2 + 0 · x3 + · · · + 1 · xn = 0.\r\nThat is, the final system of homogeneous system is given by In · x = 0. Or equivalently,\r\nthe row-reduced echelon form of the augmented matrix [A 0] is [In 0]. That is, the\r\nrow-reduced echelon form of A is In.\r\n3 =⇒ 4\r\nSuppose that the row-reduced echelon form of A is In. Then using Remark 2.2.4.4,\r\nthere exist elementary matrices E1, E2, . . . , Ek such that\r\nE1E2 · · · EkA = In. (2.2.4)\r\nNow, using Remark 2.2.4, the matrix E\r\n−1\r\nj\r\nis an elementary matrix and is the inverse of\r\nEj for 1 ≤ j ≤ k. Therefore, successively multiplying Equation (2.2.4) on the left by\r\nE\r\n−1\r\n1\r\n, E−1\r\n2\r\n, . . . , E−1\r\nk\r\n, we get\r\nA = E\r\n−1\r\nk E\r\n−1\r\nk−1\r\n· · · E\r\n−1\r\n2 E\r\n−1\r\n1\r\nand thus A is a product of elementary matrices.\r\n4 =⇒ 1\r\nSuppose A = E1E2 · · · Ek; where the Ei’s are elementary matrices. As the elementary\r\nmatrices are invertible (see Remark 2.2.4) and the product of invertible matrices is also\r\ninvertible, we get the required result.\r\nAs an immediate consequence of Theorem 2.2.5, we have the following important result.\r\nTheorem 2.2.6. Let A be a square matrix of order n.\r\n1. Suppose there exists a matrix C such that CA = In. Then A−1\r\nexists.\r\n2. Suppose there exists a matrix B such that AB = In. Then A−1exists.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/34a90b2d-9728-488b-96e1-b41d013dfec3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d9b0f3b922bcbddeb6787a9034770419c8007810a8d93cae43d7f4b1cdf51a54",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 449
      },
      {
        "segments": [
          {
            "segment_id": "416d1700-40b3-45e4-810d-112e316f96ef",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 40,
            "page_width": 612,
            "page_height": 792,
            "content": "40 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nProof. Suppose there exists a matrix C such that CA = In. Let x0 be a solution of the\r\nhomogeneous system Ax = 0. Then Ax0 = 0 and\r\nx0 = In · x0 = (CA)x0 = C(Ax0) = C0 = 0.\r\nThat is, the homogeneous system Ax = 0 has only the trivial solution. Hence, using\r\nTheorem 2.2.5, the matrix A is invertible.\r\nUsing the first part, it is clear that the matrix B in the second part, is invertible. Hence\r\nAB = In = BA.\r\nThus, A is invertible as well.\r\nRemark 2.2.7. Theorem 2.2.6 implies the following:\r\n1. “if we want to show that a square matrix A of order n is invertible, it is enough to\r\nshow the existence of\r\n(a) either a matrix B such that AB = In\r\n(b) or a matrix C such that CA = In.\r\n2. Let A be an invertible matrix of order n. Suppose there exist elementary matrices\r\nE1, E2, . . . , Ek such that E1E2 · · · EkA = In. Then A−1 = E1E2 · · · Ek.\r\nRemark 2.2.7 gives the following method of computing the inverse of a matrix.\r\nSummary: Let A be an n × n matrix. Apply the Gauss-Jordan method to the matrix\r\n[A In]. Suppose the row-reduced echelon form of the matrix [A In] is [B C]. If B = In,\r\nthen A−1 = C or else A is not invertible.\r\nExample 2.2.8. Find the inverse of the matrix\r\n\r\n\r\n\r\n0 0 1\r\n0 1 1\r\n1 1 1\r\n\r\n\r\n using the Gauss-Jordan method.\r\nSolution: let us apply the Gauss-Jordan method to the matrix\r\n\r\n\r\n\r\n0 0 1 1 0 0\r\n0 1 1 0 1 0\r\n1 1 1 0 0 1\r\n\r\n\r\n .\r\n1.\r\n\r\n\r\n\r\n0 0 1 1 0 0\r\n0 1 1 0 1 0\r\n1 1 1 0 0 1\r\n\r\n\r\n\r\n−−→\r\nR13\r\n\r\n\r\n\r\n1 1 1 0 0 1\r\n0 1 1 0 1 0\r\n0 0 1 1 0 0\r\n\r\n\r\n\r\n2.\r\n\r\n\r\n\r\n1 1 1 0 0 1\r\n0 1 1 0 1 0\r\n0 0 1 1 0 0\r\n\r\n\r\n\r\n−−−−−→\r\nR31(−1)\r\n−−−−−→\r\nR32(−1)\r\n\r\n\r\n\r\n1 1 0 −1 0 1\r\n0 1 0 −1 1 0\r\n0 0 1 1 0 0\r\n\r\n\r\n\r\n3.\r\n\r\n\r\n\r\n1 1 0 −1 0 1\r\n0 1 0 −1 1 0\r\n0 0 1 1 0 0\r\n\r\n\r\n\r\n−−−−−→\r\nR21(−1)\r\n\r\n\r\n\r\n1 0 0 0 −1 1\r\n0 1 0 −1 1 0\r\n0 0 1 1 0 0\r\n\r\n\r\n .",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/416d1700-40b3-45e4-810d-112e316f96ef.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1a0286f71f967204c33c4fd1769566f9d44de63043e82f7e9bc253bf672074a2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 466
      },
      {
        "segments": [
          {
            "segment_id": "1d15146b-efb7-428e-b243-a042f9e8e2b1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 41,
            "page_width": 612,
            "page_height": 792,
            "content": "2.2. ELEMENTARY MATRICES 41\r\nThus, the inverse of the given matrix is\r\n\r\n\r\n\r\n0 −1 1\r\n−1 1 0\r\n1 0 0\r\n\r\n\r\n.\r\nExercise 2.2.9. 1. Find the inverse of the following matrices using the Gauss-Jordan\r\nmethod.\r\n(i)\r\n\r\n\r\n\r\n1 2 3\r\n1 3 2\r\n2 4 7\r\n\r\n\r\n , (ii)\r\n\r\n\r\n\r\n1 3 3\r\n2 3 2\r\n2 4 7\r\n\r\n\r\n , (iii)\r\n\r\n\r\n\r\n2 1 1\r\n1 2 1\r\n1 1 2\r\n\r\n\r\n , (iv)\r\n\r\n\r\n\r\n0 0 2\r\n0 2 1\r\n2 1 1\r\n\r\n\r\n.\r\n2. Which of the following matrices are elementary?\r\n\r\n\r\n\r\n2 0 1\r\n0 1 0\r\n0 0 1\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n1\r\n2\r\n0 0\r\n0 1 0\r\n0 0 1\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n1 −1 0\r\n0 1 0\r\n0 0 1\r\n\r\n\r\n,\r\n\r\n\r\n\r\n1 0 0\r\n5 1 0\r\n0 0 1\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n0 0 1\r\n0 1 0\r\n1 0 0\r\n\r\n\r\n ,\r\n\r\n\r\n\r\n0 0 1\r\n1 0 0\r\n0 1 0\r\n\r\n\r\n .\r\n3. Let A =\r\n\"\r\n2 1\r\n1 2#\r\n. Find the elementary matrices E1, E2, E3 and E4 such that E4 · E3 ·\r\nE2 · E1 · A = I2.\r\n4. Let B =\r\n\r\n\r\n\r\n1 1 1\r\n0 1 1\r\n0 0 3\r\n\r\n\r\n . Determine elementary matrices E1, E2 and E3 such that E3 ·\r\nE2 · E1 · B = I3.\r\n5. In Exercise 2.2.9.3, let C = E4 · E3 · E2 · E1. Then check that AC = I2.\r\n6. In Exercise 2.2.9.4, let C = E3 · E2 · E1. Then check that BC = I3.\r\n7. Find the inverse of the three matrices given in Example 2.2.3.3.\r\n8. Let A be a 1 × 2 matrix and B be a 2 × 1 matrix having positive entries. Which of\r\nBA or AB is invertible? Give reasons.\r\n9. Let A be an n × m matrix and B be an m × n matrix. Prove that\r\n(a) the matrix I − BA is invertible if and only if the matrix I − AB is invertible\r\n[Hint: Use Theorem 2.2.5.2].\r\n(b) (I − BA)\r\n−1 = I + B(I − AB)−1A whenever I − AB is invertible.\r\n(c) (I − BA)\r\n−1B = B(I − AB)−1 whenever I − AB is invertible.\r\n(d) (A−1 + B−1)\r\n−1 = A(A + B)−1B whenever A, B and A + B are all invertible.\r\nWe end this section by giving two more equivalent conditions for a matrix to be invert\u0002ible.\r\nTheorem 2.2.10. The following statements are equivalent for an n × n matrix A.\r\n1. A is invertible.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1d15146b-efb7-428e-b243-a042f9e8e2b1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=52b0acaba7b16eb64985d4f88c159d863bb48d339756239c2fa10f363c845c60",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 488
      },
      {
        "segments": [
          {
            "segment_id": "3542fbd4-bd6d-4bec-96c3-79dfa08402d6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 42,
            "page_width": 612,
            "page_height": 792,
            "content": "42 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n2. The system Ax = b has a unique solution for every b.\r\n3. The system Ax = b is consistent for every b.\r\nProof. 1 =⇒ 2\r\nObserve that x0 = A−1b is the unique solution of the system Ax = b.\r\n2 =⇒ 3\r\nThe system Ax = b has a solution and hence by definition, the system is consistent.\r\n3 =⇒ 1\r\nFor 1 ≤ i ≤ n, define ei = (0, . . . , 0, 1\r\n|{z}\r\ni\r\nth position\r\n, 0, . . . , 0)t, and consider the linear\r\nsystem Ax = ei. By assumption, this system has a solution, say xi, for each i, 1 ≤ i ≤ n.\r\nDefine a matrix B = [x1, x2, . . . , xn]. That is, the i\r\nth column of B is the solution of the\r\nsystem Ax = ei. Then\r\nAB = A[x1, x2 . . . , xn] = [Ax1, Ax2 . . . , Axn] = [e1, e2 . . . , en] = In.\r\nTherefore, by Theorem 2.2.6, the matrix A is invertible.\r\nWe now state another important result whose proof is immediate from Theorem 2.2.10\r\nand Theorem 2.2.5 and hence the proof is omitted.\r\nTheorem 2.2.11. Let A be an n × n matrix. Then the two statements given below cannot\r\nhold together.\r\n1. The system Ax = b has a unique solution for every b.\r\n2. The system Ax = 0 has a non-trivial solution.\r\nExercise 2.2.12. 1. Let A and B be two square matrices of the same order such that\r\nB = P A for some invertible matrix P. Then, prove that A is invertible if and only\r\nif B is invertible.\r\n2. Let A and B be two m × n matrices. Then prove that the two matrices A, B are\r\nrow-equivalent if and only if B = P A, where P is product of elementary matrices.\r\nWhen is this P unique?\r\n3. Let b\r\nt = [1, 2, −1, −2]. Suppose A is a 4 × 4 matrix such that the linear system\r\nAx = b has no solution. Mark each of the statements given below as true or false?\r\n(a) The homogeneous system Ax = 0 has only the trivial solution.\r\n(b) The matrix A is invertible.\r\n(c) Let c\r\nt = [−1, −2, 1, 2]. Then the system Ax = c has no solution.\r\n(d) Let B be the row-reduced echelon form of A. Then\r\ni. the fourth row of B is [0, 0, 0, 0].\r\nii. the fourth row of B is [0, 0, 0, 1].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/3542fbd4-bd6d-4bec-96c3-79dfa08402d6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9f446763b1f192bd6e7f29141875ae089b4a99d972710d0f5d5a359ac0ba6404",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 442
      },
      {
        "segments": [
          {
            "segment_id": "b98e2482-7a55-4516-adfa-037222de6ea7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 43,
            "page_width": 612,
            "page_height": 792,
            "content": "2.3. RANK OF A MATRIX 43\r\niii. the third row of B is necessarily of the form [0, 0, 0, 0].\r\niv. the third row of B is necessarily of the form [0, 0, 0, 1].\r\nv. the third row of B is necessarily of the form [0, 0, 1, α], where α is any real\r\nnumber.\r\n2.3 Rank of a Matrix\r\nIn the previous section, we gave a few equivalent conditions for a square matrix to be\r\ninvertible. We also used the Gauss-Jordan method and the elementary matrices to compute\r\nthe inverse of a square matrix A. In this section and the subsequent sections, we will mostly\r\nbe concerned with m × n matrices.\r\nLet A by an m×n matrix. Suppose that C is the row-reduced echelon form of A. Then\r\nthe matrix C is unique (see Theorem 2.1.30). Hence, we use the matrix C to define the\r\nrank of the matrix A.\r\nDefinition 2.3.1 (Row Rank of a Matrix). Let C be the row-reduced echelon form of a\r\nmatrix A. The number of non-zero rows in C is called the row-rank of A.\r\nFor a matrix A, we write ‘row-rank (A)’ to denote the row-rank of A. By the very\r\ndefinition, it is clear that row-equivalent matrices have the same row-rank. Thus, the\r\nnumber of non-zero rows in either the row echelon form or the row-reduced echelon form\r\nof a matrix are equal. Therefore, we just need to get the row echelon form of the matrix\r\nto know its rank.\r\nExample 2.3.2. 1. Determine the row-rank of A =\r\n\r\n\r\n\r\n1 2 1 1\r\n2 3 1 2\r\n1 1 2 1\r\n\r\n\r\n .\r\nSolution: The row-reduced echelon form of A is obtained as follows:\r\n\r\n\r\n\r\n1 2 1 1\r\n2 3 1 2\r\n1 1 2 1\r\n\r\n\r\n\r\n→\r\n\r\n\r\n\r\n1 2 1 1\r\n0 −1 −1 0\r\n0 −1 1 0\r\n\r\n\r\n\r\n→\r\n\r\n\r\n\r\n1 2 1 1\r\n0 1 1 0\r\n0 0 2 0\r\n\r\n\r\n\r\n→\r\n\r\n\r\n\r\n1 0 0 1\r\n0 1 0 0\r\n0 0 1 0\r\n\r\n\r\n .\r\nThe final matrix has 3 non-zero rows. Thus row-rank(A) = 3. This also follows\r\nfrom the third matrix.\r\n2. Determine the row-rank of A =\r\n\r\n\r\n\r\n1 2 1 1 1\r\n2 3 1 2 2\r\n1 1 0 1 1\r\n\r\n\r\n .\r\nSolution: row-rank(A) = 2 as one has the following:\r\n\r\n\r\n\r\n1 2 1 1 1\r\n2 3 1 2 2\r\n1 1 0 1 1\r\n\r\n\r\n\r\n→\r\n\r\n\r\n\r\n1 2 1 1 1\r\n0 −1 −1 0 0\r\n0 −1 −1 0 0\r\n\r\n\r\n\r\n→\r\n\r\n\r\n\r\n1 2 1 1 1\r\n0 1 1 0 0\r\n0 0 0 0 0\r\n\r\n\r\n .\r\nThe following remark related to the augmented matrix is immediate as computing the\r\nrank only involves the row operations (also see Remark 2.1.29).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b98e2482-7a55-4516-adfa-037222de6ea7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8218202e45bc4982d517f64ff0eb22f6e0759e89f635b7fdda7432d8a3a0ea06",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 512
      },
      {
        "segments": [
          {
            "segment_id": "72544f3d-78ac-4bc6-ba1e-1b285a3cce4f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 44,
            "page_width": 612,
            "page_height": 792,
            "content": "44 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nRemark 2.3.3. Let Ax = b be a linear system with m equations in n unknowns. Then\r\nthe row-reduced echelon form of A agrees with the first n columns of [A b], and hence\r\nrow-rank(A) ≤ row-rank([A b]).\r\nNow, consider an m × n matrix A and an elementary matrix E of order n. Then the\r\nproduct AE corresponds to applying column transformation on the matrix A. Therefore,\r\nfor each elementary matrix, there is a corresponding column transformation as well. We\r\nsummarize these ideas as follows.\r\nDefinition 2.3.4. The column transformations obtained by right multiplication of elemen\u0002tary matrices are called column operations.\r\nExample 2.3.5. Let A =\r\n\r\n\r\n\r\n1 2 3 1\r\n2 0 3 2\r\n3 4 5 3\r\n\r\n\r\n\r\n. Then\r\nA\r\n\r\n\r\n\r\n\r\n\r\n1 0 0 0\r\n0 0 1 0\r\n0 1 0 0\r\n0 0 0 1\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n1 3 2 1\r\n2 3 0 2\r\n3 5 4 3\r\n\r\n\r\n\r\nand A\r\n\r\n\r\n\r\n\r\n\r\n1 0 0 −1\r\n0 1 0 0\r\n0 0 1 0\r\n0 0 0 1\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n1 2 3 0\r\n2 0 3 0\r\n3 4 5 0\r\n\r\n\r\n.\r\nRemark 2.3.6. After application of a finite number of elementary column operations (see\r\nDefinition 2.3.4) to a matrix A, we can obtain a matrix B having the following properties:\r\n1. The first nonzero entry in each column is 1, called the leading term.\r\n2. Column(s) containing only 0’s comes after all columns with at least one non-zero\r\nentry.\r\n3. The first non-zero entry (the leading term) in each non-zero column moves down in\r\nsuccessive columns.\r\nWe define column-rank of A as the number of non-zero columns in B.\r\nIt will be proved later that row-rank(A) = column-rank(A). Thus we are led to the\r\nfollowing definition.\r\nDefinition 2.3.7. The number of non-zero rows in the row-reduced echelon form of a\r\nmatrix A is called the rank of A, denoted rank(A).\r\nwe are now ready to prove a few results associated with the rank of a matrix.\r\nTheorem 2.3.8. Let A be a matrix of rank r. Then there exist a finite number of elemen\u0002tary matrices E1, E2, . . . , Es and F1, F2, . . . , Fℓ such that\r\nE1E2 . . . Es A F1F2 . . . Fℓ =\r\n\"\r\nIr 0\r\n0 0#\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/72544f3d-78ac-4bc6-ba1e-1b285a3cce4f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=65ec6aa982522dd6786ef3f3c614192755402a6f3e72638a54af71d77234f329",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 426
      },
      {
        "segments": [
          {
            "segment_id": "d5d1fb32-d621-45eb-9268-f07972e6cea8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 45,
            "page_width": 612,
            "page_height": 792,
            "content": "2.3. RANK OF A MATRIX 45\r\nProof. Let C be the row-reduced echelon matrix of A. As rank(A) = r, the first r rows of C\r\nare non-zero rows. So by Theorem 2.1.22, C will have r leading columns, say i1, i2, . . . , ir.\r\nNote that, for 1 ≤ s ≤ r, the i\r\nth\r\ns\r\ncolumn will have 1 in the s\r\nth row and zero, elsewhere.\r\nWe now apply column operations to the matrix C. Let D be the matrix obtained from\r\nC by successively interchanging the s\r\nth and ith\r\ns\r\ncolumn of C for 1 ≤ s ≤ r. Then D has\r\nthe form \"\r\nIr B\r\n0 0#\r\n, where B is a matrix of an appropriate size. As the (1, 1) block of D is\r\nan identity matrix, the block (1, 2) can be made the zero matrix by application of column\r\noperations to D. This gives the required result.\r\nThe next result is a corollary of Theorem 2.3.8. It gives the solution set of a homo\u0002geneous system Ax = 0. One can also obtain this result as a particular case of Corol\u0002lary 2.1.23.2 as by definition rank(A) ≤ m, the number of rows of A.\r\nCorollary 2.3.9. Let A be an m × n matrix. Suppose rank(A) = r < n. Then Ax = 0 has\r\ninfinite number of solutions. In particular, Ax = 0 has a non-trivial solution.\r\nProof. By Theorem 2.3.8, there exist elementary matrices E1, . . . , Es and F1, . . . , Fℓ such\r\nthat E1E2 · · · Es A F1F2 · · · Fℓ =\r\n\"\r\nIr 0\r\n0 0#\r\n. Define P = E1E2 · · · Es and Q = F1F2 · · · Fℓ.\r\nThen the matrix P AQ =\r\n\"\r\nIr 0\r\n0 0#\r\n. As Ei\r\n’s for 1 ≤ i ≤ s correspond only to row operations,\r\nwe get AQ =\r\nh\r\nC 0\r\ni\r\n, where C is a matrix of size m × r. Let Q1, Q2, . . . , Qn be the\r\ncolumns of the matrix Q. Then check that AQi = 0 for i = r + 1, . . . , n. Hence, the\r\nrequired results follows (use Theorem 2.1.5).\r\nExercise 2.3.10. 1. Determine ranks of the coefficient and the augmented matrices\r\nthat appear in Exercise 2.1.25.2.\r\n2. Let P and Q be invertible matrices such that the matrix product P AQ is defined.\r\nProve that rank(P AQ) = rank(A).\r\n3. Let A =\r\n\"\r\n2 4 8\r\n1 3 2#\r\nand B =\r\n\"\r\n1 0 0\r\n0 1 0#\r\n. Find P and Q such that B = P AQ.\r\n4. Let A and B be two matrices. Prove that\r\n(a) if A + B is defined, then rank(A + B) ≤ rank(A) + rank(B),\r\n(b) if AB is defined, then rank(AB) ≤ rank(A) and rank(AB) ≤ rank(B).\r\n5. Let A be a matrix of rank r. Then prove that there exists invertible matrices Bi, Ci\r\nsuch that\r\nB1A =\r\n\"\r\nR1 R2\r\n0 0 #\r\n, AC1 =\r\n\"\r\nS1 0\r\nS3 0\r\n#\r\n, B2AC2 =\r\n\"\r\nA1 0\r\n0 0#\r\nand B3AC3 =\r\n\"\r\nIr 0\r\n0 0#\r\n,\r\nwhere the (1, 1) block of each matrix is of size r × r. Also, prove that A1 is an\r\ninvertible matrix.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/d5d1fb32-d621-45eb-9268-f07972e6cea8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eba2db203578ac36043a7700e42cea96c5bc73ae34f31f876be6f2238c61afa6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 567
      },
      {
        "segments": [
          {
            "segment_id": "d5d1fb32-d621-45eb-9268-f07972e6cea8",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 45,
            "page_width": 612,
            "page_height": 792,
            "content": "2.3. RANK OF A MATRIX 45\r\nProof. Let C be the row-reduced echelon matrix of A. As rank(A) = r, the first r rows of C\r\nare non-zero rows. So by Theorem 2.1.22, C will have r leading columns, say i1, i2, . . . , ir.\r\nNote that, for 1 ≤ s ≤ r, the i\r\nth\r\ns\r\ncolumn will have 1 in the s\r\nth row and zero, elsewhere.\r\nWe now apply column operations to the matrix C. Let D be the matrix obtained from\r\nC by successively interchanging the s\r\nth and ith\r\ns\r\ncolumn of C for 1 ≤ s ≤ r. Then D has\r\nthe form \"\r\nIr B\r\n0 0#\r\n, where B is a matrix of an appropriate size. As the (1, 1) block of D is\r\nan identity matrix, the block (1, 2) can be made the zero matrix by application of column\r\noperations to D. This gives the required result.\r\nThe next result is a corollary of Theorem 2.3.8. It gives the solution set of a homo\u0002geneous system Ax = 0. One can also obtain this result as a particular case of Corol\u0002lary 2.1.23.2 as by definition rank(A) ≤ m, the number of rows of A.\r\nCorollary 2.3.9. Let A be an m × n matrix. Suppose rank(A) = r < n. Then Ax = 0 has\r\ninfinite number of solutions. In particular, Ax = 0 has a non-trivial solution.\r\nProof. By Theorem 2.3.8, there exist elementary matrices E1, . . . , Es and F1, . . . , Fℓ such\r\nthat E1E2 · · · Es A F1F2 · · · Fℓ =\r\n\"\r\nIr 0\r\n0 0#\r\n. Define P = E1E2 · · · Es and Q = F1F2 · · · Fℓ.\r\nThen the matrix P AQ =\r\n\"\r\nIr 0\r\n0 0#\r\n. As Ei\r\n’s for 1 ≤ i ≤ s correspond only to row operations,\r\nwe get AQ =\r\nh\r\nC 0\r\ni\r\n, where C is a matrix of size m × r. Let Q1, Q2, . . . , Qn be the\r\ncolumns of the matrix Q. Then check that AQi = 0 for i = r + 1, . . . , n. Hence, the\r\nrequired results follows (use Theorem 2.1.5).\r\nExercise 2.3.10. 1. Determine ranks of the coefficient and the augmented matrices\r\nthat appear in Exercise 2.1.25.2.\r\n2. Let P and Q be invertible matrices such that the matrix product P AQ is defined.\r\nProve that rank(P AQ) = rank(A).\r\n3. Let A =\r\n\"\r\n2 4 8\r\n1 3 2#\r\nand B =\r\n\"\r\n1 0 0\r\n0 1 0#\r\n. Find P and Q such that B = P AQ.\r\n4. Let A and B be two matrices. Prove that\r\n(a) if A + B is defined, then rank(A + B) ≤ rank(A) + rank(B),\r\n(b) if AB is defined, then rank(AB) ≤ rank(A) and rank(AB) ≤ rank(B).\r\n5. Let A be a matrix of rank r. Then prove that there exists invertible matrices Bi, Ci\r\nsuch that\r\nB1A =\r\n\"\r\nR1 R2\r\n0 0 #\r\n, AC1 =\r\n\"\r\nS1 0\r\nS3 0\r\n#\r\n, B2AC2 =\r\n\"\r\nA1 0\r\n0 0#\r\nand B3AC3 =\r\n\"\r\nIr 0\r\n0 0#\r\n,\r\nwhere the (1, 1) block of each matrix is of size r × r. Also, prove that A1 is an\r\ninvertible matrix.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/d5d1fb32-d621-45eb-9268-f07972e6cea8.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=eba2db203578ac36043a7700e42cea96c5bc73ae34f31f876be6f2238c61afa6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 567
      },
      {
        "segments": [
          {
            "segment_id": "2230e6fd-bf84-4166-98cf-70fb2279a2c6",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 46,
            "page_width": 612,
            "page_height": 792,
            "content": "46 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\n6. Let A be an m × n matrix of rank r. Then prove that A can be written as A = BC,\r\nwhere both B and C have rank r and B is of size m × r and C is of size r × n.\r\n7. Let A and B be two matrices such that AB is defined and rank(A) = rank(AB).\r\nThen prove that A = ABX for some matrix X. Similarly, if BA is defined and\r\nrank (A) = rank (BA), then A = Y BA for some matrix Y. [Hint: Choose invertible\r\nmatrices P, Q satisfying P AQ =\r\n\u0014\r\nA1 0\r\n0 0\u0015\r\n, P(AB) = (P AQ)(Q−1B) = \u0014\r\nA2 A3\r\n0 0 \u0015\r\n. Now find\r\nR an invertible matrix with P(AB)R =\r\n\u0014\r\nC 0\r\n0 0\u0015\r\n. Define X = R\r\n\u0014\r\nC\r\n−1A1 0\r\n0 0\u0015\r\nQ−1.]\r\n8. Suppose the matrices B and C are invertible and the involved partitioned products\r\nare defined, then prove that\r\n\"\r\nA B\r\nC 0\r\n#−1\r\n=\r\n\"\r\n0 C\r\n−1\r\nB−1 −B−1AC−1\r\n#\r\n.\r\n9. Suppose A−1 = B with A =\r\n\"\r\nA11 A12\r\nA21 A22#\r\nand B =\r\n\"\r\nB11 B12\r\nB21 B22#\r\n. Also, assume that\r\nA11 is invertible and define P = A22 − A21A\r\n−1\r\n11 A12. Then prove that\r\n(a) A is row-equivalent to the matrix \"\r\nA11 A12\r\n0 A22 − A21A\r\n−1\r\n11 A12#\r\n,\r\n(b) P is invertible and B =\r\n\"\r\nA\r\n−1\r\n11 + (A\r\n−1\r\n11 A12)P\r\n−1\r\n(A21A\r\n−1\r\n11 ) −(A\r\n−1\r\n11 A12)P\r\n−1\r\n−P\r\n−1\r\n(A21A\r\n−1\r\n11 ) P\r\n−1\r\n#\r\n.\r\nWe end this section by giving another equivalent condition for a square matrix to be\r\ninvertible. To do so, we need the following definition.\r\nDefinition 2.3.11. A n × n matrix A is said to be of full rank if rank(A) = n.\r\nTheorem 2.3.12. Let A be a square matrix of order n. Then the following statements are\r\nequivalent.\r\n1. A is invertible.\r\n2. A has full rank.\r\n3. The row-reduced form of A is In.\r\nProof. 1 =⇒ 2\r\nLet if possible rank(A) = r < n. Then there exists an invertible matrix P (a product\r\nof elementary matrices) such that P A =\r\n\"\r\nB1 B2\r\n0 0 #\r\n, where B1 is an r × r matrix. Since A\r\nis invertible, let A−1 =\r\n\"\r\nC1\r\nC2\r\n#\r\n, where C1 is an r × n matrix. Then\r\nP = P In = P(AA−1) = (P A)A\r\n−1 =\r\n\"\r\nB1 B2\r\n0 0 # \"C1C2\r\n#\r\n=\r\n\"\r\nB1C1 + B2C2\r\n0\r\n#\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/2230e6fd-bf84-4166-98cf-70fb2279a2c6.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e89d75445467010729a1ea0e2811cd5f0aea8acf99b1b9d3fb20e6d0ca0c522a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 453
      },
      {
        "segments": [
          {
            "segment_id": "3f6debf2-3ddc-406f-84cb-3ff2f7d863e3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 47,
            "page_width": 612,
            "page_height": 792,
            "content": "2.4. EXISTENCE OF SOLUTION OF AX = B 47\r\nThus, P has n − r rows consisting of only zeros. Hence, P cannot be invertible. A\r\ncontradiction. Thus, A is of full rank.\r\n2 =⇒ 3\r\nSuppose A is of full rank. This implies, the row-reduced echelon form of A has all\r\nnon-zero rows. But A has as many columns as rows and therefore, the last row of the\r\nrow-reduced echelon form of A is [0, 0, . . . , 0, 1]. Hence, the row-reduced echelon form of A\r\nis In.\r\n3 =⇒ 1\r\nUsing Theorem 2.2.5.3, the required result follows.\r\n2.4 Existence of Solution of Ax = b\r\nIn Section 2.2, we studied the system of linear equations in which the matrix A was a square\r\nmatrix. We will now use the rank of a matrix to study the system of linear equations even\r\nwhen A is not a square matrix. Before proceeding with our main result, we give an example\r\nfor motivation and observations. Based on these observations, we will arrive at a better\r\nunderstanding, related to the existence and uniqueness results for the linear system Ax = b.\r\nConsider a linear system Ax = b. Suppose the application of the Gauss-Jordan method\r\nhas reduced the augmented matrix [A b] to\r\n[C d] =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 0 2 −1 0 0 2 8\r\n0 1 1 3 0 0 5 1\r\n0 0 0 0 1 0 −1 2\r\n0 0 0 0 0 1 1 4\r\n0 0 0 0 0 0 0 0\r\n0 0 0 0 0 0 0 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nThen to get the solution set, we observe the following.\r\nObservations:\r\n1. The number of non-zero rows in C is 4. This number is also equal to the number of\r\nnon-zero rows in [C d]. So, there are 4 leading columns/basic variables.\r\n2. The leading terms appear in columns 1, 2, 5 and 6. Thus, the respective variables\r\nx1, x2, x5 and x6 are the basic variables.\r\n3. The remaining variables, x3, x4 and x7 are free variables.\r\nHence, the solution set is given by\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\nx3\r\nx4\r\nx5\r\nx6\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n8 − 2x3 + x4 − 2x7\r\n1 − x3 − 3x4 − 5x7\r\nx3\r\nx4\r\n2 + x7\r\n4 − x7\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n8\r\n1\r\n0\r\n0\r\n2\r\n4\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x3\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−2\r\n−1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n−3\r\n0\r\n1\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−2\r\n−5\r\n0\r\n0\r\n1\r\n−1\r\n1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/3f6debf2-3ddc-406f-84cb-3ff2f7d863e3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5a9321db4e4c1ead4ab1f6643dfcc3fdfad19c53f938294eed450f19287556dc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 582
      },
      {
        "segments": [
          {
            "segment_id": "3f6debf2-3ddc-406f-84cb-3ff2f7d863e3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 47,
            "page_width": 612,
            "page_height": 792,
            "content": "2.4. EXISTENCE OF SOLUTION OF AX = B 47\r\nThus, P has n − r rows consisting of only zeros. Hence, P cannot be invertible. A\r\ncontradiction. Thus, A is of full rank.\r\n2 =⇒ 3\r\nSuppose A is of full rank. This implies, the row-reduced echelon form of A has all\r\nnon-zero rows. But A has as many columns as rows and therefore, the last row of the\r\nrow-reduced echelon form of A is [0, 0, . . . , 0, 1]. Hence, the row-reduced echelon form of A\r\nis In.\r\n3 =⇒ 1\r\nUsing Theorem 2.2.5.3, the required result follows.\r\n2.4 Existence of Solution of Ax = b\r\nIn Section 2.2, we studied the system of linear equations in which the matrix A was a square\r\nmatrix. We will now use the rank of a matrix to study the system of linear equations even\r\nwhen A is not a square matrix. Before proceeding with our main result, we give an example\r\nfor motivation and observations. Based on these observations, we will arrive at a better\r\nunderstanding, related to the existence and uniqueness results for the linear system Ax = b.\r\nConsider a linear system Ax = b. Suppose the application of the Gauss-Jordan method\r\nhas reduced the augmented matrix [A b] to\r\n[C d] =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 0 2 −1 0 0 2 8\r\n0 1 1 3 0 0 5 1\r\n0 0 0 0 1 0 −1 2\r\n0 0 0 0 0 1 1 4\r\n0 0 0 0 0 0 0 0\r\n0 0 0 0 0 0 0 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nThen to get the solution set, we observe the following.\r\nObservations:\r\n1. The number of non-zero rows in C is 4. This number is also equal to the number of\r\nnon-zero rows in [C d]. So, there are 4 leading columns/basic variables.\r\n2. The leading terms appear in columns 1, 2, 5 and 6. Thus, the respective variables\r\nx1, x2, x5 and x6 are the basic variables.\r\n3. The remaining variables, x3, x4 and x7 are free variables.\r\nHence, the solution set is given by\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\nx3\r\nx4\r\nx5\r\nx6\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n8 − 2x3 + x4 − 2x7\r\n1 − x3 − 3x4 − 5x7\r\nx3\r\nx4\r\n2 + x7\r\n4 − x7\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n8\r\n1\r\n0\r\n0\r\n2\r\n4\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x3\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−2\r\n−1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n−3\r\n0\r\n1\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−2\r\n−5\r\n0\r\n0\r\n1\r\n−1\r\n1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/3f6debf2-3ddc-406f-84cb-3ff2f7d863e3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5a9321db4e4c1ead4ab1f6643dfcc3fdfad19c53f938294eed450f19287556dc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 582
      },
      {
        "segments": [
          {
            "segment_id": "654ca1ea-abe3-4115-bfda-5c5afadf9bda",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 48,
            "page_width": 612,
            "page_height": 792,
            "content": "48 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nwhere x3, x4 and x7 are arbitrary.\r\nLet x0 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n8\r\n1\r\n0\r\n0\r\n2\r\n4\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n, u1 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−2\r\n−1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n, u2 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n−3\r\n0\r\n1\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nand u3 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−2\r\n−5\r\n0\r\n0\r\n1\r\n−1\r\n1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nThen it can easily be verified that Cx0 = d, and for 1 ≤ i ≤ 3, Cui = 0. Hence, it follows\r\nthat Ax0 = d, and for 1 ≤ i ≤ 3, Aui = 0.\r\nA similar idea is used in the proof of the next theorem and is omitted. The proof\r\nappears on page 87 as Theorem 3.3.26.\r\nTheorem 2.4.1 (Existence/Non-Existence Result). Consider a linear system Ax = b,\r\nwhere A is an m × n matrix, and x, b are vectors of orders n × 1, and m × 1, respectively.\r\nSuppose rank (A) = r and rank([A b]) = ra. Then exactly one of the following statement\r\nholds:\r\n1. If r < ra, the linear system has no solution.\r\n2. if ra = r, then the linear system is consistent. Furthermore,\r\n(a) if r = n then the solution set contains a unique vector x0 satisfying Ax0 = b.\r\n(b) if r < n then the solution set has the form\r\n{x0 + k1u1 + k2u2 + · · · + kn−run−r : ki ∈ R, 1 ≤ i ≤ n − r},\r\nwhere Ax0 = b and Aui = 0 for 1 ≤ i ≤ n − r.\r\nRemark 2.4.2. Let A be an m × n matrix. Then Theorem 2.4.1 implies that\r\n1. the linear system Ax = b is consistent if and only if rank(A) = rank([A b]).\r\n2. the vectors ui, for 1 ≤ i ≤ n − r, correspond to each of the free variables.\r\nExercise 2.4.3. In the introduction, we gave 3 figures (see Figure 2) to show the cases\r\nthat arise in the Euclidean plane (2 equations in 2 unknowns). It is well known that in the\r\ncase of Euclidean space (3 equations in 3 unknowns), there\r\n1. is a figure to indicate the system has a unique solution.\r\n2. are 4 distinct figures to indicate the system has no solution.\r\n3. are 3 distinct figures to indicate the system has infinite number of solutions.\r\nDetermine all the figures.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/654ca1ea-abe3-4115-bfda-5c5afadf9bda.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=569a441df44e42727d3ac2bef6de0fc937176d643db8f69e3db9464b6bfe7e55",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 492
      },
      {
        "segments": [
          {
            "segment_id": "a29ec0a6-9148-4181-a1ab-58f101ed5546",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 49,
            "page_width": 612,
            "page_height": 792,
            "content": "2.5. DETERMINANT 49\r\n2.5 Determinant\r\nIn this section, we associate a number with each square matrix. To do so, we start with the\r\nfollowing notation. Let A be an n×n matrix. Then for each positive integers αi’s 1 ≤ i ≤ k\r\nand βj ’s for 1 ≤ j ≤ ℓ, we write A(α1, . . . , αk\r\n\f\r\n\fβ1, . . . , βℓ) to mean that submatrix of A,\r\nthat is obtained by deleting the rows corresponding to αi’s and the columns corresponding\r\nto βj ’s of A.\r\nExample 2.5.1. Let A =\r\n\r\n\r\n\r\n1 2 3\r\n1 3 2\r\n2 4 7\r\n\r\n\r\n . Then A(1|2) = \"\r\n1 2\r\n2 7#\r\n, A(1|3) = \"\r\n1 3\r\n2 4#\r\nand\r\nA(1, 2|1, 3) = [4].\r\nWith the notations as above, we have the following inductive definition of determinant\r\nof a matrix. This definition is commonly known as the expansion of the determinant along\r\nthe first row. The students with a knowledge of symmetric groups/permutations can find\r\nthe definition of the determinant in Appendix 7.1.15. It is also proved in Appendix that\r\nthe definition given below does correspond to the expansion of determinant along the first\r\nrow.\r\nDefinition 2.5.2 (Determinant of a Square Matrix). Let A be a square matrix of order n.\r\nThe determinant of A, denoted det(A) (or |A|) is defined by\r\ndet(A) =\r\n\r\n\r\n\r\na, if A = [a] (n = 1),\r\nPn\r\nj=1\r\n(−1)1+ja1j detA(1|j)\r\n\u0001\r\n, otherwise.\r\nExample 2.5.3. 1. Let A = [−2]. Then det(A) = |A| = −2.\r\n2. Let A =\r\n\"\r\na b\r\nc d#\r\n. Then, det(A) = |A| = a\r\n\f\r\n\fA(1|1)\r\n\f\r\n\f − b\r\n\f\r\n\fA(1|2)\r\n\f\r\n\f = ad − bc. For\r\nexample, if A =\r\n\"\r\n1 2\r\n3 5#\r\nthen det(A) =\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1 2\r\n3 5\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= 1 · 5 − 2 · 3 = −1.\r\n3. Let A =\r\n\r\n\r\n\r\na11 a12 a13\r\na21 a22 a23\r\na31 a32 a33\r\n\r\n\r\n . Then,\r\ndet(A) = |A| = a11 det(A(1|1)) − a12 det(A(1|2)) + a13 det(A(1|3))\r\n= a11\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\na22 a23\r\na32 a33\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n− a12\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\na21 a23\r\na31 a33\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n+ a13\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\na21 a22\r\na31 a32\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= a11(a22a33 − a23a32) − a12(a21a33 − a31a23)\r\n+a13(a21a32 − a31a22) (2.5.1)\r\nLet A =\r\n\r\n\r\n\r\n1 2 3\r\n2 3 1\r\n1 2 2\r\n\r\n\r\n\r\n. Then |A| = 1·\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n3 1\r\n2 2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n−2·\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n2 1\r\n1 2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n+ 3·\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n2 3\r\n1 2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= 4−2(3)+ 3(1) = 1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a29ec0a6-9148-4181-a1ab-58f101ed5546.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=56676f717444009404494bd40048eab4906a48ebb8c6500f84fbe90282d2aba5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 421
      },
      {
        "segments": [
          {
            "segment_id": "8e0c6bf2-b1af-4aa4-8297-c523bd69bd62",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 50,
            "page_width": 612,
            "page_height": 792,
            "content": "50 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nExercise 2.5.4. Find the determinant of the following matrices.\r\ni)\r\n\r\n\r\n\r\n\r\n\r\n1 2 7 8\r\n0 4 3 2\r\n0 0 2 3\r\n0 0 0 5\r\n\r\n\r\n\r\n\r\n\r\n, ii)\r\n\r\n\r\n\r\n\r\n\r\n3 0 0 1\r\n0 2 0 5\r\n6 −7 1 0\r\n3 2 0 6\r\n\r\n\r\n\r\n\r\n\r\n, iii)\r\n\r\n\r\n\r\n1 a a2\r\n1 b b2\r\n1 c c2\r\n\r\n\r\n.\r\nDefinition 2.5.5 (Singular, Non-Singular). A matrix A is said to be a singular if\r\ndet(A) = 0. It is called non-singular if det(A) 6= 0.\r\nWe omit the proof of the next theorem that relates the determinant of a square matrix\r\nwith row operations. The interested reader is advised to go through Appendix 7.2.\r\nTheorem 2.5.6. Let A be an n × n matrix. If\r\n1. B is obtained from A by interchanging two rows then det(B) = − det(A),\r\n2. B is obtained from A by multiplying a row by c then det(B) = c det(A),\r\n3. B is obtained from A by replacing the jth row by jth row plus c times the ith row,\r\nwhere i 6= j then det(B) = det(A),\r\n4. all the elements of one row or column of A are 0 then det(A) = 0,\r\n5. two rows of A are equal then det(A) = 0.\r\n6. A is a triangular matrix then det(A) is product of diagonal entries.\r\nSince det(In) = 1, where In is the n × n identity matrix, the following remark gives the\r\ndeterminant of the elementary matrices. The proof is omitted as it is a direct application\r\nof Theorem 2.5.6.\r\nRemark 2.5.7. Fix a positive integer n. Then\r\n1. det(Eij ) = −1, where Eij corresponds to the interchange of the i\r\nth and the jth row\r\nof In.\r\n2. For c 6= 0, det(Ek(c)) = c, where Ek(c) is obtained by multiplying the k\r\nth row of In\r\nby c.\r\n3. For c 6= 0, det(Eij (c)) = 1, where Eij (c) is obtained by replacing the j\r\nth row of In\r\nby the j\r\nth row of In plus c times the ith row of In.\r\nRemark 2.5.8. Theorem 2.5.6.1 implies that “one can also calculate the determinant by\r\nexpanding along any row.” Hence, the computation of determinant using the k-th row for\r\n1 ≤ k ≤ n is given by\r\ndet(A) = Xn\r\nj=1\r\n(−1)k+jakj detA(k|j)\r\n\u0001\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/8e0c6bf2-b1af-4aa4-8297-c523bd69bd62.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ad377c09baafbd6e0d32b78102580133dc91056efc8971e5a530c3e11f9508cb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 421
      },
      {
        "segments": [
          {
            "segment_id": "2f49bc3f-5fcd-4bcc-98cf-3af9959df978",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 51,
            "page_width": 612,
            "page_height": 792,
            "content": "2.5. DETERMINANT 51\r\nExample 2.5.9. 1. Let A =\r\n\r\n\r\n\r\n2 2 6\r\n1 3 2\r\n1 1 2\r\n\r\n\r\n . Determine det(A).\r\nSolution: Check that\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n2 2 6\r\n1 3 2\r\n1 1 2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n−−−→\r\nR1(2)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1 1 3\r\n1 3 2\r\n1 1 2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n−−−−−→\r\nR21(−1)\r\n−−−−−→\r\nR31(−1)\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1 1 3\r\n0 2 −1\r\n0 0 −1\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n. Thus, using\r\nTheorem 2.5.6, det(A) = 2 · 1 · 2 · (−1) = −4.\r\n2. Let A =\r\n\r\n\r\n\r\n\r\n\r\n2 2 6 8\r\n1 1 2 4\r\n1 3 2 6\r\n3 3 5 8\r\n\r\n\r\n\r\n\r\n\r\n. Determine det(A).\r\nSolution: The successive application of row operations R1(2), R21(−1), R31(−1),\r\nR41(−3), R23 and R34(−4) and the application of Theorem 2.5.6 implies\r\ndet(A) = 2 · (−1) ·\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1 1 3 4\r\n0 2 −1 2\r\n0 0 −1 0\r\n0 0 0 −4\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= −16.\r\nObserve that the row operation R1(2) gives 2 as the first product and the row operation\r\nR23 gives −1 as the second product.\r\nRemark 2.5.10. 1. Let u\r\nt = (u1, u2) and vt = (v1, v2) be two vectors in R2\r\n. Consider\r\nthe parallelogram on vertices P = (0, 0)t, Q = u, R = u+v and S = v (see Figure 3).\r\nThen Area (P QRS) = |u1v2 − u2v1|, the absolute value of\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\nu1 u2\r\nv1 v2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n.\r\nP\r\nQ\r\nS\r\nR\r\nT\r\nu\r\nv\r\nw\r\nu × v\r\nθ\r\nγ\r\nFigure 3: Parallelepiped with vertices P, Q, R and S as base\r\nRecall the following: The dot product of u\r\nt = (u1, u2) and vt = (v1, v2), denoted\r\nu • v, equals u • v = u1v1 + u2v2, and the length of a vector u, denoted ℓ(u) equals\r\nℓ(u) = pu\r\n2\r\n1 + u\r\n2\r\n2\r\n. Also, if θ is the angle between u and v then we know that cos(θ) =",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/2f49bc3f-5fcd-4bcc-98cf-3af9959df978.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e0a77fd0d462c7b52c1b9ae0f197ea33cdf4bfd7a5b739cf31c5832d2ac647b4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 329
      },
      {
        "segments": [
          {
            "segment_id": "cc0a9dd2-31b0-4540-b054-5c6223de48a9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 52,
            "page_width": 612,
            "page_height": 792,
            "content": "52 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nu•v\r\nℓ(u)ℓ(v)\r\n. Therefore\r\nArea(P QRS) = ℓ(u)ℓ(v) sin(θ) = ℓ(u)ℓ(v)\r\ns\r\n1 −\r\n\u0012\r\nu • v\r\nℓ(u)ℓ(v)\r\n\u00132\r\n=\r\np\r\nℓ(u)\r\n2 + ℓ(v)2 − (u • v)2 =\r\np\r\n(u1v2 − u2v1)\r\n2\r\n= |u1v2 − u2v1|.\r\nThat is, in R\r\n2\r\n, the determinant is ± times the area of the parallelogram.\r\n2. Consider Figure 3 again. Let u\r\nt = (u1, u2, u3), vt = (v1, v2, v3) and wt = (w1, w2, w3)\r\nbe three vectors in R\r\n3\r\n. Then u • v = u1v1 + u2v2 + u3v3 and the cross product of u\r\nand v, denoted u × v, equals\r\nu × v = (u2v3 − u3v2, u3v1 − u1v3, u1v2 − u2v1).\r\nThe vector u × v is perpendicular to the plane containing both u and v. Note that if\r\nu3 = v3 = 0, then we can think of u and v as vectors in the XY -plane and in this\r\ncase ℓ(u × v) = |u1v2 − u2v1| = Area(P QRS). Hence, if γ is the angle between the\r\nvector w and the vector u × v, then\r\nvolume (P) = Area(P QRS) · height = |w • (u × v)| = ±\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\nw1 w2 w3\r\nu1 u2 u3\r\nv1 v2 v3\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n.\r\nIn general, for any n × n matrix A, it can be proved that | det(A)| is indeed equal to the\r\nvolume of the n-dimensional parallelepiped. The actual proof is beyond the scope of this\r\nbook.\r\nExercise 2.5.11. In each of the questions given below, use Theorem 2.5.6 to arrive at\r\nyour answer.\r\n1. Let A =\r\n\r\n\r\n\r\na b c\r\ne f g\r\nh j ℓ\r\n\r\n\r\n , B =\r\n\r\n\r\n\r\na b αc\r\ne f αg\r\nh j αℓ\r\n\r\n\r\n\r\nand C =\r\n\r\n\r\n\r\na b αa + βb + c\r\ne f αe + βf + g\r\nh j αh + βj + ℓ\r\n\r\n\r\n for some\r\ncomplex numbers α and β. Prove that det(B) = α det(A) and det(C) = det(A).\r\n2. Let A =\r\n\r\n\r\n\r\n1 3 2\r\n2 3 1\r\n1 5 3\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n1 −1 0\r\n1 0 −1\r\n0 −1 1\r\n\r\n\r\n\r\n. Prove that 3 divides det(A) and\r\ndet(B) = 0.\r\n2.5.1 Adjoint of a Matrix\r\nDefinition 2.5.12 (Minor, Cofactor of a Matrix). The number det (A(i|j)) is called the\r\n(i, j)\r\nth minor of A. We write Aij = det (A(i|j)) . The (i, j)th cofactor of A, denoted Cij ,\r\nis the number (−1)i+jAij .",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/cc0a9dd2-31b0-4540-b054-5c6223de48a9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=14e6b9291ebbf37ee64e3aa0105695cb4134bdc4ff03d258bbeaf9c055d4a25c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 452
      },
      {
        "segments": [
          {
            "segment_id": "355c5844-c863-486f-8f21-df70ffaaec95",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 53,
            "page_width": 612,
            "page_height": 792,
            "content": "2.5. DETERMINANT 53\r\nDefinition 2.5.13 (Adjoint of a Matrix). Let A be an n × n matrix. The matrix B = [bij ]\r\nwith bij = Cji, for 1 ≤ i, j ≤ n is called the Adjoint of A, denoted Adj(A).\r\nExample 2.5.14. Let A =\r\n\r\n\r\n\r\n1 2 3\r\n2 3 1\r\n1 2 2\r\n\r\n\r\n . Then Adj(A) =\r\n\r\n\r\n\r\n4 2 −7\r\n−3 −1 5\r\n1 0 −1\r\n\r\n\r\n\r\nas\r\nC11 = (−1)1+1A11 = 4, C21 = (−1)2+1A21 = 2, . . . , C33 = (−1)3+3A33 = −1.\r\nTheorem 2.5.15. Let A be an n × n matrix. Then\r\n1. for 1 ≤ i ≤ n, Pn\r\nj=1\r\naij Cij =\r\nPn\r\nj=1\r\naij (−1)i+j Aij = det(A),\r\n2. for i 6= ℓ, Pn\r\nj=1\r\naij Cℓj =\r\nPn\r\nj=1\r\naij (−1)ℓ+j Aℓj = 0, and\r\n3. A(Adj(A)) = det(A)In. Thus,\r\nwhenever det(A) 6= 0 one has A\r\n−1 =\r\n1\r\ndet(A)\r\nAdj(A). (2.5.2)\r\nProof. Part 1: It directly follows from Remark 2.5.8 and the definition of the cofactor.\r\nPart 2: Fix positive integers i, ℓ with 1 ≤ i 6= ℓ ≤ n. And let B = [bij ] be a square\r\nmatrix whose ℓ\r\nth row equals the ith row of A and the remaining rows of B are the same\r\nas that of A.\r\nThen by construction, the i\r\nth and ℓth rows of B are equal. Thus, by Theorem 2.5.6.5,\r\ndet(B) = 0. As A(ℓ|j) = B(ℓ|j) for 1 ≤ j ≤ n, using Remark 2.5.8, we have\r\n0 = det(B) = Xn\r\nj=1\r\n(−1)ℓ+jbℓj detB(ℓ|j)\r\n\u0001\r\n=\r\nXn\r\nj=1\r\n(−1)ℓ+jaij detB(ℓ|j)\r\n\u0001\r\n=\r\nXn\r\nj=1\r\n(−1)ℓ+jaij detA(ℓ|j)\r\n\u0001\r\n=\r\nXn\r\nj=1\r\naijCℓj . (2.5.3)\r\nThis completes the proof of Part 2.\r\nPart 3:, Using Equation (2.5.3) and Remark 2.5.8, observe that\r\n\u0014\r\nA\r\n\r\nAdj(A)\r\n\u0001\r\n\u0015\r\nij\r\n=\r\nXn\r\nk=1\r\naik\r\nAdj(A)\r\n\u0001\r\nkj =\r\nXn\r\nk=1\r\naikCjk =\r\n(\r\n0, if i 6= j,\r\ndet(A), if i = j.\r\nThus, A(Adj(A)) = det(A)In. Therefore, if det(A) 6= 0 then A\r\n\u0010\r\n1\r\ndet(A)Adj(A)\r\n\u0011\r\n= In.\r\nHence, by Theorem 2.2.6,\r\nA\r\n−1 =\r\n1\r\ndet(A)\r\nAdj(A).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/355c5844-c863-486f-8f21-df70ffaaec95.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4e9b715f2c1a4c1d478ad6a94c19c2d9d077935f4dd25a2e9ab7070f82b18680",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 365
      },
      {
        "segments": [
          {
            "segment_id": "6a0a603c-4b6d-448c-b097-2f56925314ea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 54,
            "page_width": 612,
            "page_height": 792,
            "content": "54 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nExample 2.5.16. For A =\r\n\r\n\r\n\r\n1 −1 0\r\n0 1 1\r\n1 2 1\r\n\r\n\r\n , Adj(A) =\r\n\r\n\r\n\r\n−1 1 −1\r\n1 1 −1\r\n−1 −3 1\r\n\r\n\r\n\r\nand det(A) = −2.\r\nThus, by Theorem 2.5.15.3, A−1 =\r\n\r\n\r\n\r\n1/2 −1/2 1/2\r\n−1/2 −1/2 1/2\r\n1/2 3/2 −1/2\r\n\r\n\r\n.\r\nThe next corollary is a direct consequence of Theorem 2.5.15.3 and hence the proof is\r\nomitted.\r\nCorollary 2.5.17. Let A be a non-singular matrix. Then\r\n\r\nAdj(A)\r\n\u0001\r\nA = det(A) In and Xn\r\ni=1\r\naij Cik =\r\n(\r\ndet(A), if j = k,\r\n0, if j 6= k.\r\nThe next result gives another equivalent condition for a square matrix to be invertible.\r\nTheorem 2.5.18. A square matrix A is non-singular if and only if A is invertible.\r\nProof. Let A be non-singular. Then det(A) 6= 0 and hence A−1 =\r\n1\r\ndet(A)Adj(A) as .\r\nNow, let us assume that A is invertible. Then, using Theorem 2.2.5, A = E1E2 · · · Ek, a\r\nproduct of elementary matrices. Also, by Remark 2.5.7, det(Ei) 6= 0 for each i, 1 ≤ i ≤ k.\r\nThus, by a repeated application of the first three parts of Theorem 2.5.6 gives det(A) 6= 0.\r\nHence, the required result follows.\r\nWe are now ready to prove a very important result that related the determinant of\r\nproduct of two matrices with their determinants.\r\nTheorem 2.5.19. Let A and B be square matrices of order n. Then\r\ndet(AB) = det(A) det(B) = det(BA).\r\nProof. Step 1. Let A be non-singular. Then by Theorem 2.5.15.3, A is invertible. Hence,\r\nusing Theorem 2.2.5, A = E1E2 · · · Ek, a product of elementary matrices. Then a repeated\r\napplication of the first three parts of Theorem 2.5.6 gives\r\ndet(AB) = det(E1E2 · · · EkB) = det(E1) det(E2 · · · EkB)\r\n= det(E1) det(E2) det(E3 · · · EkB)\r\n= det(E1E2) det(E3) det(E4 · · · EkB)\r\n=\r\n.\r\n.\r\n.\r\n= det(E1E2 · · · Ek) det(B) = det(A) det(B).\r\nThus, if A is non-singular then det(AB) = det(A) det(B). This will be used in the second\r\nstep.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6a0a603c-4b6d-448c-b097-2f56925314ea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5b0de373021ea01212d2b79a51b7ddf7689c3cba7eeba1009c53865c42cdab7a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 371
      },
      {
        "segments": [
          {
            "segment_id": "d703c2ff-0317-4949-a6d1-5dc5c9dc17db",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 55,
            "page_width": 612,
            "page_height": 792,
            "content": "2.5. DETERMINANT 55\r\nStep 2. Let A be singular. Then using Theorem 2.5.18 A is not invertible. Hence,\r\nthere exists an invertible matrix P such that P A = C, where C =\r\n\"\r\nC1\r\n0\r\n#\r\n. So, A = P\r\n−1C\r\nand therefore\r\ndet(AB) = det((P\r\n−1C)B) = det(P−1\r\n(CB)) = det P\r\n−1\r\n\"\r\nC1B\r\n0\r\n#!\r\n= det(P\r\n−1\r\n) · det \"C1B\r\n0\r\n#! as P\r\n−1\r\nis non-singular\r\n= det(P) · 0 = 0 = 0 · det(B) = det(A) det(B).\r\nThus, the proof of the theorem is complete.\r\nThe next result relates the determinant of a matrix with the determinant of its trans\u0002pose. As an application of this result, determinant can be computed by expanding along\r\nany column as well.\r\nTheorem 2.5.20. Let A be a square matrix. Then det(A) = det(At\r\n).\r\nProof. If A is a non-singular, Corollary 2.5.17 gives det(A) = det(At).\r\nIf A is singular, then by Theorem 2.5.18, A is not invertible. Therefore, Atis also\r\nnot invertible (as Atis invertible implies A−1 =\r\n\r\n(At)\r\n−1\r\n\u0001t\r\n)). Thus, using Theorem 2.5.18\r\nagain, det(At) = 0 = det(A). Hence the required result follows.\r\n2.5.2 Cramer’s Rule\r\nLet A be a square matrix. Then using Theorem 2.2.10 and Theorem 2.5.18, one has the\r\nfollowing result.\r\nTheorem 2.5.21. Let A be a square matrix. Then the following statements are equivalent:\r\n1. A is invertible.\r\n2. The linear system Ax = b has a unique solution for every b.\r\n3. det(A) 6= 0.\r\nThus, Ax = b has a unique solution for every b if and only if det(A) 6= 0. The next\r\ntheorem gives a direct method of finding the solution of the linear system Ax = b when\r\ndet(A) 6= 0.\r\nTheorem 2.5.22 (Cramer’s Rule). Let A be an n × n matrix. If det(A) 6= 0 then the\r\nunique solution of the linear system Ax = b is\r\nxj =\r\ndet(Aj )\r\ndet(A)\r\n, for j = 1, 2, . . . , n,\r\nwhere Aj is the matrix obtained from A by replacing the jth column of A by the column\r\nvector b.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/d703c2ff-0317-4949-a6d1-5dc5c9dc17db.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8d981130c591ce059f4834cad673b0644756b7223b6b348dfa391f5d5a43f736",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 355
      },
      {
        "segments": [
          {
            "segment_id": "0acdae26-3a59-4d04-b273-2249abed6b83",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 56,
            "page_width": 612,
            "page_height": 792,
            "content": "56 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\nProof. Since det(A) 6= 0, A is invertible and hence the row-reduced echelon form of A is\r\nI. Thus, for some invertible matrix P,\r\nRREF[A|b] = P[A|b] = [P A|Pb] = [I|d],\r\nwhere d = Ab. Hence, the system Ax = b has the unique solution xj = dj , for 1 ≤ j ≤ n.\r\nAlso,\r\n[e1, e2, . . . , en] = I = P A = [P A[:, 1], P A[:, 2], . . . , P A[:, n]].\r\nThus,\r\nP Aj = P[A[:, 1], . . . , A[:, j − 1], b, A[:, j + 1], . . . , A[:, n]]\r\n= [P A[:, 1], . . . , P A[:, j − 1], Pb, P A[:, j + 1], . . . , P A[:, n]]\r\n= [e1, . . . , ej−1, d, ej+1, . . . , en]\r\nand hence det(P Aj ) = dj , for 1 ≤ j ≤ n. Therefore,\r\ndet(Aj )\r\ndet(A)\r\n=\r\ndet(P) det(Aj )\r\ndet(P) det(A)\r\n=\r\ndet(P Aj )\r\ndet(P A)\r\n=\r\ndj\r\n1\r\n= dj .\r\nHence, xj =\r\ndet(Aj )\r\ndet(A)\r\nand the required result follows.\r\nIn Theorem 2.5.22 A1 =\r\n\r\n\r\n\r\n\r\n\r\n\r\nb1 a12 · · · a1n\r\nb2 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nbn an2 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\n, A2 =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 b1 a13 · · · a1n\r\na21 b2 a23 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 bn an3 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\nand so\r\non till An =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 · · · a1n−1 b1\r\na12 · · · a2n−1 b2\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\na1n · · · ann−1 bn\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nExample 2.5.23. Solve Ax = b using Cramer’s rule, where A =\r\n\r\n\r\n\r\n1 2 3\r\n2 3 1\r\n1 2 2\r\n\r\n\r\n\r\nand b =\r\n\r\n\r\n\r\n1\r\n1\r\n1\r\n\r\n\r\n.\r\nSolution: Check that det(A) = 1 and x\r\nt = (−1, 1, 0) as\r\nx1 =\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1 2 3\r\n1 3 1\r\n1 2 2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= −1, x2 =\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1 1 3\r\n2 1 1\r\n1 1 2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= 1, and x3 =\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n1 2 1\r\n2 3 1\r\n1 2 1\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n= 0.\r\n2.6 Miscellaneous Exercises\r\nExercise 2.6.1. 1. Show that a triangular matrix A is invertible if and only if each\r\ndiagonal entry of A is non-zero.\r\n2. Let A be an orthogonal matrix. Prove that det A = ±1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0acdae26-3a59-4d04-b273-2249abed6b83.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=068c86d95f9c2c09c9f8dbbe5f7aed128b475c879374cc74e98843238128bebf",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 480
      },
      {
        "segments": [
          {
            "segment_id": "aa4a1791-9941-4776-9a47-a1fe0d249790",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 57,
            "page_width": 612,
            "page_height": 792,
            "content": "2.6. MISCELLANEOUS EXERCISES 57\r\n3. Prove that every 2 × 2 matrix A satisfying tr(A) = 0 and det(A) = 0 is a nilpotent\r\nmatrix.\r\n4. Let A and B be two non-singular matrices. Are the matrices A + B and A − B\r\nnon-singular? Justify your answer.\r\n5. Let A be an n × n matrix. Prove that the following statements are equivalent:\r\n(a) A is not invertible.\r\n(b) rank(A) 6= n.\r\n(c) det(A) = 0.\r\n(d) A is not row-equivalent to In.\r\n(e) The homogeneous system Ax = 0 has a non-trivial solution.\r\n(f) The system Ax = b is either inconsistent or it is consistent and in this case it\r\nhas an infinite number of solutions.\r\n(g) A is not a product of elementary matrices.\r\n6. For what value(s) of λ does the following systems have non-trivial solutions? Also,\r\nfor each value of λ, determine a non-trivial solution.\r\n(a) (λ − 2)x + y = 0, x + (λ + 2)y = 0.\r\n(b) λx + 3y = 0, (λ + 6)y = 0.\r\n7. Let x1, x2, . . . , xn be fixed reals numbers and define A = [aij ]n×n with aij = x\r\nj−1\r\ni\r\n.\r\nProve that det(A) = Q\r\n1≤i<j≤n\r\n(xj − xi). This matrix is usually called the Van-der\r\nmonde matrix.\r\n8. Let A = [aij ]n×n with aij = max{i, j}. Prove that det A = (−1)n−1n.\r\n9. Let A = [aij ]n×n with aij = 1\r\ni+j−1\r\n. Using induction, prove that A is invertible. This\r\nmatrix is commonly known as the Hilbert matrix.\r\n10. Solve the following system of equations by Cramer’s rule.\r\ni) x + y + z − w = 1, x + y − z + w = 2, 2x + y + z − w = 7, x + y + z + w = 3.\r\nii) x − y + z − w = 1, x + y − z + w = 2, 2x + y − z − w = 7, x − y − z + w = 3.\r\n11. Suppose A = [aij ] and B = [bij ] are two n × n matrices with bij = p\r\ni−jaij for\r\n1 ≤ i, j ≤ n for some non-zero p ∈ R. Then compute det(B) in terms of det(A).\r\n12. The position of an element aij of a determinant is called even or odd according as\r\ni + j is even or odd. Show that\r\n(a) If all the entries in odd positions are multiplied with −1 then the value of the\r\ndeterminant doesn’t change.\r\n(b) If all entries in even positions are multiplied with −1 then the determinant",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/aa4a1791-9941-4776-9a47-a1fe0d249790.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e35f4b7c7846742f3c446be3bbd18795e2a43d99e8b2a3c3766659bd64cb643c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 452
      },
      {
        "segments": [
          {
            "segment_id": "f3a37db3-b8fa-4e9e-8c2d-e857a0d9c3f5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 58,
            "page_width": 612,
            "page_height": 792,
            "content": "58 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS\r\ni. does not change if the matrix is of even order.\r\nii. is multiplied by −1 if the matrix is of odd order.\r\n13. Let A be a Hermitian (A∗ = At) matrix. Prove that det A is a real number.\r\n14. Let A be an n × n matrix. Then A is invertible if and only if Adj(A) is invertible.\r\n15. Let A and B be invertible matrices. Prove that Adj(AB) = Adj(B)Adj(A).\r\n16. Let P =\r\n\"\r\nA B\r\nC D#\r\nbe a rectangular matrix with A a square matrix of order n and\r\n|A| 6= 0. Then show that rank (P) = n if and only if D = CA−1B.\r\n2.7 Summary\r\nIn this chapter, we started with a system of linear equations Ax = b and related it to the\r\naugmented matrix [A |b]. We applied row operations to [A |b] to get its row echelon form\r\nand the row-reduced echelon forms. Depending on the row echelon matrix, say [C |d], thus\r\nobtained, we had the following result:\r\n1. If [C |d] has a row of the form [0 |1] then the linear system Ax = b has not solution.\r\n2. Suppose [C |d] does not have any row of the form [0 |1] then the linear system Ax = b\r\nhas at least one solution.\r\n(a) If the number of leading terms equals the number of unknowns then the system\r\nAx = b has a unique solution.\r\n(b) If the number of leading terms is less than the number of unknowns then the\r\nsystem Ax = b has an infinite number of solutions.\r\nThe following conditions are equivalent for an n × n matrix A.\r\n1. A is invertible.\r\n2. The homogeneous system Ax = 0 has only the trivial solution.\r\n3. The row reduced echelon form of A is I.\r\n4. A is a product of elementary matrices.\r\n5. The system Ax = b has a unique solution for every b.\r\n6. The system Ax = b has a solution for every b.\r\n7. rank(A) = n.\r\n8. det(A) 6= 0.\r\nSuppose the matrix A in the linear system Ax = b is of size m × n. Then exactly one\r\nof the following statement holds:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/f3a37db3-b8fa-4e9e-8c2d-e857a0d9c3f5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=89839c9bbb756e7c5dd3d28824089b17c16c064bd9531be41b217b4c313fb169",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 379
      },
      {
        "segments": [
          {
            "segment_id": "58b5f2e9-9fba-49d9-9ea8-9f5e193c58ef",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 59,
            "page_width": 612,
            "page_height": 792,
            "content": "2.7. SUMMARY 59\r\n1. if rank(A) < rank([A |b]), then the system Ax = b has no solution.\r\n2. if rank(A) = rank([A |b]), then the system Ax = b is consistent. Furthermore,\r\n(a) if rank(A) = n then the system Ax = b has a unique solution.\r\n(b) if rank(A) < n then the system Ax = b has an infinite number of solutions.\r\nWe also dealt with the following type of problems:\r\n1. Solving the linear system Ax = b. In the next chapter, we will see that this leads us\r\nto the question “is the vector b a linear combination of the columns of A”?\r\n2. Solving the linear system Ax = 0. In the next chapter, we will see that this leads us\r\nto the question “are the columns of A linearly independent/dependent”?\r\n(a) If Ax = 0 has a unique solution, the trivial solution, then the columns of A are\r\nlinear independent.\r\n(b) If Ax = 0 has an infinite number of solutions then the columns of A are linearly\r\ndependent.\r\n3. Let b\r\nt = [b1, b2, . . . , bm]. Find conditions of the bi\r\n’s such that the linear system\r\nAx = b always has a solution. Observe that for different choices of x the vector Ax\r\ngives rise to vectors that are linear combination of the columns of A. This idea will\r\nbe used in the next chapter, to get the geometrical representation of the linear span\r\nof the columns of A.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/58b5f2e9-9fba-49d9-9ea8-9f5e193c58ef.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cf1b2216090349671099197c0d64da5557a6bd72fb9352e861c020d8cbafdb72",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "9b3ed470-aca2-4591-a70c-9074302c497d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 60,
            "page_width": 612,
            "page_height": 792,
            "content": "60 CHAPTER 2. SYSTEM OF LINEAR EQUATIONS",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9b3ed470-aca2-4591-a70c-9074302c497d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d65857779833f9bcdc83df0f8c56e3ae5c77f32ba2ce0ec2f9e09aae96696798",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 259
      },
      {
        "segments": [
          {
            "segment_id": "cc158db9-f3f0-4219-9551-ee1db3f4abd7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 61,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 3\r\nFinite Dimensional Vector Spaces\r\n3.1 Finite Dimensional Vector Spaces\r\nRecall that the set of real numbers were denoted by R and the set of complex numbers\r\nwere denoted by C. Also, we wrote F to denote either the set R or the set C.\r\nLet A be an m×n complex matrix. Then using Theorem 2.1.5, we see that the solution\r\nset of the homogeneous system Ax = 0, denoted V , satisfies the following properties:\r\n1. The vector 0 ∈ V as A0 = 0.\r\n2. If x ∈ V then A(αx) = α(Ax) = 0 for all α ∈ C. Hence, αx ∈ V for any complex\r\nnumber α. In particular, −x ∈ V whenever x ∈ V .\r\n3. Let x, y ∈ V . Then for any α, β ∈ C, αx, βy ∈ V and A(αx + βy) = 0 + 0 = 0. In\r\nparticular, x + y ∈ V and x + y = y + x. Also, (x + y) + z = x + (y + z).\r\nThat is, the solution set of a homogeneous linear system satisfies some nice properties. We\r\nuse these properties to define a set and devote this chapter to the study of the structure\r\nof such sets. We will also see that the set of real numbers, R, the Euclidean plane, R\r\n2 and\r\nthe Euclidean space, R\r\n3\r\n, are examples of this set. We start with the following definition.\r\nDefinition 3.1.1 (Vector Space). A vector space over F, denoted V (F) or in short V (if\r\nthe field F is clear from the context), is a non-empty set, satisfying the following axioms:\r\n1. Vector Addition: To every pair u, v ∈ V there corresponds a unique element\r\nu ⊕ v in V (called the addition of vectors) such that\r\n(a) u ⊕ v = v ⊕ u (Commutative law).\r\n(b) (u ⊕ v) ⊕ w = u ⊕ (v ⊕ w) (Associative law).\r\n(c) There is a unique element 0 in V (the zero vector) such that u ⊕ 0 = u, for\r\nevery u ∈ V (called the additive identity).\r\n(d) For every u ∈ V there is a unique element −u ∈ V such that u ⊕ (−u) = 0\r\n(called the additive inverse).\r\n61",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/cc158db9-f3f0-4219-9551-ee1db3f4abd7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fd2c179c39f59d2c0e2257013b67c0b450088fb8d08af831fb9494a3a845c096",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 384
      },
      {
        "segments": [
          {
            "segment_id": "b0e508a7-0325-4ad2-ae19-07fc0529341d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 62,
            "page_width": 612,
            "page_height": 792,
            "content": "62 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n2. Scalar Multiplication: For each u ∈ V and α ∈ F, there corresponds a unique\r\nelement α ⊙ u in V (called the scalar multiplication) such that\r\n(a) α · (β ⊙ u) = (αβ) ⊙ u for every α, β ∈ F and u ∈ V.\r\n(b) 1 ⊙ u = u for every u ∈ V, where 1 ∈ R.\r\n3. Distributive Laws: relating vector addition with scalar multiplication\r\nFor any α, β ∈ F and u, v ∈ V, the following distributive laws hold:\r\n(a) α ⊙ (u ⊕ v) = (α ⊙ u) ⊕ (α ⊙ v).\r\n(b) (α + β) ⊙ u = (α ⊙ u) ⊕ (β ⊙ u).\r\nNote: the number 0 is the element of F whereas 0 is the zero vector.\r\nRemark 3.1.2. The elements of F are called scalars, and that of V are called vectors.\r\nIf F = R, the vector space is called a real vector space. If F = C, the vector space is\r\ncalled a complex vector space.\r\nSome interesting consequences of Definition 3.1.1 is the following useful result. Intu\u0002itively, these results seem to be obvious but for better understanding of the axioms it is\r\ndesirable to go through the proof.\r\nTheorem 3.1.3. Let V be a vector space over F. Then\r\n1. u ⊕ v = u implies v = 0.\r\n2. α ⊙ u = 0 if and only if either u is the zero vector or α = 0.\r\n3. (−1) ⊙ u = −u for every u ∈ V.\r\nProof. Part 1: For each u ∈ V, by Axiom 3.1.1.1d there exists −u ∈ V such that −u⊕u =\r\n0. Hence, u ⊕ v = u is equivalent to\r\n−u ⊕ (u ⊕ v) = −u ⊕ u ⇐⇒ (−u ⊕ u) ⊕ v = 0 ⇐⇒ 0 ⊕ v = 0 ⇐⇒ v = 0.\r\nPart 2: As 0 = 0 ⊕ 0, using Axiom 3.1.1.3, we have\r\nα ⊙ 0 = α ⊙ (0 ⊕ 0) = (α ⊙ 0) ⊕ (α ⊙ 0).\r\nThus, for any α ∈ F, Axiom 3.1.1.3a gives α ⊙ 0 = 0. In the same way,\r\n0 ⊙ u = (0 + 0) ⊙ u = (0 ⊙ u) ⊕ (0 ⊙ u).\r\nHence, using Axiom 3.1.1.3a, one has 0 ⊙ u = 0 for any u ∈ V.\r\nNow suppose α⊙u = 0. If α = 0 then the proof is over. Therefore, let us assume α 6= 0\r\n(note that α is a real or complex number, hence 1\r\nα\r\nexists and\r\n0 =\r\n1\r\nα\r\n⊙ 0 =\r\n1\r\nα\r\n⊙ (α ⊙ u) = ( 1\r\nα\r\nα) ⊙ u = 1 ⊙ u = u\r\nas 1 ⊙ u = u for every vector u ∈ V. Thus, if α 6= 0 and α ⊙ u = 0 then u = 0.\r\nPart 3: As 0 = 0u = (1 + (−1))u = u + (−1)u, one has (−1)u = −u.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b0e508a7-0325-4ad2-ae19-07fc0529341d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ac7399be3a97be0e5894c0432ddf812b80441a23dae79c4c2b728be9ba1d8d11",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 510
      },
      {
        "segments": [
          {
            "segment_id": "16a8d3e8-402b-4eeb-8a65-192bb64dc0eb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 63,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. FINITE DIMENSIONAL VECTOR SPACES 63\r\nExample 3.1.4. The readers are advised to justify the statements made in the examples\r\ngiven below.\r\n1. Let A be an m × n matrix with complex entries and suppose rank(A) = r ≤ n. Let\r\nV denote the solution set of Ax = 0. Then using Theorem 2.4.1, we know that V\r\ncontains at least the trivial solution, the 0 vector. Thus, check that the set V satisfies\r\nall the axioms stated in Definition 3.1.1 (some of them were proved to motivate this\r\nchapter).\r\n2. The set R of real numbers, with the usual addition and multiplication of real numbers\r\n(i.e., ⊕ ≡ + and ⊙ ≡ ·) forms a vector space over R.\r\n3. Let R\r\n2 = {(x1, x2) : x1, x2 ∈ R}. Then for x1, x2, y1, y2 ∈ R and α ∈ R, define\r\n(x1, x2) ⊕ (y1, y2) = (x1 + y1, x2 + y2) and α ⊙ (x1, x2) = (αx1, αx2).\r\nThen R\r\n2\r\nis a real vector space.\r\n4. Let R\r\nn = {(a1, a2, . . . , an) : ai ∈ R, 1 ≤ i ≤ n} be the set of n-tuples of real numbers.\r\nFor u = (a1, . . . , an), v = (b1, . . . , bn) in V and α ∈ R, we define\r\nu ⊕ v = (a1 + b1, . . . , an + bn) and α ⊙ u = (αa1, . . . , αan)\r\n(called component wise operations). Then V is a real vector space. This vector space\r\nR\r\nn\r\nis called the real vector space of n-tuples.\r\nRecall that the symbol i represents the complex number √−1.\r\n5. Consider the set C = {x + iy : x, y ∈ R} of complex numbers and let z1 = x1 + iy1\r\nand z2 = x2 + iy2. Define\r\nz1 ⊕ z2 = (x1 + x2) + i(y1 + y2), and\r\n(a) for any α ∈ R, define α ⊙ z1 = (αx1) + i(αy1). Then C is a real vector space as\r\nthe scalars are the real numbers.\r\n(b) (α + iβ) ⊙ (x1 + iy1) = (αx1 − βy1) + i(αy1 + βx1) for any α + iβ ∈ C. Here,\r\nthe scalars are complex numbers and hence C forms a complex vector space.\r\n6. Let C\r\nn = {(z1, z2, . . . , zn) : zi ∈ C, 1 ≤ i ≤ n}. For (z1, . . . , zn),(w1, . . . , wn) ∈ Cn\r\nand α ∈ F, define\r\n(z1, . . . , zn) ⊕ (w1, . . . , wn) = (z1 + w1, . . . , zn + wn), and\r\nα ⊙ (z1, . . . , zn) = (αz1, . . . , αzn).\r\nThen it can be verified that C\r\nn\r\nforms a vector space over C (called complex vector\r\nspace) as well as over R (called real vector space). Whenever there is no mention of\r\nscalars, it will always be assumed to be C, the complex numbers.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/16a8d3e8-402b-4eeb-8a65-192bb64dc0eb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0cac41e29299435937e06e29e6ce3eaae80aa73f613dd297bdd85de5a496a0b0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 520
      },
      {
        "segments": [
          {
            "segment_id": "16a8d3e8-402b-4eeb-8a65-192bb64dc0eb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 63,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. FINITE DIMENSIONAL VECTOR SPACES 63\r\nExample 3.1.4. The readers are advised to justify the statements made in the examples\r\ngiven below.\r\n1. Let A be an m × n matrix with complex entries and suppose rank(A) = r ≤ n. Let\r\nV denote the solution set of Ax = 0. Then using Theorem 2.4.1, we know that V\r\ncontains at least the trivial solution, the 0 vector. Thus, check that the set V satisfies\r\nall the axioms stated in Definition 3.1.1 (some of them were proved to motivate this\r\nchapter).\r\n2. The set R of real numbers, with the usual addition and multiplication of real numbers\r\n(i.e., ⊕ ≡ + and ⊙ ≡ ·) forms a vector space over R.\r\n3. Let R\r\n2 = {(x1, x2) : x1, x2 ∈ R}. Then for x1, x2, y1, y2 ∈ R and α ∈ R, define\r\n(x1, x2) ⊕ (y1, y2) = (x1 + y1, x2 + y2) and α ⊙ (x1, x2) = (αx1, αx2).\r\nThen R\r\n2\r\nis a real vector space.\r\n4. Let R\r\nn = {(a1, a2, . . . , an) : ai ∈ R, 1 ≤ i ≤ n} be the set of n-tuples of real numbers.\r\nFor u = (a1, . . . , an), v = (b1, . . . , bn) in V and α ∈ R, we define\r\nu ⊕ v = (a1 + b1, . . . , an + bn) and α ⊙ u = (αa1, . . . , αan)\r\n(called component wise operations). Then V is a real vector space. This vector space\r\nR\r\nn\r\nis called the real vector space of n-tuples.\r\nRecall that the symbol i represents the complex number √−1.\r\n5. Consider the set C = {x + iy : x, y ∈ R} of complex numbers and let z1 = x1 + iy1\r\nand z2 = x2 + iy2. Define\r\nz1 ⊕ z2 = (x1 + x2) + i(y1 + y2), and\r\n(a) for any α ∈ R, define α ⊙ z1 = (αx1) + i(αy1). Then C is a real vector space as\r\nthe scalars are the real numbers.\r\n(b) (α + iβ) ⊙ (x1 + iy1) = (αx1 − βy1) + i(αy1 + βx1) for any α + iβ ∈ C. Here,\r\nthe scalars are complex numbers and hence C forms a complex vector space.\r\n6. Let C\r\nn = {(z1, z2, . . . , zn) : zi ∈ C, 1 ≤ i ≤ n}. For (z1, . . . , zn),(w1, . . . , wn) ∈ Cn\r\nand α ∈ F, define\r\n(z1, . . . , zn) ⊕ (w1, . . . , wn) = (z1 + w1, . . . , zn + wn), and\r\nα ⊙ (z1, . . . , zn) = (αz1, . . . , αzn).\r\nThen it can be verified that C\r\nn\r\nforms a vector space over C (called complex vector\r\nspace) as well as over R (called real vector space). Whenever there is no mention of\r\nscalars, it will always be assumed to be C, the complex numbers.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/16a8d3e8-402b-4eeb-8a65-192bb64dc0eb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0cac41e29299435937e06e29e6ce3eaae80aa73f613dd297bdd85de5a496a0b0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 520
      },
      {
        "segments": [
          {
            "segment_id": "b20cd503-f3eb-4b1e-bf0c-dd0bb33696ea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 64,
            "page_width": 612,
            "page_height": 792,
            "content": "64 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nRemark 3.1.5. If the scalars are C then i(1, 0) = (i, 0) is allowed. Whereas, if the\r\nscalars are R then i(1, 0) 6= (i, 0).\r\n7. Fix a positive integer n and let Pn(R) denote the set of all polynomials in x of degree\r\n≤ n with coefficients from R. Algebraically,\r\nPn(R) = {a0 + a1x + a2x\r\n2 + · · · + anxn\r\n: ai ∈ R, 0 ≤ i ≤ n}.\r\nLet f(x) = a0 + a1x + a2x\r\n2 + · · · + anxn\r\n, g(x) = b0 + b1x + b2x\r\n2 + · · · + bnxn ∈ Pn(R)\r\nfor some ai, bi ∈ R, 0 ≤ i ≤ n. It can be verified that Pn(R) is a real vector space\r\nwith the addition and scalar multiplication defined by\r\nf(x) ⊕ g(x) = (a0 + b0) + (a1 + b1)x + · · · + (an + bn)x\r\nn\r\n, and\r\nα ⊙ f(x) = αa0 + αa1x + · · · + αanx\r\nn\r\nfor α ∈ R.\r\n8. Let P(R) be the set of all polynomials with real coefficients. As any polynomial\r\na0 + a1x + · · · + amx\r\nm also equals a0 + a1x + · · · + amxm + 0 · xm+1 + · · · + 0 · xp\r\n,\r\nwhenever p > m, let f(x) = a0 +a1x+· · ·+apx\r\np\r\n, g(x) = b0 +b1x+· · ·+bpx\r\np ∈ P(R)\r\nfor some ai, bi ∈ R, 0 ≤ i ≤ p. So, with vector addition and scalar multiplication is\r\ndefined below (called coefficient-wise), P(R) forms a real vector space.\r\nf(x) ⊕ g(x) = (a0 + b0) + (a1 + b1)x + · · · + (ap + bp)x\r\np\r\nand\r\nα ⊙ f(x) = αa0 + αa1x + · · · + αapx\r\np\r\nfor α ∈ R.\r\n9. Let P(C) be the set of all polynomials with complex coefficients. Then with respect to\r\nvector addition and scalar multiplication defined coefficient-wise, the set P(C) forms\r\na vector space.\r\n10. Let V = R\r\n+ = {x ∈ R : x > 0}. This is not a vector space under usual operations\r\nof addition and scalar multiplication (why?). But R\r\n+ is a real vector space with 1 as\r\nthe additive identity if we define vector addition and scalar multiplication by\r\nu ⊕ v = u · v and α ⊙ u = u\r\nα\r\nfor all u, v ∈ R\r\n+ and α ∈ R.\r\n11. Let V = {(x, y) : x, y ∈ R}. For any α ∈ R and x = (x1, x2), y = (y1, y2) ∈ V , let\r\nx ⊕ y = (x1 + y1 + 1, x2 + y2 − 3) and α ⊙ x = (αx1 + α − 1, αx2 − 3α + 3).\r\nThen V is a real vector space with (−1, 3) as the additive identity.\r\n12. Let M2(C) denote the set of all 2 × 2 matrices with complex entries. Then M2(C)\r\nforms a vector space with vector addition and scalar multiplication defined by\r\nA ⊕ B =\r\n\"\r\na1 a2\r\na3 a4\r\n#\r\n⊕\r\n\"\r\nb1 b2\r\nb3 b4\r\n#\r\n=\r\n\"\r\na1 + b1 a2 + b2\r\na3 + b3 a4 + b4\r\n#\r\n, α ⊙ A =\r\n\"\r\nαa1 αa2\r\nαa3 αa4\r\n#\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b20cd503-f3eb-4b1e-bf0c-dd0bb33696ea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb130a0f02c3e6bdfc018fd35772da69f5db2a33048df51592762c8873fb3881",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 576
      },
      {
        "segments": [
          {
            "segment_id": "b20cd503-f3eb-4b1e-bf0c-dd0bb33696ea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 64,
            "page_width": 612,
            "page_height": 792,
            "content": "64 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nRemark 3.1.5. If the scalars are C then i(1, 0) = (i, 0) is allowed. Whereas, if the\r\nscalars are R then i(1, 0) 6= (i, 0).\r\n7. Fix a positive integer n and let Pn(R) denote the set of all polynomials in x of degree\r\n≤ n with coefficients from R. Algebraically,\r\nPn(R) = {a0 + a1x + a2x\r\n2 + · · · + anxn\r\n: ai ∈ R, 0 ≤ i ≤ n}.\r\nLet f(x) = a0 + a1x + a2x\r\n2 + · · · + anxn\r\n, g(x) = b0 + b1x + b2x\r\n2 + · · · + bnxn ∈ Pn(R)\r\nfor some ai, bi ∈ R, 0 ≤ i ≤ n. It can be verified that Pn(R) is a real vector space\r\nwith the addition and scalar multiplication defined by\r\nf(x) ⊕ g(x) = (a0 + b0) + (a1 + b1)x + · · · + (an + bn)x\r\nn\r\n, and\r\nα ⊙ f(x) = αa0 + αa1x + · · · + αanx\r\nn\r\nfor α ∈ R.\r\n8. Let P(R) be the set of all polynomials with real coefficients. As any polynomial\r\na0 + a1x + · · · + amx\r\nm also equals a0 + a1x + · · · + amxm + 0 · xm+1 + · · · + 0 · xp\r\n,\r\nwhenever p > m, let f(x) = a0 +a1x+· · ·+apx\r\np\r\n, g(x) = b0 +b1x+· · ·+bpx\r\np ∈ P(R)\r\nfor some ai, bi ∈ R, 0 ≤ i ≤ p. So, with vector addition and scalar multiplication is\r\ndefined below (called coefficient-wise), P(R) forms a real vector space.\r\nf(x) ⊕ g(x) = (a0 + b0) + (a1 + b1)x + · · · + (ap + bp)x\r\np\r\nand\r\nα ⊙ f(x) = αa0 + αa1x + · · · + αapx\r\np\r\nfor α ∈ R.\r\n9. Let P(C) be the set of all polynomials with complex coefficients. Then with respect to\r\nvector addition and scalar multiplication defined coefficient-wise, the set P(C) forms\r\na vector space.\r\n10. Let V = R\r\n+ = {x ∈ R : x > 0}. This is not a vector space under usual operations\r\nof addition and scalar multiplication (why?). But R\r\n+ is a real vector space with 1 as\r\nthe additive identity if we define vector addition and scalar multiplication by\r\nu ⊕ v = u · v and α ⊙ u = u\r\nα\r\nfor all u, v ∈ R\r\n+ and α ∈ R.\r\n11. Let V = {(x, y) : x, y ∈ R}. For any α ∈ R and x = (x1, x2), y = (y1, y2) ∈ V , let\r\nx ⊕ y = (x1 + y1 + 1, x2 + y2 − 3) and α ⊙ x = (αx1 + α − 1, αx2 − 3α + 3).\r\nThen V is a real vector space with (−1, 3) as the additive identity.\r\n12. Let M2(C) denote the set of all 2 × 2 matrices with complex entries. Then M2(C)\r\nforms a vector space with vector addition and scalar multiplication defined by\r\nA ⊕ B =\r\n\"\r\na1 a2\r\na3 a4\r\n#\r\n⊕\r\n\"\r\nb1 b2\r\nb3 b4\r\n#\r\n=\r\n\"\r\na1 + b1 a2 + b2\r\na3 + b3 a4 + b4\r\n#\r\n, α ⊙ A =\r\n\"\r\nαa1 αa2\r\nαa3 αa4\r\n#\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b20cd503-f3eb-4b1e-bf0c-dd0bb33696ea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb130a0f02c3e6bdfc018fd35772da69f5db2a33048df51592762c8873fb3881",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 576
      },
      {
        "segments": [
          {
            "segment_id": "06f74e6d-dde9-4f26-9543-488689e8e41c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 65,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. FINITE DIMENSIONAL VECTOR SPACES 65\r\n13. Fix positive integers m and n and let Mm×n(C) denote the set of all m × n matrices\r\nwith complex entries. Then Mm×n(C) is a vector space with vector addition and\r\nscalar multiplication defined by\r\nA ⊕ B = [aij ] ⊕ [bij ] = [aij + bij ], α ⊙ A = α ⊙ [aij ] = [αaij ].\r\nIn case m = n, the vector space Mm×n(C) will be denoted by Mn(C).\r\n14. Let C([−1, 1]) be the set of all real valued continuous functions on the interval [−1, 1].\r\nThen C([−1, 1]) forms a real vector space if for all x ∈ [−1, 1], we define\r\n(f ⊕ g)(x) = f(x) + g(x) for all f, g ∈ C([−1, 1]) and\r\n(α ⊙ f)(x) = αf(x) for all α ∈ R and f ∈ C([−1, 1]).\r\n15. Let V and W be vector spaces over F, with operations (+, •) and (⊕, ⊙), respectively.\r\nLet V × W = {(v, w) : v ∈ V, w ∈ W}. Then V × W forms a vector space over F, if\r\nfor every (v1, w1),(v2, w2) ∈ V × W and α ∈ R, we define\r\n(v1, w1) ⊕\r\n′\r\n(v2, w2) = (v1 + v2, w1 ⊕ w2), and\r\nα ◦ (v1, w1) = (α • v1, α ⊙ w1).\r\nv1 + v2 and w1 ⊕ w2 on the right hand side mean vector addition in V and W,\r\nrespectively. Similarly, α • v1 and α ⊙ w1 correspond to scalar multiplication in V\r\nand W, respectively.\r\nFrom now on, we will use ‘u + v’ for ‘u ⊕ v’ and ‘α · u or αu’ for ‘α ⊙ u’.\r\nExercise 3.1.6. 1. Verify all the axioms are satisfied in all the examples of vector\r\nspaces considered in Example 3.1.4.\r\n2. Prove that the set Mm×n(R) for fixed positive integers m and n forms a real vector\r\nspace with usual operations of matrix addition and scalar multiplication.\r\n3. Let V = {(x, y) : x, y ∈ R\r\n2}. For x = (x1, x2), y = (y1, y2) ∈ V , define\r\nx + y = (x1 + y1, x2 + y2) and αx = (αx1, 0)\r\nfor all α ∈ R. Is V a vector space? Give reasons for your answer.\r\n4. Let a, b ∈ R with a < b. Then prove that C([a, b]), the set of all complex valued\r\ncontinuous functions on [a, b] forms a vector space if for all x ∈ [a, b], we define\r\n(f ⊕ g)(x) = f(x) + g(x) for all f, g ∈ C([a, b]) and\r\n(α ⊙ f)(x) = αf(x) for all α ∈ R and f ∈ C([a, b]).\r\n5. Prove that C(R), the set of all real valued continuous functions on R forms a vector\r\nspace if for all x ∈ R, we define\r\n(f ⊕ g)(x) = f(x) + g(x) for all f, g ∈ C(R) and\r\n(α ⊙ f)(x) = αf(x) for all α ∈ R and f ∈ C(R).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/06f74e6d-dde9-4f26-9543-488689e8e41c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a2c8ec348b4d91c13b3ea1d45d8625761781300855f65980037bc9c6ac995c60",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "06f74e6d-dde9-4f26-9543-488689e8e41c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 65,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. FINITE DIMENSIONAL VECTOR SPACES 65\r\n13. Fix positive integers m and n and let Mm×n(C) denote the set of all m × n matrices\r\nwith complex entries. Then Mm×n(C) is a vector space with vector addition and\r\nscalar multiplication defined by\r\nA ⊕ B = [aij ] ⊕ [bij ] = [aij + bij ], α ⊙ A = α ⊙ [aij ] = [αaij ].\r\nIn case m = n, the vector space Mm×n(C) will be denoted by Mn(C).\r\n14. Let C([−1, 1]) be the set of all real valued continuous functions on the interval [−1, 1].\r\nThen C([−1, 1]) forms a real vector space if for all x ∈ [−1, 1], we define\r\n(f ⊕ g)(x) = f(x) + g(x) for all f, g ∈ C([−1, 1]) and\r\n(α ⊙ f)(x) = αf(x) for all α ∈ R and f ∈ C([−1, 1]).\r\n15. Let V and W be vector spaces over F, with operations (+, •) and (⊕, ⊙), respectively.\r\nLet V × W = {(v, w) : v ∈ V, w ∈ W}. Then V × W forms a vector space over F, if\r\nfor every (v1, w1),(v2, w2) ∈ V × W and α ∈ R, we define\r\n(v1, w1) ⊕\r\n′\r\n(v2, w2) = (v1 + v2, w1 ⊕ w2), and\r\nα ◦ (v1, w1) = (α • v1, α ⊙ w1).\r\nv1 + v2 and w1 ⊕ w2 on the right hand side mean vector addition in V and W,\r\nrespectively. Similarly, α • v1 and α ⊙ w1 correspond to scalar multiplication in V\r\nand W, respectively.\r\nFrom now on, we will use ‘u + v’ for ‘u ⊕ v’ and ‘α · u or αu’ for ‘α ⊙ u’.\r\nExercise 3.1.6. 1. Verify all the axioms are satisfied in all the examples of vector\r\nspaces considered in Example 3.1.4.\r\n2. Prove that the set Mm×n(R) for fixed positive integers m and n forms a real vector\r\nspace with usual operations of matrix addition and scalar multiplication.\r\n3. Let V = {(x, y) : x, y ∈ R\r\n2}. For x = (x1, x2), y = (y1, y2) ∈ V , define\r\nx + y = (x1 + y1, x2 + y2) and αx = (αx1, 0)\r\nfor all α ∈ R. Is V a vector space? Give reasons for your answer.\r\n4. Let a, b ∈ R with a < b. Then prove that C([a, b]), the set of all complex valued\r\ncontinuous functions on [a, b] forms a vector space if for all x ∈ [a, b], we define\r\n(f ⊕ g)(x) = f(x) + g(x) for all f, g ∈ C([a, b]) and\r\n(α ⊙ f)(x) = αf(x) for all α ∈ R and f ∈ C([a, b]).\r\n5. Prove that C(R), the set of all real valued continuous functions on R forms a vector\r\nspace if for all x ∈ R, we define\r\n(f ⊕ g)(x) = f(x) + g(x) for all f, g ∈ C(R) and\r\n(α ⊙ f)(x) = αf(x) for all α ∈ R and f ∈ C(R).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/06f74e6d-dde9-4f26-9543-488689e8e41c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a2c8ec348b4d91c13b3ea1d45d8625761781300855f65980037bc9c6ac995c60",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "36b5592b-fa5c-4cff-b0e7-c7c814bf1f50",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 66,
            "page_width": 612,
            "page_height": 792,
            "content": "66 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n3.1.1 Subspaces\r\nDefinition 3.1.7 (Vector Subspace). Let S be a non-empty subset of V. The set S over\r\nF is said to be a subspace of V (F) if S in itself is a vector space, where the vector addition\r\nand scalar multiplication are the same as that of V (F).\r\nExample 3.1.8. 1. Let V (F) be a vector space. Then the sets given below are subspaces\r\nof V. They are called trivial subspaces.\r\n(a) S = {0}, consisting only of the zero vector 0 and\r\n(b) S = V , the whole space.\r\n2. Let S = {(x, y, z) ∈ R\r\n3\r\n: x + 2y − z = 0}. Then S is a subspace of R\r\n3\r\n(S is a plane\r\nin R\r\n3 passing through the origin).\r\n3. Let S = {(x, y, z) ∈ R\r\n3\r\n: x + y + z = 0, x − y − z = 0}. Then S is a subspace of R\r\n3\r\n(S is a line in R\r\n3 passing through the origin).\r\n4. Let S = {(x, y, z) ∈ R\r\n3\r\n: z − 3x = 0}. Then S is a subspace of R\r\n3\r\n.\r\n5. The vector space Pn(R) is a subspace of the vector space P(R).\r\n6. Prove that S = {(x, y, z) ∈ R\r\n3\r\n: x + y + z = 3} is not a subspace of R\r\n3\r\n(S is still a\r\nplane in R\r\n3\r\nbut it does not pass through the origin).\r\n7. Prove that W = {(x, 0) ∈ R\r\n2\r\n: x ∈ R} is a subspace of R\r\n2\r\n.\r\n8. Let W = {(x, 0) ∈ V : x ∈ R}, where V is the vector space of Example 3.1.4.11.\r\nThen (x, 0) ⊕ (y, 0) = (x + y + 1, −3) 6∈ W. Hence W is not a subspace of V but\r\nS = {(x, 3) : x ∈ R} is a subspace of V . Note that the zero vector (−1, 3) ∈ V .\r\n9. Let W =\r\n(\"a b\r\nc d#\r\n∈ M2(C) : a = d\r\n)\r\n. Then the condition a = d forces us to have\r\nα = α for any scalar α ∈ C. Hence,\r\n(a) W is not a vector subspace of the complex vector space M2(C), but\r\n(b) W is a vector subspace of the real vector space M2(C).\r\nWe are now ready to prove a very important result in the study of vector subspaces.\r\nThis result basically tells us that if we want to prove that a non-empty set W is a subspace\r\nof a vector space V (F) then we just need to verify only one condition. That is, we don’t\r\nhave to prove all the axioms stated in Definition 3.1.1.\r\nTheorem 3.1.9. Let V (F) be a vector space and let W be a non-empty subset of V . Then\r\nW is a subspace of V if and only if αu + βv ∈ W whenever α, β ∈ F and u, v ∈ W.\r\nProof. Let W be a subspace of V and let u, v ∈ W. Then for every α, β ∈ F, αu, βv ∈ W\r\nand hence αu + βv ∈ W.\r\nNow, let us assume that αu+βv ∈ W whenever α, β ∈ F and u, v ∈ W. Need to show,\r\nW is a subspace of V . To do so, observe the following:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/36b5592b-fa5c-4cff-b0e7-c7c814bf1f50.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1bb9bbc9285ac330ece7366456fe708d3ae741d880d903c5612ce3b42a389f2f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 590
      },
      {
        "segments": [
          {
            "segment_id": "36b5592b-fa5c-4cff-b0e7-c7c814bf1f50",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 66,
            "page_width": 612,
            "page_height": 792,
            "content": "66 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n3.1.1 Subspaces\r\nDefinition 3.1.7 (Vector Subspace). Let S be a non-empty subset of V. The set S over\r\nF is said to be a subspace of V (F) if S in itself is a vector space, where the vector addition\r\nand scalar multiplication are the same as that of V (F).\r\nExample 3.1.8. 1. Let V (F) be a vector space. Then the sets given below are subspaces\r\nof V. They are called trivial subspaces.\r\n(a) S = {0}, consisting only of the zero vector 0 and\r\n(b) S = V , the whole space.\r\n2. Let S = {(x, y, z) ∈ R\r\n3\r\n: x + 2y − z = 0}. Then S is a subspace of R\r\n3\r\n(S is a plane\r\nin R\r\n3 passing through the origin).\r\n3. Let S = {(x, y, z) ∈ R\r\n3\r\n: x + y + z = 0, x − y − z = 0}. Then S is a subspace of R\r\n3\r\n(S is a line in R\r\n3 passing through the origin).\r\n4. Let S = {(x, y, z) ∈ R\r\n3\r\n: z − 3x = 0}. Then S is a subspace of R\r\n3\r\n.\r\n5. The vector space Pn(R) is a subspace of the vector space P(R).\r\n6. Prove that S = {(x, y, z) ∈ R\r\n3\r\n: x + y + z = 3} is not a subspace of R\r\n3\r\n(S is still a\r\nplane in R\r\n3\r\nbut it does not pass through the origin).\r\n7. Prove that W = {(x, 0) ∈ R\r\n2\r\n: x ∈ R} is a subspace of R\r\n2\r\n.\r\n8. Let W = {(x, 0) ∈ V : x ∈ R}, where V is the vector space of Example 3.1.4.11.\r\nThen (x, 0) ⊕ (y, 0) = (x + y + 1, −3) 6∈ W. Hence W is not a subspace of V but\r\nS = {(x, 3) : x ∈ R} is a subspace of V . Note that the zero vector (−1, 3) ∈ V .\r\n9. Let W =\r\n(\"a b\r\nc d#\r\n∈ M2(C) : a = d\r\n)\r\n. Then the condition a = d forces us to have\r\nα = α for any scalar α ∈ C. Hence,\r\n(a) W is not a vector subspace of the complex vector space M2(C), but\r\n(b) W is a vector subspace of the real vector space M2(C).\r\nWe are now ready to prove a very important result in the study of vector subspaces.\r\nThis result basically tells us that if we want to prove that a non-empty set W is a subspace\r\nof a vector space V (F) then we just need to verify only one condition. That is, we don’t\r\nhave to prove all the axioms stated in Definition 3.1.1.\r\nTheorem 3.1.9. Let V (F) be a vector space and let W be a non-empty subset of V . Then\r\nW is a subspace of V if and only if αu + βv ∈ W whenever α, β ∈ F and u, v ∈ W.\r\nProof. Let W be a subspace of V and let u, v ∈ W. Then for every α, β ∈ F, αu, βv ∈ W\r\nand hence αu + βv ∈ W.\r\nNow, let us assume that αu+βv ∈ W whenever α, β ∈ F and u, v ∈ W. Need to show,\r\nW is a subspace of V . To do so, observe the following:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/36b5592b-fa5c-4cff-b0e7-c7c814bf1f50.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1bb9bbc9285ac330ece7366456fe708d3ae741d880d903c5612ce3b42a389f2f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 590
      },
      {
        "segments": [
          {
            "segment_id": "919f2a7f-58ad-471a-909b-0a8d7340d9f5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 67,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. FINITE DIMENSIONAL VECTOR SPACES 67\r\n1. Taking α = 1 and β = 1, we see that u + v ∈ W for every u, v ∈ W.\r\n2. Taking α = 0 and β = 0, we see that 0 ∈ W.\r\n3. Taking β = 0, we see that αu ∈ W for every α ∈ F and u ∈ W and hence using\r\nTheorem 3.1.3.3, −u = (−1)u ∈ W as well.\r\n4. The commutative and associative laws of vector addition hold as they hold in V .\r\n5. The axioms related with scalar multiplication and the distributive laws also hold as\r\nthey hold in V .\r\nThus, we have the required result.\r\nExercise 3.1.10. 1. Determine all the subspaces of R, R\r\n2 and R3\r\n.\r\n2. Prove that a line in R\r\n2\r\nis a subspace if and only if it passes through (0, 0) ∈ R\r\n2\r\n.\r\n3. Let V = {(a, b) : a, b ∈ R}. Is V a vector space over R if (a, b) ⊕ (c, d) = (a + c, 0)\r\nand α ⊙ (a, b) = (αa, 0)? Give reasons for your answer.\r\n4. Let V = R. Define x ⊕ y = x − y and α ⊙ x = −αx. Which vector space axioms are\r\nnot satisfied here?\r\n5. Which of the following are correct statements (why!)?\r\n(a) S = {(x, y, z) ∈ R\r\n3\r\n: z = x\r\n2} is a subspace of R3\r\n.\r\n(b) S = {αx : α ∈ F} forms a vector subspace of V (F) for each fixed x ∈ V .\r\n(c) S = {α(1, 1, 1) + β(1, −1, 0) : α, β ∈ R} is a vector subspace of R\r\n3\r\n.\r\n(d) All the sets given below are subspaces of C([−1, 1]) (see Example 3.1.4.14).\r\ni. W = {f ∈ C([−1, 1]) : f(1/2) = 0}.\r\nii. W = {f ∈ C([−1, 1]) : f(0) = 0, f(1/2) = 0}.\r\niii. W = {f ∈ C([−1, 1]) : f(−1/2) = 0, f(1/2) = 0}.\r\niv. W = {f ∈ C([−1, 1]) : f\r\n′\r\n(\r\n1\r\n4\r\n)exists }.\r\n(e) All the sets given below are subspaces of P(R)?\r\ni. W = {f(x) ∈ P(R) : deg(f(x)) = 3}.\r\nii. W = {f(x) ∈ P(R) : deg(f(x)) = 0}.\r\niii. W = {f(x) ∈ P(R) : f(1) = 0}.\r\niv. W = {f(x) ∈ P(R) : f(0) = 0, f(1) = 0}.\r\n(f) Let A =\r\n\"\r\n1 2 1\r\n2 1 1#\r\nand b =\r\n\"\r\n1\r\n−1\r\n#\r\n. Then {x : Ax = b} is a subspace of R\r\n3\r\n.\r\n(g) Let A =\r\n\"\r\n1 2 1\r\n2 1 1#\r\n. Then {x : Ax = 0} is a subspace of R\r\n3\r\n.\r\n6. Which of the following are subspaces of R\r\nn\r\n(R)?",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/919f2a7f-58ad-471a-909b-0a8d7340d9f5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f417490f7fea85ccca37443093b88ee8d758b46d19e23f582e172b43b1f30550",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 487
      },
      {
        "segments": [
          {
            "segment_id": "f069d8b6-4c97-4d64-9352-0fe2dc0428be",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 68,
            "page_width": 612,
            "page_height": 792,
            "content": "68 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n(a) {(x1, x2, . . . , xn) : x1 ≥ 0}.\r\n(b) {(x1, x2, . . . , xn) : x1 + 2x2 = 4x3}.\r\n(c) {(x1, x2, . . . , xn) : x1 is rational }.\r\n(d) {(x1, x2, . . . , xn) : x1 = x\r\n2\r\n3\r\n}.\r\n(e) {(x1, x2, . . . , xn) : either x1 or x2 or both are 0}.\r\n(f) {(x1, x2, . . . , xn) : |x1| ≤ 1}.\r\n7. Which of the following are subspaces of i)C\r\nn\r\n(R) ii)C\r\nn\r\n(C)?\r\n(a) {(z1, z2, . . . , zn) : z1 is real }.\r\n(b) {(z1, z2, . . . , zn) : z1 + z2 = z3}.\r\n(c) {(z1, z2, . . . , zn) :| z1 |=| z2 |}.\r\n8. Let A =\r\n\r\n\r\n\r\n1 1 1\r\n2 0 1\r\n1 −1 0\r\n\r\n\r\n . Are the sets given below subspaces of R\r\n3?\r\n(a) W = {x\r\nt ∈ R3\r\n: Ax = 0}.\r\n(b) W = {b\r\nt ∈ R3\r\n: there exists x\r\nt ∈ R3 with Ax = b}.\r\n(c) W = {x\r\nt ∈ R3\r\n: x\r\ntA = 0}.\r\n(d) W = {b\r\nt ∈ R3\r\n: there exists x\r\nt ∈ R3 with xtA = bt}.\r\n9. Fix a positive integer n. Then Mn(R) is a real vector space with usual operations\r\nof matrix addition and scalar multiplication. Prove that the sets W ⊂ Mn(R), given\r\nbelow, are subspaces of Mn(R).\r\n(a) W = {A : At = A}, the set of symmetric matrices.\r\n(b) W = {A : At = −A}, the set of skew-symmetric matrices.\r\n(c) W = {A : A is an upper triangular matrix}.\r\n(d) W = {A : A is a lower triangular matrix}.\r\n(e) W = {A : A is a diagonal matrix}.\r\n(f) W = {A : trace(A) = 0}.\r\n(g) W = {A = (aij ) : a11 + a22 = 0}.\r\n(h) W = {A = (aij ) : a21 + a22 + · · · + a2n = 0}.\r\n10. Fix a positive integer n. Then Mn(C) is a complex vector space with usual operations\r\nof matrix addition and scalar multiplication. Are the sets W ⊂ Mn(C), given below,\r\nsubspaces of Mn(C)? Give reasons.\r\n(a) W = {A : A∗ = A}, the set of Hermitian matrices.\r\n(b) W = {A : A∗ = −A}, the set of skew-Hermitian matrices.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/f069d8b6-4c97-4d64-9352-0fe2dc0428be.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb4a1b5d4b78f8bde0dc0841462dbd1f887c9293abfc228a50c027fcdcefb93e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 426
      },
      {
        "segments": [
          {
            "segment_id": "92f2f0b1-ccb1-40c5-965d-baf77d5d6681",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 69,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. FINITE DIMENSIONAL VECTOR SPACES 69\r\n(c) W = {A : A is an upper triangular matrix}.\r\n(d) W = {A : A is a lower triangular matrix}.\r\n(e) W = {A : A is a diagonal matrix}.\r\n(f) W = {A : trace(A) = 0}.\r\n(g) W = {A = (aij ) : a11 + a22 = 0}.\r\n(h) W = {A = (aij ) : a21 + a22 + · · · + a2n = 0}.\r\nWhat happens if Mn(C) is a real vector space?\r\n11. Prove that the following sets are not subspaces of Mn(R).\r\n(a) G = {A ∈ Mn(R) : det(A) = 0}.\r\n(b) G = {A ∈ Mn(R) : det(A) 6= 0}.\r\n(c) G = {A ∈ Mn(R) : det(A) = 1}.\r\n3.1.2 Linear Span\r\nDefinition 3.1.11 (Linear Combination). Let u1, u2, . . . , un be a collection of vectors\r\nfrom a vector space V (F). A vector u ∈ V is said to be a linear combination of the vectors\r\nu1, . . . , un if we can find scalars α1, . . . , αn ∈ F such that u = α1u1 + α2u2 + · · · + αnun.\r\nExample 3.1.12. 1. Is (4, 5, 5) a linear combination of (1, 0, 0), (2, 1, 0), and (3, 3, 1)?\r\nSolution: The vector (4, 5, 5) is a linear combination if the linear system\r\na(1, 0, 0) + b(2, 1, 0) + c(3, 3, 1) = (4, 5, 5) (3.1.1)\r\nin the unknowns a, b, c ∈ R has a solution. The augmented matrix of Equation (3.1.1)\r\nequals\r\n\r\n\r\n\r\n1 2 3 4\r\n0 1 3 5\r\n0 0 1 5\r\n\r\n\r\n\r\nand it has the solution α1 = 4, α2 = −10 and α3 = 5.\r\n2. Is (4, 5, 5) a linear combination of the vectors (1, 2, 3),(−1, 1, 4) and (3, 3, 2)?\r\nSolution: The vector (4, 5, 5) is a linear combination if the linear system\r\na(1, 2, 3) + b(−1, 1, 4) + c(3, 3, 2) = (4, 5, 5) (3.1.2)\r\nin the unknowns a, b, c ∈ R has a solution. The row reduced echelon form of the\r\naugmented matrix of Equation (3.1.2) equals\r\n\r\n\r\n\r\n1 0 2 3\r\n0 1 −1 −1\r\n0 0 0 0\r\n\r\n\r\n . Thus, one has an\r\ninfinite number of solutions. For example, (4, 5, 5) = 3(1, 2, 3) − (−1, 1, 4).\r\n3. Is (4, 5, 5) a linear combination of the vectors (1, 2, 1),(1, 0, −1) and (1, 1, 0).\r\nSolution: The vector (4, 5, 5) is a linear combination if the linear system\r\na(1, 2, 1) + b(1, 0, −1) + c(1, 1, 0) = (4, 5, 5) (3.1.3)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/92f2f0b1-ccb1-40c5-965d-baf77d5d6681.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=dff4d9ab1808ab51ce182c0ba230d3c56df09143cceae1b17ead84cddb242754",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 467
      },
      {
        "segments": [
          {
            "segment_id": "006fa190-cbe9-4eb3-9dd9-72b9b893373e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 70,
            "page_width": 612,
            "page_height": 792,
            "content": "70 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nin the unknowns a, b, c ∈ R has a solution. An application of Gauss elimination\r\nmethod to Equation (3.1.3) gives\r\n\r\n\r\n\r\n1 1 1 4\r\n0 1 1\r\n2\r\n3\r\n2\r\n0 0 0 1\r\n\r\n\r\n . Thus, Equation (3.1.3) has no so\u0002lution and hence (4, 5, 5) is not a linear combination of the given vectors.\r\nExercise 3.1.13. 1. Prove that every x ∈ R\r\n3\r\nis a unique linear combination of the\r\nvectors (1, 0, 0), (2, 1, 0), and (3, 3, 1).\r\n2. Find condition(s) on x, y and z such that (x, y, z) is a linear combination of (1, 2, 3),(−1, 1, 4)\r\nand (3, 3, 2)?\r\n3. Find condition(s) on x, y and z such that (x, y, z) is a linear combination of the\r\nvectors (1, 2, 1),(1, 0, −1) and (1, 1, 0).\r\nDefinition 3.1.14 (Linear Span). Let S = {u1, u2, . . . , un} be a non-empty subset of a\r\nvector space V (F). The linear span of S is the set defined by\r\nL(S) = {α1u1 + α2u2 + · · · + αnun : αi ∈ F, 1 ≤ i ≤ n}\r\nIf S is an empty set we define L(S) = {0}.\r\nExample 3.1.15. 1. Let S = {(1, 0),(0, 1)} ⊂ R\r\n2\r\n. Determine L(S).\r\nSolution: By definition, the required linear span is\r\nL(S) = {a(1, 0) + b(0, 1) : a, b ∈ R} = {(a, b) : a, b ∈ R} = R\r\n2\r\n. (3.1.4)\r\n2. For each S ⊂ R\r\n3\r\n, determine the geometrical representation of L(S).\r\n(a) S = {(1, 1, 1),(2, 1, 3)}.\r\nSolution: By definition, the required linear span is\r\nL(S) = {a(1, 1, 1) + b(2, 1, 3) : a, b ∈ R} = {(a + 2b, a + b, a + 3b) : a, b ∈ R(3.1.5) }.\r\nNote that finding all vectors of the form (a + 2b, a + b, a + 3b) is equivalent to\r\nfinding conditions on x, y and z such that (a + 2b, a + b, a + 3b) = (x, y, z), or\r\nequivalently, the system\r\na + 2b = x, a + b = y, a + 3b = z\r\nalways has a solution. Check that the row reduced form of the augmented matrix\r\nequals\r\n\r\n\r\n\r\n1 0 2y − x\r\n0 1 x − y\r\n0 0 z + y − 2x\r\n\r\n\r\n. Thus, we need 2x − y − z = 0 and hence\r\nL(S) = {a(1, 1, 1) + b(2, 1, 3) : a, b ∈ R} = {(x, y, z) ∈ R\r\n3\r\n: 2x − y − z = 0(3.1.6) }.\r\nEquation (3.1.5) is called an algebraic representation of L(S) whereas Equa\u0002tion (3.1.6) gives its geometrical representation as a subspace of R\r\n3\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/006fa190-cbe9-4eb3-9dd9-72b9b893373e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f5bb7b4128b725acc5469885f9a0e0c060cf94378b43496375df18993436f0a3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 490
      },
      {
        "segments": [
          {
            "segment_id": "5471e1a6-0327-4896-a15a-ba5db56199cc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 71,
            "page_width": 612,
            "page_height": 792,
            "content": "3.1. FINITE DIMENSIONAL VECTOR SPACES 71\r\n(b) S = {(1, 2, 1),(1, 0, −1),(1, 1, 0)}.\r\nSolution: As in Example 3.1.15.2, we need to find condition(s) on x, y, z such\r\nthat the linear system\r\na(1, 2, 1) + b(1, 0, −1) + c(1, 1, 0) = (x, y, z) (3.1.7)\r\nin the unknowns a, b, c is always consistent. An application of Gauss elimination\r\nmethod to Equation (3.1.7) gives\r\n\r\n\r\n\r\n1 1 1 x\r\n0 1 1\r\n2\r\n2x−y\r\n3\r\n0 0 0 x − y + z\r\n\r\n\r\n. Thus,\r\nL(S) = {(x, y, z) : x − y + z = 0}.\r\n(c) S = {(1, 2, 3),(−1, 1, 4),(3, 3, 2)}.\r\nSolution: We need to find condition(s) on x, y, z such that the linear system\r\na(1, 2, 3) + b(−1, 1, 4) + c(3, 3, 2) = (x, y, z)\r\nin the unknowns a, b, c is always consistent. An application of Gauss elimination\r\nmethod gives 5x − 7y + 3z = 0 as the required condition. Thus,\r\nL(S) = {(x, y, z) : 5x − 7y + 3z = 0}.\r\n3. S = {(1, 2, 3, 4),(−1, 1, 4, 5),(3, 3, 2, 3)} ⊂ R\r\n4\r\n. Determine L(S).\r\nSolution: The readers are advised to show that\r\nL(S) = {(x, y, z, w) : 2x − 3y + w = 0, 5x − 7y + 3z = 0}.\r\nExercise 3.1.16. For each of the sets S, determine the geometric representation of L(S).\r\n1. S = {−1} ⊂ R.\r\n2. S = {\r\n1\r\n104 } ⊂ R.\r\n3. S = {\r\n√\r\n15} ⊂ R.\r\n4. S = {(1, 0, 0),(0, 1, 0),(0, 0, 1)} ⊂ R\r\n3\r\n.\r\n5. S = {(1, 0, 1),(0, 1, 0),(3, 0, 3)} ⊂ R\r\n3\r\n.\r\n6. S = {(1, 0, 1),(1, 1, 0),(3, −4, 3)} ⊂ R\r\n3\r\n.\r\n7. S = {(1, 2, 1),(2, 0, 1),(1, 1, 1)} ⊂ R\r\n3\r\n.\r\n8. S = {(1, 0, 1, 1),(0, 1, 0, 1),(3, 0, 3, 1)} ⊂ R\r\n4\r\n.\r\nDefinition 3.1.17 (Finite Dimensional Vector Space). A vector space V (F) is said to be\r\nfinite dimensional if we can find a subset S of V , having finite number of elements, such\r\nthat V = L(S). If such a subset does not exist then V is called an infinite dimensional\r\nvector space.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5471e1a6-0327-4896-a15a-ba5db56199cc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e31f9dc4cf6f788e2e412da06c2da3d92396675b25d8193cb1dec5855ab3c3af",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 400
      },
      {
        "segments": [
          {
            "segment_id": "67242ee4-b791-4713-8106-ddea020832f9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 72,
            "page_width": 612,
            "page_height": 792,
            "content": "72 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nExample 3.1.18. 1. The set {(1, 2),(2, 1)} spans R\r\n2 and hence R2\r\nis a finite dimen\u0002sional vector space.\r\n2. The set {1, 1 + x, 1 − x + x\r\n2\r\n, x3, x4, x5} spans P5(C) and hence P5(C) is a finite\r\ndimensional vector space.\r\n3. Fix a positive integer n and consider the vector space Pn(R). Then Pn(C) is a finite\r\ndimensional vector space as Pn(C) = L({1, x, x2, . . . , xn}).\r\n4. Recall P(C), the vector space of all polynomials with complex coefficients. Since degree\r\nof a polynomial can be any large positive integer, P(C) cannot be a finite dimensional\r\nvector space. Indeed, checked that P(C) = L({1, x, x2, . . . , xn, . . .}).\r\nLemma 3.1.19 (Linear Span is a Subspace). Let S be a non-empty subset of a vector\r\nspace V (F). Then L(S) is a subspace of V (F).\r\nProof. By definition, S ⊂ L(S) and hence L(S) is non-empty subset of V. Let u, v ∈ L(S).\r\nThen, there exist a positive integer n, vectors wi ∈ S and scalars αi, βi ∈ F such that\r\nu = α1w1 + α2w2 + · · · + αnwn and v = β1w1 + β2w2 + · · · + βnwn. Hence,\r\nau + bv = (aα1 + bβ1)w1 + · · · + (aαn + bβn)wn ∈ L(S)\r\nfor every a, b ∈ F as aαi + bβi ∈ F for i = 1, . . . , n. Thus using Theorem 3.1.9, L(S) is a\r\nvector subspace of V (F).\r\nRemark 3.1.20. Let W be a subspace of a vector space V (F). If S ⊂ W then L(S) is a\r\nsubspace of W as W is a vector space in its own right.\r\nTheorem 3.1.21. Let S be a non-empty subset of a vector space V. Then L(S) is the\r\nsmallest subspace of V containing S.\r\nProof. For every u ∈ S, u = 1.u ∈ L(S) and hence S ⊆ L(S). To show L(S) is the\r\nsmallest subspace of V containing S, consider any subspace W of V containing S. Then by\r\nRemark 3.1.20, L(S) ⊆ W and hence the result follows.\r\nExercise 3.1.22. 1. Find all the vector subspaces of R\r\n2 and R3\r\n.\r\n2. Prove that {(x, y, z) ∈ R\r\n3\r\n: ax+ by + cz = d} is a subspace of R\r\n3\r\nif and only if d = 0.\r\n3. Let W be a set that consists of all polynomials of degree 5. Prove that W is not a\r\nsubspace P(R).\r\n4. Determine all vector subspaces of V , the vector space in Example 3.1.4.11.\r\n5. Let P and Q be two subspaces of a vector space V.\r\n(a) Prove that P ∩ Q is a subspace of V .\r\n(b) Give examples of P and Q such that P ∪ Q is not a subspace of V.\r\n(c) Determine conditions on P and Q such that P ∪ Q a subspace of V ?",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/67242ee4-b791-4713-8106-ddea020832f9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d57428d20365a82c54a83647c1d4b6ec9385109e9b953396e1a8c2d936aa23a3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 515
      },
      {
        "segments": [
          {
            "segment_id": "67242ee4-b791-4713-8106-ddea020832f9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 72,
            "page_width": 612,
            "page_height": 792,
            "content": "72 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nExample 3.1.18. 1. The set {(1, 2),(2, 1)} spans R\r\n2 and hence R2\r\nis a finite dimen\u0002sional vector space.\r\n2. The set {1, 1 + x, 1 − x + x\r\n2\r\n, x3, x4, x5} spans P5(C) and hence P5(C) is a finite\r\ndimensional vector space.\r\n3. Fix a positive integer n and consider the vector space Pn(R). Then Pn(C) is a finite\r\ndimensional vector space as Pn(C) = L({1, x, x2, . . . , xn}).\r\n4. Recall P(C), the vector space of all polynomials with complex coefficients. Since degree\r\nof a polynomial can be any large positive integer, P(C) cannot be a finite dimensional\r\nvector space. Indeed, checked that P(C) = L({1, x, x2, . . . , xn, . . .}).\r\nLemma 3.1.19 (Linear Span is a Subspace). Let S be a non-empty subset of a vector\r\nspace V (F). Then L(S) is a subspace of V (F).\r\nProof. By definition, S ⊂ L(S) and hence L(S) is non-empty subset of V. Let u, v ∈ L(S).\r\nThen, there exist a positive integer n, vectors wi ∈ S and scalars αi, βi ∈ F such that\r\nu = α1w1 + α2w2 + · · · + αnwn and v = β1w1 + β2w2 + · · · + βnwn. Hence,\r\nau + bv = (aα1 + bβ1)w1 + · · · + (aαn + bβn)wn ∈ L(S)\r\nfor every a, b ∈ F as aαi + bβi ∈ F for i = 1, . . . , n. Thus using Theorem 3.1.9, L(S) is a\r\nvector subspace of V (F).\r\nRemark 3.1.20. Let W be a subspace of a vector space V (F). If S ⊂ W then L(S) is a\r\nsubspace of W as W is a vector space in its own right.\r\nTheorem 3.1.21. Let S be a non-empty subset of a vector space V. Then L(S) is the\r\nsmallest subspace of V containing S.\r\nProof. For every u ∈ S, u = 1.u ∈ L(S) and hence S ⊆ L(S). To show L(S) is the\r\nsmallest subspace of V containing S, consider any subspace W of V containing S. Then by\r\nRemark 3.1.20, L(S) ⊆ W and hence the result follows.\r\nExercise 3.1.22. 1. Find all the vector subspaces of R\r\n2 and R3\r\n.\r\n2. Prove that {(x, y, z) ∈ R\r\n3\r\n: ax+ by + cz = d} is a subspace of R\r\n3\r\nif and only if d = 0.\r\n3. Let W be a set that consists of all polynomials of degree 5. Prove that W is not a\r\nsubspace P(R).\r\n4. Determine all vector subspaces of V , the vector space in Example 3.1.4.11.\r\n5. Let P and Q be two subspaces of a vector space V.\r\n(a) Prove that P ∩ Q is a subspace of V .\r\n(b) Give examples of P and Q such that P ∪ Q is not a subspace of V.\r\n(c) Determine conditions on P and Q such that P ∪ Q a subspace of V ?",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/67242ee4-b791-4713-8106-ddea020832f9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d57428d20365a82c54a83647c1d4b6ec9385109e9b953396e1a8c2d936aa23a3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 515
      },
      {
        "segments": [
          {
            "segment_id": "6c31b8c6-31f9-4422-9cbd-b4621ab790b0",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 73,
            "page_width": 612,
            "page_height": 792,
            "content": "3.2. LINEAR INDEPENDENCE 73\r\n(d) Define P + Q = {u + v : u ∈ P, v ∈ Q}. Prove that P + Q is a subspace of V .\r\n(e) Prove that L(P ∪ Q) = P + Q.\r\n6. Let x1 = (1, 0, 0), x2 = (1, 1, 0), x3 = (1, 2, 0), x4 = (1, 1, 1) and let S = {x1, x2, x3, x4}.\r\nDetermine all xi such that L(S) = L(S \\ {xi}).\r\n7. Let P = L{(1, 0, 0),(1, 1, 0)} and Q = L{(1, 1, 1)} be subspaces of R\r\n3\r\n. Show that\r\nP + Q = R\r\n3 and P ∩ Q = {0}. If u ∈ R3\r\n, determine uP , uQ such that u = uP + uQ\r\nwhere uP ∈ P and uQ ∈ Q. Is it necessary that uP and uQ are unique?\r\n8. Let P = L{(1, −1, 0),(1, 1, 0)} and Q = L{(1, 1, 1),(1, 2, 1)} be subspaces of R\r\n3\r\n. Show\r\nthat P + Q = R\r\n3 and P ∩ Q 6= {0}. Also, find a vector u ∈ R3\r\nsuch that u cannot be\r\nwritten uniquely in the form u = uP + uQ where uP ∈ P and uQ ∈ Q.\r\nIn this section, we saw that a vector space has infinite number of vectors. Hence, one\r\ncan start with any finite collection of vectors and obtain their span. It means that any\r\nvector space contains infinite number of other vector subspaces. Therefore, the following\r\nquestions arise:\r\n1. What are the conditions under which, the linear span of two distinct sets are the\r\nsame?\r\n2. Is it possible to find/choose vectors so that the linear span of the chosen vectors is\r\nthe whole vector space itself?\r\n3. Suppose we are able to choose certain vectors whose linear span is the whole space.\r\nCan we find the minimum number of such vectors?\r\nWe try to answer these questions in the subsequent sections.\r\n3.2 Linear Independence\r\nDefinition 3.2.1 (Linear Independence and Dependence). Let S = {u1, u2, . . . , um} be a\r\nnon-empty subset of a vector space V (F). The set S is said to be linearly independent if the\r\nsystem of equations\r\nα1u1 + α2u2 + · · · + αmum = 0, (3.2.1)\r\nin the unknowns αi\r\n’s 1 ≤ i ≤ m, has only the trivial solution. If the system (3.2.1) has a\r\nnon-trivial solution then the set S is said to be linearly dependent.\r\nExample 3.2.2. Is the set S a linear independent set? Give reasons.\r\n1. Let S = {(1, 2, 1),(2, 1, 4),(3, 3, 5)}.\r\nSolution: Consider the linear system a(1, 2, 1) + b(2, 1, 4) + c(3, 3, 5) = (0, 0, 0) in\r\nthe unknowns a, b and c. It can be checked that this system has infinite number of\r\nsolutions. Hence S is a linearly dependent subset of R\r\n3\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6c31b8c6-31f9-4422-9cbd-b4621ab790b0.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=09da424fee34cc2493e1d43018154faf62070e3cd0ad2da7d2027db6da198693",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 497
      },
      {
        "segments": [
          {
            "segment_id": "e56f3d5a-c926-472a-8a36-76c8d4e1197c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 74,
            "page_width": 612,
            "page_height": 792,
            "content": "74 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n2. Let S = {(1, 1, 1),(1, 1, 0),(1, 0, 1)}.\r\nSolution: Consider the system a(1, 1, 1) + b(1, 1, 0) + c(1, 0, 1) = (0, 0, 0) in the\r\nunknowns a, b and c. Check that this system has only the trivial solution. Hence S\r\nis a linearly independent subset of R\r\n3\r\n.\r\nIn other words, if S = {u1, . . . , um} is a non-empty subset of a vector space V, then\r\none needs to solve the linear system of equations\r\nα1u1 + α2u2 + · · · + αmum = 0 (3.2.2)\r\nin the unknowns α1, . . . , αn. If α1 = α2 = · · · = αm = 0 is the only solution of (3.2.2),\r\nthen S is a linearly independent subset of V. Otherwise, the set S is a linearly dependent\r\nsubset of V. We are now ready to state the following important results. The proof of only\r\nthe first part is given. The reader is required to supply the proof of other parts.\r\nProposition 3.2.3. Let V (F) be a vector space.\r\n1. Then the zero-vector cannot belong to a linearly independent set.\r\n2. A non-empty subset of a linearly independent set of V is also linearly independent.\r\n3. Every set containing a linearly dependent set of V is also linearly dependent.\r\nProof. Let S = {0 = u1, u2, . . . , un} be a set consisting of the zero vector. Then for any\r\nγ 6= o, γu1+ou2+· · ·+0un = 0. Hence, the system α1u1+α2u2+· · ·+αmum = 0, has a non\u0002trivial solution (α1, α2, . . . , αn) = (γ, 0 . . . , 0). Thus, the set S is linearly dependent.\r\nTheorem 3.2.4. Let {v1, v2, . . . , vp} be a linearly independent subset of a vector space\r\nV (F). If for some v ∈ V , the set {v1, v2, . . . , vp, v} is a linearly dependent, then v is a\r\nlinear combination of v1, v2, . . . , vp.\r\nProof. Since {v1, . . . , vp, v} is linearly dependent, there exist scalars c1, . . . , cp+1, not all\r\nzero, such that\r\nc1v1 + c2v2 + · · · + cpvp + cp+1v = 0. (3.2.3)\r\nClaim: cp+1 6= 0.\r\nLet if possible cp+1 = 0. As the scalars in Equation (3.2.3) are not all zero, the linear system\r\nα1v1+· · ·+αpvp = 0 in the unknowns α1, . . . , αp has a non-trivial solution (c1, . . . , cp). This\r\nby definition of linear independence implies that the set {v1, . . . , vp} is linearly dependent,\r\na contradiction to our hypothesis. Thus, cp+1 6= 0 and we get\r\nv = −\r\n1\r\ncp+1\r\n(c1v1 + · · · + cpvp) ∈ L(v1, v2, . . . , vp)\r\nas −\r\nci\r\ncp+1\r\n∈ F for 1 ≤ i ≤ p. Thus, the result follows.\r\nWe now state a very important corollary of Theorem 3.2.4 without proof. The readers\r\nare advised to supply the proof for themselves.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/e56f3d5a-c926-472a-8a36-76c8d4e1197c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a4019461acc619076f2ab8f296ee5314d24fe93326bf07d199cdb91c9891a5cb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 531
      },
      {
        "segments": [
          {
            "segment_id": "e56f3d5a-c926-472a-8a36-76c8d4e1197c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 74,
            "page_width": 612,
            "page_height": 792,
            "content": "74 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n2. Let S = {(1, 1, 1),(1, 1, 0),(1, 0, 1)}.\r\nSolution: Consider the system a(1, 1, 1) + b(1, 1, 0) + c(1, 0, 1) = (0, 0, 0) in the\r\nunknowns a, b and c. Check that this system has only the trivial solution. Hence S\r\nis a linearly independent subset of R\r\n3\r\n.\r\nIn other words, if S = {u1, . . . , um} is a non-empty subset of a vector space V, then\r\none needs to solve the linear system of equations\r\nα1u1 + α2u2 + · · · + αmum = 0 (3.2.2)\r\nin the unknowns α1, . . . , αn. If α1 = α2 = · · · = αm = 0 is the only solution of (3.2.2),\r\nthen S is a linearly independent subset of V. Otherwise, the set S is a linearly dependent\r\nsubset of V. We are now ready to state the following important results. The proof of only\r\nthe first part is given. The reader is required to supply the proof of other parts.\r\nProposition 3.2.3. Let V (F) be a vector space.\r\n1. Then the zero-vector cannot belong to a linearly independent set.\r\n2. A non-empty subset of a linearly independent set of V is also linearly independent.\r\n3. Every set containing a linearly dependent set of V is also linearly dependent.\r\nProof. Let S = {0 = u1, u2, . . . , un} be a set consisting of the zero vector. Then for any\r\nγ 6= o, γu1+ou2+· · ·+0un = 0. Hence, the system α1u1+α2u2+· · ·+αmum = 0, has a non\u0002trivial solution (α1, α2, . . . , αn) = (γ, 0 . . . , 0). Thus, the set S is linearly dependent.\r\nTheorem 3.2.4. Let {v1, v2, . . . , vp} be a linearly independent subset of a vector space\r\nV (F). If for some v ∈ V , the set {v1, v2, . . . , vp, v} is a linearly dependent, then v is a\r\nlinear combination of v1, v2, . . . , vp.\r\nProof. Since {v1, . . . , vp, v} is linearly dependent, there exist scalars c1, . . . , cp+1, not all\r\nzero, such that\r\nc1v1 + c2v2 + · · · + cpvp + cp+1v = 0. (3.2.3)\r\nClaim: cp+1 6= 0.\r\nLet if possible cp+1 = 0. As the scalars in Equation (3.2.3) are not all zero, the linear system\r\nα1v1+· · ·+αpvp = 0 in the unknowns α1, . . . , αp has a non-trivial solution (c1, . . . , cp). This\r\nby definition of linear independence implies that the set {v1, . . . , vp} is linearly dependent,\r\na contradiction to our hypothesis. Thus, cp+1 6= 0 and we get\r\nv = −\r\n1\r\ncp+1\r\n(c1v1 + · · · + cpvp) ∈ L(v1, v2, . . . , vp)\r\nas −\r\nci\r\ncp+1\r\n∈ F for 1 ≤ i ≤ p. Thus, the result follows.\r\nWe now state a very important corollary of Theorem 3.2.4 without proof. The readers\r\nare advised to supply the proof for themselves.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/e56f3d5a-c926-472a-8a36-76c8d4e1197c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a4019461acc619076f2ab8f296ee5314d24fe93326bf07d199cdb91c9891a5cb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 531
      },
      {
        "segments": [
          {
            "segment_id": "feb34c7f-c1bd-4a29-a878-e2d0152be9ae",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 75,
            "page_width": 612,
            "page_height": 792,
            "content": "3.2. LINEAR INDEPENDENCE 75\r\nCorollary 3.2.5. Let S = {u1, . . . , un} be a subset of a vector space V (F). If S is linearly\r\n1. dependent then there exists a k, 2 ≤ k ≤ n with L(u1, . . . , uk) = L(u1, . . . , uk−1).\r\n2. independent and there is a vector v ∈ V with v 6∈ L(S) then {u1, . . . , un, v} is also\r\na linearly independent subset of V.\r\nExercise 3.2.6. 1. Consider the vector space R\r\n2\r\n. Let u1 = (1, 0). Find all choices for\r\nthe vector u2 such that {u1, u2} is linearly independent subset of R\r\n2\r\n. Does there exist\r\nvectors u2 and u3 such that {u1, u2, u3} is linearly independent subset of R\r\n2?\r\n2. Let S = {(1, 1, 1, 1),(1, −1, 1, 2),(1, 1, −1, 1)} ⊂ R\r\n4\r\n. Does (1, 1, 2, 1) ∈ L(S)? Further\u0002more, determine conditions on x, y, z and u such that (x, y, z, u) ∈ L(S).\r\n3. Show that S = {(1, 2, 3),(−2, 1, 1),(8, 6, 10)} ⊂ R\r\n3\r\nis linearly dependent.\r\n4. Show that S = {(1, 0, 0),(1, 1, 0),(1, 1, 1)} ⊂ R\r\n3\r\nis linearly independent.\r\n5. Prove that {u1, u2, . . . , un} is a linearly independent subset of V (F) if and only if\r\n{u1, u1 + u2, . . . , u1 + · · · + un} is linearly independent subset of V (F).\r\n6. Find 3 vectors u, v and w in R\r\n4\r\nsuch that {u, v, w} is linearly dependent whereas\r\n{u, v}, {u, w} and {v, w} are linearly independent.\r\n7. What is the maximum number of linearly independent vectors in R\r\n3\r\n?\r\n8. Show that any set of k vectors in R\r\n3\r\nis linearly dependent if k ≥ 4.\r\n9. Is {(1, 0),(i, 0)} a linearly independent subset of C\r\n2\r\n(R)?\r\n10. Suppose V is a collection of vectors such that V (C) as well as V (R) are vector spaces.\r\nProve that the set {u1, . . . , uk, iu1, . . . , iuk} is a linearly independent subset of V (R)\r\nif and only if {u1, . . . , uk} is a linear independent subset of V (C).\r\n11. Let M be a subspace of V and let u, v ∈ V . Define K = L(M, u) and H = L(M, v).\r\nIf v ∈ K and v 6∈ M prove that u ∈ H.\r\n12. Let A ∈ Mn(R) and let x and y be two non-zero vectors such that Ax = 3x and\r\nAy = 2y. Prove that x and y are linearly independent.\r\n13. Let A =\r\n\r\n\r\n\r\n2 1 3\r\n4 −1 3\r\n3 −2 5\r\n\r\n\r\n. Determine non-zero vectors x, y and z such that Ax = 6x,\r\nAy = 2y and Az = −2z. Use the vectors x, y and z obtained here to prove the\r\nfollowing.\r\n(a) A2v = 4v, where v = cy + dz for any real numbers c and d.\r\n(b) The set {x, y, z} is linearly independent.\r\n(c) Let P = [x, y, z] be a 3 × 3 matrix. Then P is invertible.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/feb34c7f-c1bd-4a29-a878-e2d0152be9ae.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=66aa52d387efdf1dcb7e9786266b90c634d43ed2df009c75ab4f2641823ddab8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "feb34c7f-c1bd-4a29-a878-e2d0152be9ae",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 75,
            "page_width": 612,
            "page_height": 792,
            "content": "3.2. LINEAR INDEPENDENCE 75\r\nCorollary 3.2.5. Let S = {u1, . . . , un} be a subset of a vector space V (F). If S is linearly\r\n1. dependent then there exists a k, 2 ≤ k ≤ n with L(u1, . . . , uk) = L(u1, . . . , uk−1).\r\n2. independent and there is a vector v ∈ V with v 6∈ L(S) then {u1, . . . , un, v} is also\r\na linearly independent subset of V.\r\nExercise 3.2.6. 1. Consider the vector space R\r\n2\r\n. Let u1 = (1, 0). Find all choices for\r\nthe vector u2 such that {u1, u2} is linearly independent subset of R\r\n2\r\n. Does there exist\r\nvectors u2 and u3 such that {u1, u2, u3} is linearly independent subset of R\r\n2?\r\n2. Let S = {(1, 1, 1, 1),(1, −1, 1, 2),(1, 1, −1, 1)} ⊂ R\r\n4\r\n. Does (1, 1, 2, 1) ∈ L(S)? Further\u0002more, determine conditions on x, y, z and u such that (x, y, z, u) ∈ L(S).\r\n3. Show that S = {(1, 2, 3),(−2, 1, 1),(8, 6, 10)} ⊂ R\r\n3\r\nis linearly dependent.\r\n4. Show that S = {(1, 0, 0),(1, 1, 0),(1, 1, 1)} ⊂ R\r\n3\r\nis linearly independent.\r\n5. Prove that {u1, u2, . . . , un} is a linearly independent subset of V (F) if and only if\r\n{u1, u1 + u2, . . . , u1 + · · · + un} is linearly independent subset of V (F).\r\n6. Find 3 vectors u, v and w in R\r\n4\r\nsuch that {u, v, w} is linearly dependent whereas\r\n{u, v}, {u, w} and {v, w} are linearly independent.\r\n7. What is the maximum number of linearly independent vectors in R\r\n3\r\n?\r\n8. Show that any set of k vectors in R\r\n3\r\nis linearly dependent if k ≥ 4.\r\n9. Is {(1, 0),(i, 0)} a linearly independent subset of C\r\n2\r\n(R)?\r\n10. Suppose V is a collection of vectors such that V (C) as well as V (R) are vector spaces.\r\nProve that the set {u1, . . . , uk, iu1, . . . , iuk} is a linearly independent subset of V (R)\r\nif and only if {u1, . . . , uk} is a linear independent subset of V (C).\r\n11. Let M be a subspace of V and let u, v ∈ V . Define K = L(M, u) and H = L(M, v).\r\nIf v ∈ K and v 6∈ M prove that u ∈ H.\r\n12. Let A ∈ Mn(R) and let x and y be two non-zero vectors such that Ax = 3x and\r\nAy = 2y. Prove that x and y are linearly independent.\r\n13. Let A =\r\n\r\n\r\n\r\n2 1 3\r\n4 −1 3\r\n3 −2 5\r\n\r\n\r\n. Determine non-zero vectors x, y and z such that Ax = 6x,\r\nAy = 2y and Az = −2z. Use the vectors x, y and z obtained here to prove the\r\nfollowing.\r\n(a) A2v = 4v, where v = cy + dz for any real numbers c and d.\r\n(b) The set {x, y, z} is linearly independent.\r\n(c) Let P = [x, y, z] be a 3 × 3 matrix. Then P is invertible.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/feb34c7f-c1bd-4a29-a878-e2d0152be9ae.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=66aa52d387efdf1dcb7e9786266b90c634d43ed2df009c75ab4f2641823ddab8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "21db443f-0dd9-4a94-ad89-9ddd332a2210",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 76,
            "page_width": 612,
            "page_height": 792,
            "content": "76 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n(d) Let D =\r\n\r\n\r\n\r\n6 0 0\r\n0 2 0\r\n0 0 −2\r\n\r\n\r\n . Then AP = P D.\r\n14. Let P and Q be subspaces of R\r\nn\r\nsuch that P + Q = R\r\nn and P ∩ Q = {0}. Prove that\r\neach u ∈ R\r\nn\r\nis uniquely expressible as u = uP + uQ, where uP ∈ P and uQ ∈ Q.\r\n3.3 Bases\r\nDefinition 3.3.1 (Basis of a Vector Space). A basis of a vector space V is a subset B of\r\nV such that B is a linearly independent set in V and the linear span of B is V . Also, any\r\nelement of B is called a basis vector.\r\nRemark 3.3.2. Let B be a basis of a vector space V (F). Then for each v ∈ V , there\r\nexist vectors u1, u2, . . . , un ∈ B such that v =\r\nPn\r\ni=1\r\nαiui, where αi ∈ F, for 1 ≤ i ≤ n. By\r\nconvention, the linear span of an empty set is {0}. Hence, the empty set is a basis of the\r\nvector space {0}.\r\nLemma 3.3.3. Let B be a basis of a vector space V (F). Then each v ∈ V is a unique\r\nlinear combination of the basis vectors.\r\nProof. On the contrary, assume that there exists v ∈ V that is can be expressed in at least\r\ntwo ways as linear combination of basis vectors. That means, there exists a positive integer\r\np, scalars αi, βi ∈ F and vi ∈ B such that\r\nv = α1v1 + α2v2 + · · · + αpvp and v = β1v1 + β2v2 + · · · + βpvp.\r\nEquating the two expressions of v leads to the expression\r\n(α1 − β1)v1 + (α2 − β2)v2 + · · · + (αp − βp)vp = 0. (3.3.1)\r\nSince the vectors are from B, by definition (see Definition 3.3.1) the set S = {v1, v2, . . . , vp}\r\nis a linearly independent subset of V . This implies that the linear system c1v1 + c2v2 +\r\n· · · + cpvp = 0 in the unknowns c1, c2, . . . , cp has only the trivial solution. Thus, each of\r\nthe scalars αi − βi appearing in Equation (3.3.1) must be equal to 0. That is, αi − βi = 0\r\nfor 1 ≤ i ≤ p. Thus, for 1 ≤ i ≤ p, αi = βi and the result follows.\r\nExample 3.3.4. 1. The set {1} is a basis of the vector space R(R).\r\n2. The set {(1, 1),(1, −1)} is a basis of the vector space R\r\n2\r\n(R).\r\n3. Fix a positive integer n and let ei = (0, . . . , 0, 1\r\n|{z}\r\nith place\r\n, 0, . . . , 0) ∈ R\r\nn\r\nfor 1 ≤ i ≤ n.\r\nThen B = {e1, e2, . . . , en} is called the standard basis of R\r\nn\r\n.\r\n(a) B = {e1} = {1} is a standard basis of R(R).\r\n(b) B = {e1, e2} with e1 = (1, 0) and e2 = (0, 1) is the standard basis of R\r\n2\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/21db443f-0dd9-4a94-ad89-9ddd332a2210.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=43e4addd6a08a4bedbadd8a34b0b063da74beef136426d1a87d70f7681d4ce34",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 553
      },
      {
        "segments": [
          {
            "segment_id": "21db443f-0dd9-4a94-ad89-9ddd332a2210",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 76,
            "page_width": 612,
            "page_height": 792,
            "content": "76 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n(d) Let D =\r\n\r\n\r\n\r\n6 0 0\r\n0 2 0\r\n0 0 −2\r\n\r\n\r\n . Then AP = P D.\r\n14. Let P and Q be subspaces of R\r\nn\r\nsuch that P + Q = R\r\nn and P ∩ Q = {0}. Prove that\r\neach u ∈ R\r\nn\r\nis uniquely expressible as u = uP + uQ, where uP ∈ P and uQ ∈ Q.\r\n3.3 Bases\r\nDefinition 3.3.1 (Basis of a Vector Space). A basis of a vector space V is a subset B of\r\nV such that B is a linearly independent set in V and the linear span of B is V . Also, any\r\nelement of B is called a basis vector.\r\nRemark 3.3.2. Let B be a basis of a vector space V (F). Then for each v ∈ V , there\r\nexist vectors u1, u2, . . . , un ∈ B such that v =\r\nPn\r\ni=1\r\nαiui, where αi ∈ F, for 1 ≤ i ≤ n. By\r\nconvention, the linear span of an empty set is {0}. Hence, the empty set is a basis of the\r\nvector space {0}.\r\nLemma 3.3.3. Let B be a basis of a vector space V (F). Then each v ∈ V is a unique\r\nlinear combination of the basis vectors.\r\nProof. On the contrary, assume that there exists v ∈ V that is can be expressed in at least\r\ntwo ways as linear combination of basis vectors. That means, there exists a positive integer\r\np, scalars αi, βi ∈ F and vi ∈ B such that\r\nv = α1v1 + α2v2 + · · · + αpvp and v = β1v1 + β2v2 + · · · + βpvp.\r\nEquating the two expressions of v leads to the expression\r\n(α1 − β1)v1 + (α2 − β2)v2 + · · · + (αp − βp)vp = 0. (3.3.1)\r\nSince the vectors are from B, by definition (see Definition 3.3.1) the set S = {v1, v2, . . . , vp}\r\nis a linearly independent subset of V . This implies that the linear system c1v1 + c2v2 +\r\n· · · + cpvp = 0 in the unknowns c1, c2, . . . , cp has only the trivial solution. Thus, each of\r\nthe scalars αi − βi appearing in Equation (3.3.1) must be equal to 0. That is, αi − βi = 0\r\nfor 1 ≤ i ≤ p. Thus, for 1 ≤ i ≤ p, αi = βi and the result follows.\r\nExample 3.3.4. 1. The set {1} is a basis of the vector space R(R).\r\n2. The set {(1, 1),(1, −1)} is a basis of the vector space R\r\n2\r\n(R).\r\n3. Fix a positive integer n and let ei = (0, . . . , 0, 1\r\n|{z}\r\nith place\r\n, 0, . . . , 0) ∈ R\r\nn\r\nfor 1 ≤ i ≤ n.\r\nThen B = {e1, e2, . . . , en} is called the standard basis of R\r\nn\r\n.\r\n(a) B = {e1} = {1} is a standard basis of R(R).\r\n(b) B = {e1, e2} with e1 = (1, 0) and e2 = (0, 1) is the standard basis of R\r\n2\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/21db443f-0dd9-4a94-ad89-9ddd332a2210.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=43e4addd6a08a4bedbadd8a34b0b063da74beef136426d1a87d70f7681d4ce34",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 553
      },
      {
        "segments": [
          {
            "segment_id": "5986edb5-7234-446d-a7d2-8d6ba1412d12",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 77,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 77\r\n(c) B = {e1, e2, e3} with e1 = (1, 0, 0), e2 = (0, 1, 0) and e3 = (0, 0, 1) is the standard\r\nbasis of R\r\n3\r\n.\r\n(d) B = {(1, 0, 0, 0),(0, 1, 0, 0),(0, 0, 1, 0),(0, 0, 0, 1)} is the standard basis of R\r\n4\r\n.\r\n4. Let V = {(x, y, 0) : x, y ∈ R} ⊂ R\r\n3\r\n. Then B = {(2, 0, 0),(1, 3, 0)} is a basis of V.\r\n5. Let V = {(x, y, z) ∈ R\r\n3\r\n: x + y − z = 0} be a vector subspace of R\r\n3\r\n. As each element\r\n(x, y, z) ∈ V satisfies x + y − z = 0. Or equivalently z = x + y and hence\r\n(x, y, z) = (x, y, x + y) = (x, 0, x) + (0, y, y) = x(1, 0, 1) + y(0, 1, 1).\r\nHence {(1, 0, 1),(0, 1, 1)} forms a basis of V.\r\n6. Let V = {a + ib : a, b ∈ R} be a complex vector space. Then any element a + ib ∈ V\r\nequals a + ib = (a + ib) · 1. Hence a basis of V is {1}.\r\n7. Let V = {a + ib : a, b ∈ R} be a real vector space. Then {1, i} is a basis of V (R) as\r\na + ib = a · 1 + b · i for a, b ∈ R and {1, i} is a linearly independent subset of V (R).\r\n8. In C\r\n2\r\n, (a + ib, c + id) = (a + ib)(1, 0) + (c + id)(0, 1). So, {(1, 0),(0, 1)} is a basis of\r\nthe complex vector space C\r\n2\r\n.\r\n9. In case of the real vector space C\r\n2\r\n, (a + ib, c + id) = a(1, 0) + b(i, 0) + c(0, 1) + d(0, i).\r\nHence {(1, 0),(i, 0),(0, 1),(0, i)} is a basis.\r\n10. B = {e1, e2, . . . , en} is the standard basis of C\r\nn\r\n. But B is not a basis of the real\r\nvector space C\r\nn\r\n.\r\nBefore coming to the end of this section, we give an algorithm to obtain a basis of\r\nany finite dimensional vector space V . This will be done by a repeated application of\r\nCorollary 3.2.5. The algorithm proceeds as follows:\r\nStep 1: Let v1 ∈ V with v1 6= 0. Then {v1} is linearly independent.\r\nStep 2: If V = L(v1), we have got a basis of V. Else, pick v2 ∈ V such that v2 6∈ L(v1).\r\nThen by Corollary 3.2.5.2, {v1, v2} is linearly independent.\r\nStep i: Either V = L(v1, v2, . . . , vi) or L(v1, v2, . . . , vi) 6= V.\r\nIn the first case, {v1, v2, . . . , vi} is a basis of V. In the second case, pick vi+1 ∈ V\r\nwith vi+1 6∈ L(v1, v2, . . . , vi). Then, by Corollary 3.2.5.2, the set {v1, v2, . . . , vi+1}\r\nis linearly independent.\r\nThis process will finally end as V is a finite dimensional vector space.\r\nExercise 3.3.5. 1. Let u1, u2, . . . , un be basis vectors of a vector space V . Then prove\r\nthat whenever Pn\r\ni=1\r\nαiui = 0, we must have αi = 0 for each i = 1, . . . , n.\r\n2. Find a basis of R\r\n3\r\ncontaining the vector (1, 1, −2).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5986edb5-7234-446d-a7d2-8d6ba1412d12.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=00b844dd239d30899b9224c107ec880d106513d34a205ab3408a73cc4e5f3a43",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 606
      },
      {
        "segments": [
          {
            "segment_id": "5986edb5-7234-446d-a7d2-8d6ba1412d12",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 77,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 77\r\n(c) B = {e1, e2, e3} with e1 = (1, 0, 0), e2 = (0, 1, 0) and e3 = (0, 0, 1) is the standard\r\nbasis of R\r\n3\r\n.\r\n(d) B = {(1, 0, 0, 0),(0, 1, 0, 0),(0, 0, 1, 0),(0, 0, 0, 1)} is the standard basis of R\r\n4\r\n.\r\n4. Let V = {(x, y, 0) : x, y ∈ R} ⊂ R\r\n3\r\n. Then B = {(2, 0, 0),(1, 3, 0)} is a basis of V.\r\n5. Let V = {(x, y, z) ∈ R\r\n3\r\n: x + y − z = 0} be a vector subspace of R\r\n3\r\n. As each element\r\n(x, y, z) ∈ V satisfies x + y − z = 0. Or equivalently z = x + y and hence\r\n(x, y, z) = (x, y, x + y) = (x, 0, x) + (0, y, y) = x(1, 0, 1) + y(0, 1, 1).\r\nHence {(1, 0, 1),(0, 1, 1)} forms a basis of V.\r\n6. Let V = {a + ib : a, b ∈ R} be a complex vector space. Then any element a + ib ∈ V\r\nequals a + ib = (a + ib) · 1. Hence a basis of V is {1}.\r\n7. Let V = {a + ib : a, b ∈ R} be a real vector space. Then {1, i} is a basis of V (R) as\r\na + ib = a · 1 + b · i for a, b ∈ R and {1, i} is a linearly independent subset of V (R).\r\n8. In C\r\n2\r\n, (a + ib, c + id) = (a + ib)(1, 0) + (c + id)(0, 1). So, {(1, 0),(0, 1)} is a basis of\r\nthe complex vector space C\r\n2\r\n.\r\n9. In case of the real vector space C\r\n2\r\n, (a + ib, c + id) = a(1, 0) + b(i, 0) + c(0, 1) + d(0, i).\r\nHence {(1, 0),(i, 0),(0, 1),(0, i)} is a basis.\r\n10. B = {e1, e2, . . . , en} is the standard basis of C\r\nn\r\n. But B is not a basis of the real\r\nvector space C\r\nn\r\n.\r\nBefore coming to the end of this section, we give an algorithm to obtain a basis of\r\nany finite dimensional vector space V . This will be done by a repeated application of\r\nCorollary 3.2.5. The algorithm proceeds as follows:\r\nStep 1: Let v1 ∈ V with v1 6= 0. Then {v1} is linearly independent.\r\nStep 2: If V = L(v1), we have got a basis of V. Else, pick v2 ∈ V such that v2 6∈ L(v1).\r\nThen by Corollary 3.2.5.2, {v1, v2} is linearly independent.\r\nStep i: Either V = L(v1, v2, . . . , vi) or L(v1, v2, . . . , vi) 6= V.\r\nIn the first case, {v1, v2, . . . , vi} is a basis of V. In the second case, pick vi+1 ∈ V\r\nwith vi+1 6∈ L(v1, v2, . . . , vi). Then, by Corollary 3.2.5.2, the set {v1, v2, . . . , vi+1}\r\nis linearly independent.\r\nThis process will finally end as V is a finite dimensional vector space.\r\nExercise 3.3.5. 1. Let u1, u2, . . . , un be basis vectors of a vector space V . Then prove\r\nthat whenever Pn\r\ni=1\r\nαiui = 0, we must have αi = 0 for each i = 1, . . . , n.\r\n2. Find a basis of R\r\n3\r\ncontaining the vector (1, 1, −2).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5986edb5-7234-446d-a7d2-8d6ba1412d12.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=00b844dd239d30899b9224c107ec880d106513d34a205ab3408a73cc4e5f3a43",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 606
      },
      {
        "segments": [
          {
            "segment_id": "85329a87-0721-456e-97cf-2c23d4179a49",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 78,
            "page_width": 612,
            "page_height": 792,
            "content": "78 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n3. Find a basis of R\r\n3\r\ncontaining the vector (1, 1, −2) and (1, 2, −1).\r\n4. Is it possible to find a basis of R\r\n4\r\ncontaining the vectors (1, 1, 1, −2), (1, 2, −1, 1) and\r\n(1, −2, 7, −11)?\r\n5. Let S = {v1, v2, . . . , vp} be a subset of a vector space V (F). Suppose L(S) = V but S\r\nis not a linearly independent set. Then prove that each vector in V can be expressed\r\nin more than one way as a linear combination of vectors from S.\r\n6. Show that the set {(1, 0, 1),(1, i, 0),(1, 1, 1 − i)} is a basis of C\r\n3\r\n.\r\n7. Find a basis of the real vector space C\r\nn\r\ncontaining the basis B given in Example 10.\r\n8. Find a basis of V = {(x, y, z, u) ∈ R\r\n4\r\n: x − y − z = 0, x + z − u = 0}.\r\n9. Let A =\r\n\r\n\r\n\r\n1 0 1 1 0\r\n0 1 2 3 0\r\n0 0 0 0 1\r\n\r\n\r\n . Find a basis of V = {x\r\nt ∈ R5\r\n: Ax = 0}.\r\n10. Prove that {1, x, x2, . . . , xn, . . .} is a basis of the vector space P(R). This basis has\r\nan infinite number of vectors. This is also called the standard basis of P(R).\r\n11. Let u\r\nt = (1, 1, −2), vt = (−1, 2, 3) and wt = (1, 10, 1). Find a basis of L(u, v, w).\r\nDetermine a geometrical representation of L(u, v, w)?\r\n12. Prove that {(1, 0, 0),(1, 1, 0),(1, 1, 1)} is a basis of C\r\n3\r\n. Is it a basis of C\r\n3\r\n(R)?\r\n3.3.1 Dimension of a Finite Dimensional Vector Space\r\nWe first prove a result which helps us in associating a non-negative integer to every finite\r\ndimensional vector space.\r\nTheorem 3.3.6. Let V be a vector space with basis {v1, v2, . . . , vn}. Let m be a positive\r\ninteger with m > n. Then the set S = {w1, w2, . . . , wm} ⊂ V is linearly dependent.\r\nProof. We need to show that the linear system\r\nα1w1 + α2w2 + · · · + αmwm = 0 (3.3.2)\r\nin the unknowns α1, α2, . . . , αm has a non-trivial solution. We start by expressing the\r\nvectors wi\r\nin terms of the basis vectors vj ’s.\r\nAs {v1, v2, . . . , vn} is a basis of V , for each wi ∈ V, 1 ≤ i ≤ m, there exist unique\r\nscalars aij , 1 ≤ i ≤ n, 1 ≤ j ≤ m, such that\r\nw1 = a11v1 + a21v2 + · · · + an1vn,\r\nw2 = a12v1 + a22v2 + · · · + an2vn,\r\n.\r\n.\r\n. =\r\n.\r\n.\r\n.\r\nwm = a1mv1 + a2mv2 + · · · + anmvn.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/85329a87-0721-456e-97cf-2c23d4179a49.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5ddf713d1372ba2e1556f01ad44c0ddbfad8dcc0bc2e2ea2d955eb0f301fb78e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 516
      },
      {
        "segments": [
          {
            "segment_id": "85329a87-0721-456e-97cf-2c23d4179a49",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 78,
            "page_width": 612,
            "page_height": 792,
            "content": "78 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n3. Find a basis of R\r\n3\r\ncontaining the vector (1, 1, −2) and (1, 2, −1).\r\n4. Is it possible to find a basis of R\r\n4\r\ncontaining the vectors (1, 1, 1, −2), (1, 2, −1, 1) and\r\n(1, −2, 7, −11)?\r\n5. Let S = {v1, v2, . . . , vp} be a subset of a vector space V (F). Suppose L(S) = V but S\r\nis not a linearly independent set. Then prove that each vector in V can be expressed\r\nin more than one way as a linear combination of vectors from S.\r\n6. Show that the set {(1, 0, 1),(1, i, 0),(1, 1, 1 − i)} is a basis of C\r\n3\r\n.\r\n7. Find a basis of the real vector space C\r\nn\r\ncontaining the basis B given in Example 10.\r\n8. Find a basis of V = {(x, y, z, u) ∈ R\r\n4\r\n: x − y − z = 0, x + z − u = 0}.\r\n9. Let A =\r\n\r\n\r\n\r\n1 0 1 1 0\r\n0 1 2 3 0\r\n0 0 0 0 1\r\n\r\n\r\n . Find a basis of V = {x\r\nt ∈ R5\r\n: Ax = 0}.\r\n10. Prove that {1, x, x2, . . . , xn, . . .} is a basis of the vector space P(R). This basis has\r\nan infinite number of vectors. This is also called the standard basis of P(R).\r\n11. Let u\r\nt = (1, 1, −2), vt = (−1, 2, 3) and wt = (1, 10, 1). Find a basis of L(u, v, w).\r\nDetermine a geometrical representation of L(u, v, w)?\r\n12. Prove that {(1, 0, 0),(1, 1, 0),(1, 1, 1)} is a basis of C\r\n3\r\n. Is it a basis of C\r\n3\r\n(R)?\r\n3.3.1 Dimension of a Finite Dimensional Vector Space\r\nWe first prove a result which helps us in associating a non-negative integer to every finite\r\ndimensional vector space.\r\nTheorem 3.3.6. Let V be a vector space with basis {v1, v2, . . . , vn}. Let m be a positive\r\ninteger with m > n. Then the set S = {w1, w2, . . . , wm} ⊂ V is linearly dependent.\r\nProof. We need to show that the linear system\r\nα1w1 + α2w2 + · · · + αmwm = 0 (3.3.2)\r\nin the unknowns α1, α2, . . . , αm has a non-trivial solution. We start by expressing the\r\nvectors wi\r\nin terms of the basis vectors vj ’s.\r\nAs {v1, v2, . . . , vn} is a basis of V , for each wi ∈ V, 1 ≤ i ≤ m, there exist unique\r\nscalars aij , 1 ≤ i ≤ n, 1 ≤ j ≤ m, such that\r\nw1 = a11v1 + a21v2 + · · · + an1vn,\r\nw2 = a12v1 + a22v2 + · · · + an2vn,\r\n.\r\n.\r\n. =\r\n.\r\n.\r\n.\r\nwm = a1mv1 + a2mv2 + · · · + anmvn.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/85329a87-0721-456e-97cf-2c23d4179a49.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5ddf713d1372ba2e1556f01ad44c0ddbfad8dcc0bc2e2ea2d955eb0f301fb78e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 516
      },
      {
        "segments": [
          {
            "segment_id": "0dca734c-60cb-45f7-bce5-9b3b1235e664",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 79,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 79\r\nHence, Equation (3.3.2) can be rewritten as\r\nα1\r\n\r\n\r\nXn\r\nj=1\r\naj1vj\r\n\r\n + α2\r\n\r\n\r\nXn\r\nj=1\r\naj2vj\r\n\r\n + · · · + αm\r\n\r\n\r\nXn\r\nj=1\r\najmvj\r\n\r\n = 0.\r\nOr equivalently,\r\n Xm\r\ni=1\r\nαia1i\r\n!\r\nv1 +\r\n Xm\r\ni=1\r\nαia2i\r\n!\r\nv2 + · · · +\r\n Xm\r\ni=1\r\nαiani!vn = 0. (3.3.3)\r\nSince {v1, . . . , vn} is a basis, using Exercise 3.3.5.1, we get\r\nXm\r\ni=1\r\nαia1i =\r\nXm\r\ni=1\r\nαia2i = · · · =\r\nXm\r\ni=1\r\nαiani = 0.\r\nTherefore, finding αi\r\n’s satisfying Equation (3.3.2) reduces to solving the homogeneous\r\nsystem Aα = 0 where α =\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1\r\nα2\r\n.\r\n.\r\n.\r\nαm\r\n\r\n\r\n\r\n\r\n\r\n\r\nand A =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1m\r\na21 a22 · · · a2m\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 an2 · · · anm\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nSince n < m, Corollary 2.1.23.2 (here the matrix A is m × n) implies that Aα = 0\r\nhas a non-trivial solution. hence Equation (3.3.2) has a non-trivial solution and thus\r\n{w1, w2, . . . , wm} is a linearly dependent set.\r\nCorollary 3.3.7. Let B1 = {u1, . . . , un} and B2 = {v1, . . . , vm} be two bases of a finite\r\ndimensional vector space V . Then m = n.\r\nProof. Let if possible, m > n. Then by Theorem 3.3.6, {v1, . . . , vm} is a linearly dependent\r\nsubset of V , contradicting the assumption that B2 is a basis of V . Hence we must have\r\nm ≤ n. A similar argument implies n ≤ m and hence m = n.\r\nLet V be a finite dimensional vector space. Then Corollary 3.3.7 implies that the\r\nnumber of elements in any basis of V is the same. This number is used to define the\r\ndimension of any finite dimensional vector space.\r\nDefinition 3.3.8 (Dimension of a Finite Dimensional Vector Space). Let V be a finite\r\ndimensional vector space. Then the dimension of V , denoted dim(V ), is the number of\r\nelements in a basis of V .\r\nNote that Corollary 3.2.5.2 can be used to generate a basis of any non-trivial finite\r\ndimensional vector space.\r\nExample 3.3.9. The dimension of vector spaces in Example 3.3.4 are as follows:\r\n1. dim(R) = 1 in Example 3.3.4.1.\r\n2. dim(R\r\n2\r\n) = 2 in Example 3.3.4.2.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0dca734c-60cb-45f7-bce5-9b3b1235e664.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9ff0b1aa0c60dd4b2bbfff3b4db7fb558dbe468cea524122d671d9015337ac8a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 438
      },
      {
        "segments": [
          {
            "segment_id": "c96c675b-6d09-4397-abe0-5ef24646100d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 80,
            "page_width": 612,
            "page_height": 792,
            "content": "80 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n3. dim(V ) = 2 in Example 3.3.4.4.\r\n4. dim(V ) = 2 in Example 3.3.4.5.\r\n5. dim(V ) = 1 in Example 3.3.4.6.\r\n6. dim(V ) = 2 in Example 3.3.4.7.\r\n7. dim(C\r\n2\r\n) = 2 in Example 3.3.4.8.\r\n8. dim(C\r\n2\r\n(R)) = 4 in Example 3.3.4.9.\r\n9. For fixed positive integer n, dim(R\r\nn\r\n) = n in Example 3.3.4.3 and in Example 3.3.4.10,\r\none has dim(C\r\nn\r\n) = n and dim(C\r\nn\r\n(R)) = 2n.\r\nThus, we see that the dimension of a vector space dependents on the set of scalars.\r\nExample 3.3.10. Let V be the set of all functions f : R\r\nn−→R with the property that\r\nf(x+ y) = f(x) + f(y) and f(αx) = αf(x) for all x, y ∈ R\r\nn and α ∈ R. For any f, g ∈ V,\r\nand t ∈ R, define\r\n(f ⊕ g)(x) = f(x) + g(x) and (t ⊙ f)(x) = f(tx).\r\nThen it can be easily verified that V is a real vector space. Also, for 1 ≤ i ≤ n, define\r\nthe functions ei(x) = ei\r\n\r\n(x1, x2, . . . , xn)\r\n\u0001\r\n= xi. Then it can be easily verified that the set\r\n{e1, e2, . . . , en} is a basis of V and hence dim(V ) = n.\r\nThe next theorem follows directly from Corollary 3.2.5.2 and Theorem 3.3.6. Hence,\r\nthe proof is omitted.\r\nTheorem 3.3.11. Let S be a linearly independent subset of a finite dimensional vector\r\nspace V. Then the set S can be extended to form a basis of V.\r\nTheorem 3.3.11 is equivalent to the following statement:\r\nLet V be a vector space of dimension n. Suppose, we have found a linearly independent\r\nsubset {v1, . . . , vr} of V with r < n. Then it is possible to find vectors vr+1, . . . , vn in V\r\nsuch that {v1, v2, . . . , vn} is a basis of V. Thus, one has the following important corollary.\r\nCorollary 3.3.12. Let V be a vector space of dimension n. Then\r\n1. any set consisting of n linearly independent vectors forms a basis of V.\r\n2. any subset S of V having n vectors with L(S) = V forms a basis of V .\r\nExercise 3.3.13. 1. Determine dim(Pn(R)). Is dim(P(R)) finite?\r\n2. Let W1 and W2 be two subspaces of a vector space V such that W1 ⊂ W2. Show that\r\nW1 = W2 if and only if dim(W1) = dim(W2).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c96c675b-6d09-4397-abe0-5ef24646100d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ebeba32c642e8eaa47300a4556fea78df0ece323a396563c05a413fcc92d1c44",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 431
      },
      {
        "segments": [
          {
            "segment_id": "d378b573-1de5-45c8-8783-bad59b30dd0e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 81,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 81\r\n3. Consider the vector space C([−π, π]). For each integer n, define en(x) = sin(nx).\r\nProve that {en : n = 1, 2, . . .} is a linearly independent set.\r\n[Hint: For any positive integer ℓ, consider the set {ek1, . . . , ekℓ} and the linear system\r\nα1 sin(k1x) + α2 sin(k2x) + · · · + αℓ sin(kℓx) = 0 for all x ∈ [−π, π]\r\nin the unknowns α1, . . . , αn. Now for suitable values of m, consider the integral\r\nZ π\r\n−π\r\nsin(mx) (α1 sin(k1x) + α2 sin(k2x) + · · · + αℓ sin(kℓx)) dx\r\nto get the required result.]\r\n4. Determine a basis and dimension of W = {(x, y, z, w) ∈ R\r\n4\r\n: x + y − z + w = 0}.\r\n5. Let W1 be a subspace of a vector space V . If dim(V ) = n and dim(W1) = k with\r\nk ≥ 1 then prove that there exists a subspace W2 of V such that W1 ∩ W2 = {0},\r\nW1 + W2 = V and dim(W2) = n − k. Also, prove that for each v ∈ V there exist\r\nunique vectors w1 ∈ W1 and w2 ∈ W2 such that v = w1 + w2. The subspace W2 is\r\ncalled the complementary subspace of W1 in V.\r\n6. Is the set, W = {p(x) ∈ P4(R) : p(−1) = p(1) = 0} a subspace of P4(R)? If yes,\r\nfind its dimension.\r\n3.3.2 Application to the study of C\r\nn\r\nIn this subsection, we will study results that are intrinsic to the understanding of linear\r\nalgebra, especially results associated with matrices. We start with a few exercises that\r\nshould have appeared in previous sections of this chapter.\r\nExercise 3.3.14. 1. Let V = {A ∈ M2(C) : tr(A) = 0}, where tr(A) stands for the\r\ntrace of the matrix A. Show that V is a real vector space and find its basis. Is\r\nW =\r\n(\"a b\r\nc −a\r\n#\r\n: c = −b\r\n)\r\na subspace of V ?\r\n2. In each of the questions given below, determine whether the given set is a vector space\r\nor not? If it is a vector space, find the dimension and a basis.\r\n(a) sln(R) = {A ∈ Mn(R) : tr(A) = 0}.\r\n(b) Sn(R) = {A ∈ Mn(R) : A = At}.\r\n(c) An(R) = {A ∈ Mn(R) : A + At = 0}.\r\n(d) sln(C) = {A ∈ Mn(C) : tr(A) = 0}.\r\n(e) Sn(C) = {A ∈ Mn(C) : A = A∗}.\r\n(f) An(C) = {A ∈ Mn(C) : A + A∗ = 0}.\r\n3. Does there exist an A ∈ M2(C) satisfying A2 6= 0 but A3 = 0.\r\n4. Prove that there does not exist an A ∈ Mn(C) satisfying An 6= 0 but An+1 = 0. That\r\nis, if A is an n × n nilpotent matrix then the order of nilpotency ≤ n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/d378b573-1de5-45c8-8783-bad59b30dd0e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=21b2ecc55dbaeff30663d6d2ab3feb564cdeba309e760d81a24768143d3e2014",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 500
      },
      {
        "segments": [
          {
            "segment_id": "75f4034d-1b8d-4e81-a8a9-a092acf1deaa",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 82,
            "page_width": 612,
            "page_height": 792,
            "content": "82 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n5. Let A ∈ Mn(C) be a triangular matrix. Then the rows/columns of A are linearly\r\nindependent subset of C\r\nn\r\nif and only if aii 6= 0 for 1 ≤ i ≤ n.\r\n6. Prove that the rows/columns of A ∈ Mn(C) are linearly independent if and only if\r\ndet(A) 6= 0.\r\n7. Prove that the rows/columns of A ∈ Mn(C) span C\r\nn\r\nif and only if A is an invertible\r\nmatrix.\r\n8. Let A be a skew-symmetric matrix of odd order. Prove that the rows/columns of A\r\nare linearly dependent. Hint: What is det(A)?\r\nWe now define subspaces that are associated with matrices.\r\nDefinition 3.3.15. Let A ∈ Mm×n(C) and let R1, R2, . . . , Rm ∈ C\r\nn\r\nbe the rows of A and\r\na1, a2, . . . , an ∈ C\r\nm be its columns. We define\r\n1. Column Space(A), denoted Col(A), as Col(A) = L(a1, a2, . . . , an) = {Ax : x ∈\r\nC\r\nn} ⊂ Cm,\r\n2. Column Space(A∗), as Col(A∗) = {A∗x : x ∈ C\r\nm} ⊂ Cn\r\n,\r\n3. Null Space(A), denoted N (A), as N (A) = {x ∈ C\r\nn\r\n: Ax = 0}.\r\n4. Range(A), denoted R(A), as Im(A) = R(A) = {y : Ax = y for some x ∈ C\r\nn}.\r\nNote that the “column space” of A consists of all b such that Ax = b has a solution.\r\nHence, Col(A) = Im(A). We illustrate the above definitions with the help of an example\r\nand then ask the readers to solve the exercises that appear after the example.\r\nExample 3.3.16. Compute the above mentioned subspaces for A =\r\n\r\n\r\n\r\n1 1 1 −2\r\n1 2 −1 1\r\n1 −2 7 −11\r\n\r\n\r\n.\r\nSolution: Verify the following\r\n1. R(A) = L(R1, R2, R3) = {(x, y, z, u) ∈ C\r\n4\r\n: 3x − 2y = z, 5x − 3y + u = 0} = C(A∗)\r\n2. C(A) = L(a1, a2, a3, a4) = {(x, y, z) ∈ C\r\n3\r\n: 4x − 3y − z = 0} = R(A∗)\r\n3. N (A) = {(x, y, z, u) ∈ C\r\n4\r\n: x + 3z − 5u = 0, y − 2z + 3u = 0}.\r\n4. N (A∗) = {(x, y, z) ∈ C\r\n3\r\n: x + 4z = 0, y − 3z = 0}.\r\nExercise 3.3.17. 1. Let A ∈ Mm×n(C). Then prove that\r\n(a) R(A) is a subspace of C\r\nn\r\n,\r\n(b) C(A) is a subspace of C\r\nm,\r\n(c) N (A) is a subspace of C\r\nn\r\n,\r\n(d) N (At\r\n) is a subspace of C\r\nm,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/75f4034d-1b8d-4e81-a8a9-a092acf1deaa.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0bdfb91032b2a2e5fba46070784de6cf541532602a8c04d0a77ca1e964d72f20",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 456
      },
      {
        "segments": [
          {
            "segment_id": "cb594966-ab79-4b84-b314-19d78fee3ce7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 83,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 83\r\n(e) R(A) = C(At\r\n) and C(A) = R(At).\r\n2. Let A =\r\n\r\n\r\n\r\n\r\n\r\n1 2 1 3 2\r\n0 2 2 2 4\r\n2 −2 4 0 8\r\n4 2 5 6 10\r\n\r\n\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n\r\n\r\n2 4 0 6\r\n−1 0 −2 5\r\n−3 −5 1 −4\r\n−1 −1 1 2\r\n\r\n\r\n\r\n\r\n\r\n.\r\n(a) Find the row-reduced echelon forms of A and B.\r\n(b) Find P1 and P2 such that P1A and P2B are in row-reduced echelon form.\r\n(c) Find a basis each for the row spaces of A and B.\r\n(d) Find a basis each for the range spaces of A and B.\r\n(e) Find bases of the null spaces of A and B.\r\n(f) Find the dimensions of all the vector subspaces so obtained.\r\nLemma 3.3.18. Let A ∈ Mm×n(C) and let B = EA for some elementary matrix E. Then\r\nR(A) = R(B) and dim(R(A)) = dim(R(B)).\r\nProof. We prove the result for the elementary matrix Eij (c), where c 6= 0 and 1 ≤ i <\r\nj ≤ m. The readers are advised to prove the results for other elementary matrices. Let\r\nR1, R2, . . . , Rm be the rows of A. Then B = Eij (c)A implies\r\nR(B) = L(R1, . . . , Ri−1, Ri + cRj , Ri+1, . . . , Rm)\r\n= {α1R1 + · · · + αi−1Ri−1 + αi(Ri + cRj ) + · · ·\r\n+αmRm : αℓ ∈ R, 1 ≤ ℓ ≤ m}\r\n=\r\n(Xm\r\nℓ=1\r\nαℓRℓ + αi(cRj ) : αℓ ∈ R, 1 ≤ ℓ ≤ m\r\n)\r\n=\r\n(Xm\r\nℓ=1\r\nβℓRℓ: βℓ ∈ R, 1 ≤ ℓ ≤ m\r\n)\r\n= L(R1, . . . , Rm) = R(A)\r\nHence, the proof of the lemma is complete.\r\nWe omit the proof of the next result as the proof is similar to the proof of Lemma 3.3.18.\r\nLemma 3.3.19. Let A ∈ Mm×n(C) and let C = AE for some elementary matrix E. Then\r\nC(A) = C(C) and dim(C(A)) = dim(C(C)).\r\nThe first and second part of the next result are a repeated application of Lemma 3.3.18\r\nand Lemma 3.3.19, respectively. Hence the proof is omitted. This result is also helpful in\r\nfinding a basis of a subspace of C\r\nn\r\n.\r\nCorollary 3.3.20. Let A ∈ Mm×n(C). If\r\n1. B is in row-reduced echelon form of A then R(A) = R(B). In particular, the non-zero\r\nrows of B form a basis of R(A) and dim(R(A)) = dim(R(B)) = Row rank(A).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/cb594966-ab79-4b84-b314-19d78fee3ce7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c465009e8f0434e68848726c2edde89334a0f57aa8a647865c4bf16a78e4fdce",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 442
      },
      {
        "segments": [
          {
            "segment_id": "db53e157-a96b-4710-9b3c-bcc81bc2076a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 84,
            "page_width": 612,
            "page_height": 792,
            "content": "84 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n2. the application of column operations gives a matrix C that has the form given in\r\nRemark 2.3.6, then dim(C(A)) = dim(C(C)) = Column rank(A) and the non-zero\r\ncolumns of C form a basis of C(A).\r\nBefore proceeding with applications of Corollary 3.3.20, we first prove that for any\r\nA ∈ Mm×n(C), Row rank(A) = Column rank(A).\r\nTheorem 3.3.21. Let A ∈ Mm×n(C). Then Row rank(A) = Column rank(A).\r\nProof. Let R1, R2, . . . , Rm be the rows of A and C1, C2, . . . , Cn be the columns of A. Let\r\nRow rank(A) = r. Then by Corollary 3.3.20.1, dimL(R1, R2, . . . , Rm)\r\n\u0001\r\n= r. Hence, there\r\nexists vectors\r\nu\r\nt\r\n1 = (u11, . . . , u1n), u\r\nt\r\n2 = (u21, . . . , u2n), . . . , u\r\nt\r\nr = (ur1, . . . , urn) ∈ R\r\nn\r\nwith\r\nRi ∈ L(u\r\nt\r\n1\r\n, u\r\nt\r\n2\r\n, . . . , u\r\nt\r\nr\r\n) ∈ R\r\nn\r\n, for all i, 1 ≤ i ≤ m.\r\nTherefore, there exist real numbers αij , 1 ≤ i ≤ m, 1 ≤ j ≤ r such that\r\nR1 = α11u\r\nt\r\n1 + · · · + α1ru\r\nt\r\nr =\r\n Xr\r\ni=1\r\nα1iui1,\r\nXr\r\ni=1\r\nα1iui2, . . . ,Xr\r\ni=1\r\nα1iuin!,\r\nR2 = α21u\r\nt\r\n1 + · · · + α2ru\r\nt\r\nr =\r\n Xr\r\ni=1\r\nα2iui1,\r\nXr\r\ni=1\r\nα2iui2, . . . ,Xr\r\ni=1\r\nα2iuin!,\r\nand so on, till\r\nRm = αm1u\r\nt\r\n1 + · · · + αmru\r\nt\r\nr =\r\n Xr\r\ni=1\r\nαmiui1,\r\nXr\r\ni=1\r\nαmiui2, . . . ,Xr\r\ni=1\r\nαmiuin!.\r\nSo,\r\nC1 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPr\r\ni=1\r\nα1iui1\r\n.\r\n.\r\n.\r\nPr\r\ni=1\r\nαmiui1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n= u11\r\n\r\n\r\n\r\n\r\n\r\n\r\nα11\r\nα21\r\n.\r\n.\r\n.\r\nαm1\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ u21\r\n\r\n\r\n\r\n\r\n\r\n\r\nα12\r\nα22\r\n.\r\n.\r\n.\r\nαm2\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ · · · + ur1\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1r\r\nα2r\r\n.\r\n.\r\n.\r\nαmr\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nIn general, for 1 ≤ j ≤ n, we have\r\nCj =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPr\r\ni=1\r\nα1iuij\r\n.\r\n.\r\n.\r\nPr\r\ni=1\r\nαmiuij\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n= u1j\r\n\r\n\r\n\r\n\r\n\r\n\r\nα11\r\nα21\r\n.\r\n.\r\n.\r\nαm1\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ u2j\r\n\r\n\r\n\r\n\r\n\r\n\r\nα12\r\nα22\r\n.\r\n.\r\n.\r\nαm2\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ · · · + urj\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1r\r\nα2r\r\n.\r\n.\r\n.\r\nαmr\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nTherefore, C1, C2, . . . , Cn are linear combination of the r vectors\r\n(α11, α21, . . . , αm1)\r\nt\r\n,(α12, α22, . . . , αm2)\r\nt\r\n, . . . ,(α1r , α2r, . . . , αmr)\r\nt\r\n.\r\nThus, by Corollary 3.3.20.2, Column rank(A) = dimC(A)\r\n\u0001\r\n≤ r = Row rank(A). A similar\r\nargument gives Row rank(A) ≤ Column rank(A). Hence, we have the required result.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/db53e157-a96b-4710-9b3c-bcc81bc2076a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=345ce1bf020ed272e13d4dc2195a13caf5be3dad83756d55c56be1685c4efaa4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "db53e157-a96b-4710-9b3c-bcc81bc2076a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 84,
            "page_width": 612,
            "page_height": 792,
            "content": "84 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n2. the application of column operations gives a matrix C that has the form given in\r\nRemark 2.3.6, then dim(C(A)) = dim(C(C)) = Column rank(A) and the non-zero\r\ncolumns of C form a basis of C(A).\r\nBefore proceeding with applications of Corollary 3.3.20, we first prove that for any\r\nA ∈ Mm×n(C), Row rank(A) = Column rank(A).\r\nTheorem 3.3.21. Let A ∈ Mm×n(C). Then Row rank(A) = Column rank(A).\r\nProof. Let R1, R2, . . . , Rm be the rows of A and C1, C2, . . . , Cn be the columns of A. Let\r\nRow rank(A) = r. Then by Corollary 3.3.20.1, dimL(R1, R2, . . . , Rm)\r\n\u0001\r\n= r. Hence, there\r\nexists vectors\r\nu\r\nt\r\n1 = (u11, . . . , u1n), u\r\nt\r\n2 = (u21, . . . , u2n), . . . , u\r\nt\r\nr = (ur1, . . . , urn) ∈ R\r\nn\r\nwith\r\nRi ∈ L(u\r\nt\r\n1\r\n, u\r\nt\r\n2\r\n, . . . , u\r\nt\r\nr\r\n) ∈ R\r\nn\r\n, for all i, 1 ≤ i ≤ m.\r\nTherefore, there exist real numbers αij , 1 ≤ i ≤ m, 1 ≤ j ≤ r such that\r\nR1 = α11u\r\nt\r\n1 + · · · + α1ru\r\nt\r\nr =\r\n Xr\r\ni=1\r\nα1iui1,\r\nXr\r\ni=1\r\nα1iui2, . . . ,Xr\r\ni=1\r\nα1iuin!,\r\nR2 = α21u\r\nt\r\n1 + · · · + α2ru\r\nt\r\nr =\r\n Xr\r\ni=1\r\nα2iui1,\r\nXr\r\ni=1\r\nα2iui2, . . . ,Xr\r\ni=1\r\nα2iuin!,\r\nand so on, till\r\nRm = αm1u\r\nt\r\n1 + · · · + αmru\r\nt\r\nr =\r\n Xr\r\ni=1\r\nαmiui1,\r\nXr\r\ni=1\r\nαmiui2, . . . ,Xr\r\ni=1\r\nαmiuin!.\r\nSo,\r\nC1 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPr\r\ni=1\r\nα1iui1\r\n.\r\n.\r\n.\r\nPr\r\ni=1\r\nαmiui1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n= u11\r\n\r\n\r\n\r\n\r\n\r\n\r\nα11\r\nα21\r\n.\r\n.\r\n.\r\nαm1\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ u21\r\n\r\n\r\n\r\n\r\n\r\n\r\nα12\r\nα22\r\n.\r\n.\r\n.\r\nαm2\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ · · · + ur1\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1r\r\nα2r\r\n.\r\n.\r\n.\r\nαmr\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nIn general, for 1 ≤ j ≤ n, we have\r\nCj =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPr\r\ni=1\r\nα1iuij\r\n.\r\n.\r\n.\r\nPr\r\ni=1\r\nαmiuij\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n= u1j\r\n\r\n\r\n\r\n\r\n\r\n\r\nα11\r\nα21\r\n.\r\n.\r\n.\r\nαm1\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ u2j\r\n\r\n\r\n\r\n\r\n\r\n\r\nα12\r\nα22\r\n.\r\n.\r\n.\r\nαm2\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ · · · + urj\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1r\r\nα2r\r\n.\r\n.\r\n.\r\nαmr\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nTherefore, C1, C2, . . . , Cn are linear combination of the r vectors\r\n(α11, α21, . . . , αm1)\r\nt\r\n,(α12, α22, . . . , αm2)\r\nt\r\n, . . . ,(α1r , α2r, . . . , αmr)\r\nt\r\n.\r\nThus, by Corollary 3.3.20.2, Column rank(A) = dimC(A)\r\n\u0001\r\n≤ r = Row rank(A). A similar\r\nargument gives Row rank(A) ≤ Column rank(A). Hence, we have the required result.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/db53e157-a96b-4710-9b3c-bcc81bc2076a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=345ce1bf020ed272e13d4dc2195a13caf5be3dad83756d55c56be1685c4efaa4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "9ec6dfcf-e441-4072-8fb0-9b951b32e083",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 85,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 85\r\nLet M and N be two subspaces a vector space V (F). Then recall that (see Exer\u0002cise 3.1.22.5d) M + N = {u + v : u ∈ M, v ∈ N} is the smallest subspace of V containing\r\nboth M and N. We now state a very important result that relates the dimensions of the\r\nthree subspaces M, N and M + N (for a proof, see Appendix 7.3.1).\r\nTheorem 3.3.22. Let M and N be two subspaces of a finite dimensional vector space\r\nV (F). Then\r\ndim(M) + dim(N) = dim(M + N) + dim(M ∩ N). (3.3.4)\r\nLet S be a subset of R\r\nn and let V = L(S). Then Theorem 3.3.6 and Corollary 3.3.20.1\r\nto obtain a basis of V . The algorithm proceeds as follows:\r\n1. Construct a matrix A whose rows are the vectors in S.\r\n2. Apply row operations on A to get B, a matrix in row echelon form.\r\n3. Let B be the set of non-zero rows of B. Then B is a basis of L(S) = V.\r\nExample 3.3.23. 1. Let S = {(1, 1, 1, 1),(1, 1, −1, 1),(1, 1, 0, 1),(1, −1, 1, 1)} ⊂ R\r\n4\r\n.\r\nFind a basis of L(S).\r\nSolution: Here A =\r\n\r\n\r\n\r\n\r\n\r\n1 1 1 1\r\n1 1 −1 1\r\n1 1 0 1\r\n1 −1 1 1\r\n\r\n\r\n\r\n\r\n\r\n. Then B =\r\n\r\n\r\n\r\n\r\n\r\n1 1 1 1\r\n0 1 0 0\r\n0 0 1 0\r\n0 0 0 0\r\n\r\n\r\n\r\n\r\n\r\nis the row echelon\r\nform of A and hence B = {(1, 1, 1, 1),(0, 1, 0, 0),(0, 0, 1, 0)} is a basis of L(S). Ob\u0002serve that the non-zero rows of B can be obtained, using the first, second and fourth or\r\nthe first, third and fourth rows of A. Hence the subsets {(1, 1, 1, 1), (1, 1, 0, 1), (1, −1, 1, 1)}\r\nand {(1, 1, 1, 1), (1, 1, −1, 1), (1, −1, 1, 1)} of S are also bases of L(S).\r\n2. Let V = {(v, w, x, y, z) ∈ R\r\n5\r\n: v + x + z = 3y} and W = {(v, w, x, y, z) ∈ R\r\n5\r\n:\r\nw − x = z, v = y} be two subspaces of R\r\n5\r\n. Find bases of V and W containing a basis\r\nof V ∩ W.\r\nSolution: Let us find a basis of V ∩ W. The solution set of the linear equations\r\nv + x − 3y + z = 0, w − x − z = 0 and v = y\r\nis\r\n(v, w, x, y, z)\r\nt = (y, 2y, x, y, 2y − x)t = y(1, 2, 0, 1, 2)t + x(0, 0, 1, 0, −1)t\r\n.\r\nThus, a basis of V ∩ W is B = {(1, 2, 0, 1, 2),(0, 0, 1, 0, −1)}. Similarly, a basis of\r\nV is B1 = {(−1, 0, 1, 0, 0),(0, 1, 0, 0, 0),(3, 0, 0, 1, 0),(−1, 0, 0, 0, 1)} and that of W is\r\nB2 = {(1, 0, 0, 1, 0),(0, 1, 1, 0, 0),(0, 1, 0, 0, 1)}. To find a basis of V containing a basis\r\nof V ∩W, form a matrix whose rows are the vectors in B and B1 (see the first matrix\r\nin Equation(3.3.5)) and apply row operations without disturbing the first two rows",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9ec6dfcf-e441-4072-8fb0-9b951b32e083.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cf25f0930c59c89cb146c002ded30aa0eb6c9ae4f054801475d8cb9dcd670ada",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 584
      },
      {
        "segments": [
          {
            "segment_id": "9ec6dfcf-e441-4072-8fb0-9b951b32e083",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 85,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 85\r\nLet M and N be two subspaces a vector space V (F). Then recall that (see Exer\u0002cise 3.1.22.5d) M + N = {u + v : u ∈ M, v ∈ N} is the smallest subspace of V containing\r\nboth M and N. We now state a very important result that relates the dimensions of the\r\nthree subspaces M, N and M + N (for a proof, see Appendix 7.3.1).\r\nTheorem 3.3.22. Let M and N be two subspaces of a finite dimensional vector space\r\nV (F). Then\r\ndim(M) + dim(N) = dim(M + N) + dim(M ∩ N). (3.3.4)\r\nLet S be a subset of R\r\nn and let V = L(S). Then Theorem 3.3.6 and Corollary 3.3.20.1\r\nto obtain a basis of V . The algorithm proceeds as follows:\r\n1. Construct a matrix A whose rows are the vectors in S.\r\n2. Apply row operations on A to get B, a matrix in row echelon form.\r\n3. Let B be the set of non-zero rows of B. Then B is a basis of L(S) = V.\r\nExample 3.3.23. 1. Let S = {(1, 1, 1, 1),(1, 1, −1, 1),(1, 1, 0, 1),(1, −1, 1, 1)} ⊂ R\r\n4\r\n.\r\nFind a basis of L(S).\r\nSolution: Here A =\r\n\r\n\r\n\r\n\r\n\r\n1 1 1 1\r\n1 1 −1 1\r\n1 1 0 1\r\n1 −1 1 1\r\n\r\n\r\n\r\n\r\n\r\n. Then B =\r\n\r\n\r\n\r\n\r\n\r\n1 1 1 1\r\n0 1 0 0\r\n0 0 1 0\r\n0 0 0 0\r\n\r\n\r\n\r\n\r\n\r\nis the row echelon\r\nform of A and hence B = {(1, 1, 1, 1),(0, 1, 0, 0),(0, 0, 1, 0)} is a basis of L(S). Ob\u0002serve that the non-zero rows of B can be obtained, using the first, second and fourth or\r\nthe first, third and fourth rows of A. Hence the subsets {(1, 1, 1, 1), (1, 1, 0, 1), (1, −1, 1, 1)}\r\nand {(1, 1, 1, 1), (1, 1, −1, 1), (1, −1, 1, 1)} of S are also bases of L(S).\r\n2. Let V = {(v, w, x, y, z) ∈ R\r\n5\r\n: v + x + z = 3y} and W = {(v, w, x, y, z) ∈ R\r\n5\r\n:\r\nw − x = z, v = y} be two subspaces of R\r\n5\r\n. Find bases of V and W containing a basis\r\nof V ∩ W.\r\nSolution: Let us find a basis of V ∩ W. The solution set of the linear equations\r\nv + x − 3y + z = 0, w − x − z = 0 and v = y\r\nis\r\n(v, w, x, y, z)\r\nt = (y, 2y, x, y, 2y − x)t = y(1, 2, 0, 1, 2)t + x(0, 0, 1, 0, −1)t\r\n.\r\nThus, a basis of V ∩ W is B = {(1, 2, 0, 1, 2),(0, 0, 1, 0, −1)}. Similarly, a basis of\r\nV is B1 = {(−1, 0, 1, 0, 0),(0, 1, 0, 0, 0),(3, 0, 0, 1, 0),(−1, 0, 0, 0, 1)} and that of W is\r\nB2 = {(1, 0, 0, 1, 0),(0, 1, 1, 0, 0),(0, 1, 0, 0, 1)}. To find a basis of V containing a basis\r\nof V ∩W, form a matrix whose rows are the vectors in B and B1 (see the first matrix\r\nin Equation(3.3.5)) and apply row operations without disturbing the first two rows",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9ec6dfcf-e441-4072-8fb0-9b951b32e083.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cf25f0930c59c89cb146c002ded30aa0eb6c9ae4f054801475d8cb9dcd670ada",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 584
      },
      {
        "segments": [
          {
            "segment_id": "fedbeab9-5e31-430f-9d90-edfa119e00a9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 86,
            "page_width": 612,
            "page_height": 792,
            "content": "86 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nthat have come from B. Then after a few row operations, we get\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 2 0 1 2\r\n0 0 1 0 −1\r\n−1 0 1 0 0\r\n0 1 0 0 0\r\n3 0 0 1 0\r\n−1 0 0 0 1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−→\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 2 0 1 2\r\n0 0 1 0 −1\r\n0 1 0 0 0\r\n0 0 0 1 3\r\n0 0 0 0 0\r\n0 0 0 0 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n. (3.3.5)\r\nThus, a required basis of V is {(1, 2, 0, 1, 2),(0, 0, 1, 0, −1),(0, 1, 0, 0, 0),(0, 0, 0, 1, 3)}.\r\nSimilarly, a required basis of W is {(1, 2, 0, 1, 2),(0, 0, 1, 0, −1),(0, 0, −1, 0, 1)}.\r\nExercise 3.3.24. 1. If M and N are 4-dimensional subspaces of a vector space V of\r\ndimension 7 then show that M and N have at least one vector in common other than\r\nthe zero vector.\r\n2. Let V = {(x, y, z, w) ∈ R\r\n4\r\n: x + y − z + w = 0, x + y + z + w = 0, x + 2y = 0} and\r\nW = {(x, y, z, w) ∈ R\r\n4\r\n: x − y − z + w = 0, x + 2y − w = 0} be two subspaces of R\r\n4\r\n.\r\nFind bases and dimensions of V, W, V ∩ W and V + W.\r\n3. Let W1 and W2 be two subspaces of a vector space V . If dim(W1) + dim(W2) >\r\ndim(V ), then prove that W1 ∩ W2 contains a non-zero vector.\r\n4. Give examples to show that the Column Space of two row-equivalent matrices need\r\nnot be same.\r\n5. Let A ∈ Mm×n(C) with m < n. Prove that the columns of A are linearly dependent.\r\n6. Suppose a sequence of matrices A = B0 −→ B1 −→ · · · −→ Bk−1 −→ Bk = B\r\nsatisfies R(Bl) ⊂ R(Bl−1) for 1 ≤ l ≤ k. Then prove that R(B) ⊂ R(A).\r\nBefore going to the next section, we prove the rank-nullity theorem and the main\r\ntheorem of system of linear equations (see Theorem 2.4.1).\r\nTheorem 3.3.25 (Rank-Nullity Theorem). For any matrix A ∈ Mm×n(C),\r\ndim(C(A)) + dim(N (A)) = n.\r\nProof. Let dim(N (A)) = r < n and let {u1, u2, . . . , ur} be a basis of N (A). Since {u1, . . . , ur}\r\nis a linearly independent subset in R\r\nn\r\n, there exist vectors ur+1, . . . , un ∈ R\r\nn\r\n(see Corol\u0002lary 3.2.5.2) such that {u1, . . . , un} is a basis of R\r\nn\r\n. Then by definition,\r\nC(A) = L(Au1, Au2, . . . , Aun)\r\n= L(0, . . . , 0, Aur+1, Aur+2, . . . , Aun) = L(Aur+1, . . . , Aun).\r\nWe need to prove that {Aur+1, . . . , Aun} is a linearly independent set. Consider the linear\r\nsystem\r\nα1Aur+1 + α2Aur+2 + · · · + αn−rAun = 0. (3.3.6)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fedbeab9-5e31-430f-9d90-edfa119e00a9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e57ccc9dd512f7d5ed2ef20ccc8082752db7dafc59e8382d73c8b0b66158019c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 558
      },
      {
        "segments": [
          {
            "segment_id": "fedbeab9-5e31-430f-9d90-edfa119e00a9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 86,
            "page_width": 612,
            "page_height": 792,
            "content": "86 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nthat have come from B. Then after a few row operations, we get\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 2 0 1 2\r\n0 0 1 0 −1\r\n−1 0 1 0 0\r\n0 1 0 0 0\r\n3 0 0 1 0\r\n−1 0 0 0 1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−→\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 2 0 1 2\r\n0 0 1 0 −1\r\n0 1 0 0 0\r\n0 0 0 1 3\r\n0 0 0 0 0\r\n0 0 0 0 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n. (3.3.5)\r\nThus, a required basis of V is {(1, 2, 0, 1, 2),(0, 0, 1, 0, −1),(0, 1, 0, 0, 0),(0, 0, 0, 1, 3)}.\r\nSimilarly, a required basis of W is {(1, 2, 0, 1, 2),(0, 0, 1, 0, −1),(0, 0, −1, 0, 1)}.\r\nExercise 3.3.24. 1. If M and N are 4-dimensional subspaces of a vector space V of\r\ndimension 7 then show that M and N have at least one vector in common other than\r\nthe zero vector.\r\n2. Let V = {(x, y, z, w) ∈ R\r\n4\r\n: x + y − z + w = 0, x + y + z + w = 0, x + 2y = 0} and\r\nW = {(x, y, z, w) ∈ R\r\n4\r\n: x − y − z + w = 0, x + 2y − w = 0} be two subspaces of R\r\n4\r\n.\r\nFind bases and dimensions of V, W, V ∩ W and V + W.\r\n3. Let W1 and W2 be two subspaces of a vector space V . If dim(W1) + dim(W2) >\r\ndim(V ), then prove that W1 ∩ W2 contains a non-zero vector.\r\n4. Give examples to show that the Column Space of two row-equivalent matrices need\r\nnot be same.\r\n5. Let A ∈ Mm×n(C) with m < n. Prove that the columns of A are linearly dependent.\r\n6. Suppose a sequence of matrices A = B0 −→ B1 −→ · · · −→ Bk−1 −→ Bk = B\r\nsatisfies R(Bl) ⊂ R(Bl−1) for 1 ≤ l ≤ k. Then prove that R(B) ⊂ R(A).\r\nBefore going to the next section, we prove the rank-nullity theorem and the main\r\ntheorem of system of linear equations (see Theorem 2.4.1).\r\nTheorem 3.3.25 (Rank-Nullity Theorem). For any matrix A ∈ Mm×n(C),\r\ndim(C(A)) + dim(N (A)) = n.\r\nProof. Let dim(N (A)) = r < n and let {u1, u2, . . . , ur} be a basis of N (A). Since {u1, . . . , ur}\r\nis a linearly independent subset in R\r\nn\r\n, there exist vectors ur+1, . . . , un ∈ R\r\nn\r\n(see Corol\u0002lary 3.2.5.2) such that {u1, . . . , un} is a basis of R\r\nn\r\n. Then by definition,\r\nC(A) = L(Au1, Au2, . . . , Aun)\r\n= L(0, . . . , 0, Aur+1, Aur+2, . . . , Aun) = L(Aur+1, . . . , Aun).\r\nWe need to prove that {Aur+1, . . . , Aun} is a linearly independent set. Consider the linear\r\nsystem\r\nα1Aur+1 + α2Aur+2 + · · · + αn−rAun = 0. (3.3.6)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fedbeab9-5e31-430f-9d90-edfa119e00a9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e57ccc9dd512f7d5ed2ef20ccc8082752db7dafc59e8382d73c8b0b66158019c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 558
      },
      {
        "segments": [
          {
            "segment_id": "a8d33419-5e76-48cd-88f0-974f95a66d6a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 87,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 87\r\nin the unknowns α1, . . . , αn−r. This linear system is equivalent to\r\nA(α1ur+1 + α2ur+2 + · · · + αn−run) = 0.\r\nHence, by definition of N (A), α1ur+1 + · · · + αn−run ∈ N (A) = L(u1, . . . , ur). Therefore,\r\nthere exists scalars βi, 1 ≤ i ≤ r such that\r\nα1ur+1 + α2ur+2 + · · · + αn−run = β1u1 + β2u2 + · · · + βrur.\r\nOr equivalently,\r\nβ1u1 + · · · + βrur − α1ur+1 − · · · − αn−run = 0. (3.3.7)\r\nAs {u1, . . . , un} is a linearly independent set, the only solution of Equation (3.3.7) is\r\nαi = 0 for 1 ≤ i ≤ n − r and βj = 0 for 1 ≤ j ≤ r.\r\nIn other words, we have shown that the only solution of Equation (3.3.6) is the trivial\r\nsolution (αi = 0 for all i, 1 ≤ i ≤ n − r). Hence, the set {Aur+1, . . . , Aun} is a linearly\r\nindependent and is a basis of C(A). Thus\r\ndim(C(A)) + dim(N (A)) = (n − r) + r = n\r\nand the proof of the theorem is complete.\r\nTheorem 3.3.25 is part of what is known as the fundamental theorem of linear algebra\r\n(see Theorem 5.2.15). As the final result in this direction, We now prove the main theorem\r\non linear systems stated on page 48 (see Theorem 2.4.1) whose proof was omitted.\r\nTheorem 3.3.26. Consider a linear system Ax = b, where A is an m × n matrix, and\r\nx, b are vectors of orders n × 1, and m × 1, respectively. Suppose rank (A) = r and\r\nrank([A b]) = ra. Then exactly one of the following statement holds:\r\n1. If r < ra, the linear system has no solution.\r\n2. if ra = r, then the linear system is consistent. Furthermore,\r\n(a) if r = n, then the solution set of the linear system has a unique n × 1 vector x0\r\nsatisfying Ax0 = b.\r\n(b) if r < n, then the set of solutions of the linear system is an infinite set and has\r\nthe form\r\n{x0 + k1u1 + k2u2 + · · · + kn−run−r : ki ∈ R, 1 ≤ i ≤ n − r},\r\nwhere x0, u1, . . . , un−r are n × 1 vectors satisfying Ax0 = b and Aui = 0 for\r\n1 ≤ i ≤ n − r.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a8d33419-5e76-48cd-88f0-974f95a66d6a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b3b57dd35c73748d9b2fd031245e7f1a5464774e9eb76849d90b270df92c2ccd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 429
      },
      {
        "segments": [
          {
            "segment_id": "0d2c6957-aebc-4cbe-9da6-eb2e7fbd637e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 88,
            "page_width": 612,
            "page_height": 792,
            "content": "88 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nProof. Proof of Part 1. As r < ra, the (r + 1)-th row of the row-reduced echelon form of\r\n[A b] has the form [0, 1]. Thus, by Theorem 1, the system Ax = b is inconsistent.\r\nProof of Part 2a and Part 2b. As r = ra, using Corollary 3.3.20, C(A) = C([A, b]).\r\nHence, the vector b ∈ C(A) and therefore there exist scalars c1, c2, . . . , cn such that b =\r\nc1a1 +c2a2 +· · · cnan, where a1, a2, . . . , an are the columns of A. That is, we have a vector\r\nx\r\nt\r\n0 = [c1, c2, . . . , cn] that satisfies Ax = b.\r\nIf in addition r = n, then the system Ax = b has no free variables in its solution set\r\nand thus we have a unique solution (see Theorem 2.1.22.2a).\r\nWhereas the condition r < n implies that the system Ax = b has n−r free variables in\r\nits solution set and thus we have an infinite number of solutions (see Theorem 2.1.22.2b).\r\nTo complete the proof of the theorem, we just need to show that the solution set in this\r\ncase has the form {x0 + k1u1 + k2u2 + · · · + kn−run−r : ki ∈ R, 1 ≤ i ≤ n − r}, where\r\nAx0 = b and Aui = 0 for 1 ≤ i ≤ n − r.\r\nTo get this, note that using the rank-nullity theorem (see Theorem 3.3.25) rank(A) = r\r\nimplies that dim(N (A)) = n − r. Let {u1, u2, . . . , un−r} be a basis of N (A). Then by\r\ndefinition Aui = 0 for 1 ≤ i ≤ n − r and hence\r\nA(x0 + k1u1 + k2u2 + · · · + kn−run−r) = Ax0 + k10 + · · · + kn−r0 = b.\r\nThus, the required result follows.\r\nExample 3.3.27. Let A =\r\n\r\n\r\n\r\n1 1 0 1 1 0 −1\r\n0 0 1 2 3 0 −2\r\n0 0 0 0 0 1 1\r\n\r\n\r\n\r\nand V = {x\r\nt ∈ R7\r\n: Ax = 0}. Find\r\na basis and dimension of V .\r\nSolution: Observe that x1, x3 and x6 are the basic variables and the rest are the free\r\nvariables. Writing the basic variables in terms of free variables, we get\r\nx1 = x7 − x2 − x4 − x5, x3 = 2x7 − 2x4 − 3x5 and x6 = −x7.\r\nHence,\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\nx3\r\nx4\r\nx5\r\nx6\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx7 − x2 − x4 − x5\r\nx2\r\n2x7 − 2x4 − 3x5\r\nx4\r\nx5\r\n−x7\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n= x2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−1\r\n0\r\n−2\r\n1\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x5\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−1\r\n0\r\n−3\r\n0\r\n1\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n0\r\n2\r\n0\r\n0\r\n−1\r\n1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n. (3.3.8)\r\nTherefore, if we let u\r\nt\r\n1 =\r\nh\r\n−1, 1, 0, 0, 0, 0, 0\r\ni\r\n, u\r\nt\r\n2 =\r\nh\r\n−1, 0, −2, 1, 0, 0, 0\r\ni\r\n, u\r\nt\r\n3 =\r\nh\r\n−1, 0, −3, 0, 1, 0, 0\r\ni\r\nand u\r\nt\r\n4 =\r\nh\r\n1, 0, 2, 0, 0, −1, 1\r\ni\r\nthen S = {u1, u2, u3, u4} is the basis of V . The reasons are\r\nas follows:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0d2c6957-aebc-4cbe-9da6-eb2e7fbd637e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=447c445e2250813f382e4331238cf50aa5d2de7e181be9bfc6e7c1282a0fea20",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 715
      },
      {
        "segments": [
          {
            "segment_id": "0d2c6957-aebc-4cbe-9da6-eb2e7fbd637e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 88,
            "page_width": 612,
            "page_height": 792,
            "content": "88 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\nProof. Proof of Part 1. As r < ra, the (r + 1)-th row of the row-reduced echelon form of\r\n[A b] has the form [0, 1]. Thus, by Theorem 1, the system Ax = b is inconsistent.\r\nProof of Part 2a and Part 2b. As r = ra, using Corollary 3.3.20, C(A) = C([A, b]).\r\nHence, the vector b ∈ C(A) and therefore there exist scalars c1, c2, . . . , cn such that b =\r\nc1a1 +c2a2 +· · · cnan, where a1, a2, . . . , an are the columns of A. That is, we have a vector\r\nx\r\nt\r\n0 = [c1, c2, . . . , cn] that satisfies Ax = b.\r\nIf in addition r = n, then the system Ax = b has no free variables in its solution set\r\nand thus we have a unique solution (see Theorem 2.1.22.2a).\r\nWhereas the condition r < n implies that the system Ax = b has n−r free variables in\r\nits solution set and thus we have an infinite number of solutions (see Theorem 2.1.22.2b).\r\nTo complete the proof of the theorem, we just need to show that the solution set in this\r\ncase has the form {x0 + k1u1 + k2u2 + · · · + kn−run−r : ki ∈ R, 1 ≤ i ≤ n − r}, where\r\nAx0 = b and Aui = 0 for 1 ≤ i ≤ n − r.\r\nTo get this, note that using the rank-nullity theorem (see Theorem 3.3.25) rank(A) = r\r\nimplies that dim(N (A)) = n − r. Let {u1, u2, . . . , un−r} be a basis of N (A). Then by\r\ndefinition Aui = 0 for 1 ≤ i ≤ n − r and hence\r\nA(x0 + k1u1 + k2u2 + · · · + kn−run−r) = Ax0 + k10 + · · · + kn−r0 = b.\r\nThus, the required result follows.\r\nExample 3.3.27. Let A =\r\n\r\n\r\n\r\n1 1 0 1 1 0 −1\r\n0 0 1 2 3 0 −2\r\n0 0 0 0 0 1 1\r\n\r\n\r\n\r\nand V = {x\r\nt ∈ R7\r\n: Ax = 0}. Find\r\na basis and dimension of V .\r\nSolution: Observe that x1, x3 and x6 are the basic variables and the rest are the free\r\nvariables. Writing the basic variables in terms of free variables, we get\r\nx1 = x7 − x2 − x4 − x5, x3 = 2x7 − 2x4 − 3x5 and x6 = −x7.\r\nHence,\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\nx3\r\nx4\r\nx5\r\nx6\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx7 − x2 − x4 − x5\r\nx2\r\n2x7 − 2x4 − 3x5\r\nx4\r\nx5\r\n−x7\r\nx7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n= x2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−1\r\n0\r\n−2\r\n1\r\n0\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x5\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n−1\r\n0\r\n−3\r\n0\r\n1\r\n0\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n+ x7\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n0\r\n2\r\n0\r\n0\r\n−1\r\n1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n. (3.3.8)\r\nTherefore, if we let u\r\nt\r\n1 =\r\nh\r\n−1, 1, 0, 0, 0, 0, 0\r\ni\r\n, u\r\nt\r\n2 =\r\nh\r\n−1, 0, −2, 1, 0, 0, 0\r\ni\r\n, u\r\nt\r\n3 =\r\nh\r\n−1, 0, −3, 0, 1, 0, 0\r\ni\r\nand u\r\nt\r\n4 =\r\nh\r\n1, 0, 2, 0, 0, −1, 1\r\ni\r\nthen S = {u1, u2, u3, u4} is the basis of V . The reasons are\r\nas follows:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0d2c6957-aebc-4cbe-9da6-eb2e7fbd637e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=447c445e2250813f382e4331238cf50aa5d2de7e181be9bfc6e7c1282a0fea20",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 715
      },
      {
        "segments": [
          {
            "segment_id": "89d44025-7d80-4cb6-b1b8-cf68f5ae52df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 89,
            "page_width": 612,
            "page_height": 792,
            "content": "3.3. BASES 89\r\n1. For Linear independence, we consider the homogeneous system\r\nc1u1 + c2u2 + c3u3 + c4u4 = 0 (3.3.9)\r\nin the unknowns c1, c2, c3 and c4. Then relating the unknowns with the free variables\r\nx2, x4, x5 and x7 and then comparing Equations (3.3.8) and (3.3.9), we get\r\n(a) c1 = 0 as the 2-nd coordinate consists only of c1.\r\n(b) c2 = 0 as the 4-th coordinate consists only of c2.\r\n(c) c3 = 0 as the 5-th coordinate consists only of c3.\r\n(d) c4 = 0 as the 7-th coordinate consists only of c4.\r\nHence, the set S is linearly independent.\r\n2. L(S) = V is obvious as any vector of V has the form mentioned as the first equality\r\nin Equation (3.3.8).\r\nThe understanding built in Example 3.3.27 gives us the following remark.\r\nRemark 3.3.28. The vectors u1, u2, . . . , un−r in Theorem 3.3.26.2b correspond to express\u0002ing the solution set with the help of the free variables. This is done by writing the basic\r\nvariables in terms of the free variables and then writing the solution set in such a way that\r\neach ui corresponds to a specific free variable.\r\nThe following are some of the consequences of the rank-nullity theorem. The proof is\r\nleft as an exercise for the reader.\r\nExercise 3.3.29. 1. Let A be an m × n real matrix. Then\r\n(a) if n > m, then the system Ax = 0 has infinitely many solutions,\r\n(b) if n < m, then there exists a non-zero vector b = (b1, b2, . . . , bm)\r\nt\r\nsuch that the\r\nsystem Ax = b does not have any solution.\r\n2. The following statements are equivalent for an m × n matrix A.\r\n(a) Rank (A) = k.\r\n(b) There exist a set of k rows of A that are linearly independent.\r\n(c) There exist a set of k columns of A that are linearly independent.\r\n(d) dim(C(A)) = k.\r\n(e) There exists a k×k submatrix B of A with det(B) 6= 0 and determinant of every\r\n(k + 1) × (k + 1) submatrix of A is zero.\r\n(f) There exists a linearly independent subset {b1, b2, . . . , bk} of R\r\nm such that the\r\nsystem Ax = bifor 1 ≤ i ≤ k is consistent.\r\n(g) dim(N (A)) = n − k.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/89d44025-7d80-4cb6-b1b8-cf68f5ae52df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c60ac99eab99a5b9cb1173a4067f21ccfc433174a68a40835c665f6206d31b49",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 399
      },
      {
        "segments": [
          {
            "segment_id": "b72bb7b1-3957-4459-9c8c-2723306d02c2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 90,
            "page_width": 612,
            "page_height": 792,
            "content": "90 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n3.4 Ordered Bases\r\nLet B = {u1, u2, . . . , un} be a basis of a vector space V . As B is a set, there is no ordering\r\nof its elements. In this section, we want to associate an order among the vectors in any\r\nbasis of V as this helps in getting a better understanding about finite dimensional vector\r\nspaces and its relationship with matrices.\r\nDefinition 3.4.1 (Ordered Basis). Let V be a vector space of dimension n. Then an\r\nordered basis for V is a basis {u1, u2, . . . , un} together with a one-to-one correspondence\r\nbetween the sets {u1, u2, . . . , un} and {1, 2, 3, . . . , n}.\r\nIf the ordered basis has u1 as the first vector, u2 as the second vector and so on, then\r\nwe denote this by writing the ordered basis as (u1, u2, . . . , un).\r\nExample 3.4.2. 1. Consider the vector space P2(R) with basis {1− x, 1 + x, x2}. Then\r\none can take either B1 =\r\n\r\n1− x, 1 + x, x2\r\n\u0001\r\nor B2 =\r\n\r\n1 + x, 1− x, x2\r\n\u0001\r\nas ordered bases.\r\nAlso for any element a0 + a1x + a2x\r\n2 ∈ P2(R), one has\r\na0 + a1x + a2x\r\n2 =\r\na0 − a1\r\n2\r\n(1 − x) + a0 + a1\r\n2\r\n(1 + x) + a2x\r\n2\r\n.\r\nThus, a0 + a1x + a2x\r\n2\r\nin the ordered basis\r\n(a) B1, has a0−a1\r\n2\r\nas the coefficient of the first element, a0+a1\r\n2\r\nas the coefficient of\r\nthe second element and a2 as the coefficient the third element of B1.\r\n(b) B2, has a0+a1\r\n2\r\nas the coefficient of the first element, a0−a1\r\n2\r\nas the coefficient of\r\nthe second element and a2 as the coefficient the third element of B2.\r\n2. Let V = {(x, y, z) : x+y = z} and let B = {(−1, 1, 0),(1, 0, 1)} be a basis of V . Then\r\ncheck that (3, 4, 7) = 4(−1, 1, 0) + 7(1, 0, 1) ∈ V.\r\nThat is, as ordered bases (u1, u2, . . . , un), (u2, u3, . . . , un, u1) and (un, un−1, . . . , u2, u1)\r\nare different even though they have the same set of vectors as elements. To proceed further,\r\nwe now define the notion of coordinates of a vector depending on the chosen ordered basis.\r\nDefinition 3.4.3 (Coordinates of a Vector). Let B = (v1, v2, . . . , vn) be an ordered basis\r\nof a vector space V and let v ∈ V . Suppose\r\nv = β1v1 + β2v2 + · · · + βnvn for some scalars β1, β2, . . . , βn.\r\nThen the tuple (β1, β2, . . . , βn)\r\nt\r\nis called the coordinate of the vector v with respect to the\r\nordered basis B and is denoted by [v]B = (β1, . . . , βn)\r\nt\r\n, a column vector.\r\nExample 3.4.4. 1. In Example 3.4.2.1, let p(x) = a0 + a1x + a2x\r\n2\r\n. Then\r\n[p(x)]B1 =\r\n\r\n\r\n\r\na0−a1\r\n2\r\na0+a1\r\n2\r\na2\r\n\r\n\r\n , [p(x)]B2 =\r\n\r\n\r\n\r\na0+a1\r\n2\r\na0−a1\r\n2\r\na2\r\n\r\n\r\n\r\nand [p(x)]B3 =\r\n\r\n\r\n\r\na2\r\na0−a1\r\n2\r\na0+a1\r\n2\r\n\r\n\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b72bb7b1-3957-4459-9c8c-2723306d02c2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6529bf2e3bc3a5524d006cc20ce9c7e04b48de4cc259baa0bc2771b8e6ad16e4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 577
      },
      {
        "segments": [
          {
            "segment_id": "b72bb7b1-3957-4459-9c8c-2723306d02c2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 90,
            "page_width": 612,
            "page_height": 792,
            "content": "90 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n3.4 Ordered Bases\r\nLet B = {u1, u2, . . . , un} be a basis of a vector space V . As B is a set, there is no ordering\r\nof its elements. In this section, we want to associate an order among the vectors in any\r\nbasis of V as this helps in getting a better understanding about finite dimensional vector\r\nspaces and its relationship with matrices.\r\nDefinition 3.4.1 (Ordered Basis). Let V be a vector space of dimension n. Then an\r\nordered basis for V is a basis {u1, u2, . . . , un} together with a one-to-one correspondence\r\nbetween the sets {u1, u2, . . . , un} and {1, 2, 3, . . . , n}.\r\nIf the ordered basis has u1 as the first vector, u2 as the second vector and so on, then\r\nwe denote this by writing the ordered basis as (u1, u2, . . . , un).\r\nExample 3.4.2. 1. Consider the vector space P2(R) with basis {1− x, 1 + x, x2}. Then\r\none can take either B1 =\r\n\r\n1− x, 1 + x, x2\r\n\u0001\r\nor B2 =\r\n\r\n1 + x, 1− x, x2\r\n\u0001\r\nas ordered bases.\r\nAlso for any element a0 + a1x + a2x\r\n2 ∈ P2(R), one has\r\na0 + a1x + a2x\r\n2 =\r\na0 − a1\r\n2\r\n(1 − x) + a0 + a1\r\n2\r\n(1 + x) + a2x\r\n2\r\n.\r\nThus, a0 + a1x + a2x\r\n2\r\nin the ordered basis\r\n(a) B1, has a0−a1\r\n2\r\nas the coefficient of the first element, a0+a1\r\n2\r\nas the coefficient of\r\nthe second element and a2 as the coefficient the third element of B1.\r\n(b) B2, has a0+a1\r\n2\r\nas the coefficient of the first element, a0−a1\r\n2\r\nas the coefficient of\r\nthe second element and a2 as the coefficient the third element of B2.\r\n2. Let V = {(x, y, z) : x+y = z} and let B = {(−1, 1, 0),(1, 0, 1)} be a basis of V . Then\r\ncheck that (3, 4, 7) = 4(−1, 1, 0) + 7(1, 0, 1) ∈ V.\r\nThat is, as ordered bases (u1, u2, . . . , un), (u2, u3, . . . , un, u1) and (un, un−1, . . . , u2, u1)\r\nare different even though they have the same set of vectors as elements. To proceed further,\r\nwe now define the notion of coordinates of a vector depending on the chosen ordered basis.\r\nDefinition 3.4.3 (Coordinates of a Vector). Let B = (v1, v2, . . . , vn) be an ordered basis\r\nof a vector space V and let v ∈ V . Suppose\r\nv = β1v1 + β2v2 + · · · + βnvn for some scalars β1, β2, . . . , βn.\r\nThen the tuple (β1, β2, . . . , βn)\r\nt\r\nis called the coordinate of the vector v with respect to the\r\nordered basis B and is denoted by [v]B = (β1, . . . , βn)\r\nt\r\n, a column vector.\r\nExample 3.4.4. 1. In Example 3.4.2.1, let p(x) = a0 + a1x + a2x\r\n2\r\n. Then\r\n[p(x)]B1 =\r\n\r\n\r\n\r\na0−a1\r\n2\r\na0+a1\r\n2\r\na2\r\n\r\n\r\n , [p(x)]B2 =\r\n\r\n\r\n\r\na0+a1\r\n2\r\na0−a1\r\n2\r\na2\r\n\r\n\r\n\r\nand [p(x)]B3 =\r\n\r\n\r\n\r\na2\r\na0−a1\r\n2\r\na0+a1\r\n2\r\n\r\n\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b72bb7b1-3957-4459-9c8c-2723306d02c2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6529bf2e3bc3a5524d006cc20ce9c7e04b48de4cc259baa0bc2771b8e6ad16e4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 577
      },
      {
        "segments": [
          {
            "segment_id": "6b5e073a-fed7-4681-a33d-1a8ba7c02063",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 91,
            "page_width": 612,
            "page_height": 792,
            "content": "3.4. ORDERED BASES 91\r\n2. In Example 3.4.2.2, \u0002(3, 4, 7)\u0003\r\nB\r\n=\r\n\"\r\n4\r\n7\r\n#\r\nand \u0002(x, y, z)\r\n\u0003\r\nB\r\n=\r\n\u0002\r\n(z − y, y, z)\r\n\u0003\r\nB\r\n=\r\n\"\r\ny\r\nz\r\n#\r\n.\r\n3. Let the ordered bases of R\r\n3\r\nbe B1 =\r\n\r\n(1, 0, 0),(0, 1, 0),(0, 0, 1)\u0001, B2 =\r\n\r\n(1, 0, 0),(1, 1, 0),(1, 1, 1)\u0001\r\nand B3 =\r\n\r\n(1, 1, 1),(1, 1, 0),(1, 0, 0)\u0001. Then\r\n(1, −1, 1) = 1 · (1, 0, 0) + (−1) · (0, 1, 0) + 1 · (0, 0, 1).\r\n= 2 · (1, 0, 0) + (−2) · (1, 1, 0) + 1 · (1, 1, 1).\r\n= 1 · (1, 1, 1) + (−2) · (1, 1, 0) + 2 · (1, 0, 0).\r\nTherefore, if we write u = (1, −1, 1), then\r\n[u]B1 = (1, −1, 1)t, [u]B2 = (2, −2, 1)t, [u]B3 = (1, −2, 2)t.\r\nIn general, let V be an n-dimensional vector space with B1 = (u1, u2, . . . , un) and\r\nB2 = (v1, v2, . . . , vn). Since B1 is a basis of V, there exist unique scalars aij , 1 ≤ i, j ≤ n,\r\nsuch that\r\nvi =\r\nXn\r\nl=1\r\naliul, or equivalently, [vi]B1 = (a1i, a2i, . . . , ani)\r\nt\r\nfor 1 ≤ i ≤ n.\r\nSuppose v ∈ V with [v]B2 = (α1, α2, . . . , αn)\r\nt\r\n. Then\r\nv =\r\nXn\r\ni=1\r\nαivi =\r\nXn\r\ni=1\r\nαi\r\n\r\n\r\nXn\r\nj=1\r\najiuj\r\n\r\n =\r\nXn\r\nj=1 Xni=1\r\najiαi\r\n!\r\nuj .\r\nSince B1 is a basis this representation of v in terms of ui\r\n’s is unique. So,\r\n[v]B1 =\r\n Xn\r\ni=1\r\na1iαi,\r\nXn\r\ni=1\r\na2iαi, . . . ,Xn\r\ni=1\r\naniαi\r\n!t\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 · · · a1n\r\na21 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1\r\nα2\r\n.\r\n.\r\n.\r\nαn\r\n\r\n\r\n\r\n\r\n\r\n\r\n= A[v]B2,\r\nwhere A =\r\n\u0014\r\n[v1]B1\r\n, [v2]B1, . . . , [vn]B1\r\n\u0015\r\n. Hence, we have proved the following theorem.\r\nTheorem 3.4.5. Let V be an n-dimensional vector space with bases B1 = (u1, u2, . . . , un)\r\nand B2 = (v1, v2, . . . , vn). Define an n × n matrix A by A =\r\n\u0014\r\n[v1]B1, [v2]B1, . . . , [vn]B1\r\n\u0015\r\n.\r\nThen, A is an invertible matrix (see Exercise 3.3.14.7) and\r\n[v]B1 = A[v]B2\r\nfor all v ∈ V.\r\nTheorem 3.4.5 states that the coordinates of a vector with respect to different bases\r\nare related via an invertible matrix A.\r\nExample 3.4.6. Let B1 =\r\n\r\n(1, 0, 0),(1, 1, 0),(1, 1, 1)\u0001and B2 =\r\n\r\n(1, 1, 1),(1, −1, 1),(1, 1, 0)\u0001\r\nbe two bases of R\r\n3\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6b5e073a-fed7-4681-a33d-1a8ba7c02063.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7200dd29ec4babd8de7a46c53c91f861ac3db93b159f3c00316a8d78cf666e29",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 499
      },
      {
        "segments": [
          {
            "segment_id": "1887f451-13a6-4747-b7e0-7e25dbb73775",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 92,
            "page_width": 612,
            "page_height": 792,
            "content": "92 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n1. Then [(x, y, z)]B1 = (x − y, y − z, z)\r\nt and [(x, y, z)]B2 = ( y−x\r\n2 + z,\r\nx−y\r\n2\r\n, x − z)\r\nt\r\n.\r\n2. Check that A =\r\n\u0014\r\n[(1, 1, 1)]B1, [(1, −1, 1)]B1, [(1, 1, 0)]B1\r\n\u0015\r\n=\r\n\r\n\r\n\r\n0 2 0\r\n0 −2 1\r\n1 1 0\r\n\r\n\r\n\r\nas\r\n[(1, 1, 1)]B1= 0 · (1, 0, 0) + 0 · (1, 1, 0) + 1 · (1, 1, 1) = (0, 0, 1)t,\r\n[(1, −1, 1)]B1= 2 · (1, 0, 0) + (−2) · (1, 1, 0) + 1 · (1, 1, 1) = (2, −2, 1)tand\r\n[(1, 1, 0)]B1\r\n= 0 · (1, 0, 0) + 1 · (1, 1, 0) + 0 · (1, 1, 1) = (0, 1, 0)t.\r\n3. Thus, for any (x, y, z) ∈ R\r\n3\r\n,\r\n[(x, y, z)]B1 =\r\n\r\n\r\n\r\nx − y\r\ny − z\r\nz\r\n\r\n\r\n =\r\n\r\n\r\n\r\n0 2 0\r\n0 −2 1\r\n1 1 0\r\n\r\n\r\n\r\n\r\n\r\n\r\ny−x\r\n2 + z\r\nx−y\r\n2\r\nx − z\r\n\r\n\r\n = A [(x, y, z)]B2\r\n.\r\n4. Observe that the matrix A is invertible and hence [(x, y, z)]B2 = A−1\r\n[(x, y, z)]B1.\r\nIn the next chapter, we try to understand Theorem 3.4.5 again using the ideas of ‘linear\r\ntransformations/functions’.\r\nExercise 3.4.7. 1. Consider the vector space P3(R).\r\n(a) Prove that B1 = (1−x, 1+x\r\n2\r\n, 1−x\r\n3\r\n, 3+x\r\n2−x3\r\n) and B2 = (1, 1−x, 1+x\r\n2\r\n, 1−x\r\n3\r\n)\r\nare bases of P3(R).\r\n(b) Find the coordinates of u = 1 + x + x\r\n2 + x3 with respect to B1 and B2.\r\n(c) Find the matrix A such that [u]B2 = A[u]B1.\r\n(d) Let v = a0 + a1x + a2x\r\n2 + a3x3\r\n. Then verify that\r\n[v]B1 =\r\n\r\n\r\n\r\n\r\n\r\n−a1\r\n−a0 − a1 + 2a2 − a3\r\n−a0 − a1 + a2 − 2a3\r\na0 + a1 − a2 + a3\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n0 1 0 0\r\n−1 0 1 0\r\n−1 0 0 1\r\n1 0 0 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\na0 + a1 − a2 + a3\r\n−a1\r\na2\r\n−a3\r\n\r\n\r\n\r\n\r\n\r\n= [v]B2\r\n.\r\n2. Let B =\r\n\r\n(2, 1, 0),(2, 1, 1),(2, 2, 1)\u0001be an ordered basis of R\r\n3\r\n. Determine the coordi\u0002nates of (1, 2, 1) and (4, −2, 2) with respect B.\r\n3.5 Summary\r\nIn this chapter, we started with the definition of vector spaces over F, the set of scalars.\r\nThe set F was either R, the set of real numbers or C, the set of complex numbers.\r\nIt was important to note that given a non-empty set V of vectors with a set F of scalars,\r\nwe need to do the following:\r\n1. first define vector addition and scalar multiplication and",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1887f451-13a6-4747-b7e0-7e25dbb73775.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=592a1eeed90b6f49902746731876ea8c7b32b1f576ac3a42da9c9972e90c6d7b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 517
      },
      {
        "segments": [
          {
            "segment_id": "1887f451-13a6-4747-b7e0-7e25dbb73775",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 92,
            "page_width": 612,
            "page_height": 792,
            "content": "92 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n1. Then [(x, y, z)]B1 = (x − y, y − z, z)\r\nt and [(x, y, z)]B2 = ( y−x\r\n2 + z,\r\nx−y\r\n2\r\n, x − z)\r\nt\r\n.\r\n2. Check that A =\r\n\u0014\r\n[(1, 1, 1)]B1, [(1, −1, 1)]B1, [(1, 1, 0)]B1\r\n\u0015\r\n=\r\n\r\n\r\n\r\n0 2 0\r\n0 −2 1\r\n1 1 0\r\n\r\n\r\n\r\nas\r\n[(1, 1, 1)]B1= 0 · (1, 0, 0) + 0 · (1, 1, 0) + 1 · (1, 1, 1) = (0, 0, 1)t,\r\n[(1, −1, 1)]B1= 2 · (1, 0, 0) + (−2) · (1, 1, 0) + 1 · (1, 1, 1) = (2, −2, 1)tand\r\n[(1, 1, 0)]B1\r\n= 0 · (1, 0, 0) + 1 · (1, 1, 0) + 0 · (1, 1, 1) = (0, 1, 0)t.\r\n3. Thus, for any (x, y, z) ∈ R\r\n3\r\n,\r\n[(x, y, z)]B1 =\r\n\r\n\r\n\r\nx − y\r\ny − z\r\nz\r\n\r\n\r\n =\r\n\r\n\r\n\r\n0 2 0\r\n0 −2 1\r\n1 1 0\r\n\r\n\r\n\r\n\r\n\r\n\r\ny−x\r\n2 + z\r\nx−y\r\n2\r\nx − z\r\n\r\n\r\n = A [(x, y, z)]B2\r\n.\r\n4. Observe that the matrix A is invertible and hence [(x, y, z)]B2 = A−1\r\n[(x, y, z)]B1.\r\nIn the next chapter, we try to understand Theorem 3.4.5 again using the ideas of ‘linear\r\ntransformations/functions’.\r\nExercise 3.4.7. 1. Consider the vector space P3(R).\r\n(a) Prove that B1 = (1−x, 1+x\r\n2\r\n, 1−x\r\n3\r\n, 3+x\r\n2−x3\r\n) and B2 = (1, 1−x, 1+x\r\n2\r\n, 1−x\r\n3\r\n)\r\nare bases of P3(R).\r\n(b) Find the coordinates of u = 1 + x + x\r\n2 + x3 with respect to B1 and B2.\r\n(c) Find the matrix A such that [u]B2 = A[u]B1.\r\n(d) Let v = a0 + a1x + a2x\r\n2 + a3x3\r\n. Then verify that\r\n[v]B1 =\r\n\r\n\r\n\r\n\r\n\r\n−a1\r\n−a0 − a1 + 2a2 − a3\r\n−a0 − a1 + a2 − 2a3\r\na0 + a1 − a2 + a3\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n0 1 0 0\r\n−1 0 1 0\r\n−1 0 0 1\r\n1 0 0 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\na0 + a1 − a2 + a3\r\n−a1\r\na2\r\n−a3\r\n\r\n\r\n\r\n\r\n\r\n= [v]B2\r\n.\r\n2. Let B =\r\n\r\n(2, 1, 0),(2, 1, 1),(2, 2, 1)\u0001be an ordered basis of R\r\n3\r\n. Determine the coordi\u0002nates of (1, 2, 1) and (4, −2, 2) with respect B.\r\n3.5 Summary\r\nIn this chapter, we started with the definition of vector spaces over F, the set of scalars.\r\nThe set F was either R, the set of real numbers or C, the set of complex numbers.\r\nIt was important to note that given a non-empty set V of vectors with a set F of scalars,\r\nwe need to do the following:\r\n1. first define vector addition and scalar multiplication and",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1887f451-13a6-4747-b7e0-7e25dbb73775.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=592a1eeed90b6f49902746731876ea8c7b32b1f576ac3a42da9c9972e90c6d7b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 517
      },
      {
        "segments": [
          {
            "segment_id": "4117d0e7-5057-4bb2-a2d0-4d6f14e3c045",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 93,
            "page_width": 612,
            "page_height": 792,
            "content": "3.5. SUMMARY 93\r\n2. then verify the axioms in Definition 3.1.1.\r\nIf all the axioms are satisfied then V is a vector space over F. To check whether a non\u0002empty subset W of a vector space V over F is a subspace of V , we only need to check\r\nwhether u + v ∈ W for all u, v ∈ W and αu ∈ W for all α ∈ F and u ∈ W.\r\nWe then came across the definition of linear combination of vectors and the linear span\r\nof vectors. It was also shown that the linear span of a subset S of a vector space V is the\r\nsmallest subspace of V containing S. Also, to check whether a given vector v is a linear\r\ncombination of the vectors u1, u2, . . . , un, we need to solve the linear system\r\nc1u1 + c2u2 + · · · + cnun = v\r\nin the unknowns c1, . . . , cn. This corresponds to solving the linear system Ax = b. It was\r\nalso shown that the geometrical representation of the linear span of S = {u1, u2, . . . , un}\r\nis equivalent to finding conditions on the coordinates of the vector b such that the linear\r\nsystem Ax = b is consistent, where the matrix A is formed with the coordinates of the\r\nvector ui as the i-th column of the matrix A.\r\nBy definition, S = {u1, u2, . . . , un} is linearly independent subset in V (F) if the ho\u0002mogeneous system Ax = 0 has only the trivial solution in F, else S is linearly dependent,\r\nwhere the matrix A is formed with the coordinates of the vector ui as the i-th column of\r\nthe matrix A.\r\nWe then had the notion of the basis of a finite dimensional vector space V and the\r\nfollowing results were proved.\r\n1. A linearly independent set can be extended to form a basis of V .\r\n2. Any two bases of V have the same number of elements.\r\nThis number was defined as the dimension of V and we denoted it by dim(V ).\r\nThe following conditions are equivalent for an n × n matrix A.\r\n1. A is invertible.\r\n2. The homogeneous system Ax = 0 has only the trivial solution.\r\n3. The row reduced echelon form of A is I.\r\n4. A is a product of elementary matrices.\r\n5. The system Ax = b has a unique solution for every b.\r\n6. The system Ax = b has a solution for every b.\r\n7. rank(A) = n.\r\n8. det(A) 6= 0.\r\n9. The row space of A is R\r\nn\r\n.\r\n10. The column space of A is R\r\nn\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/4117d0e7-5057-4bb2-a2d0-4d6f14e3c045.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=428c55d6029ae90c9241a77f72746ec5d25bf4bc4821bd5b62c921623a31f4a0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 462
      },
      {
        "segments": [
          {
            "segment_id": "deaca757-2538-4a79-adba-4c4475250316",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 94,
            "page_width": 612,
            "page_height": 792,
            "content": "94 CHAPTER 3. FINITE DIMENSIONAL VECTOR SPACES\r\n11. The rows of A form a basis of R\r\nn\r\n.\r\n12. The columns of A form a basis of R\r\nn\r\n.\r\n13. The null space of A is {0}.\r\nLet A be an m × n matrix. Then we proved the rank-nullity theorem which states that\r\nrank(A) + nullity(A) = n, the number of columns. This implied that if rank(A) = r then\r\nthe solution set of the linear system Ax = b is of the form x0 + c1u1 + · · · + cn−run−r,\r\nwhere Ax0 = b and Aui = 0 for 1 ≤ i ≤ n − r. Also, the vectors u1, u2, . . . , un−r are\r\nlinearly independent.\r\nLet V be a vector space of R\r\nn\r\nfor some positive integer n with dim(V ) = k. Then V\r\nmay not have a standard basis. Even if V may have a basis that looks like an standard\r\nbasis, our problem may force us to look for some other basis. In such a case, it is always\r\nhelpful to fix an ordered basis B and then express each vector in V as a linear combination\r\nof elements from B. This idea helps us in writing each element of V as a column vector\r\nof size k. We will also see its use in the study of linear transformations and the study of\r\neigenvalues and eigenvectors.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/deaca757-2538-4a79-adba-4c4475250316.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=417df889a5e4bbca589755bb96bb6b39febec01767db50faab9cd9d0dcae9ba3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 241
      },
      {
        "segments": [
          {
            "segment_id": "5f3a6956-cce6-4034-ac4b-c6f4af286e10",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 95,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 4\r\nLinear Transformations\r\n4.1 Definitions and Basic Properties\r\nIn this chapter, it will be shown that if V is a real vector space with dim(V ) = n then V\r\nlooks like R\r\nn\r\n. On similar lines a complex vector space of dimension n has all the properties\r\nthat are satisfied by C\r\nn\r\n. To do so, we start with the definition of functions over vector\r\nspaces that commute with the operations of vector addition and scalar multiplication.\r\nDefinition 4.1.1 (Linear Transformation, Linear Operator). Let V and W be vector spaces\r\nover the same scalar set F. A function (map) T : V −→W is called a linear transformation\r\nif for all α ∈ F and u, v ∈ V the function T satisfies\r\nT(α · u) = α ⊙ T(u) and T(u + v) = T(u) ⊕ T(v),\r\nwhere +, · are binary operations in V and ⊕, ⊙ are the binary operations in W. In partic\u0002ular, if W = V then the linear transformation T is called a linear operator.\r\nWe now give a few examples of linear transformations.\r\nExample 4.1.2. 1. Define T : R−→R\r\n2\r\nby T(x) = (x, 3x) for all x ∈ R. Then T is a\r\nlinear transformation as\r\nT(αx) = (αx, 3αx) = α(x, 3x) = αT(x) and\r\nT(x + y) = (x + y, 3(x + y) = (x, 3x) + (y, 3y) = T(x) + T(y).\r\n2. Let V, W and Z be vector spaces over F. Also, let T : V −→W and S : W−→Z be\r\nlinear transformations. Then, for each v ∈ V , the composition of T and S is defined\r\nby S ◦ T(v) = S\r\n\r\nT(v)\r\n\u0001\r\n. It is easy to verify that S ◦ T is a linear transformation. In\r\nparticular, if V = W, one writes T\r\n2\r\nin place of T ◦ T.\r\n3. Let x\r\nt = (x1, x2, . . . , xn) ∈ Rn\r\n. Then for a fixed vector a\r\nt = (a1, a2, . . . , an) ∈ Rn\r\n,\r\ndefine T : R\r\nn −→ R by T(xt\r\n) = Pn\r\ni=1\r\naixifor all x\r\nt ∈ Rn\r\n. Then T is a linear\r\ntransformation. In particular,\r\n95",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5f3a6956-cce6-4034-ac4b-c6f4af286e10.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ae754277b9bbb39353010808156363916b693589c836c2d482d477039b0aa403",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 378
      },
      {
        "segments": [
          {
            "segment_id": "0c887cda-d73e-41c6-9905-d9936b4949dc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 96,
            "page_width": 612,
            "page_height": 792,
            "content": "96 CHAPTER 4. LINEAR TRANSFORMATIONS\r\n(a) T(x\r\nt\r\n) = Pn\r\ni=1\r\nxifor all x\r\nt ∈ Rn\r\nif ai = 1, for 1 ≤ i ≤ n.\r\n(b) if a = eifor a fixed i, 1 ≤ i ≤ n, one can define Ti(x\r\nt\r\n) = xifor all x\r\nt ∈ Rn\r\n.\r\n4. Define T : R\r\n2−→R3\r\nby T(x, y) = (x + y, 2x − y, x + 3y). Then T is a linear trans\u0002formation with T(1, 0) = (1, 2, 1) and T(0, 1) = (1, −1, 3).\r\n5. Let A ∈ Mm×n(C). Define a map TA : C\r\nn−→Cm by TA(xt\r\n) = Ax for every x\r\nt =\r\n(x1, x2, . . . , xn) ∈ Cn. Then TA is a linear transformation. That is, every m × n\r\ncomplex matrix defines a linear transformation from C\r\nn\r\nto C\r\nm.\r\n6. Define T : R\r\nn+1−→Pn(R) by T(a1, a2, . . . , an+1) = a1 + a2x + · · · + an+1xn\r\nfor\r\n(a1, a2, . . . , an+1) ∈ R\r\nn+1\r\n. Then T is a linear transformation.\r\n7. Fix A ∈ Mn(C). Then TA : Mn(C)−→Mn(C) and SA : Mn(C)−→C are both linear\r\ntransformations, where\r\nTA(B) = BA∗and SA(B) = tr(BA∗) for every B ∈ Mn(C).\r\nBefore proceeding further with some more definitions and results associated with linear\r\ntransformations, we prove that any linear transformation sends the zero vector to a zero\r\nvector.\r\nProposition 4.1.3. Let T : V −→W be a linear transformation. Suppose that 0V is the\r\nzero vector in V and 0W is the zero vector of W. Then T(0V ) = 0W .\r\nProof. Since 0V = 0V + 0V , we have\r\nT(0V ) = T(0V + 0V ) = T(0V ) + T(0V ).\r\nSo T(0V ) = 0W as T(0V ) ∈ W.\r\nFrom now on, we write 0 for both the zero vector of the domain and codomain.\r\nDefinition 4.1.4 (Zero Transformation). Let V and W be two vector spaces over F and\r\ndefine T : V −→W by T(v) = 0 for every v ∈ V. Then T is a linear transformation and is\r\nusually called the zero transformation, denoted 0.\r\nDefinition 4.1.5 (Identity Operator). Let V be a vector space over F and define T :\r\nV −→V by T(v) = v for every v ∈ V. Then T is a linear transformation and is usually\r\ncalled the Identity transformation, denoted I.\r\nDefinition 4.1.6 (Equality of two Linear Operators). Let V be a vector space and let\r\nT, S : V −→V be a linear operators. The operators T and S are said to be equal if T(x) =\r\nS(x) for all x ∈ V .\r\nWe now prove a result that relates a linear transformation T with its value on a basis\r\nof the domain space.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0c887cda-d73e-41c6-9905-d9936b4949dc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e59fab2c10f8eb865344733e35f18f8d44ff2c5b18c99c9f98009fd6fb590264",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 481
      },
      {
        "segments": [
          {
            "segment_id": "959c09ef-e881-4da0-9837-720b1695a899",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 97,
            "page_width": 612,
            "page_height": 792,
            "content": "4.1. DEFINITIONS AND BASIC PROPERTIES 97\r\nTheorem 4.1.7. Let V and W be two vector spaces over F and let T : V −→W be a\r\nlinear transformation. If B =\r\n\r\nu1, . . . , un\r\n\u0001\r\nis an ordered basis of V then for each v ∈ V ,\r\nthe vector T(v) is a linear combination of T(u1), . . . , T(un) ∈ W. That is, we have full\r\ninformation of T if we know T(u1), . . . , T(un) ∈ W, the image of basis vectors in W.\r\nProof. As B is a basis of V, for every v ∈ V, we can find c1, . . . , cn ∈ F such that v =\r\nc1u1 + · · · + cnun, or equivalently [v]B = (α1, . . . , αn)\r\nt\r\n. Hence, by definition\r\nT(v) = T(c1u1 + · · · + cnun) = c1T(u1) + · · · + cnT(un).\r\nThat is, we just need to know the vectors T(u1), T(u2), . . . , T(un) in W to get T(v) as\r\n[v]B = (α1, . . . , αn)\r\nt\r\nis known in V . Hence, the required result follows.\r\nExercise 4.1.8. 1. Are the maps T : V −→W given below, linear transformations?\r\n(a) Let V = R\r\n2 and W = R3 with T(x, y) = (x + y + 1, 2x − y, x + 3y).\r\n(b) Let V = W = R\r\n2 with T(x, y) = (x − y, x2 − y2\r\n).\r\n(c) Let V = W = R\r\n2 with T(x, y) = (x − y, |x|).\r\n(d) Let V = R\r\n2 and W = R4 with T(x, y) = (x + y, x − y, 2x + y, 3x − 4y).\r\n(e) Let V = W = R\r\n4 with T(x, y, z, w) = (z, x, w, y).\r\n2. Which of the following maps T : M2(R)−→M2(R) are linear operators?\r\n(a) T(A) = At(b) T(A) = I + A (c) T(A) = A2\r\n(d) T(A) = BAB−1, where B is a fixed 2 × 2 matrix.\r\n3. Prove that a map T : R −→ R is a linear transformation if and only if there exists a\r\nunique c ∈ R such that T(x) = cx for every x ∈ R.\r\n4. Let A ∈ Mn(C) and define TA : C\r\nn−→Cn\r\nby TA(x\r\nt\r\n) = Ax for every x\r\nt ∈ Cn\r\n. Prove\r\nthat for any positive integer k, T\r\nk\r\nA\r\n(x\r\nt\r\n) = Akx.\r\n5. Use matrices to give examples of linear operators T, S : R\r\n3−→R3\r\nthat satisfy:\r\n(a) T 6= 0, T2 6= 0, T3 = 0.\r\n(b) T 6= 0, S 6= 0, S ◦ T 6= 0, T ◦ S = 0.\r\n(c) S\r\n2 = T2\r\n, S 6= T.\r\n(d) T\r\n2 = I, T 6= I.\r\n6. Let T : R\r\nn −→ Rn\r\nbe a linear operator with T 6= 0 and T\r\n2 = 0. Prove that there\r\nexists a vector x ∈ R\r\nn\r\nsuch that the set {x, T(x)} is linearly independent.\r\n7. Fix a positive integer p and let T : R\r\nn −→ Rn\r\nbe a linear operator with T\r\nk 6= 0 for\r\n1 ≤ k ≤ p and T\r\np+1 = 0. Then prove that there exists a vector x ∈ Rn\r\nsuch that the\r\nset {x, T(x), . . . , Tp(x)} is linearly independent.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/959c09ef-e881-4da0-9837-720b1695a899.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=571590d66d79fa4ddb267f7c3e6943159aef44fe537372d1bb1b788b6c742973",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 589
      },
      {
        "segments": [
          {
            "segment_id": "959c09ef-e881-4da0-9837-720b1695a899",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 97,
            "page_width": 612,
            "page_height": 792,
            "content": "4.1. DEFINITIONS AND BASIC PROPERTIES 97\r\nTheorem 4.1.7. Let V and W be two vector spaces over F and let T : V −→W be a\r\nlinear transformation. If B =\r\n\r\nu1, . . . , un\r\n\u0001\r\nis an ordered basis of V then for each v ∈ V ,\r\nthe vector T(v) is a linear combination of T(u1), . . . , T(un) ∈ W. That is, we have full\r\ninformation of T if we know T(u1), . . . , T(un) ∈ W, the image of basis vectors in W.\r\nProof. As B is a basis of V, for every v ∈ V, we can find c1, . . . , cn ∈ F such that v =\r\nc1u1 + · · · + cnun, or equivalently [v]B = (α1, . . . , αn)\r\nt\r\n. Hence, by definition\r\nT(v) = T(c1u1 + · · · + cnun) = c1T(u1) + · · · + cnT(un).\r\nThat is, we just need to know the vectors T(u1), T(u2), . . . , T(un) in W to get T(v) as\r\n[v]B = (α1, . . . , αn)\r\nt\r\nis known in V . Hence, the required result follows.\r\nExercise 4.1.8. 1. Are the maps T : V −→W given below, linear transformations?\r\n(a) Let V = R\r\n2 and W = R3 with T(x, y) = (x + y + 1, 2x − y, x + 3y).\r\n(b) Let V = W = R\r\n2 with T(x, y) = (x − y, x2 − y2\r\n).\r\n(c) Let V = W = R\r\n2 with T(x, y) = (x − y, |x|).\r\n(d) Let V = R\r\n2 and W = R4 with T(x, y) = (x + y, x − y, 2x + y, 3x − 4y).\r\n(e) Let V = W = R\r\n4 with T(x, y, z, w) = (z, x, w, y).\r\n2. Which of the following maps T : M2(R)−→M2(R) are linear operators?\r\n(a) T(A) = At(b) T(A) = I + A (c) T(A) = A2\r\n(d) T(A) = BAB−1, where B is a fixed 2 × 2 matrix.\r\n3. Prove that a map T : R −→ R is a linear transformation if and only if there exists a\r\nunique c ∈ R such that T(x) = cx for every x ∈ R.\r\n4. Let A ∈ Mn(C) and define TA : C\r\nn−→Cn\r\nby TA(x\r\nt\r\n) = Ax for every x\r\nt ∈ Cn\r\n. Prove\r\nthat for any positive integer k, T\r\nk\r\nA\r\n(x\r\nt\r\n) = Akx.\r\n5. Use matrices to give examples of linear operators T, S : R\r\n3−→R3\r\nthat satisfy:\r\n(a) T 6= 0, T2 6= 0, T3 = 0.\r\n(b) T 6= 0, S 6= 0, S ◦ T 6= 0, T ◦ S = 0.\r\n(c) S\r\n2 = T2\r\n, S 6= T.\r\n(d) T\r\n2 = I, T 6= I.\r\n6. Let T : R\r\nn −→ Rn\r\nbe a linear operator with T 6= 0 and T\r\n2 = 0. Prove that there\r\nexists a vector x ∈ R\r\nn\r\nsuch that the set {x, T(x)} is linearly independent.\r\n7. Fix a positive integer p and let T : R\r\nn −→ Rn\r\nbe a linear operator with T\r\nk 6= 0 for\r\n1 ≤ k ≤ p and T\r\np+1 = 0. Then prove that there exists a vector x ∈ Rn\r\nsuch that the\r\nset {x, T(x), . . . , Tp(x)} is linearly independent.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/959c09ef-e881-4da0-9837-720b1695a899.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=571590d66d79fa4ddb267f7c3e6943159aef44fe537372d1bb1b788b6c742973",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 589
      },
      {
        "segments": [
          {
            "segment_id": "1bd71322-919f-46be-a9da-be200311ca68",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 98,
            "page_width": 612,
            "page_height": 792,
            "content": "98 CHAPTER 4. LINEAR TRANSFORMATIONS\r\n8. Let T : R\r\nn −→ Rm be a linear transformation with T(x0) = y0 for some x0 ∈ Rn\r\nand y0 ∈ R\r\nm. Define T−1\r\n(y0) = {x ∈ R\r\nn\r\n: T(x) = y0}. Then prove that for every\r\nx ∈ T\r\n−1\r\n(y0) there exists z ∈ T\r\n−1\r\n(0) such that x = x0 + z. Also, prove that T\r\n−1\r\n(y0)\r\nis a subspace of R\r\nn\r\nif and only if 0 ∈ T\r\n−1\r\n(y0).\r\n9. Define a map T : C −→ C by T(z) = z, the complex conjugate of z. Is T a linear\r\ntransformation over C(R)?\r\n10. Prove that there exists infinitely many linear transformations T : R\r\n3 −→ R2\r\nsuch\r\nthat T(1, −1, 1) = (1, 2) and T(−1, 1, 2) = (1, 0)?\r\n11. Does there exist a linear transformation T : R\r\n3 −→ R2\r\nsuch that T(1, 0, 1) = (1, 2),\r\nT(0, 1, 1) = (1, 0) and T(1, 1, 1) = (2, 3)?\r\n12. Does there exist a linear transformation T : R\r\n3 −→ R2\r\nsuch that T(1, 0, 1) = (1, 2),\r\nT(0, 1, 1) = (1, 0) and T(1, 1, 2) = (2, 3)?\r\n13. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x+ 3y + 4z, x+ y + z, x+ y + 3z). Find\r\nthe value of k for which there exists a vector x\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = (9, 3, k).\r\n14. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x−2y + 2z, −2x+ 5y + 2z, 8x+y + 4z).\r\nFind a vector x\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = (1, 1, −1).\r\n15. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x + y + 3z, 4x − y + 3z, 3x − 2y + 5z).\r\nDetermine non-zero vectors x\r\nt\r\n, y\r\nt\r\n, z\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = 6x, T(y\r\nt\r\n) = 2y and\r\nT(z\r\nt\r\n) = −2z. Is the set {x, y, z} linearly independent?\r\n16. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x + 3y + 4z, −y, −3y + 4z). Determine\r\nnon-zero vectors x\r\nt\r\n, y\r\nt\r\n, z\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = 2x, T(y\r\nt\r\n) = 4y and T(z\r\nt\r\n) = −z.\r\nIs the set {x, y, z} linearly independent?\r\n17. Let n be any positive integer. Prove that there does not exist a linear transformation\r\nT : R\r\n3 −→ Rn\r\nsuch that T(1, 1, −2) = x\r\nt\r\n, T(−1, 2, 3) = y\r\nt and T(1, 10, 1) = zt where\r\nz = x + y. Does there exist real numbers c, d such that z = cx + dy and T is indeed\r\na linear transformation?\r\n18. Find all functions f : R\r\n2 −→ R2\r\nthat fixes the line y = x and sends (x1, y1) for\r\nx1 6= y1 to its mirror image along the line y = x. Or equivalently, f satisfies\r\n(a) f(x, x) = (x, x) and\r\n(b) f(x, y) = (y, x) for all (x, y) ∈ R\r\n2\r\n.\r\n19. Consider the complex vector space C\r\n3 and let f : C3−→C3\r\nbe a linear transformation.\r\nSuppose there exist non-zero vectors x, y, z ∈ C\r\n3\r\nsuch that f(x) = x, f(y) = (1 + i)y\r\nand f(z) = (2 + 3i)z. Then prove that\r\n(a) the vectors {x, y, z} are linearly independent subset of C\r\n3\r\n.\r\n(b) the set {x, y, z} form a basis of C\r\n3\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1bd71322-919f-46be-a9da-be200311ca68.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=990611bb2808b1f891228ee706a989da90331b912e632a99196d7473855e1893",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 639
      },
      {
        "segments": [
          {
            "segment_id": "1bd71322-919f-46be-a9da-be200311ca68",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 98,
            "page_width": 612,
            "page_height": 792,
            "content": "98 CHAPTER 4. LINEAR TRANSFORMATIONS\r\n8. Let T : R\r\nn −→ Rm be a linear transformation with T(x0) = y0 for some x0 ∈ Rn\r\nand y0 ∈ R\r\nm. Define T−1\r\n(y0) = {x ∈ R\r\nn\r\n: T(x) = y0}. Then prove that for every\r\nx ∈ T\r\n−1\r\n(y0) there exists z ∈ T\r\n−1\r\n(0) such that x = x0 + z. Also, prove that T\r\n−1\r\n(y0)\r\nis a subspace of R\r\nn\r\nif and only if 0 ∈ T\r\n−1\r\n(y0).\r\n9. Define a map T : C −→ C by T(z) = z, the complex conjugate of z. Is T a linear\r\ntransformation over C(R)?\r\n10. Prove that there exists infinitely many linear transformations T : R\r\n3 −→ R2\r\nsuch\r\nthat T(1, −1, 1) = (1, 2) and T(−1, 1, 2) = (1, 0)?\r\n11. Does there exist a linear transformation T : R\r\n3 −→ R2\r\nsuch that T(1, 0, 1) = (1, 2),\r\nT(0, 1, 1) = (1, 0) and T(1, 1, 1) = (2, 3)?\r\n12. Does there exist a linear transformation T : R\r\n3 −→ R2\r\nsuch that T(1, 0, 1) = (1, 2),\r\nT(0, 1, 1) = (1, 0) and T(1, 1, 2) = (2, 3)?\r\n13. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x+ 3y + 4z, x+ y + z, x+ y + 3z). Find\r\nthe value of k for which there exists a vector x\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = (9, 3, k).\r\n14. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x−2y + 2z, −2x+ 5y + 2z, 8x+y + 4z).\r\nFind a vector x\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = (1, 1, −1).\r\n15. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x + y + 3z, 4x − y + 3z, 3x − 2y + 5z).\r\nDetermine non-zero vectors x\r\nt\r\n, y\r\nt\r\n, z\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = 6x, T(y\r\nt\r\n) = 2y and\r\nT(z\r\nt\r\n) = −2z. Is the set {x, y, z} linearly independent?\r\n16. Let T : R\r\n3 −→ R3\r\nbe defined by T(x, y, z) = (2x + 3y + 4z, −y, −3y + 4z). Determine\r\nnon-zero vectors x\r\nt\r\n, y\r\nt\r\n, z\r\nt ∈ R3\r\nsuch that T(x\r\nt\r\n) = 2x, T(y\r\nt\r\n) = 4y and T(z\r\nt\r\n) = −z.\r\nIs the set {x, y, z} linearly independent?\r\n17. Let n be any positive integer. Prove that there does not exist a linear transformation\r\nT : R\r\n3 −→ Rn\r\nsuch that T(1, 1, −2) = x\r\nt\r\n, T(−1, 2, 3) = y\r\nt and T(1, 10, 1) = zt where\r\nz = x + y. Does there exist real numbers c, d such that z = cx + dy and T is indeed\r\na linear transformation?\r\n18. Find all functions f : R\r\n2 −→ R2\r\nthat fixes the line y = x and sends (x1, y1) for\r\nx1 6= y1 to its mirror image along the line y = x. Or equivalently, f satisfies\r\n(a) f(x, x) = (x, x) and\r\n(b) f(x, y) = (y, x) for all (x, y) ∈ R\r\n2\r\n.\r\n19. Consider the complex vector space C\r\n3 and let f : C3−→C3\r\nbe a linear transformation.\r\nSuppose there exist non-zero vectors x, y, z ∈ C\r\n3\r\nsuch that f(x) = x, f(y) = (1 + i)y\r\nand f(z) = (2 + 3i)z. Then prove that\r\n(a) the vectors {x, y, z} are linearly independent subset of C\r\n3\r\n.\r\n(b) the set {x, y, z} form a basis of C\r\n3\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1bd71322-919f-46be-a9da-be200311ca68.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=990611bb2808b1f891228ee706a989da90331b912e632a99196d7473855e1893",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 639
      },
      {
        "segments": [
          {
            "segment_id": "58d257eb-2963-4036-ba2f-b75a488ec6ba",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 99,
            "page_width": 612,
            "page_height": 792,
            "content": "4.2. MATRIX OF A LINEAR TRANSFORMATION 99\r\n4.2 Matrix of a linear transformation\r\nIn the previous section, we learnt the definition of a linear transformation. We also saw\r\nin Example 4.1.2.5 that for each A ∈ Mm×n(C), there exists a linear transformation TA :\r\nC\r\nn−→Cm given by TA(xt\r\n) = Ax for each x\r\nt ∈ Cn\r\n. In this section, we prove that every\r\nlinear transformation over finite dimensional vector spaces corresponds to a matrix. Before\r\nproceeding further, we advise the reader to recall the results on ordered basis, studied in\r\nSection 3.4.\r\nLet V and W be finite dimensional vector spaces over F with dimensions n and m,\r\nrespectively. Also, let B1 = (v1, . . . , vn) and B2 = (w1, . . . , wm) be ordered bases of V\r\nand W, respectively. If T : V −→W is a linear transformation then Theorem 4.1.7 implies\r\nthat T(v) ∈ W is a linear combination of the vectors T(v1), . . . , T(vn). So, let us find the\r\ncoordinate vectors [T(vj )]B2\r\nfor each j = 1, 2, . . . , n. Let us assume that\r\n[T(v1)]B2\r\n= (a11, . . . , am1)\r\nt\r\n, [T(v2)]B2= (a12, . . . , am2)\r\nt\r\n, . . . , [T(vn)]B2= (a1n, . . . , amn)\r\nt\r\n.\r\nOr equivalently,\r\nT(vj ) = a1jw1 + a2jw2 + · · · + amjwm =\r\nXm\r\ni=1\r\naijwifor j = 1, 2, . . . , n. (4.2.1)\r\nTherefore, for a fixed x ∈ V , if [x]B1 = (x1, x2, . . . , xn)\r\nt\r\nthen\r\nT(x) = T\r\n\r\n\r\nXn\r\nj=1\r\nxjvj\r\n\r\n =\r\nXn\r\nj=1\r\nxjT(vj ) = Xn\r\nj=1\r\nxj\r\n Xm\r\ni=1\r\naijwi\r\n!\r\n=\r\nXm\r\ni=1\r\n\r\n\r\nXn\r\nj=1\r\naijxj\r\n\r\n wi\r\n.\r\n(4.2.2)\r\nHence, using Equation (4.2.2), the coordinates of T(x) with respect to the basis B2 equals\r\n[T(x)]B2 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPn\r\nj=1\r\na1jxj\r\n.\r\n.\r\n.\r\nPn\r\nj=1\r\namjxj\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\n.\r\n.\r\n.\r\nxn\r\n\r\n\r\n\r\n\r\n\r\n\r\n= A [x]B1\r\n,\r\nwhere\r\nA =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\u0002\r\n[T(v1)]B2, [T(v2)]B2, . . . , [T(vn)]B2\r\n\u0003\r\n. (4.2.3)\r\nThe above observations lead to the following theorem and the subsequent definition.\r\nTheorem 4.2.1. Let V and W be finite dimensional vector spaces over F with dimensions\r\nn and m, respectively. Let T : V −→W be a linear transformation. Also, let B1 and B2 be\r\nordered bases of V and W, respectively. Then there exists a matrix A ∈ Mm×n(F), denoted\r\nA = T[B1,B2], with A =\r\n\u0002\r\n[T(v1)]B2, [T(v2)]B2, . . . , [T(vn)]B2\r\n\u0003\r\nsuch that\r\n[T(x)]B2 = A [x]B1\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/58d257eb-2963-4036-ba2f-b75a488ec6ba.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b3fc1228e7a178f2220845bdd263993ac4ebbd6c57b7564071e4a179980d0b40",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 555
      },
      {
        "segments": [
          {
            "segment_id": "58d257eb-2963-4036-ba2f-b75a488ec6ba",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 99,
            "page_width": 612,
            "page_height": 792,
            "content": "4.2. MATRIX OF A LINEAR TRANSFORMATION 99\r\n4.2 Matrix of a linear transformation\r\nIn the previous section, we learnt the definition of a linear transformation. We also saw\r\nin Example 4.1.2.5 that for each A ∈ Mm×n(C), there exists a linear transformation TA :\r\nC\r\nn−→Cm given by TA(xt\r\n) = Ax for each x\r\nt ∈ Cn\r\n. In this section, we prove that every\r\nlinear transformation over finite dimensional vector spaces corresponds to a matrix. Before\r\nproceeding further, we advise the reader to recall the results on ordered basis, studied in\r\nSection 3.4.\r\nLet V and W be finite dimensional vector spaces over F with dimensions n and m,\r\nrespectively. Also, let B1 = (v1, . . . , vn) and B2 = (w1, . . . , wm) be ordered bases of V\r\nand W, respectively. If T : V −→W is a linear transformation then Theorem 4.1.7 implies\r\nthat T(v) ∈ W is a linear combination of the vectors T(v1), . . . , T(vn). So, let us find the\r\ncoordinate vectors [T(vj )]B2\r\nfor each j = 1, 2, . . . , n. Let us assume that\r\n[T(v1)]B2\r\n= (a11, . . . , am1)\r\nt\r\n, [T(v2)]B2= (a12, . . . , am2)\r\nt\r\n, . . . , [T(vn)]B2= (a1n, . . . , amn)\r\nt\r\n.\r\nOr equivalently,\r\nT(vj ) = a1jw1 + a2jw2 + · · · + amjwm =\r\nXm\r\ni=1\r\naijwifor j = 1, 2, . . . , n. (4.2.1)\r\nTherefore, for a fixed x ∈ V , if [x]B1 = (x1, x2, . . . , xn)\r\nt\r\nthen\r\nT(x) = T\r\n\r\n\r\nXn\r\nj=1\r\nxjvj\r\n\r\n =\r\nXn\r\nj=1\r\nxjT(vj ) = Xn\r\nj=1\r\nxj\r\n Xm\r\ni=1\r\naijwi\r\n!\r\n=\r\nXm\r\ni=1\r\n\r\n\r\nXn\r\nj=1\r\naijxj\r\n\r\n wi\r\n.\r\n(4.2.2)\r\nHence, using Equation (4.2.2), the coordinates of T(x) with respect to the basis B2 equals\r\n[T(x)]B2 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPn\r\nj=1\r\na1jxj\r\n.\r\n.\r\n.\r\nPn\r\nj=1\r\namjxj\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nx1\r\nx2\r\n.\r\n.\r\n.\r\nxn\r\n\r\n\r\n\r\n\r\n\r\n\r\n= A [x]B1\r\n,\r\nwhere\r\nA =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nam1 am2 · · · amn\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\u0002\r\n[T(v1)]B2, [T(v2)]B2, . . . , [T(vn)]B2\r\n\u0003\r\n. (4.2.3)\r\nThe above observations lead to the following theorem and the subsequent definition.\r\nTheorem 4.2.1. Let V and W be finite dimensional vector spaces over F with dimensions\r\nn and m, respectively. Let T : V −→W be a linear transformation. Also, let B1 and B2 be\r\nordered bases of V and W, respectively. Then there exists a matrix A ∈ Mm×n(F), denoted\r\nA = T[B1,B2], with A =\r\n\u0002\r\n[T(v1)]B2, [T(v2)]B2, . . . , [T(vn)]B2\r\n\u0003\r\nsuch that\r\n[T(x)]B2 = A [x]B1\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/58d257eb-2963-4036-ba2f-b75a488ec6ba.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b3fc1228e7a178f2220845bdd263993ac4ebbd6c57b7564071e4a179980d0b40",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 555
      },
      {
        "segments": [
          {
            "segment_id": "0b7c3559-8d34-4ba1-b759-10be87110216",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 100,
            "page_width": 612,
            "page_height": 792,
            "content": "100 CHAPTER 4. LINEAR TRANSFORMATIONS\r\nDefinition 4.2.2 (Matrix of a Linear Transformation). Let V and W be finite dimensional\r\nvector spaces over F with dimensions n and m, respectively. Let T : V −→W be a linear\r\ntransformation. Then the matrix T[B1,B2] is called the matrix of the linear transformation\r\nwith respect to the ordered bases B1 and B2.\r\nRemark 4.2.3. Let B1 = (v1, . . . , vn) and B2 = (w1, . . . , wm) be ordered bases of V and\r\nW, respectively. Also, let T : V −→ W be a linear transformation. Then writing T[B1,B2]\r\nin place of the matrix A, Equation (4.2.1) can be rewritten as\r\nT(vj ) = Xm\r\ni=1\r\nT[B1,B2]ijwi, for 1 ≤ j ≤ n. (4.2.4)\r\nWe now give a few examples to understand the above discussion and Theorem 4.2.1.\r\nQ = (0, 1)\r\nP = (1, 0) θ\r\nθ\r\nQ′ = (− sin θ, cos θ)\r\nP\r\n′ = (cos θ,sin θ)\r\nθ\r\nα\r\nP = (x, y)\r\nP\r\n′ = (x′\r\n, y′)\r\nFigure 4.1: Counter-clockwise Rotation by an angle θ\r\nExample 4.2.4. 1. Let T : R\r\n2−→R2\r\nbe a function that counterclockwise rotates every\r\npoint in R\r\n2\r\nby an angle θ, 0 ≤ θ < 2π. Then using Figure 4.1 it can be checked that\r\nx\r\n′ = OP′\r\ncos(α + θ) = OPcos α cos θ − sin α sin θ\r\n\u0001\r\n= x cos θ − y sin θ and similarly\r\ny\r\n′ = x sin θ + y cos θ. Or equivalently, if B = (e1, e2) is the standard ordered basis of\r\nR\r\n2\r\n, then using T(1, 0) = (cos θ,sin θ) and T(0, 1) = (− sin θ, cos θ), we get\r\nT[B,B] = h\r\n[T(1, 0)]B, [T(0, 1)]B\r\ni\r\n=\r\n\"\r\ncos θ − sin θ\r\nsin θ cos θ\r\n#\r\n. (4.2.5)\r\n2. Let B1 =\r\n\r\n(1, 0),(0, 1)\u0001and B2 =\r\n\r\n(1, 1),(1, −1)\u0001be two ordered bases of R\r\n2\r\n. Then\r\nCompute T[B1,B1] and T[B2,B2] for the linear transformation T : R\r\n2−→R2 defined\r\nby T(x, y) = (x + y, x − 2y).\r\nSolution: Observe that for (x, y) ∈ R\r\n2\r\n, [(x, y)]B1 =\r\n\"\r\nx\r\ny\r\n#\r\nand [(x, y)]B2 =\r\n\"\r\nx+y\r\n2\r\nx−y\r\n2\r\n#\r\n.\r\nAlso, T(1, 0) = (1, 1), T(0, 1) = (1, −2), T(1, 1) = (2, −1) and T(1, −1) = (0, 3).\r\nThus, we have\r\nT[B1,B1] = \u0002\r\n[T\r\n\r\n1, 0)\u0001]B1, [T\r\n\r\n0, 1)\u0001]B1\r\n\u0003\r\n=\r\n\u0002\r\n[(1, 1)]B1\r\n, [(1, −2)]B1\r\n\u0003\r\n=\r\n\"\r\n1 1\r\n1 −2\r\n#\r\nand\r\nT[B2,B2] = \u0002\r\n[T\r\n\r\n1, 1)\u0001]B2, [T\r\n\r\n1, −1)\u0001]B2\r\n\u0003\r\n=\r\n\u0002\r\n[(2, −1)]B2, [(0, 3)]B2\r\n\u0003\r\n=\r\n\"\r\n1\r\n2\r\n3\r\n2\r\n3\r\n2 −\r\n3\r\n2\r\n#\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0b7c3559-8d34-4ba1-b759-10be87110216.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7c3a6b31a9fda9b573ba9a2581e0ded9ac23913975cbef8e6e2caeadfbbba565",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 467
      },
      {
        "segments": [
          {
            "segment_id": "54a2d139-d44f-4018-99fd-bff4f3ea30bd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 101,
            "page_width": 612,
            "page_height": 792,
            "content": "4.2. MATRIX OF A LINEAR TRANSFORMATION 101\r\nHence, we see that\r\n[T(x, y)]B1=\r\n\u0002\r\n(x + y, x − 2y)\r\n\u0003\r\nB1\r\n=\r\n\"\r\nx + y\r\nx − 2y\r\n#\r\n=\r\n\"\r\n1 1\r\n1 −2\r\n# \"x\r\ny\r\n#\r\nand\r\n[T(x, y)]B2\r\n=\r\n\u0002\r\n(x + y, x − 2y)\r\n\u0003\r\nB2\r\n=\r\n\"\r\n2x−y\r\n2\r\n3y\r\n2\r\n#\r\n=\r\n\"\r\n1\r\n2\r\n3\r\n2\r\n3\r\n2 −\r\n3\r\n2\r\n# \" x+y\r\n2\r\nx−y\r\n2\r\n#\r\n3. Let B1 =\r\n\r\n(1, 0, 0),(0, 1, 0),(0, 0, 1)\u0001and B2 =\r\n\r\n(1, 0),(0, 1)\u0001be ordered bases of R\r\n3\r\nand R\r\n2\r\n, respectively. Define T : R\r\n3−→R2\r\nby T(x, y, z) = (x + y − z, x + z). Then\r\nT[B1,B2] = \u0014[(1, 0, 0)]B2, [(0, 1, 0)]B2, [(0, 0, 1)]B2\r\n\u0015\r\n=\r\n\"\r\n1 1 −1\r\n1 0 1 #\r\n.\r\nCheck that [T(x, y, z)]B2 = (x + y − z, x + z)\r\nt = T[B1,B2] [(x, y, z)]B1\r\n.\r\n4. Let B1 =\r\n\r\n(1, 0, 0),(0, 1, 0),(0, 0, 1)\u0001, B2 =\r\n\r\n(1, 0, 0),(1, 1, 0),(1, 1, 1)\u0001be two ordered\r\nbases of R\r\n3\r\n. Define T : R\r\n3−→R3\r\nby T(x\r\nt\r\n) = x for all x\r\nt ∈ R3\r\n. Then\r\n[T(1, 0, 0)]B2= 1 · (1, 0, 0) + 0 · (1, 1, 0) + 0 · (1, 1, 1) = (1, 0, 0)t,\r\n[T(0, 1, 0)]B2= −1 · (1, 0, 0) + 1 · (1, 1, 0) + 0 · (1, 1, 1) = (−1, 1, 0)t, and\r\n[T(0, 0, 1)]B2= 0 · (1, 0, 0) + (−1) · (1, 1, 0) + 1 · (1, 1, 1) = (0, −1, 1)t.\r\nThus, check that\r\nT[B1,B2] = [[T(1, 0, 0)]B2\r\n, [T(0, 1, 0)]B2, [T(0, 0, 1)]B2]\r\n= [(1, 0, 0)t\r\n, (−1, 1, 0)t, (0, −1, 1)t] =\r\n\r\n\r\n\r\n1 −1 0\r\n0 1 −1\r\n0 0 1\r\n\r\n\r\n ,\r\nT[B2,B1] = [[T(1, 0, 0)]B1, [T(1, 1, 0)]B1, [T(1, 1, 1)]B1] =\r\n\r\n\r\n\r\n1 1 1\r\n0 1 1\r\n0 0 1\r\n\r\n\r\n,\r\nT[B1,B1] = I3 = T[B2,B2] and T[B2,B1]\r\n−1 = T[B1,B2].\r\nRemark 4.2.5. 1. Let V and W be finite dimensional vector spaces over F with order\r\nbases B1 = (v1, . . . , vn) and B2 of V and W, respectively. If T : V −→W is a linear\r\ntransformation then\r\n(a) T[B1,B2] = \u0002[T(v1)]B2, [T(v2)]B2, . . . , [T(vn)]B2\r\n\u0003\r\n.\r\n(b) [T(x)]B2 = T[B1,B2] [x]B1for all x ∈ V . That is, the coordinate vector of\r\nT(x) ∈ W is obtained by multiplying the matrix of the linear transformation\r\nwith the coordinate vector of x ∈ V .\r\n2. Let A ∈ Mm×n(R). Then A induces a linear transformation TA : R\r\nn−→Rm defined\r\nby TA(x\r\nt\r\n) = Ax for all x\r\nt ∈ Rn\r\n. Let B1 and B2 be the standard ordered bases of R\r\nn\r\nand R\r\nm, respectively. Then it can be easily verified that TA[B1,B2] = A.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/54a2d139-d44f-4018-99fd-bff4f3ea30bd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6e3930b5ff804901a46952e6477fbcf6a89931393f216131505e92034aeae058",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 513
      },
      {
        "segments": [
          {
            "segment_id": "54a2d139-d44f-4018-99fd-bff4f3ea30bd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 101,
            "page_width": 612,
            "page_height": 792,
            "content": "4.2. MATRIX OF A LINEAR TRANSFORMATION 101\r\nHence, we see that\r\n[T(x, y)]B1=\r\n\u0002\r\n(x + y, x − 2y)\r\n\u0003\r\nB1\r\n=\r\n\"\r\nx + y\r\nx − 2y\r\n#\r\n=\r\n\"\r\n1 1\r\n1 −2\r\n# \"x\r\ny\r\n#\r\nand\r\n[T(x, y)]B2\r\n=\r\n\u0002\r\n(x + y, x − 2y)\r\n\u0003\r\nB2\r\n=\r\n\"\r\n2x−y\r\n2\r\n3y\r\n2\r\n#\r\n=\r\n\"\r\n1\r\n2\r\n3\r\n2\r\n3\r\n2 −\r\n3\r\n2\r\n# \" x+y\r\n2\r\nx−y\r\n2\r\n#\r\n3. Let B1 =\r\n\r\n(1, 0, 0),(0, 1, 0),(0, 0, 1)\u0001and B2 =\r\n\r\n(1, 0),(0, 1)\u0001be ordered bases of R\r\n3\r\nand R\r\n2\r\n, respectively. Define T : R\r\n3−→R2\r\nby T(x, y, z) = (x + y − z, x + z). Then\r\nT[B1,B2] = \u0014[(1, 0, 0)]B2, [(0, 1, 0)]B2, [(0, 0, 1)]B2\r\n\u0015\r\n=\r\n\"\r\n1 1 −1\r\n1 0 1 #\r\n.\r\nCheck that [T(x, y, z)]B2 = (x + y − z, x + z)\r\nt = T[B1,B2] [(x, y, z)]B1\r\n.\r\n4. Let B1 =\r\n\r\n(1, 0, 0),(0, 1, 0),(0, 0, 1)\u0001, B2 =\r\n\r\n(1, 0, 0),(1, 1, 0),(1, 1, 1)\u0001be two ordered\r\nbases of R\r\n3\r\n. Define T : R\r\n3−→R3\r\nby T(x\r\nt\r\n) = x for all x\r\nt ∈ R3\r\n. Then\r\n[T(1, 0, 0)]B2= 1 · (1, 0, 0) + 0 · (1, 1, 0) + 0 · (1, 1, 1) = (1, 0, 0)t,\r\n[T(0, 1, 0)]B2= −1 · (1, 0, 0) + 1 · (1, 1, 0) + 0 · (1, 1, 1) = (−1, 1, 0)t, and\r\n[T(0, 0, 1)]B2= 0 · (1, 0, 0) + (−1) · (1, 1, 0) + 1 · (1, 1, 1) = (0, −1, 1)t.\r\nThus, check that\r\nT[B1,B2] = [[T(1, 0, 0)]B2\r\n, [T(0, 1, 0)]B2, [T(0, 0, 1)]B2]\r\n= [(1, 0, 0)t\r\n, (−1, 1, 0)t, (0, −1, 1)t] =\r\n\r\n\r\n\r\n1 −1 0\r\n0 1 −1\r\n0 0 1\r\n\r\n\r\n ,\r\nT[B2,B1] = [[T(1, 0, 0)]B1, [T(1, 1, 0)]B1, [T(1, 1, 1)]B1] =\r\n\r\n\r\n\r\n1 1 1\r\n0 1 1\r\n0 0 1\r\n\r\n\r\n,\r\nT[B1,B1] = I3 = T[B2,B2] and T[B2,B1]\r\n−1 = T[B1,B2].\r\nRemark 4.2.5. 1. Let V and W be finite dimensional vector spaces over F with order\r\nbases B1 = (v1, . . . , vn) and B2 of V and W, respectively. If T : V −→W is a linear\r\ntransformation then\r\n(a) T[B1,B2] = \u0002[T(v1)]B2, [T(v2)]B2, . . . , [T(vn)]B2\r\n\u0003\r\n.\r\n(b) [T(x)]B2 = T[B1,B2] [x]B1for all x ∈ V . That is, the coordinate vector of\r\nT(x) ∈ W is obtained by multiplying the matrix of the linear transformation\r\nwith the coordinate vector of x ∈ V .\r\n2. Let A ∈ Mm×n(R). Then A induces a linear transformation TA : R\r\nn−→Rm defined\r\nby TA(x\r\nt\r\n) = Ax for all x\r\nt ∈ Rn\r\n. Let B1 and B2 be the standard ordered bases of R\r\nn\r\nand R\r\nm, respectively. Then it can be easily verified that TA[B1,B2] = A.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/54a2d139-d44f-4018-99fd-bff4f3ea30bd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6e3930b5ff804901a46952e6477fbcf6a89931393f216131505e92034aeae058",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 513
      },
      {
        "segments": [
          {
            "segment_id": "8a27fa1f-977d-4f08-8ea5-62cd70fd68f3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 102,
            "page_width": 612,
            "page_height": 792,
            "content": "102 CHAPTER 4. LINEAR TRANSFORMATIONS\r\nExercise 4.2.6. 1. Let T : R\r\n2−→R2\r\nbe a linear transformation that reflects every point\r\nin R\r\n2 about the line y = mx. Find its matrix with respect to the standard ordered\r\nbasis of R\r\n2\r\n.\r\n2. Let T : R\r\n3−→R3\r\nbe a linear transformation that reflects every point in R\r\n3 about the\r\nX-axis. Find its matrix with respect to the standard ordered basis of R\r\n3\r\n.\r\n3. Let T : R\r\n3−→R3\r\nbe a linear transformation that counterclockwise rotates every point\r\nin R\r\n3 around the positive Z-axis by an angle θ, 0 ≤ θ < 2π. Prove that T is a linear\r\noperator and find its matrix with respect to the standard ordered basis of R\r\n3\r\n.[Hint: Is\r\n\r\n\r\n\r\ncos θ − sin θ 0\r\nsin θ cos θ 0\r\n0 0 1\r\n\r\n\r\n the required matrix?]\r\n4. Define a function D : Pn(R)−→Pn(R) by\r\nD(a0 + a1x + a2x\r\n2 + · · · + anxn\r\n) = a1 + 2a2x + · · · + nanx\r\nn−1\r\n.\r\nProve that D is a linear operator and find the matrix of D with respect to the standard\r\nordered basis of Pn(R). Observe that the image of D is contained in Pn−1(R).\r\n5. Let T be a linear operator in R\r\n2\r\nsatisfying T(3, 4) = (0, 1) and T(−1, 1) = (2, 3). Let\r\nB =\r\n\r\n(1, 0),(1, 1)\u0001be an ordered basis of R\r\n2\r\n. Compute T[B,B].\r\n6. For each linear transformation given in Example 4.1.2, find its matrix of the linear\r\ntransform with respect to standard ordered bases.\r\n4.3 Rank-Nullity Theorem\r\nWe are now ready to related the rank-nullity theorem (see Theorem 3.3.25 on 86) with the\r\nrank-nullity theorem for linear transformation. To do so, we first define the range space\r\nand the null space of any linear transformation.\r\nDefinition 4.3.1 (Range Space and Null Space). Let V be finite dimensional vector space\r\nover F and let W be any vector space over F. Then for a linear transformation T : V −→W,\r\nwe define\r\n1. C(T) = {T(x) : x ∈ V } as the range space of T and\r\n2. N (T) = {x ∈ V : T(x) = 0} as the null space of T.\r\nWe now prove some results associated with the above definitions.\r\nProposition 4.3.2. Let V be a vector space over F with basis {v1, . . . , vn}. Also, let W\r\nbe a vector spaces over F. Then for any linear transformation T : V −→W,\r\n1. C(T) = L(T(v1), . . . , T(vn)) is a subspace of W and dim(C(T) ≤ dim(W).\r\n2. N (T) is a subspace of V and dim(N (T) ≤ dim(V ).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/8a27fa1f-977d-4f08-8ea5-62cd70fd68f3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=59402bf36332b5f1afb22dae2b114d242a30c1a65eb92fc5be9bc28ea2c694d5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 467
      },
      {
        "segments": [
          {
            "segment_id": "8219ceb0-ea71-422b-acc1-1b55281984cf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 103,
            "page_width": 612,
            "page_height": 792,
            "content": "4.3. RANK-NULLITY THEOREM 103\r\n3. The following statements are equivalent.\r\n(a) T is one-one.\r\n(b) N (T) = {0}.\r\n(c) {T(ui) : 1 ≤ i ≤ n} is a basis of C(T).\r\n4. dim(C(T) = dim(V ) if and only if N (T) = {0}.\r\nProof. Parts 1 and 2 The results about C(T) and N (T) can be easily proved. We thus\r\nleave the proof for the readers.\r\nWe now assume that T is one-one. We need to show that N (T) = {0}.\r\nLet u ∈ N (T). Then by definition, T(u) = 0. Also for any linear transformation (see\r\nProposition 4.1.3), T(0) = 0. Thus T(u) = T(0). So, T is one-one implies u = 0. That is,\r\nN (T) = {0}.\r\nLet N (T) = {0}. We need to show that T is one-one. So, let us assume that for\r\nsome u, v ∈ V, T(u) = T(v). Then, by linearity of T, T(u − v) = 0. This implies,\r\nu − v ∈ N (T) = {0}. This in turn implies u = v. Hence, T is one-one.\r\nThe other parts can be similarly proved.\r\nRemark 4.3.3. 1. C(T) is called the range space and N (T) the null space of T.\r\n2. dim(C(T) is denoted by ρ(T) and is called the rank of T.\r\n3. dim(N (T) is denoted by ν(T) and is called the nullity of T.\r\nExample 4.3.4. Determine the range and null space of the linear transformation\r\nT : R\r\n3−→R4 with T(x, y, z) = (x − y + z, y − z, x, 2x − 5y + 5z).\r\nSolution: By Definition\r\nR(T) = L\r\n\r\n(1, 0, 1, 2),(−1, 1, 0, −5),(1, −1, 0, 5)\u0001\r\n= L\r\n\r\n(1, 0, 1, 2),(1, −1, 0, 5)\u0001\r\n= {α(1, 0, 1, 2) + β(1, −1, 0, 5) : α, β ∈ R}\r\n= {(α + β, −β, α, 2α + 5β) : α, β ∈ R}\r\n= {(x, y, z, w) ∈ R\r\n4\r\n: x + y − z = 0, 5y − 2z + w = 0}\r\nand\r\nN (T) = {(x, y, z) ∈ R\r\n3\r\n: T(x, y, z) = 0}\r\n= {(x, y, z) ∈ R\r\n3\r\n: (x − y + z, y − z, x, 2x − 5y + 5z) = 0}\r\n= {(x, y, z) ∈ R\r\n3\r\n: x − y + z = 0, y − z = 0,\r\nx = 0, 2x − 5y + 5z = 0}\r\n= {(x, y, z) ∈ R\r\n3\r\n: y − z = 0, x = 0}\r\n= {(0, y, y) ∈ R\r\n3\r\n: y ∈ R} = L((0, 1, 1))",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/8219ceb0-ea71-422b-acc1-1b55281984cf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d820509666b5cbbb097607d6962c8d823b74d2bc4f55628fc174efaf4997f6b7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 447
      },
      {
        "segments": [
          {
            "segment_id": "5ac4aa57-163e-4bd6-8e37-c27e9ad806c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 104,
            "page_width": 612,
            "page_height": 792,
            "content": "104 CHAPTER 4. LINEAR TRANSFORMATIONS\r\nExercise 4.3.5. 1. Define a linear operator D : Pn(R)−→Pn(R) by\r\nD(a0 + a1x + a2x\r\n2 + · · · + anxn\r\n) = a1 + 2a2x + · · · + nanx\r\nn−1\r\n.\r\nDescribe N (D) and C(D). Note that C(D) ⊂ Pn−1(R).\r\n2. Let T : V −→W be a linear transformation. If {T(v1), . . . , T(vn)} is linearly inde\u0002pendent subset in C(T) then prove that {v1, . . . , vn} ⊂ V is linearly independent.\r\n3. Define a linear operator T : R\r\n3 −→ R3\r\nby T(1, 0, 0) = (0, 0, 1), T(1, 1, 0) = (1, 1, 1)\r\nand T(1, 1, 1) = (1, 1, 0). Then\r\n(a) determine T(x, y, z) for x, y, z ∈ R.\r\n(b) determine C(T) and N (T). Also calculate ρ(T) and ν(T).\r\n(c) prove that T\r\n3 = T and find the matrix of T with respect to the standard basis.\r\n4. Find a linear operator T : R\r\n3 −→ R3\r\nfor which C(T) = L\r\n\r\n(1, 2, 0),(0, 1, 1),(1, 3, 1)\u0001?\r\n5. Let {v1, v2, . . . , vn} be a basis of a vector space V (F). If W(F) is a vector space\r\nand w1, w2, . . . , wn ∈ W then prove that there exists a unique linear transformation\r\nT : V −→W such that T(vi) = wi\r\nfor all i = 1, 2, . . . , n.\r\nWe now state the rank-nullity theorem for linear transformation. The proof of this\r\nresult is similar to the proof of Theorem 3.3.25 and it also follows from Proposition 4.3.2.\r\nHence, we omit the proof.\r\nTheorem 4.3.6 (Rank Nullity Theorem). Let V be a finite dimensional vector space and\r\nlet T : V −→W be a linear transformation. Then ρ(T) + ν(T) = dim(V ). That is,\r\ndim(R(T)) + dim(N (T)) = dim(V ).\r\nTheorem 4.3.7. Let V and W be finite dimensional vector spaces over F and let T :\r\nV −→W be a linear transformation. Also assume that T is one-one and onto. Then\r\n1. for each w ∈ W, the set T\r\n−1\r\n(w) is a set consisting of a single element.\r\n2. the map T\r\n−1\r\n: W−→V defined by T\r\n−1\r\n(w) = v whenever T(v) = w is a linear\r\ntransformation.\r\nProof. Since T is onto, for each w ∈ W there exists v ∈ V such that T(v) = w. So, the\r\nset T\r\n−1\r\n(w) is non-empty.\r\nSuppose there exist vectors v1, v2 ∈ V such that T(v1) = T(v2). Then the assumption,\r\nT is one-one implies v1 = v2. This completes the proof of Part 1.\r\nWe are now ready to prove that T\r\n−1\r\n, as defined in Part 2, is a linear transformation. Let\r\nw1, w2 ∈ W. Then by Part 1, there exist unique vectors v1, v2 ∈ V such that T\r\n−1\r\n(w1) = v1\r\nand T\r\n−1\r\n(w2) = v2. Or equivalently, T(v1) = w1 and T(v2) = w2. So, for any α1, α2 ∈ F,\r\nT(α1v1 + α2v2) = α1w1 + α2w2. Hence, by definition, for any α1, α2 ∈ F, T −1(α1w1 +\r\nα2w2) = α1v1 + α2v2 = α1T\r\n−1\r\n(w1) + α2T\r\n−1\r\n(w2). Thus the proof of Part 2 is over.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5ac4aa57-163e-4bd6-8e37-c27e9ad806c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7af4fec7f0520b0569fa83b768370837c8fe0e838f3b853396bf56387eac99fc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "5ac4aa57-163e-4bd6-8e37-c27e9ad806c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 104,
            "page_width": 612,
            "page_height": 792,
            "content": "104 CHAPTER 4. LINEAR TRANSFORMATIONS\r\nExercise 4.3.5. 1. Define a linear operator D : Pn(R)−→Pn(R) by\r\nD(a0 + a1x + a2x\r\n2 + · · · + anxn\r\n) = a1 + 2a2x + · · · + nanx\r\nn−1\r\n.\r\nDescribe N (D) and C(D). Note that C(D) ⊂ Pn−1(R).\r\n2. Let T : V −→W be a linear transformation. If {T(v1), . . . , T(vn)} is linearly inde\u0002pendent subset in C(T) then prove that {v1, . . . , vn} ⊂ V is linearly independent.\r\n3. Define a linear operator T : R\r\n3 −→ R3\r\nby T(1, 0, 0) = (0, 0, 1), T(1, 1, 0) = (1, 1, 1)\r\nand T(1, 1, 1) = (1, 1, 0). Then\r\n(a) determine T(x, y, z) for x, y, z ∈ R.\r\n(b) determine C(T) and N (T). Also calculate ρ(T) and ν(T).\r\n(c) prove that T\r\n3 = T and find the matrix of T with respect to the standard basis.\r\n4. Find a linear operator T : R\r\n3 −→ R3\r\nfor which C(T) = L\r\n\r\n(1, 2, 0),(0, 1, 1),(1, 3, 1)\u0001?\r\n5. Let {v1, v2, . . . , vn} be a basis of a vector space V (F). If W(F) is a vector space\r\nand w1, w2, . . . , wn ∈ W then prove that there exists a unique linear transformation\r\nT : V −→W such that T(vi) = wi\r\nfor all i = 1, 2, . . . , n.\r\nWe now state the rank-nullity theorem for linear transformation. The proof of this\r\nresult is similar to the proof of Theorem 3.3.25 and it also follows from Proposition 4.3.2.\r\nHence, we omit the proof.\r\nTheorem 4.3.6 (Rank Nullity Theorem). Let V be a finite dimensional vector space and\r\nlet T : V −→W be a linear transformation. Then ρ(T) + ν(T) = dim(V ). That is,\r\ndim(R(T)) + dim(N (T)) = dim(V ).\r\nTheorem 4.3.7. Let V and W be finite dimensional vector spaces over F and let T :\r\nV −→W be a linear transformation. Also assume that T is one-one and onto. Then\r\n1. for each w ∈ W, the set T\r\n−1\r\n(w) is a set consisting of a single element.\r\n2. the map T\r\n−1\r\n: W−→V defined by T\r\n−1\r\n(w) = v whenever T(v) = w is a linear\r\ntransformation.\r\nProof. Since T is onto, for each w ∈ W there exists v ∈ V such that T(v) = w. So, the\r\nset T\r\n−1\r\n(w) is non-empty.\r\nSuppose there exist vectors v1, v2 ∈ V such that T(v1) = T(v2). Then the assumption,\r\nT is one-one implies v1 = v2. This completes the proof of Part 1.\r\nWe are now ready to prove that T\r\n−1\r\n, as defined in Part 2, is a linear transformation. Let\r\nw1, w2 ∈ W. Then by Part 1, there exist unique vectors v1, v2 ∈ V such that T\r\n−1\r\n(w1) = v1\r\nand T\r\n−1\r\n(w2) = v2. Or equivalently, T(v1) = w1 and T(v2) = w2. So, for any α1, α2 ∈ F,\r\nT(α1v1 + α2v2) = α1w1 + α2w2. Hence, by definition, for any α1, α2 ∈ F, T −1(α1w1 +\r\nα2w2) = α1v1 + α2v2 = α1T\r\n−1\r\n(w1) + α2T\r\n−1\r\n(w2). Thus the proof of Part 2 is over.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5ac4aa57-163e-4bd6-8e37-c27e9ad806c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=7af4fec7f0520b0569fa83b768370837c8fe0e838f3b853396bf56387eac99fc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 559
      },
      {
        "segments": [
          {
            "segment_id": "9d1ed8bc-ceeb-4d2f-bf8c-1942b615b8c1",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 105,
            "page_width": 612,
            "page_height": 792,
            "content": "4.3. RANK-NULLITY THEOREM 105\r\nDefinition 4.3.8 (Inverse Linear Transformation). Let V and W be finite dimensional\r\nvector spaces over F and let T : V −→W be a linear transformation. If the map T is\r\none-one and onto, then the map T\r\n−1\r\n: W−→V defined by\r\nT\r\n−1\r\n(w) = v whenever T(v) = w\r\nis called the inverse of the linear transformation T.\r\nExample 4.3.9. 1. Let T : R\r\n2−→R2\r\nbe defined by T(x, y) = (x + y, x − y). Then\r\nT\r\n−1\r\n: R\r\n2−→R2\r\nis defined by T\r\n−1\r\n(x, y) = (x+y\r\n2\r\n,\r\nx−y\r\n2\r\n). One can see that\r\nT ◦ T\r\n−1\r\n(x, y) = T(T\r\n−1\r\n(x, y)) = T(\r\nx + y\r\n2\r\n,\r\nx − y\r\n2\r\n)\r\n= (x + y\r\n2\r\n+\r\nx − y\r\n2\r\n,\r\nx + y\r\n2\r\n−\r\nx − y\r\n2\r\n) = (x, y) = I(x, y),\r\nwhere I is the identity operator. Hence, T ◦ T\r\n−1 = I. Verify that T−1 ◦ T = I. Thus,\r\nthe map T\r\n−1\r\nis indeed the inverse of T.\r\n2. For (a1, . . . , an+1) ∈ R\r\nn+1, define the linear transformation T : Rn+1−→Pn(R) by\r\nT(a1, a2, . . . , an+1) = a1 + a2x + · · · + an+1x\r\nn\r\n.\r\nThen it can be checked that T\r\n−1\r\n: Pn(R)−→R\r\nn+1 is defined by T−1\r\n(a1 + a2x + · · · +\r\nan+1x\r\nn\r\n) = (a1, a2, . . . , an+1) for all a1 + a2x + · · · + an+1x\r\nn ∈ Pn(R).\r\nUsing the Rank-nullity theorem, we give a short proof of the following result.\r\nCorollary 4.3.10. Let V be a finite dimensional vector space and let T : V −→V be a\r\nlinear operator. Then the following statements are equivalent.\r\n1. T is one-one.\r\n2. T is onto.\r\n3. T is invertible.\r\nProof. By Proposition 4.3.2, T is one-one if and only if N (T) = {0}. By Theorem 4.3.6\r\nN (T) = {0} implies dim(C(T)) = dim(V ). Or equivalently, T is onto.\r\nNow, we know that T is invertible if T is one-one and onto. But we have just shown\r\nthat T is one-one if and only if T is onto. Thus, we have the required result.\r\nRemark 4.3.11. Let V be a finite dimensional vector space and let T : V −→V be a linear\r\noperator. If either T is one-one or T is onto then T is invertible.\r\nExercise 4.3.12. 1. Let V be a finite dimensional vector space and let T : V −→W be\r\na linear transformation. Then prove that\r\n(a) N (T) and C(T) are also finite dimensional.\r\n(b) i. if dim(V ) < dim(W) then T cannot be onto.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9d1ed8bc-ceeb-4d2f-bf8c-1942b615b8c1.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=628a9e6568f408e2f28a3ce9a1ecc04793b119c876d5b45c15a244fa3f33acff",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 472
      },
      {
        "segments": [
          {
            "segment_id": "0c38750d-f16a-456b-b1ad-22a2908f2fad",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 106,
            "page_width": 612,
            "page_height": 792,
            "content": "106 CHAPTER 4. LINEAR TRANSFORMATIONS\r\nii. if dim(V ) > dim(W) then T cannot be one-one.\r\n2. Let V be a vector space of dimension n and let B = (v1, . . . , vn) be an ordered basis of\r\nV . For i = 1, . . . , n, let wi ∈ V with [wi]B = [a1i, a2i, . . . , ani ]\r\nt\r\n. Also, let A = [aij ].\r\nThen prove that w1, . . . , wn is a basis of V if and only if A is invertible.\r\n3. Let T, S : V −→V be linear transformations with dim(V ) = n.\r\n(a) Show that C(T + S) ⊂ C(T) + C(S). Deduce that ρ(T + S) ≤ ρ(T) + ρ(S).\r\n(b) Now, use Theorem 4.3.6 to prove ν(T + S) ≥ ν(T) + ν(S) − n.\r\n4. Let z1, z2, . . . , zk be k distinct complex numbers and define a linear transformation T :\r\nPn(C) −→ C\r\nk\r\nby T\r\n\r\nP(z)\r\n\u0001\r\n=\r\n\r\nP(z1), P(z2), . . . , P(zk)\r\n\u0001\r\n. For each k ≥ 1, determine\r\ndim(C(T)).\r\n5. Fix A ∈ Mn(R) satisfying A2 = A and define TA : R\r\nn −→ Rn\r\nby TA(v\r\nt\r\n) = Av, for\r\nall v\r\nt ∈ Rn\r\n. Then prove that\r\n(a) TA ◦ TA = TA. Equivalently, TA ◦ (I − TA) = 0, where I : R\r\nn −→ Rn\r\nis the\r\nidentity map and 0 : R\r\nn −→ Rn\r\nis the zero map.\r\n(b) N (TA) ∩ C(TA) = {0}.\r\n(c) R\r\nn = C(TA) + N (TA). [Hint: x = TA(x) + (I − TA)(x)]\r\n4.4 Similarity of Matrices\r\nLet V be a finite dimensional vector space with ordered basis B. Then we saw that any\r\nlinear operator T : V −→V corresponds to a square matrix of order dim(V ) and this matrix\r\nwas denoted by T[B,B]. In this section, we will try to understand the relationship between\r\nT[B1,B1] and T[B2,B2], where B1 and B2 are distinct ordered bases of V . This will enable\r\nus to understand the reason for defining the matrix product somewhat differently.\r\nTheorem 4.4.1 (Composition of Linear Transformations). Let V, W and Z be finite\r\ndimensional vector spaces with ordered bases B1,B2 and B3, respectively. Also, let T :\r\nV −→W and S : W−→Z be linear transformations. Then the composition map S ◦ T :\r\nV −→Z (see Figure 4.2) is a linear transformation and\r\n(V,B1, n) (W,B2, m) (Z,B3, p)\r\nT[B1,B2]m×n S[B2,B3]p×m\r\n(S ◦ T)[B1,B3]p×n = S[B2,B3] · T[B1,B2]\r\nFigure 4.2: Composition of Linear Transformations\r\n(S ◦ T) [B1,B3] = S[B2,B3] · T[B1,B2].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0c38750d-f16a-456b-b1ad-22a2908f2fad.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3a347220e747adf81ea60af466258f8e34a83f24777ac5ce060f7c9e22e40531",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 450
      },
      {
        "segments": [
          {
            "segment_id": "a0309a01-86f5-4ef0-b927-3db84b7594c3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 107,
            "page_width": 612,
            "page_height": 792,
            "content": "4.4. SIMILARITY OF MATRICES 107\r\nProof. Let B1 = (u1, . . . , un), B2 = (v1, . . . , vm) and B3 = (w1, . . . , wp) be ordered bases\r\nof V, W and Z, respectively. Then using Equation (4.2.4), we have\r\n(S ◦ T) (ut) = S(T(ut)) = S\r\n\u0012Xm\r\nj=1\r\n(T[B1,B2])jtvj\r\n\u0013\r\n=\r\nXm\r\nj=1\r\n(T[B1,B2])jtS(vj )\r\n=\r\nXm\r\nj=1\r\n(T[B1,B2])jtX\r\np\r\nk=1\r\n(S[B2,B3])kjwk =\r\nX\r\np\r\nk=1\r\n(\r\nXm\r\nj=1\r\n(S[B2,B3])kj (T[B1,B2])jt)wk\r\n=\r\nX\r\np\r\nk=1\r\n(S[B2,B3] T[B1,B2])ktwk.\r\nThus, using matrix multiplication, the t-th column of (S ◦ T) [B1,B3] is given by\r\n[(S ◦ T) (ut)]B3 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nS[B2,B3] T[B1,B2]\r\n\u0001\r\n\r\n1t\r\nS[B2,B3] T[B1,B2]\r\n\u0001\r\n2t\r\n.\r\n.\r\n.\r\n\r\nS[B2,B3] T[B1,B2]\r\n\u0001\r\npt\r\n\r\n\r\n\r\n\r\n\r\n\r\n= S[B2,B3]\r\n\r\n\r\n\r\n\r\n\r\n\r\nT[B1,B2]1t\r\nT[B1,B2]2t\r\n.\r\n.\r\n.\r\nT[B1,B2]pt\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nHence, (S ◦ T)[B1,B3] = \u0002\r\n[(S ◦ T)(u1)]B3, . . . , [(S ◦ T)(un)]B3\r\n\u0003\r\n= S[B2,B3] · T[B1,B2] and\r\nthe proof of the theorem is over.\r\nProposition 4.4.2. Let V be a finite dimensional vector space and let T, S : V −→V be\r\ntwo linear operators. Then ν(T) + ν(S) ≥ ν(T ◦ S) ≥ max{ν(T), ν(S)}.\r\nProof. We first prove the second inequality.\r\nSuppose v ∈ N (S). Then (T ◦ S)(v) = T(S(v) = T(0) = 0 gives N (S) ⊂ N (T ◦ S).\r\nTherefore, ν(S) ≤ ν(T ◦ S).\r\nWe now use Theorem 4.3.6 to see that the inequality ν(T) ≤ ν(T ◦ S) is equivalent to\r\nshowing C(T ◦ S) ⊂ C(T). But this holds true as C(S) ⊂ V and hence T(C(S)) ⊂ T(V ).\r\nThus, the proof of the second inequality is over.\r\nFor the proof of the first inequality, assume that k = ν(S) and {v1, . . . , vk} is a basis\r\nof N (S). Then {v1, . . . , vk} ⊂ N (T ◦ S) as T(0) = 0. So, let us extend it to get a basis\r\n{v1, . . . , vk, u1, . . . , uℓ} of N (T ◦ S).\r\nClaim: {S(u1), S(u2), . . . , S(uℓ)} is a linearly independent subset of N (T).\r\nIt is easily seen that {S(u1), . . . , S(uℓ)} is a subset of N (T). So, let us solve the linear\r\nsystem c1S(u1) + c2S(u2) + · · · + cℓS(uℓ) = 0 in the unknowns c1, c2, . . . , cℓ\r\n. This system\r\nis equivalent to S(c1u1 + c2u2 + · · · + cℓuℓ) = 0. That is, P\r\nℓ\r\ni=1\r\nciui ∈ N (S). Hence, P\r\nℓ\r\ni=1\r\nciui\r\nis a unique linear combination of the vectors v1, . . . , vk. Thus,\r\nc1u1 + c2u2 + · · · + cℓuℓ = α1v1 + α2v2 + · · · + αkvk (4.4.1)\r\nfor some scalars α1, α2, . . . , αk. But by assumption, {v1, . . . , vk, u1, . . . , uℓ} is a basis of\r\nN (T ◦ S) and hence linearly independent. Therefore, the only solution of Equation (4.4.1)\r\nis given by ci = 0 for 1 ≤ i ≤ ℓ and αj = 0 for 1 ≤ j ≤ k.\r\nThus, {S(u1), S(u2), . . . , S(uℓ)} is a linearly independent subset of N (T) and so ν(T) ≥\r\nℓ. Hence, ν(T ◦ S) = k + ℓ ≤ ν(S) + ν(T).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a0309a01-86f5-4ef0-b927-3db84b7594c3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6d9bd8741f7093adfa05d3b8ef958235db25fdce2260404a4c6b86c62d85fccc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 591
      },
      {
        "segments": [
          {
            "segment_id": "a0309a01-86f5-4ef0-b927-3db84b7594c3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 107,
            "page_width": 612,
            "page_height": 792,
            "content": "4.4. SIMILARITY OF MATRICES 107\r\nProof. Let B1 = (u1, . . . , un), B2 = (v1, . . . , vm) and B3 = (w1, . . . , wp) be ordered bases\r\nof V, W and Z, respectively. Then using Equation (4.2.4), we have\r\n(S ◦ T) (ut) = S(T(ut)) = S\r\n\u0012Xm\r\nj=1\r\n(T[B1,B2])jtvj\r\n\u0013\r\n=\r\nXm\r\nj=1\r\n(T[B1,B2])jtS(vj )\r\n=\r\nXm\r\nj=1\r\n(T[B1,B2])jtX\r\np\r\nk=1\r\n(S[B2,B3])kjwk =\r\nX\r\np\r\nk=1\r\n(\r\nXm\r\nj=1\r\n(S[B2,B3])kj (T[B1,B2])jt)wk\r\n=\r\nX\r\np\r\nk=1\r\n(S[B2,B3] T[B1,B2])ktwk.\r\nThus, using matrix multiplication, the t-th column of (S ◦ T) [B1,B3] is given by\r\n[(S ◦ T) (ut)]B3 =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nS[B2,B3] T[B1,B2]\r\n\u0001\r\n\r\n1t\r\nS[B2,B3] T[B1,B2]\r\n\u0001\r\n2t\r\n.\r\n.\r\n.\r\n\r\nS[B2,B3] T[B1,B2]\r\n\u0001\r\npt\r\n\r\n\r\n\r\n\r\n\r\n\r\n= S[B2,B3]\r\n\r\n\r\n\r\n\r\n\r\n\r\nT[B1,B2]1t\r\nT[B1,B2]2t\r\n.\r\n.\r\n.\r\nT[B1,B2]pt\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nHence, (S ◦ T)[B1,B3] = \u0002\r\n[(S ◦ T)(u1)]B3, . . . , [(S ◦ T)(un)]B3\r\n\u0003\r\n= S[B2,B3] · T[B1,B2] and\r\nthe proof of the theorem is over.\r\nProposition 4.4.2. Let V be a finite dimensional vector space and let T, S : V −→V be\r\ntwo linear operators. Then ν(T) + ν(S) ≥ ν(T ◦ S) ≥ max{ν(T), ν(S)}.\r\nProof. We first prove the second inequality.\r\nSuppose v ∈ N (S). Then (T ◦ S)(v) = T(S(v) = T(0) = 0 gives N (S) ⊂ N (T ◦ S).\r\nTherefore, ν(S) ≤ ν(T ◦ S).\r\nWe now use Theorem 4.3.6 to see that the inequality ν(T) ≤ ν(T ◦ S) is equivalent to\r\nshowing C(T ◦ S) ⊂ C(T). But this holds true as C(S) ⊂ V and hence T(C(S)) ⊂ T(V ).\r\nThus, the proof of the second inequality is over.\r\nFor the proof of the first inequality, assume that k = ν(S) and {v1, . . . , vk} is a basis\r\nof N (S). Then {v1, . . . , vk} ⊂ N (T ◦ S) as T(0) = 0. So, let us extend it to get a basis\r\n{v1, . . . , vk, u1, . . . , uℓ} of N (T ◦ S).\r\nClaim: {S(u1), S(u2), . . . , S(uℓ)} is a linearly independent subset of N (T).\r\nIt is easily seen that {S(u1), . . . , S(uℓ)} is a subset of N (T). So, let us solve the linear\r\nsystem c1S(u1) + c2S(u2) + · · · + cℓS(uℓ) = 0 in the unknowns c1, c2, . . . , cℓ\r\n. This system\r\nis equivalent to S(c1u1 + c2u2 + · · · + cℓuℓ) = 0. That is, P\r\nℓ\r\ni=1\r\nciui ∈ N (S). Hence, P\r\nℓ\r\ni=1\r\nciui\r\nis a unique linear combination of the vectors v1, . . . , vk. Thus,\r\nc1u1 + c2u2 + · · · + cℓuℓ = α1v1 + α2v2 + · · · + αkvk (4.4.1)\r\nfor some scalars α1, α2, . . . , αk. But by assumption, {v1, . . . , vk, u1, . . . , uℓ} is a basis of\r\nN (T ◦ S) and hence linearly independent. Therefore, the only solution of Equation (4.4.1)\r\nis given by ci = 0 for 1 ≤ i ≤ ℓ and αj = 0 for 1 ≤ j ≤ k.\r\nThus, {S(u1), S(u2), . . . , S(uℓ)} is a linearly independent subset of N (T) and so ν(T) ≥\r\nℓ. Hence, ν(T ◦ S) = k + ℓ ≤ ν(S) + ν(T).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a0309a01-86f5-4ef0-b927-3db84b7594c3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6d9bd8741f7093adfa05d3b8ef958235db25fdce2260404a4c6b86c62d85fccc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 591
      },
      {
        "segments": [
          {
            "segment_id": "ce1d5659-3fda-4b13-8a06-2fc3c21e7bea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 108,
            "page_width": 612,
            "page_height": 792,
            "content": "108 CHAPTER 4. LINEAR TRANSFORMATIONS\r\nRemark 4.4.3. Using Theorem 4.3.6 and Proposition 4.4.2, we see that if A and B are\r\ntwo n × n matrices then\r\nmin{ρ(A), ρ(B)} ≥ ρ(AB) ≥ n − ρ(A) − ρ(B).\r\nLet V be a finite dimensional vector space and let T : V −→V be an invertible linear\r\noperator. Then using Theorem 4.3.7, the map T\r\n−1\r\n: V −→V is a linear operator defined\r\nby T\r\n−1\r\n(u) = v whenever T(v) = u. The next result relates the matrix of T and T\r\n−1\r\n. The\r\nreader is required to supply the proof (use Theorem 4.4.1).\r\nTheorem 4.4.4 (Inverse of a Linear Transformation). Let V be a finite dimensional vector\r\nspace with ordered bases B1 and B2. Also let T : V −→V be an invertible linear operator.\r\nThen the matrix of T and T\r\n−1 are related by T[B1,B2]−1 = T−1\r\n[B2,B1].\r\nExercise 4.4.5. Find the matrix of the linear transformations given below.\r\n1. Define T : R\r\n3−→R3\r\nby T(1, 1, 1) = (1, −1, 1), T(1, −1, 1) = (1, 1, −1) and T(1, 1, −1) =\r\n(1, 1, 1). Find T[B,B], where B =\r\n\r\n(1, 1, 1),(1, −1, 1),(1, 1, −1)\u0001. Is T an invertible\r\nlinear operator?\r\n2. Let B =\r\n\r\n1, x, x2\r\n, x3\r\n\u0001\r\nbe an ordered basis of P3(R). Define T : P3(R)−→P3(R) by\r\nT(1) = 1, T(x) = 1 + x, T(x\r\n2\r\n) = (1 + x)\r\n2\r\nand T(x\r\n3\r\n) = (1 + x)\r\n3\r\n.\r\nProve that T is an invertible linear operator. Also, find T[B,B] and T\r\n−1\r\n[B,B].\r\nWe end this section with definition, results and examples related with the notion of\r\nisomorphism. The result states that for each fixed positive integer n, every real vector\r\nspace of dimension n is isomorphic to R\r\nn and every complex vector space of dimension n\r\nis isomorphic to C\r\nn\r\n.\r\nDefinition 4.4.6 (Isomorphism). Let V and W be two vector spaces over F. Then V\r\nis said to be isomorphic to W if there exists a linear transformation T : V −→W that is\r\none-one, onto and invertible. We also denote it by V ∼= W.\r\nTheorem 4.4.7. Let V be a vector space over R. If dim(V ) = n then V ∼= R\r\nn\r\n.\r\nProof. Let B be the standard ordered basis of R\r\nn and let B1 =\r\n\r\nv1, . . . , vn\r\n\u0001\r\nbe an ordered\r\nbasis of V . Define a map T : V −→R\r\nn by T(vi) = ei\r\nfor 1 ≤ i ≤ n. Then it can be easily\r\nverified that T is a linear transformation that is one-one, onto and invertible (the image of\r\na basis vector is a basis vector). Hence, the result follows.\r\nA similar idea leads to the following result and hence we omit the proof.\r\nTheorem 4.4.8. Let V be a vector space over C. If dim(V ) = n then V ∼= C\r\nn\r\n.\r\nExample 4.4.9. 1. The standard ordered basis of Pn(C) is given by \r\n1, x, x2, . . . , xn\r\n\u0001\r\n.\r\nHence, define T : Pn(C)−→C\r\nn+1 by T(xi\r\n) = ei+1 for 0 ≤ i ≤ n. In general, verify\r\nthat T(a0 + a1x + · · · + anx\r\nn\r\n) = (a0, a1, . . . , an) and T is linear transformation which\r\nis one-one, onto and invertible. Thus, the vector space Pn(C) ∼= C\r\nn+1\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ce1d5659-3fda-4b13-8a06-2fc3c21e7bea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb1940f26992a6dc964273aedc1dfebacd9ed83e9785c1a9085f5ddb7979d3fd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 586
      },
      {
        "segments": [
          {
            "segment_id": "ce1d5659-3fda-4b13-8a06-2fc3c21e7bea",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 108,
            "page_width": 612,
            "page_height": 792,
            "content": "108 CHAPTER 4. LINEAR TRANSFORMATIONS\r\nRemark 4.4.3. Using Theorem 4.3.6 and Proposition 4.4.2, we see that if A and B are\r\ntwo n × n matrices then\r\nmin{ρ(A), ρ(B)} ≥ ρ(AB) ≥ n − ρ(A) − ρ(B).\r\nLet V be a finite dimensional vector space and let T : V −→V be an invertible linear\r\noperator. Then using Theorem 4.3.7, the map T\r\n−1\r\n: V −→V is a linear operator defined\r\nby T\r\n−1\r\n(u) = v whenever T(v) = u. The next result relates the matrix of T and T\r\n−1\r\n. The\r\nreader is required to supply the proof (use Theorem 4.4.1).\r\nTheorem 4.4.4 (Inverse of a Linear Transformation). Let V be a finite dimensional vector\r\nspace with ordered bases B1 and B2. Also let T : V −→V be an invertible linear operator.\r\nThen the matrix of T and T\r\n−1 are related by T[B1,B2]−1 = T−1\r\n[B2,B1].\r\nExercise 4.4.5. Find the matrix of the linear transformations given below.\r\n1. Define T : R\r\n3−→R3\r\nby T(1, 1, 1) = (1, −1, 1), T(1, −1, 1) = (1, 1, −1) and T(1, 1, −1) =\r\n(1, 1, 1). Find T[B,B], where B =\r\n\r\n(1, 1, 1),(1, −1, 1),(1, 1, −1)\u0001. Is T an invertible\r\nlinear operator?\r\n2. Let B =\r\n\r\n1, x, x2\r\n, x3\r\n\u0001\r\nbe an ordered basis of P3(R). Define T : P3(R)−→P3(R) by\r\nT(1) = 1, T(x) = 1 + x, T(x\r\n2\r\n) = (1 + x)\r\n2\r\nand T(x\r\n3\r\n) = (1 + x)\r\n3\r\n.\r\nProve that T is an invertible linear operator. Also, find T[B,B] and T\r\n−1\r\n[B,B].\r\nWe end this section with definition, results and examples related with the notion of\r\nisomorphism. The result states that for each fixed positive integer n, every real vector\r\nspace of dimension n is isomorphic to R\r\nn and every complex vector space of dimension n\r\nis isomorphic to C\r\nn\r\n.\r\nDefinition 4.4.6 (Isomorphism). Let V and W be two vector spaces over F. Then V\r\nis said to be isomorphic to W if there exists a linear transformation T : V −→W that is\r\none-one, onto and invertible. We also denote it by V ∼= W.\r\nTheorem 4.4.7. Let V be a vector space over R. If dim(V ) = n then V ∼= R\r\nn\r\n.\r\nProof. Let B be the standard ordered basis of R\r\nn and let B1 =\r\n\r\nv1, . . . , vn\r\n\u0001\r\nbe an ordered\r\nbasis of V . Define a map T : V −→R\r\nn by T(vi) = ei\r\nfor 1 ≤ i ≤ n. Then it can be easily\r\nverified that T is a linear transformation that is one-one, onto and invertible (the image of\r\na basis vector is a basis vector). Hence, the result follows.\r\nA similar idea leads to the following result and hence we omit the proof.\r\nTheorem 4.4.8. Let V be a vector space over C. If dim(V ) = n then V ∼= C\r\nn\r\n.\r\nExample 4.4.9. 1. The standard ordered basis of Pn(C) is given by \r\n1, x, x2, . . . , xn\r\n\u0001\r\n.\r\nHence, define T : Pn(C)−→C\r\nn+1 by T(xi\r\n) = ei+1 for 0 ≤ i ≤ n. In general, verify\r\nthat T(a0 + a1x + · · · + anx\r\nn\r\n) = (a0, a1, . . . , an) and T is linear transformation which\r\nis one-one, onto and invertible. Thus, the vector space Pn(C) ∼= C\r\nn+1\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ce1d5659-3fda-4b13-8a06-2fc3c21e7bea.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bb1940f26992a6dc964273aedc1dfebacd9ed83e9785c1a9085f5ddb7979d3fd",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 586
      },
      {
        "segments": [
          {
            "segment_id": "9287fd4a-a8df-46ac-8cd9-1b9477416bef",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 109,
            "page_width": 612,
            "page_height": 792,
            "content": "4.5. CHANGE OF BASIS 109\r\n2. Let V = {(x, y, z, w) ∈ R\r\n4\r\n: x − y + z − w = 0}. Suppose that B is the standard\r\nordered basis of R\r\n3 and B1 =\r\n\r\n(1, 1, 0, 0),(−1, 0, 1, 0),(1, 0, 0, 1)\u0001is the ordered basis\r\nof V . Then T : V −→R\r\n3 defined by T(v) = T(y −z +w, y, z, w) = (y, z, w) is a linear\r\ntransformation and T[B1,B] = I3. Thus, T is one-one, onto and invertible.\r\n4.5 Change of Basis\r\nLet V be a vector space with ordered bases B1 = (u1, . . . , un) and B2 = (v1, . . . , vn}. Also,\r\nrecall that the identity linear operator I : V −→V is defined by I(x) = x for every x ∈ V.\r\nIf\r\nI[B2,B1] = \u0002[I(v1)]B1, [I(v2)]B1, . . . , [I(vn)]B1\r\n\u0003\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 an2 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\nthen by definition of I[B2,B1], we see that vi = I(vi) = Pn\r\nj=1\r\najiuj for all i, 1 ≤ i ≤ n. Thus,\r\nwe have proved the following result which also appeared in another form in Theorem 3.4.5.\r\nTheorem 4.5.1 (Change of Basis Theorem). Let V be a finite dimensional vector space\r\nwith ordered bases B1 = (u1, u2, . . . , un} and B2 = (v1, v2, . . . , vn}. Suppose x ∈ V with\r\n[x]B1 = (α1, α2, . . . , αn)\r\nt and [x]B2 = (β1, β2, . . . , βn)t\r\n. Then [x]B1 = I[B2,B1] [x]B2. Or\r\nequivalently,\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1\r\nα2\r\n.\r\n.\r\n.\r\nαn\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 an2 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nβ1\r\nβ2\r\n.\r\n.\r\n.\r\nβn\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nRemark 4.5.2. Observe that the identity linear operator I : V −→V is invertible and\r\nhence by Theorem 4.4.4 I[B2,B1]\r\n−1 = I−1\r\n[B1,B2] = I[B1,B2]. Therefore, we also have\r\n[x]B2 = I[B1,B2] [x]B1.\r\nLet V be a finite dimensional vector space with ordered bases B1 and B2. Then for any\r\nlinear operator T : V −→V the next result relates T[B1,B1] and T[B2,B2].\r\nTheorem 4.5.3. Let B1 = (u1, . . . , un) and B2 = (v1, . . . , vn) be two ordered bases of a\r\nvector space V . Also, let A = [aij ] = I[B2,B1] be the matrix of the identity linear operator.\r\nThen for any linear operator T : V −→V\r\nT[B2,B2] = A\r\n−1\r\n· T[B1,B1] · A = I[B1,B2] · T[B1,B1] · I[B2,B1]. (4.5.2)\r\nProof. The proof uses Theorem 4.4.1 by representing T[B1,B2] as (I ◦ T)[B1,B2] and (T ◦\r\nI)[B1,B2], where I is the identity operator on V (see Figure 4.3). By Theorem 4.4.1, we\r\nhave\r\nT[B1,B2] = (I ◦ T)[B1,B2] = I[B1,B2] · T[B1,B1]\r\n= (T ◦ I)[B1,B2] = T[B2,B2] · I[B1,B2].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9287fd4a-a8df-46ac-8cd9-1b9477416bef.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f9f28ab6989e474fc7e7cb67304e318c8aa7b7f1886de60237d7dbe6cd41764",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 569
      },
      {
        "segments": [
          {
            "segment_id": "9287fd4a-a8df-46ac-8cd9-1b9477416bef",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 109,
            "page_width": 612,
            "page_height": 792,
            "content": "4.5. CHANGE OF BASIS 109\r\n2. Let V = {(x, y, z, w) ∈ R\r\n4\r\n: x − y + z − w = 0}. Suppose that B is the standard\r\nordered basis of R\r\n3 and B1 =\r\n\r\n(1, 1, 0, 0),(−1, 0, 1, 0),(1, 0, 0, 1)\u0001is the ordered basis\r\nof V . Then T : V −→R\r\n3 defined by T(v) = T(y −z +w, y, z, w) = (y, z, w) is a linear\r\ntransformation and T[B1,B] = I3. Thus, T is one-one, onto and invertible.\r\n4.5 Change of Basis\r\nLet V be a vector space with ordered bases B1 = (u1, . . . , un) and B2 = (v1, . . . , vn}. Also,\r\nrecall that the identity linear operator I : V −→V is defined by I(x) = x for every x ∈ V.\r\nIf\r\nI[B2,B1] = \u0002[I(v1)]B1, [I(v2)]B1, . . . , [I(vn)]B1\r\n\u0003\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 an2 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\nthen by definition of I[B2,B1], we see that vi = I(vi) = Pn\r\nj=1\r\najiuj for all i, 1 ≤ i ≤ n. Thus,\r\nwe have proved the following result which also appeared in another form in Theorem 3.4.5.\r\nTheorem 4.5.1 (Change of Basis Theorem). Let V be a finite dimensional vector space\r\nwith ordered bases B1 = (u1, u2, . . . , un} and B2 = (v1, v2, . . . , vn}. Suppose x ∈ V with\r\n[x]B1 = (α1, α2, . . . , αn)\r\nt and [x]B2 = (β1, β2, . . . , βn)t\r\n. Then [x]B1 = I[B2,B1] [x]B2. Or\r\nequivalently,\r\n\r\n\r\n\r\n\r\n\r\n\r\nα1\r\nα2\r\n.\r\n.\r\n.\r\nαn\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 a12 · · · a1n\r\na21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 an2 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nβ1\r\nβ2\r\n.\r\n.\r\n.\r\nβn\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\nRemark 4.5.2. Observe that the identity linear operator I : V −→V is invertible and\r\nhence by Theorem 4.4.4 I[B2,B1]\r\n−1 = I−1\r\n[B1,B2] = I[B1,B2]. Therefore, we also have\r\n[x]B2 = I[B1,B2] [x]B1.\r\nLet V be a finite dimensional vector space with ordered bases B1 and B2. Then for any\r\nlinear operator T : V −→V the next result relates T[B1,B1] and T[B2,B2].\r\nTheorem 4.5.3. Let B1 = (u1, . . . , un) and B2 = (v1, . . . , vn) be two ordered bases of a\r\nvector space V . Also, let A = [aij ] = I[B2,B1] be the matrix of the identity linear operator.\r\nThen for any linear operator T : V −→V\r\nT[B2,B2] = A\r\n−1\r\n· T[B1,B1] · A = I[B1,B2] · T[B1,B1] · I[B2,B1]. (4.5.2)\r\nProof. The proof uses Theorem 4.4.1 by representing T[B1,B2] as (I ◦ T)[B1,B2] and (T ◦\r\nI)[B1,B2], where I is the identity operator on V (see Figure 4.3). By Theorem 4.4.1, we\r\nhave\r\nT[B1,B2] = (I ◦ T)[B1,B2] = I[B1,B2] · T[B1,B1]\r\n= (T ◦ I)[B1,B2] = T[B2,B2] · I[B1,B2].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9287fd4a-a8df-46ac-8cd9-1b9477416bef.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4f9f28ab6989e474fc7e7cb67304e318c8aa7b7f1886de60237d7dbe6cd41764",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 569
      },
      {
        "segments": [
          {
            "segment_id": "16eaa4c4-f8bc-45f7-ac72-495701797942",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 110,
            "page_width": 612,
            "page_height": 792,
            "content": "110 CHAPTER 4. LINEAR TRANSFORMATIONS\r\n(V,B1) (V,B1)\r\n(V,B2) (V,B2)\r\nT[B1,B1]\r\nT[B2,B2]\r\nI[B1,B2] I[B1,B2]\r\nT ◦ I\r\nI ◦ T\r\nFigure 4.3: Commutative Diagram for Similarity of Matrices\r\nThus, using I[B2,B1] = I[B1,B2]\r\n−1\r\n, we get I[B1,B2] T[B1,B1] I[B2,B1] = T[B2,B2] and\r\nthe result follows.\r\nLet T : V −→V be a linear operator on V . If dim(V ) = n then each ordered basis B of\r\nV gives rise to an n × n matrix T[B,B]. Also, we know that for any vector space we have\r\ninfinite number of choices for an ordered basis. So, as we change an ordered basis, the\r\nmatrix of the linear transformation changes. Theorem 4.5.3 tells us that all these matrices\r\nare related by an invertible matrix (see Remark 4.5.2). Thus we are led to the following\r\nremark and the definition.\r\nRemark 4.5.4. The Equation (4.5.2) shows that T[B2,B2] = I[B1,B2]·T[B1,B1]·I[B2,B1].\r\nHence, the matrix I[B1,B2] is called the B1 : B2 change of basis matrix.\r\nDefinition 4.5.5 (Similar Matrices). Two square matrices B and C of the same order\r\nare said to be similar if there exists a non-singular matrix P such that P\r\n−1BP = C or\r\nequivalently BP = P C.\r\nExample 4.5.6. 1. Let B1 =\r\n\r\n1 + x, 1 + 2x + x\r\n2\r\n, 2 + x\r\n\u0001\r\nand B2 =\r\n\r\n1, 1 + x, 1 + x + x\r\n2\r\n\u0001\r\nbe ordered bases of P2(R). Then I(a + bx + cx2) = a + bx + cx2. Thus,\r\nI[B2,B1] = [[1]B1\r\n, [1 + x]B1, [1 + x + x\r\n2\r\n]B1] =\r\n\r\n\r\n\r\n−1 1 −2\r\n0 0 1\r\n1 0 1\r\n\r\n\r\n\r\nand\r\nI[B1,B2] = [[1 + x]B2, [1 + 2x + x\r\n2\r\n]B2, [2 + x]B2] =\r\n\r\n\r\n\r\n0 −1 1\r\n1 1 1\r\n0 1 0\r\n\r\n\r\n .\r\nAlso, verify that I[B1,B2]\r\n−1 = I[B2,B1].\r\n2. Let B1 =\r\n\r\n(1, 0, 0),(1, 1, 0),(1, 1, 1)\u0001and B2 =\r\n\r\n1, 1, −1),(1, 2, 1),(2, 1, 1)\u0001be two\r\nordered bases of R\r\n3\r\n. Define T : R\r\n3−→R3\r\nby T(x, y, z) = (x + y, x + y + 2z, y − z).\r\nThen T[B1,B1] =\r\n\r\n\r\n\r\n0 0 −2\r\n1 1 4\r\n0 1 0\r\n\r\n\r\n\r\nand T[B2,B2] =\r\n\r\n\r\n\r\n−4/5 1 8/5\r\n−2/5 2 9/5\r\n8/5 0 −1/5\r\n\r\n\r\n. Also, check that",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/16eaa4c4-f8bc-45f7-ac72-495701797942.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=526d934e2189643a3dcd22fd0088ce0852d0060a7466501263d081de2e1280cc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 409
      },
      {
        "segments": [
          {
            "segment_id": "9ed3bb84-6b11-40c1-8d87-88ab9e4b705a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 111,
            "page_width": 612,
            "page_height": 792,
            "content": "4.6. SUMMARY 111\r\nI[B2,B1] =\r\n\r\n\r\n\r\n0 −1 1\r\n2 1 0\r\n−1 1 1\r\n\r\n\r\n\r\nand\r\nT[B1,B1] I[B2,B1] = I[B2,B1] T[B2,B2] =\r\n\r\n\r\n\r\n2 −2 −2\r\n−2 4 5\r\n2 1 0\r\n\r\n\r\n .\r\nExercise 4.5.7. 1. Let V be an n-dimensional vector space and let T : V −→V be a\r\nlinear operator. Suppose T has the property that T\r\nn−1 6= 0 but Tn = 0.\r\n(a) Prove that there exists u ∈ V with {u, T(u), . . . , T n−1\r\n(u)}, a basis of V.\r\n(b) For B =\r\n\r\nu, T(u), . . . , T n−1\r\n(u)\r\n\u0001\r\nprove that T[B,B] =\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n0 0 0 · · · 0\r\n1 0 0 · · · 0\r\n0 1 0 · · · 0\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 · · · 1 0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n.\r\n(c) Let A be an n × n matrix satisfying An−1 6= 0 but An = 0. Then prove that A\r\nis similar to the matrix given in Part 1b.\r\n2. Define T : R\r\n3−→R3\r\nby T(x, y, z) = (x + y + 2z, x − y − 3z, 2x + 3y + z). Let B be the\r\nstandard basis and B1 =\r\n\r\n(1, 1, 1),(1, −1, 1),(1, 1, 2)\u0001be another ordered basis of R\r\n3\r\n.\r\nThen find the\r\n(a) matrices T[B,B] and T[B1,B1].\r\n(b) matrix P such that P\r\n−1T[B,B] P = T[B1,B1].\r\n3. Define T : R\r\n3−→R3\r\nby T(x, y, z) = (x, x + y, x + y + z). Let B be the standard basis\r\nand B1 =\r\n\r\n(1, 0, 0),(1, 1, 0),(1, 1, 1)\u0001be another ordered basis of R\r\n3\r\n. Then find the\r\n(a) matrices T[B,B] and T[B1,B1].\r\n(b) matrix P such that P\r\n−1T[B,B] P = T[B1,B1].\r\n4. Let B1 =\r\n\r\n(1, 2, 0),(1, 3, 2),(0, 1, 3)\u0001and B2 =\r\n\r\n(1, 2, 1),(0, 1, 2),(1, 4, 6)\u0001be two or\u0002dered bases of R\r\n3\r\n. Find the change of basis matrix\r\n(a) P from B1 to B2.\r\n(b) Q from B2 to B1.\r\n(c) from the standard basis of R\r\n3\r\nto B1. What do you notice?\r\nIs it true that P Q = I = QP? Give reasons for your answer.\r\n4.6 Summary",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9ed3bb84-6b11-40c1-8d87-88ab9e4b705a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e8ff43bf6cb5624ef2e49aec9391db3aed85e3392fb8b0d144322c6de6d814e8",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "543534b6-5696-4dde-89f3-af5e493f34f7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 112,
            "page_width": 612,
            "page_height": 792,
            "content": "112 CHAPTER 4. LINEAR TRANSFORMATIONS",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/543534b6-5696-4dde-89f3-af5e493f34f7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=06ee31915afe504fd3d1e10ff7b33562bc804045f99e4c18b7ef07bead29038f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 413
      },
      {
        "segments": [
          {
            "segment_id": "bd269bb3-b1b6-47b6-9ca3-406e63bd9d2c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 113,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 5\r\nInner Product Spaces\r\n5.1 Introduction\r\nIn the previous chapters, we learnt about vector spaces and linear transformations that are\r\nmaps (functions) between vector spaces. In this chapter, we will start with the definition\r\nof inner product that helps us to view vector spaces geometrically.\r\n5.2 Definition and Basic Properties\r\nIn R\r\n2 and R3\r\n, we had a notion of dot product between two vectors. In particular, if\r\nx\r\nt = (x1, x2, x3), yt = (y1, y2, y3) are two vectors in R3\r\nthen their dot product was defined\r\nby\r\nx · y = x1y1 + x2y2 + x3y3.\r\nNote that for any x\r\nt\r\n, y\r\nt\r\n, z\r\nt ∈ R3 and α ∈ R, the dot product satisfied the following\r\nconditions:\r\nx · (y + αz) = x · y + αx · z, x · y = y · x, and x · x ≥ 0.\r\nAlso, x·x = 0 if and only if x = 0. So, in this chapter, we generalize the idea of dot product\r\nfor arbitrary vector spaces. This generalization is commonly known as inner product which\r\nis our starting point for this chapter.\r\nDefinition 5.2.1 (Inner Product). Let V be a vector space over F. An inner product over\r\nV, denoted by h , i, is a map from V × V to F satisfying\r\n1. hau + bv, wi = ahu, wi + bhv, wi, for all u, v, w ∈ V and a, b ∈ F,\r\n2. hu, vi = hv, ui, the complex conjugate of hu, vi, for all u, v ∈ V and\r\n3. hu, ui ≥ 0 for all u ∈ V and equality holds if and only if u = 0.\r\nDefinition 5.2.2 (Inner Product Space). Let V be a vector space with an inner product\r\nh , i. Then (V,h , i) is called an inner product space (in short, ips).\r\n113",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/bd269bb3-b1b6-47b6-9ca3-406e63bd9d2c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=d79dcdd97ce8d2afe9c598eb5fcc9a16cdd8e713147d09cf55d12757b733841b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 321
      },
      {
        "segments": [
          {
            "segment_id": "8905994f-8610-4cf4-bc44-de294d4d7381",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 114,
            "page_width": 612,
            "page_height": 792,
            "content": "114 CHAPTER 5. INNER PRODUCT SPACES\r\nExample 5.2.3. The first two examples given below are called the standard inner prod\u0002uct or the dot product on R\r\nn and Cn\r\n, respectively. From now on, whenever an inner\r\nproduct is not mentioned, it will be assumed to be the standard inner product.\r\n1. Let V = R\r\nn\r\n. Define hu, vi = u1v1 + · · · + unvn = u\r\ntv for all ut = (u1, . . . , un), vt =\r\n(v1, . . . , vn) ∈ V . Then it can be easily verified that h , i satisfies all the three\r\nconditions of Definition 5.2.1. Hence, R\r\nn\r\n,h , i\r\n\u0001\r\nis an inner product space.\r\n2. Let u\r\nt = (u1, . . . , un), vt = (v1, . . . , vn) be two vectors in Cn\r\n(C). Define hu, vi =\r\nu1v1 + u2v2 + · · · + unvn = u\r\n∗v. Then it can be easily verified that \r\nC\r\nn\r\n,h , i\r\n\u0001\r\nis an\r\ninner product space.\r\n3. Let V = R\r\n2 and let A =\r\n\"\r\n4 −1\r\n−1 2 #\r\n. Define hx, yi = x\r\ntAy for xt\r\n, y\r\nt ∈ R2\r\n. Then\r\nprove that h , i is an inner product. Hint: hx, yi = 4x1y1 − x1y2 − x2y1 + 2x2y2 and\r\nhx, xi = (x1 − x2)\r\n2 + 3x2\r\n1 + x\r\n2\r\n2\r\n.\r\n4. Prove that hx, yi = 10x1y1 + 3x1y2 + 3x2y1 + 2x2y2 + x2y3 + x3y2 + x3y3 defines an\r\ninner product in R\r\n3\r\n, where x\r\nt = (x1, x2, x3) and yt = (y1, y2, y3) ∈ R3\r\n.\r\n5. For x\r\nt = (x1, x2), yt = (y1, y2) ∈ R2\r\n, we define three maps that satisfy at least one\r\ncondition out of the three conditions for an inner product. Determine the condition\r\nwhich is not satisfied. Give reasons for your answer.\r\n(a) hx, yi = x1y1.\r\n(b) hx, yi = x\r\n2\r\n1 + y\r\n2\r\n1 + x\r\n2\r\n2 + y\r\n2\r\n2\r\n.\r\n(c) hx, yi = x1y\r\n3\r\n1 + x2y\r\n3\r\n2\r\n.\r\n6. For A, B ∈ Mn(R), define hA, Bi = tr(ABt). Then\r\nhA + B, Ci = tr(A + B)C\r\nt\r\n\u0001\r\n= tr(ACt\r\n) + tr(BCt) = hA, Ci + hB, Ci.\r\nhA, Bi = tr(ABt) = tr( (ABt)\r\nt\r\n) = tr(BAt) = hB, Ai.\r\nIf A = (aij ), then hA, Ai = tr(AAt\r\n) = Pn\r\ni=1\r\n(AAt)ii =\r\nPn\r\ni,j=1\r\naijaij =\r\nPn\r\ni,j=1\r\na\r\n2\r\nij and\r\ntherefore, hA, Ai > 0 for all non-zero matrix A.\r\nExercise 5.2.4. 1. Verify that inner products defined in Examples 3 and 4, are indeed\r\ninner products.\r\n2. Let hx, yi = 0 for every vector y of an inner product space V . prove that x = 0.\r\nDefinition 5.2.5 (Length/Norm of a Vector). Let V be a vector space. Then for any\r\nvector u ∈ V, we define the length (norm) of u, denoted kuk, by kuk =\r\np\r\nhu, ui, the\r\npositive square root. A vector of norm 1 is called a unit vector.\r\nExample 5.2.6. 1. Let V be an inner product space and u ∈ V . Then for any scalar\r\nα, it is easy to verify that kαuk =\r\n\f\r\n\fα\r\n\f\r\n\f · kuk.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/8905994f-8610-4cf4-bc44-de294d4d7381.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ad3ae76c2d20c3c7f015823f1fe6acfec1112bdee10bd33ac536a5c6645b87b8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 576
      },
      {
        "segments": [
          {
            "segment_id": "8905994f-8610-4cf4-bc44-de294d4d7381",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 114,
            "page_width": 612,
            "page_height": 792,
            "content": "114 CHAPTER 5. INNER PRODUCT SPACES\r\nExample 5.2.3. The first two examples given below are called the standard inner prod\u0002uct or the dot product on R\r\nn and Cn\r\n, respectively. From now on, whenever an inner\r\nproduct is not mentioned, it will be assumed to be the standard inner product.\r\n1. Let V = R\r\nn\r\n. Define hu, vi = u1v1 + · · · + unvn = u\r\ntv for all ut = (u1, . . . , un), vt =\r\n(v1, . . . , vn) ∈ V . Then it can be easily verified that h , i satisfies all the three\r\nconditions of Definition 5.2.1. Hence, R\r\nn\r\n,h , i\r\n\u0001\r\nis an inner product space.\r\n2. Let u\r\nt = (u1, . . . , un), vt = (v1, . . . , vn) be two vectors in Cn\r\n(C). Define hu, vi =\r\nu1v1 + u2v2 + · · · + unvn = u\r\n∗v. Then it can be easily verified that \r\nC\r\nn\r\n,h , i\r\n\u0001\r\nis an\r\ninner product space.\r\n3. Let V = R\r\n2 and let A =\r\n\"\r\n4 −1\r\n−1 2 #\r\n. Define hx, yi = x\r\ntAy for xt\r\n, y\r\nt ∈ R2\r\n. Then\r\nprove that h , i is an inner product. Hint: hx, yi = 4x1y1 − x1y2 − x2y1 + 2x2y2 and\r\nhx, xi = (x1 − x2)\r\n2 + 3x2\r\n1 + x\r\n2\r\n2\r\n.\r\n4. Prove that hx, yi = 10x1y1 + 3x1y2 + 3x2y1 + 2x2y2 + x2y3 + x3y2 + x3y3 defines an\r\ninner product in R\r\n3\r\n, where x\r\nt = (x1, x2, x3) and yt = (y1, y2, y3) ∈ R3\r\n.\r\n5. For x\r\nt = (x1, x2), yt = (y1, y2) ∈ R2\r\n, we define three maps that satisfy at least one\r\ncondition out of the three conditions for an inner product. Determine the condition\r\nwhich is not satisfied. Give reasons for your answer.\r\n(a) hx, yi = x1y1.\r\n(b) hx, yi = x\r\n2\r\n1 + y\r\n2\r\n1 + x\r\n2\r\n2 + y\r\n2\r\n2\r\n.\r\n(c) hx, yi = x1y\r\n3\r\n1 + x2y\r\n3\r\n2\r\n.\r\n6. For A, B ∈ Mn(R), define hA, Bi = tr(ABt). Then\r\nhA + B, Ci = tr(A + B)C\r\nt\r\n\u0001\r\n= tr(ACt\r\n) + tr(BCt) = hA, Ci + hB, Ci.\r\nhA, Bi = tr(ABt) = tr( (ABt)\r\nt\r\n) = tr(BAt) = hB, Ai.\r\nIf A = (aij ), then hA, Ai = tr(AAt\r\n) = Pn\r\ni=1\r\n(AAt)ii =\r\nPn\r\ni,j=1\r\naijaij =\r\nPn\r\ni,j=1\r\na\r\n2\r\nij and\r\ntherefore, hA, Ai > 0 for all non-zero matrix A.\r\nExercise 5.2.4. 1. Verify that inner products defined in Examples 3 and 4, are indeed\r\ninner products.\r\n2. Let hx, yi = 0 for every vector y of an inner product space V . prove that x = 0.\r\nDefinition 5.2.5 (Length/Norm of a Vector). Let V be a vector space. Then for any\r\nvector u ∈ V, we define the length (norm) of u, denoted kuk, by kuk =\r\np\r\nhu, ui, the\r\npositive square root. A vector of norm 1 is called a unit vector.\r\nExample 5.2.6. 1. Let V be an inner product space and u ∈ V . Then for any scalar\r\nα, it is easy to verify that kαuk =\r\n\f\r\n\fα\r\n\f\r\n\f · kuk.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/8905994f-8610-4cf4-bc44-de294d4d7381.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ad3ae76c2d20c3c7f015823f1fe6acfec1112bdee10bd33ac536a5c6645b87b8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 576
      },
      {
        "segments": [
          {
            "segment_id": "fff4aa06-039f-4a80-a510-713c24369928",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 115,
            "page_width": 612,
            "page_height": 792,
            "content": "5.2. DEFINITION AND BASIC PROPERTIES 115\r\n2. Let u\r\nt = (1, −1, 2, −3) ∈ R4\r\n. Then kuk =\r\n√\r\n1 + 1 + 4 + 9 = √15. Thus, √\r\n1\r\n15u and\r\n− √\r\n1\r\n15u are vectors of norm 1 in the vector subspace L(u) of R\r\n4\r\n. Or equivalently,\r\n√\r\n1\r\n15u is a unit vector in the direction of u.\r\nExercise 5.2.7. 1. Let u\r\nt = (−1, 1, 2, 3, 7) ∈ R5\r\n. Find all α ∈ R such that kαuk = 1.\r\n2. Let u\r\nt = (−1, 1, 2, 3, 7) ∈ C5\r\n. Find all α ∈ C such that kαuk = 1.\r\n3. Prove that kx + yk\r\n2 + kx − yk2 = 2\r\nkxk\r\n2 + kyk2\r\n\u0001\r\n, for all x, y ∈ R\r\nn\r\n. This equality\r\nis commonly known as the Parallelogram Law as in a parallelogram the sum of\r\nsquare of the lengths of the diagonals equals twice the sum of squares of the lengths\r\nof the sides.\r\n4. Prove that for any two continuous functions f(x), g(x) ∈ C([−1, 1]), the map hf(x), g(x)i = R 1\r\n−1\r\nf(x) · g(x)dx defines an inner product in C([−1, 1]).\r\n5. Fix an ordered basis B = (u1, . . . , un) of a complex vector space V . Prove that h , i\r\ndefined by hu, vi =\r\nPn\r\ni=1\r\naibi, whenever [u]B = (a1, . . . , an)\r\nt and [v]B = (b1, . . . , bn)t\r\nis\r\nindeed an inner product in V .\r\nA very useful and a fundamental inequality concerning the inner product is due to\r\nCauchy and Schwarz. The next theorem gives the statement and a proof of this inequality.\r\nTheorem 5.2.8 (Cauchy-Bunyakovskii-Schwartz inequality). Let V (F) be an inner product\r\nspace. Then for any u, v ∈ V\r\n|hu, vi| ≤ kuk kvk. (5.2.1)\r\nEquality holds in Equation (5.2.1) if and only if the vectors u and v are linearly dependent.\r\nFurthermore, if u 6= 0, then in this case v = hv,\r\nu\r\nkuk\r\ni\r\nu\r\nkuk\r\n.\r\nProof. If u = 0, then the inequality (5.2.1) holds trivially. Hence, let u 6= 0. Also, by\r\nthe third property of inner product, hλu + v, λu + vi ≥ 0 for all λ ∈ F. In particular, for\r\nλ = −\r\nhv, ui\r\nkuk\r\n2\r\n,\r\n0 ≤ hλu + v, λu + vi = λλkuk\r\n2 + λhu, vi + λhv, ui + kvk2\r\n=\r\nhv, ui\r\nkuk\r\n2\r\nhv, ui\r\nkuk\r\n2\r\nkuk\r\n2 −\r\nhv, ui\r\nkuk\r\n2\r\nhu, vi − hv, ui\r\nkuk\r\n2\r\nhv, ui + kvk\r\n2\r\n= kvk\r\n2 −\r\n|hv, ui|2\r\nkuk\r\n2\r\n.\r\nOr, in other words |hv, ui|2 ≤ kuk\r\n2kvk2 and the proof of the inequality is over.\r\nIf u 6= 0 then hλu + v, λu + vi = 0 if and only of λu + v = 0. Hence, equality holds\r\nin (5.2.1) if and only if λ = −\r\nhv, ui\r\nkuk\r\n2\r\n. That is, u and v are linearly dependent and in this\r\ncase v =\r\nD\r\nv,\r\nu\r\nkuk\r\nE\r\nu\r\nkuk\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fff4aa06-039f-4a80-a510-713c24369928.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a36fb09a17de3d782a0be359a87e55e0dab3c3a420f689ca38eea111af372240",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 540
      },
      {
        "segments": [
          {
            "segment_id": "fff4aa06-039f-4a80-a510-713c24369928",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 115,
            "page_width": 612,
            "page_height": 792,
            "content": "5.2. DEFINITION AND BASIC PROPERTIES 115\r\n2. Let u\r\nt = (1, −1, 2, −3) ∈ R4\r\n. Then kuk =\r\n√\r\n1 + 1 + 4 + 9 = √15. Thus, √\r\n1\r\n15u and\r\n− √\r\n1\r\n15u are vectors of norm 1 in the vector subspace L(u) of R\r\n4\r\n. Or equivalently,\r\n√\r\n1\r\n15u is a unit vector in the direction of u.\r\nExercise 5.2.7. 1. Let u\r\nt = (−1, 1, 2, 3, 7) ∈ R5\r\n. Find all α ∈ R such that kαuk = 1.\r\n2. Let u\r\nt = (−1, 1, 2, 3, 7) ∈ C5\r\n. Find all α ∈ C such that kαuk = 1.\r\n3. Prove that kx + yk\r\n2 + kx − yk2 = 2\r\nkxk\r\n2 + kyk2\r\n\u0001\r\n, for all x, y ∈ R\r\nn\r\n. This equality\r\nis commonly known as the Parallelogram Law as in a parallelogram the sum of\r\nsquare of the lengths of the diagonals equals twice the sum of squares of the lengths\r\nof the sides.\r\n4. Prove that for any two continuous functions f(x), g(x) ∈ C([−1, 1]), the map hf(x), g(x)i = R 1\r\n−1\r\nf(x) · g(x)dx defines an inner product in C([−1, 1]).\r\n5. Fix an ordered basis B = (u1, . . . , un) of a complex vector space V . Prove that h , i\r\ndefined by hu, vi =\r\nPn\r\ni=1\r\naibi, whenever [u]B = (a1, . . . , an)\r\nt and [v]B = (b1, . . . , bn)t\r\nis\r\nindeed an inner product in V .\r\nA very useful and a fundamental inequality concerning the inner product is due to\r\nCauchy and Schwarz. The next theorem gives the statement and a proof of this inequality.\r\nTheorem 5.2.8 (Cauchy-Bunyakovskii-Schwartz inequality). Let V (F) be an inner product\r\nspace. Then for any u, v ∈ V\r\n|hu, vi| ≤ kuk kvk. (5.2.1)\r\nEquality holds in Equation (5.2.1) if and only if the vectors u and v are linearly dependent.\r\nFurthermore, if u 6= 0, then in this case v = hv,\r\nu\r\nkuk\r\ni\r\nu\r\nkuk\r\n.\r\nProof. If u = 0, then the inequality (5.2.1) holds trivially. Hence, let u 6= 0. Also, by\r\nthe third property of inner product, hλu + v, λu + vi ≥ 0 for all λ ∈ F. In particular, for\r\nλ = −\r\nhv, ui\r\nkuk\r\n2\r\n,\r\n0 ≤ hλu + v, λu + vi = λλkuk\r\n2 + λhu, vi + λhv, ui + kvk2\r\n=\r\nhv, ui\r\nkuk\r\n2\r\nhv, ui\r\nkuk\r\n2\r\nkuk\r\n2 −\r\nhv, ui\r\nkuk\r\n2\r\nhu, vi − hv, ui\r\nkuk\r\n2\r\nhv, ui + kvk\r\n2\r\n= kvk\r\n2 −\r\n|hv, ui|2\r\nkuk\r\n2\r\n.\r\nOr, in other words |hv, ui|2 ≤ kuk\r\n2kvk2 and the proof of the inequality is over.\r\nIf u 6= 0 then hλu + v, λu + vi = 0 if and only of λu + v = 0. Hence, equality holds\r\nin (5.2.1) if and only if λ = −\r\nhv, ui\r\nkuk\r\n2\r\n. That is, u and v are linearly dependent and in this\r\ncase v =\r\nD\r\nv,\r\nu\r\nkuk\r\nE\r\nu\r\nkuk\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fff4aa06-039f-4a80-a510-713c24369928.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a36fb09a17de3d782a0be359a87e55e0dab3c3a420f689ca38eea111af372240",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 540
      },
      {
        "segments": [
          {
            "segment_id": "058fb90c-b5fd-455e-9306-8f22b59f1f99",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 116,
            "page_width": 612,
            "page_height": 792,
            "content": "116 CHAPTER 5. INNER PRODUCT SPACES\r\nLet V be a real vector space. Then for every u, v ∈ V, the Cauchy-Schwarz inequality\r\n(see (5.2.1)) implies that −1 ≤\r\nhu,vi\r\nkuk kvk ≤ 1. Also, we know that cos : [0, π] −→ [−1, 1]\r\nis an one-one and onto function. We use this idea, to relate inner product with the angle\r\nbetween two vectors in an inner product space V .\r\nDefinition 5.2.9 (Angle between two vectors). Let V be a real vector space and let u, v ∈\r\nV . Suppose θ is the angle between u, v. We define\r\ncos θ =\r\nhu, vi\r\nkuk kvk\r\n.\r\n1. The real number θ, with 0 ≤ θ ≤ π, and satisfying cos θ =\r\nhu, vi\r\nkuk kvk\r\nis called the angle\r\nbetween the two vectors u and v in V.\r\nDefinition 5.2.10 (Orthogonal Vectors). Let V be a vector space.\r\n1. The vectors u and v in V are said to be orthogonal if hu, vi = 0. Orthogonality\r\ncorresponds to perpendicularity.\r\n2. A set of vectors {u1, u2, . . . , un} in V is called mutually orthogonal if hui\r\n, uj i = 0\r\nfor all 1 ≤ i 6= j ≤ n.\r\na\r\nb\r\nc\r\nA B\r\nC\r\nFigure 2: Triangle with vertices A, B and C\r\nBefore proceeding further with one more definition, recall that if ABC are vertices of\r\na triangle (see Figure 5.2) then cos(A) = b\r\n2+c2−a2\r\n2bc . We prove this as our next result.\r\nLemma 5.2.11. Let A, B and C be the sides of a triangle in a real inner product space V\r\nthen\r\ncos(A) = b\r\n2 + c2 − a2\r\n2bc .\r\nProof. Let the coordinates of the vertices A, B and C be 0, u and v, respectively. Then\r\nAB~ = u, AC~ = v and BC~ = v − u. Thus, we need to prove that\r\ncos(A) = kvk\r\n2 + kuk2 − kv − uk2\r\n2kvkkuk\r\n.\r\nNow, using the properties of an inner product and Definition 5.2.9, it follows that\r\nkvk\r\n2 + kuk2 − kv − uk2 = 2 hu, vi = 2 kvkkuk cos(A).\r\nThus, the required result follows.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/058fb90c-b5fd-455e-9306-8f22b59f1f99.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0f02da3d8ab3702856ced1ea62f3872bb78ef071c6cf4610deafffa5bb177b13",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 371
      },
      {
        "segments": [
          {
            "segment_id": "18cdd6c7-6acd-4305-b66d-0133ee006815",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 117,
            "page_width": 612,
            "page_height": 792,
            "content": "5.2. DEFINITION AND BASIC PROPERTIES 117\r\nDefinition 5.2.12 (Orthogonal Complement). Let W be a subset of a vector space V with\r\ninner product h , i. Then the orthogonal complement of W in V , denoted W⊥, is defined\r\nby\r\nW⊥ = {v ∈ V : hv, wi = 0, for all w ∈ W}.\r\nExercise 5.2.13. Let W be a subset of a vector space V . Then prove that W⊥ is a subspace\r\nof V .\r\nExample 5.2.14. 1. Let R\r\n4\r\nbe endowed with the standard inner product. Fix two vec\u0002tors u\r\nt = (1, 1, 1, 1), vt = (1, 1, −1, 0) ∈ R4\r\n. Determine two vectors z and w such\r\nthat u = z + w, z is parallel to v and w is orthogonal to v.\r\nSolution: Let z\r\nt = kvt = (k, k, −k, 0), for some k ∈ R and let wt = (a, b, c, d). As\r\nw is orthogonal to v, hw, vi = 0 and hence a + b − c = 0. Thus, c = a + b and\r\n(1, 1, 1, 1) = u\r\nt = zt + wt = (k, k, −k, 0) + (a, b, a + b, d).\r\nComparing the corresponding coordinates, we get\r\nd = 1, a + k = 1, b + k = 1 and a + b − k = 1.\r\nSolving for a, b and k gives a = b = 2\r\n3\r\nand k = 1\r\n3\r\n. Thus, z\r\nt = 1\r\n3\r\n(1, 1, −1, 0) and\r\nwt =\r\n1\r\n3\r\n(2, 2, 4, 3).\r\n2. Let R\r\n3\r\nbe endowed with the standard inner product and let P = (1, 1, 1), Q = (2, 1, 3)\r\nand R = (−1, 1, 2) be three vertices of a triangle in R\r\n3\r\n. Compute the angle between\r\nthe sides P Q and P R.\r\nSolution: Method 1: The sides are represented by the vectors\r\nP Q~ = (2, 1, 3) − (1, 1, 1) = (1, 0, 2), P R~ = (−2, 0, 1) and RQ~ = (−3, 0, −1).\r\nAs hP Q, ~ P R~ i = 0, the angle between the sides P Q and P R is\r\nπ\r\n2\r\n.\r\nMethod 2: kP Qk =\r\n√\r\n5, kP Rk =\r\n√\r\n5 and kQRk =\r\n√\r\n10. As\r\nkQRk\r\n2 = kP Qk2 + kP Rk2\r\n,\r\nby Pythagoras theorem, the angle between the sides P Q and P R is\r\nπ\r\n2\r\n.\r\nWe end this section by stating and proving the fundamental theorem of linear algebra.\r\nTo do this, recall that for a matrix A ∈ Mn(C), A∗ denotes the conjugate transpose of\r\nA, N (A) = {v ∈ C\r\nn\r\n: Av = 0} denotes the null space of A and R(A) = {Av : v ∈ C\r\nn}\r\ndenotes the range space of A. The readers are also advised to go through Theorem 3.3.25\r\n(the rank-nullity theorem for matrices) before proceeding further as the first part is stated\r\nand proved there.\r\nTheorem 5.2.15 (Fundamental Theorem of Linear Algebra). Let A be an n × n matrix\r\nwith complex entries and let N (A) and R(A) be defined as above. Then",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/18cdd6c7-6acd-4305-b66d-0133ee006815.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a7521c2e2760cbd0826b295ff93086019fb41f8faea128a8b06c666ff3f97b09",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 545
      },
      {
        "segments": [
          {
            "segment_id": "18cdd6c7-6acd-4305-b66d-0133ee006815",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 117,
            "page_width": 612,
            "page_height": 792,
            "content": "5.2. DEFINITION AND BASIC PROPERTIES 117\r\nDefinition 5.2.12 (Orthogonal Complement). Let W be a subset of a vector space V with\r\ninner product h , i. Then the orthogonal complement of W in V , denoted W⊥, is defined\r\nby\r\nW⊥ = {v ∈ V : hv, wi = 0, for all w ∈ W}.\r\nExercise 5.2.13. Let W be a subset of a vector space V . Then prove that W⊥ is a subspace\r\nof V .\r\nExample 5.2.14. 1. Let R\r\n4\r\nbe endowed with the standard inner product. Fix two vec\u0002tors u\r\nt = (1, 1, 1, 1), vt = (1, 1, −1, 0) ∈ R4\r\n. Determine two vectors z and w such\r\nthat u = z + w, z is parallel to v and w is orthogonal to v.\r\nSolution: Let z\r\nt = kvt = (k, k, −k, 0), for some k ∈ R and let wt = (a, b, c, d). As\r\nw is orthogonal to v, hw, vi = 0 and hence a + b − c = 0. Thus, c = a + b and\r\n(1, 1, 1, 1) = u\r\nt = zt + wt = (k, k, −k, 0) + (a, b, a + b, d).\r\nComparing the corresponding coordinates, we get\r\nd = 1, a + k = 1, b + k = 1 and a + b − k = 1.\r\nSolving for a, b and k gives a = b = 2\r\n3\r\nand k = 1\r\n3\r\n. Thus, z\r\nt = 1\r\n3\r\n(1, 1, −1, 0) and\r\nwt =\r\n1\r\n3\r\n(2, 2, 4, 3).\r\n2. Let R\r\n3\r\nbe endowed with the standard inner product and let P = (1, 1, 1), Q = (2, 1, 3)\r\nand R = (−1, 1, 2) be three vertices of a triangle in R\r\n3\r\n. Compute the angle between\r\nthe sides P Q and P R.\r\nSolution: Method 1: The sides are represented by the vectors\r\nP Q~ = (2, 1, 3) − (1, 1, 1) = (1, 0, 2), P R~ = (−2, 0, 1) and RQ~ = (−3, 0, −1).\r\nAs hP Q, ~ P R~ i = 0, the angle between the sides P Q and P R is\r\nπ\r\n2\r\n.\r\nMethod 2: kP Qk =\r\n√\r\n5, kP Rk =\r\n√\r\n5 and kQRk =\r\n√\r\n10. As\r\nkQRk\r\n2 = kP Qk2 + kP Rk2\r\n,\r\nby Pythagoras theorem, the angle between the sides P Q and P R is\r\nπ\r\n2\r\n.\r\nWe end this section by stating and proving the fundamental theorem of linear algebra.\r\nTo do this, recall that for a matrix A ∈ Mn(C), A∗ denotes the conjugate transpose of\r\nA, N (A) = {v ∈ C\r\nn\r\n: Av = 0} denotes the null space of A and R(A) = {Av : v ∈ C\r\nn}\r\ndenotes the range space of A. The readers are also advised to go through Theorem 3.3.25\r\n(the rank-nullity theorem for matrices) before proceeding further as the first part is stated\r\nand proved there.\r\nTheorem 5.2.15 (Fundamental Theorem of Linear Algebra). Let A be an n × n matrix\r\nwith complex entries and let N (A) and R(A) be defined as above. Then",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/18cdd6c7-6acd-4305-b66d-0133ee006815.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a7521c2e2760cbd0826b295ff93086019fb41f8faea128a8b06c666ff3f97b09",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 545
      },
      {
        "segments": [
          {
            "segment_id": "4bdfd55d-f5c5-494d-9255-aaba08a2740c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 118,
            "page_width": 612,
            "page_height": 792,
            "content": "118 CHAPTER 5. INNER PRODUCT SPACES\r\n1. dim(N (A)) + dim(R(A)) = n.\r\n2. N (A) = R(A∗)\r\n\u0001⊥\r\nand N (A∗) = R(A)\r\n\u0001⊥\r\n.\r\n3. dim(R(A)) = dim(R(A∗)).\r\nProof. Part 1: Proved in Theorem 3.3.25.\r\nPart 2: We first prove that N (A) ⊂ R(A∗)\r\n⊥. Let x ∈ N (A). Then Ax = 0 and\r\n0 = hAx, ui = u\r\n∗Ax = (A∗u)∗x = hx, A∗ui\r\nfor all u ∈ C\r\nn\r\n. Thus, x ∈ R(A∗)\r\n⊥ and hence N (A) ⊂ R(A∗\r\n)\r\n⊥.\r\nWe now prove that R(A∗)\r\n⊥ ⊂ N (A). Let x ∈ R(A∗\r\n)\r\n⊥. Then for every y ∈ Cn\r\n,\r\n0 = hx, A∗yi = (A\r\n∗y)∗x = y∗\r\n(A\r\n∗\r\n)\r\n∗x = y∗Ax = hAx, yi.\r\nIn particular, for y = Ax, we get kAxk\r\n2 = 0 and hence Ax = 0. That is, x ∈ N (A). Thus,\r\nthe proof of the first equality in Part 2 is over. We omit the second equality as it proceeds\r\non the same lines as above.\r\nPart 3: Use the first two parts to get the result.\r\nHence the proof of the fundamental theorem is complete.\r\nFor more information related with the fundamental theorem of linear algebra the inter\u0002ested readers are advised to see the article “The Fundamental Theorem of Linear Algebra,\r\nGilbert Strang, The American Mathematical Monthly, Vol. 100, No. 9, Nov., 1993, pp.\r\n848 - 855.”\r\nExercise 5.2.16. 1. Answer the following questions when R\r\n3\r\nis endowed with the stan\u0002dard inner product.\r\n(a) Let u\r\nt = (1, 1, 1). Find vectors v, w ∈ R3\r\nthat are orthogonal to u and to each\r\nother.\r\n(b) Find the equation of the line that passes through the point (1, 1, −1) and is\r\nparallel to the vector (a, b, c) 6= (0, 0, 0).\r\n(c) Find the equation of the plane that contains the point (1, 1 − 1) and the vector\r\n(a, b, c) 6= (0, 0, 0) is a normal vector to the plane.\r\n(d) Find area of the parallelogram with vertices (0, 0, 0), (1, 2, −2), (2, 3, 0) and\r\n(3, 5, −2).\r\n(e) Find the equation of the plane that contains the point (2, −2, 1) and is perpen\u0002dicular to the line with parametric equations x = t − 1, y = 3t + 2, z = t + 1.\r\n(f) Let P = (3, 0, 2), Q = (1, 2, −1) and R = (2, −1, 1) be three points in R\r\n3\r\n.\r\ni. Find the area of the triangle with vertices P, Q and R.\r\nii. Find the area of the parallelogram built on vectors P Q~ and QR~ .\r\niii. Find a nonzero vector orthogonal to the triangle with vertices P, Q and R.\r\niv. Find all vectors x orthogonal to P Q~ and QR~ with kxk =\r\n√\r\n2.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/4bdfd55d-f5c5-494d-9255-aaba08a2740c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0bad4bc75cd13c3af38eb89808c60b6372d972a66d22912111cf436e48c23997",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 484
      },
      {
        "segments": [
          {
            "segment_id": "5f4d1473-fbcc-4754-9e91-9d0f9558ac87",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 119,
            "page_width": 612,
            "page_height": 792,
            "content": "5.2. DEFINITION AND BASIC PROPERTIES 119\r\nv. Choose one of the vectors x found in part 1(f)iv. Find the volume of the\r\nparallelepiped built on vectors P Q~ and QR~ and x. Do you think the volume\r\nwould be different if you choose the other vector x?\r\n(g) Find the equation of the plane that contains the lines (x, y, z) = (1, 2, −2) +\r\nt(1, 1, 0) and (x, y, z) = (1, 2, −2) + t(0, 1, 2).\r\n(h) Let u\r\nt = (1, −1, 1) and vt = (1, k, 1). Find k such that the angle between u and\r\nv is π/3.\r\n(i) Let p1 be a plane that passes through the point A = (1, 2, 3) and has nˇ = (2, −1, 1)\r\nas its normal vector. Then\r\ni. find the equation of the plane p2 which is parallel to p1 and passes through\r\nthe point (−1, 2, −3).\r\nii. calculate the distance between the planes p1 and p2.\r\n(j) In the parallelogram ABCD, ABkDC and ADkBC and A = (−2, 1, 3), B =\r\n(−1, 2, 2), C = (−3, 1, 5). Find\r\ni. the coordinates of the point D,\r\nii. the cosine of the angle BCD.\r\niii. the area of the triangle ABC\r\niv. the volume of the parallelepiped determined by the vectors AB, AD and the\r\nvector (0, 0, −7).\r\n(k) Find the equation of a plane that contains the point (1, 1, 2) and is orthogonal\r\nto the line with parametric equation x = 2 + t, y = 3 and z = 1 − t.\r\n(l) Find a parametric equation of a line that passes through the point (1, −2, 1) and\r\nis orthogonal to the plane x + 3y + 2z = 1.\r\n2. Let {e\r\nt\r\n1\r\n, e\r\nt\r\n2\r\n, . . . , e\r\nt\r\nn} be the standard basis of R\r\nn\r\n. Then prove that with respect to the\r\nstandard inner product on R\r\nn\r\n, the vectors ei satisfy the following:\r\n(a) keik = 1 for 1 ≤ i ≤ n.\r\n(b) hei\r\n, ej i = 0 for 1 ≤ i 6= j ≤ n.\r\n3. Let x\r\nt = (x1, x2), yt = (y1, y2) ∈ R2\r\n. Then hx, yi = 4x1y1 − x1y2 − x2y1 + 2x2y2\r\ndefines an inner product. Use this inner product to find\r\n(a) the angle between e\r\nt\r\n1 = (1, 0) and e\r\nt\r\n2 = (0, 1).\r\n(b) v ∈ R\r\n2\r\nsuch that hv,(1, 0)ti = 0.\r\n(c) vectors x\r\nt\r\n, y\r\nt ∈ R2\r\nsuch that kxk = kyk = 1 and hx, yi = 0.\r\n4. Does there exist an inner product in R\r\n2\r\nsuch that\r\nk(1, 2)k = k(2, −1)k = 1 and h(1, 2), (2, −1)i = 0?",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5f4d1473-fbcc-4754-9e91-9d0f9558ac87.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=26341e65d18acf1feb9ed5e8e474566e2822cb08349644258e25fdbd02263f28",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 476
      },
      {
        "segments": [
          {
            "segment_id": "ddb18e73-a383-454d-982d-bdc12577830e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 120,
            "page_width": 612,
            "page_height": 792,
            "content": "120 CHAPTER 5. INNER PRODUCT SPACES\r\n[Hint: Consider a symmetric matrix A =\r\n\"\r\na b\r\nb c#\r\n. Define hx, yi = y\r\ntAx. Use the\r\ngiven conditions to get a linear system of 3 equations in the unknowns a, b, c. Solve\r\nthis system.]\r\n5. Let W = {(x, y, z) ∈ R\r\n3\r\n: x + y + z = 0}. Find a basis of W⊥.\r\n6. Let W be a subspace of a finite dimensional inner product space V . Prove that\r\n(W⊥)\r\n⊥ = W.\r\n7. Let x\r\nt = (x1, x2, x3), yt = (y1, y2, y3) ∈ R3\r\n. Show that\r\nhx, yi = 10x1y1 + 3x1y2 + 3x2y1 + 4x2y2 + x2y3 + x3y2 + 3x3y3\r\nis an inner product in R\r\n3\r\n(R). With respect to this inner product, find the angle between\r\nthe vectors (1, 1, 1) and (2, −5, 2).\r\n8. Recall the inner product space Mn×n(R) (see Example 5.2.3.6). Determine W⊥ for\r\nthe subspace W = {A ∈ Mn×n(R) : At = A}.\r\n9. Prove that hf(x), g(x)i =\r\nRπ\r\n−π\r\nf(x) · g(x)dx defines an inner product in C[−π, π].\r\nDefine 1(x) = 1 for all x ∈ [−π, π]. Prove that\r\nS = {1} ∪ {cos(mx) : m ≥ 1} ∪ {sin(nx) : n ≥ 1}\r\nis a linearly independent subset of C[−π, π].\r\n10. Let V be an inner product space. Prove the triangle inequality\r\nku + vk ≤ kuk + kvk for every u, v ∈ V.\r\n11. Let z1, z2, . . . , zn ∈ C. Use the Cauchy-Schwarz inequality to prove that\r\n|z1 + z2 + · · · + zn| ≤ pn(|z1|\r\n2 + |z2|2 + · · · + |zn|2).\r\nWhen does the equality hold?\r\n12. Let x, y ∈ R\r\nn\r\n. Prove the following:\r\n(a) hx, yi = 0 ⇐⇒ kx − yk\r\n2 = kxk2 + kyk2\r\n(Pythagoras Theorem).\r\n(b) kxk = kyk ⇐⇒ hx + y, x − yi = 0 (x and y form adjacent sides of a rhombus\r\nas the diagonals x + y and x − y are orthogonal).\r\n(c) 4hx, yi = kx + yk\r\n2 − kx − yk2\r\n(polarization identity).\r\nAre the above results true if x, y ∈ C\r\nn\r\n(C)?\r\n13. Let x, y ∈ C\r\nn\r\n(C). Prove that\r\n(a) 4hx, yi = kx + yk\r\n2 − kx − yk2 + ikx + iyk2 − ikx − iyk2\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ddb18e73-a383-454d-982d-bdc12577830e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cde07ac13ea6541343c67b1c19bb945afee7b80dec629be0d5ef3ae388969402",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 414
      },
      {
        "segments": [
          {
            "segment_id": "0aee1924-3470-4b34-b88b-baa13ceaca94",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 121,
            "page_width": 612,
            "page_height": 792,
            "content": "5.2. DEFINITION AND BASIC PROPERTIES 121\r\n(b) If x 6= 0 then kx + ixk\r\n2 = kxk2 + kixk2\r\n, even though hx, ixi 6= 0.\r\n(c) hx, yi = 0 whenever kx + yk\r\n2 = kxk2 + kyk2 and kx + iyk2 = kxk2 + kiyk2\r\n.\r\n14. Let h , i denote the standard inner product on C\r\nn\r\n(C) and let A ∈ Mn(C). That is,\r\nhx, yi = x\r\n∗y for all xt\r\n, y\r\nt ∈ Cn\r\n. Prove that hAx, yi = hx, A∗yi for all x, y ∈ C\r\nn\r\n.\r\n15. Let (V,h , i) be an n-dimensional inner product space and let u ∈ V be a fixed vector\r\nwith kuk = 1. Then give reasons for the following statements.\r\n(a) Let S\r\n⊥ = {v ∈ V : hv, ui = 0}. Then dim(S⊥) = n − 1.\r\n(b) Let 0 6= α ∈ F. Then S = {v ∈ V : hv, ui = α} is not a subspace of V.\r\n(c) Let v ∈ V . Then v = v0 + αu for a vector v0 ∈ S\r\n⊥ and a scalar α. That is,\r\nV = L(u, S⊥).\r\n5.2.1 Basic Results on Orthogonal Vectors\r\nWe start this subsection with the definition of an orthonormal set. Then a theorem is\r\nproved that implies that the coordinates of a vector with respect to an orthonormal basis\r\nare just the inner products with the basis vectors.\r\nDefinition 5.2.17 (Orthonormal Set). Let S = {v1, v2, . . . , vn} be a set of non-zero,\r\nmutually orthogonal vectors in an inner product space V . Then S is called an orthonormal\r\nset if kvik = 1 for 1 ≤ i ≤ n. If S is also a basis of V then S is called an orthonormal\r\nbasis of V.\r\nExample 5.2.18. 1. Consider R\r\n2 with the standard inner product. Then a few or\u0002thonormal sets in R\r\n2 are \b\r\n(1, 0),(0, 1)\t,\r\n\b\r\n√\r\n1\r\n2\r\n(1, 1), √\r\n1\r\n2\r\n(1, −1)\tand \b\r\n√\r\n1\r\n5\r\n(2, 1), √\r\n1\r\n5\r\n(1, −2)\t.\r\n2. Let R\r\nn\r\nbe endowed with the standard inner product. Then by Exercise 5.2.16.2, the\r\nstandard ordered basis (e\r\nt\r\n1\r\n, e\r\nt\r\n2\r\n, . . . , e\r\nt\r\nn\r\n) is an orthonormal set.\r\nTheorem 5.2.19. Let V be an inner product space and let {u1, u2, . . . , un} be a set of\r\nnon-zero, mutually orthogonal vectors of V.\r\n1. Then the set {u1, u2, . . . , un} is linearly independent.\r\n2. Let v =\r\nPn\r\ni=1\r\nαiui ∈ V . Then kvk\r\n2 = k\r\nPn\r\ni=1\r\nαiuik\r\n2 =\r\nPn\r\ni=1\r\n|αi|\r\n2kuik2\r\n;\r\n3. Let v =\r\nPn\r\ni=1\r\nαiui. If kuik = 1 for 1 ≤ i ≤ n then αi = hv, uii for 1 ≤ i ≤ n. That is,\r\nv =\r\nXn\r\ni=1\r\nhv, uiiui and kvk\r\n2 =\r\nXn\r\ni=1\r\n|hv, uii|2.\r\n4. Let dim(V ) = n. Then hv, uii = 0 for all i = 1, 2, . . . , n if and only if v = 0.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0aee1924-3470-4b34-b88b-baa13ceaca94.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3913aebaaef42af40ddf57eca2ed5a70b49dc5f6791836f1ea1599a694fb5a3e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "0aee1924-3470-4b34-b88b-baa13ceaca94",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 121,
            "page_width": 612,
            "page_height": 792,
            "content": "5.2. DEFINITION AND BASIC PROPERTIES 121\r\n(b) If x 6= 0 then kx + ixk\r\n2 = kxk2 + kixk2\r\n, even though hx, ixi 6= 0.\r\n(c) hx, yi = 0 whenever kx + yk\r\n2 = kxk2 + kyk2 and kx + iyk2 = kxk2 + kiyk2\r\n.\r\n14. Let h , i denote the standard inner product on C\r\nn\r\n(C) and let A ∈ Mn(C). That is,\r\nhx, yi = x\r\n∗y for all xt\r\n, y\r\nt ∈ Cn\r\n. Prove that hAx, yi = hx, A∗yi for all x, y ∈ C\r\nn\r\n.\r\n15. Let (V,h , i) be an n-dimensional inner product space and let u ∈ V be a fixed vector\r\nwith kuk = 1. Then give reasons for the following statements.\r\n(a) Let S\r\n⊥ = {v ∈ V : hv, ui = 0}. Then dim(S⊥) = n − 1.\r\n(b) Let 0 6= α ∈ F. Then S = {v ∈ V : hv, ui = α} is not a subspace of V.\r\n(c) Let v ∈ V . Then v = v0 + αu for a vector v0 ∈ S\r\n⊥ and a scalar α. That is,\r\nV = L(u, S⊥).\r\n5.2.1 Basic Results on Orthogonal Vectors\r\nWe start this subsection with the definition of an orthonormal set. Then a theorem is\r\nproved that implies that the coordinates of a vector with respect to an orthonormal basis\r\nare just the inner products with the basis vectors.\r\nDefinition 5.2.17 (Orthonormal Set). Let S = {v1, v2, . . . , vn} be a set of non-zero,\r\nmutually orthogonal vectors in an inner product space V . Then S is called an orthonormal\r\nset if kvik = 1 for 1 ≤ i ≤ n. If S is also a basis of V then S is called an orthonormal\r\nbasis of V.\r\nExample 5.2.18. 1. Consider R\r\n2 with the standard inner product. Then a few or\u0002thonormal sets in R\r\n2 are \b\r\n(1, 0),(0, 1)\t,\r\n\b\r\n√\r\n1\r\n2\r\n(1, 1), √\r\n1\r\n2\r\n(1, −1)\tand \b\r\n√\r\n1\r\n5\r\n(2, 1), √\r\n1\r\n5\r\n(1, −2)\t.\r\n2. Let R\r\nn\r\nbe endowed with the standard inner product. Then by Exercise 5.2.16.2, the\r\nstandard ordered basis (e\r\nt\r\n1\r\n, e\r\nt\r\n2\r\n, . . . , e\r\nt\r\nn\r\n) is an orthonormal set.\r\nTheorem 5.2.19. Let V be an inner product space and let {u1, u2, . . . , un} be a set of\r\nnon-zero, mutually orthogonal vectors of V.\r\n1. Then the set {u1, u2, . . . , un} is linearly independent.\r\n2. Let v =\r\nPn\r\ni=1\r\nαiui ∈ V . Then kvk\r\n2 = k\r\nPn\r\ni=1\r\nαiuik\r\n2 =\r\nPn\r\ni=1\r\n|αi|\r\n2kuik2\r\n;\r\n3. Let v =\r\nPn\r\ni=1\r\nαiui. If kuik = 1 for 1 ≤ i ≤ n then αi = hv, uii for 1 ≤ i ≤ n. That is,\r\nv =\r\nXn\r\ni=1\r\nhv, uiiui and kvk\r\n2 =\r\nXn\r\ni=1\r\n|hv, uii|2.\r\n4. Let dim(V ) = n. Then hv, uii = 0 for all i = 1, 2, . . . , n if and only if v = 0.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/0aee1924-3470-4b34-b88b-baa13ceaca94.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3913aebaaef42af40ddf57eca2ed5a70b49dc5f6791836f1ea1599a694fb5a3e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 539
      },
      {
        "segments": [
          {
            "segment_id": "9722419f-3d85-4650-8162-50cae3e13a1a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 122,
            "page_width": 612,
            "page_height": 792,
            "content": "122 CHAPTER 5. INNER PRODUCT SPACES\r\nProof. Consider the linear system\r\nc1u1 + c2u2 + · · · + cnun = 0 (5.2.2)\r\nin the unknowns c1, c2, . . . , cn. As h0, ui = 0 for each u ∈ V and huj , uii = 0 for all j 6= i,\r\nwe have\r\n0 = h0, uii = hc1u1 + c2u2 + · · · + cnun, uii =\r\nXn\r\nj=1\r\ncj huj , uii = cihui, uii.\r\nAs ui 6= 0, hui, uii 6= 0 and therefore ci = 0 for 1 ≤ i ≤ n. Thus, the linear system (5.2.2)\r\nhas only the trivial solution. Hence, the proof of Part 1 is complete.\r\nFor Part 2, we use a similar argument to get\r\nk\r\nXn\r\ni=1\r\nαiuik\r\n2 =\r\n*Xn\r\ni=1\r\nαiui,\r\nXn\r\ni=1\r\nαiui\r\n+\r\n=\r\nXn\r\ni=1\r\nαi\r\n*\r\nui\r\n,\r\nXn\r\nj=1\r\nαjuj\r\n+\r\n=\r\nXn\r\ni=1\r\nαi\r\nXn\r\nj=1\r\nαj hui, uj i =\r\nXn\r\ni=1\r\nαiαi hui, uii =\r\nXn\r\ni=1\r\n|αi|\r\n2\r\nkuik\r\n2\r\n.\r\nNote that hv, uii =\r\nDPn\r\nj=1 αjuj , ui\r\nE\r\n=\r\nPn\r\nj=1 αj huj , uii = αj . Thus, the proof of Part 3\r\nis complete.\r\nPart 4 directly follows using Part 3 as the set {u1, u2, . . . , un} is a basis of V. Therefore,\r\nwe have obtained the required result.\r\nIn view of Theorem 5.2.19, we inquire into the question of extracting an orthonormal\r\nbasis from a given basis. In the next section, we describe a process (called the Gram\u0002Schmidt Orthogonalization process) that generates an orthonormal set from a given set\r\ncontaining finitely many vectors.\r\nRemark 5.2.20. The last two parts of Theorem 5.2.19 can be rephrased as follows:\r\nLet B =\r\n\r\nv1, . . . , vn\r\n\u0001\r\nbe an ordered orthonormal basis of an inner product space V and let\r\nu ∈ V . Then\r\n[u]B = (hu, v1i,hu, v2i, . . . ,hu, vni)\r\nt\r\n.\r\nExercise 5.2.21. 1. Let B =\r\n\r\n√\r\n1\r\n2\r\n(1, 1), √\r\n1\r\n2\r\n(1, −1)\u0001be an ordered basis of R\r\n2\r\n. Deter\u0002mine [(2, 3)]B. Also, compute [(x, y)]B.\r\n2. Let B =\r\n\r\n√\r\n1\r\n3\r\n(1, 1, 1), √\r\n1\r\n2\r\n(1, −1, 0), √\r\n1\r\n6\r\n(1, 1, −2),\r\n\u0001\r\nbe an ordered basis of R\r\n3\r\n. Determine\r\n[(2, 3, 1)]B. Also, compute [(x, y, z)]B.\r\n3. Let u\r\nt = (u1, u2, u3), vt = (v1, v2, v3) be two vectors in R3\r\n. Then recall that their\r\ncross product, denoted u × v, equals\r\nu\r\nt × vt = (u2v3 − u3v2, u3v1 − u1v3, u1v2 − u2v1).\r\nUse this to find an orthonormal basis of R\r\n3\r\ncontaining the vector √\r\n1\r\n6\r\n(1, 2, 1).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9722419f-3d85-4650-8162-50cae3e13a1a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bd43929c309ddf424668364bd0e4de67766222eb5b207391f0417fd829bdea3c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 466
      },
      {
        "segments": [
          {
            "segment_id": "68d71292-0d99-41f7-a114-e44cbd7c430c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 123,
            "page_width": 612,
            "page_height": 792,
            "content": "5.3. GRAM-SCHMIDT ORTHOGONALIZATION PROCESS 123\r\n4. Let u\r\nt = (1, −1, −2). Find vectors vt\r\n, wt ∈ R\r\n3\r\nsuch that v and w are orthogonal to\r\nu and to each other as well.\r\n5. Let A be an n × n orthogonal matrix. Prove that the rows/columns of A form an\r\northonormal basis of R\r\nn\r\n.\r\n6. Let A be an n × n unitary matrix. Prove that the rows/columns of A form an or\u0002thonormal basis of C\r\nn\r\n.\r\n7. Let {u\r\nt\r\n1\r\n, u\r\nt\r\n2\r\n, . . . , u\r\nt\r\nn} be an orthonormal basis of R\r\nn\r\n. Prove that the n × n matrix\r\nA = [u1, u2, . . . , un] is an orthogonal matrix.\r\n5.3 Gram-Schmidt Orthogonalization Process\r\nSuppose we are given two non-zero vectors u and v in a plane. Then in many instances, we\r\nneed to decompose the vector v into two components, say y and z, such that y is a vector\r\nparallel to u and z is a vector perpendicular (orthogonal) to u. We do this as follows (see\r\nFigure 5.3):\r\nLet uˆ =\r\nu\r\nkuk\r\n. Then uˆ is a unit vector in the direction of u. Also, using trigonometry, we\r\nknow that cos(θ) = kOQ~ k\r\nkOP~ k\r\nand hence kOQ~ k = kOP~ k cos(θ). Or using Definition 5.2.9,\r\nkOQ~ k = kvk\r\nhv, ui\r\nkvk kuk\r\n=\r\nhv, ui\r\nkuk\r\n,\r\nwhere we need to take the absolute value of the right hand side expression as the length\r\nof a vector is always a positive quantity. Thus, we get\r\nOQ~ = kOQ~ k uˆ = hv,\r\nu\r\nkuk\r\ni\r\nu\r\nkuk\r\n.\r\nThus, we see that y = OQ~ = hv,\r\nu\r\nkuk\r\ni\r\nu\r\nkuk\r\nand z = v − hv,\r\nu\r\nkuk\r\ni\r\nu\r\nkuk\r\n. It is easy to verify that\r\nv = y + z, y is parallel to u and z is orthogonal to u. In literature, the vector y = OQ~\r\nis often called the orthogonal projection of the vector v on u and is denoted by Proju(v).\r\nThus,\r\nProju(v) = hv,\r\nu\r\nkuk\r\ni\r\nu\r\nkuk\r\nand kProju(v)k = kOQ~ k =\r\n\f\r\n\f\r\n\f\r\n\f\r\nhv, ui\r\nkuk\r\n\f\r\n\f\r\n\f\r\n\f\r\n. (5.3.1)\r\nMoreover, the distance of the vector u from the point P equals kOR~ k = kP Q~ k =\r\nkv − hv,\r\nu\r\nkuk\r\ni\r\nu\r\nkuk\r\nk.\r\nOR~ = v −\r\nhv,ui\r\nkuk\r\n2 u\r\nOQ~ =\r\nhv,ui\r\nkuk\r\n2 u\r\nR\r\nθ O\r\nQ\r\nP\r\nu\r\nv\r\nFigure 3: Decomposition of vector v",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/68d71292-0d99-41f7-a114-e44cbd7c430c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2f329f843546c1b9b647f5d136696affcc5ebe0352cab3a7c0dca7f3f1885911",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 432
      },
      {
        "segments": [
          {
            "segment_id": "5c6d0a6f-ba09-431a-8e4e-c9ee93082726",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 124,
            "page_width": 612,
            "page_height": 792,
            "content": "124 CHAPTER 5. INNER PRODUCT SPACES\r\nAlso, note that uˆ is a unit vector in the direction of u and zˆ =\r\nz\r\nkzk\r\nis a unit vector\r\northogonal to uˆ. This idea is generalized to study the Gram-Schmidt Orthogonalization\r\nprocess which is given as the next result. Before stating this result, we look at the following\r\nexample to understand the process.\r\nExample 5.3.1. 1. In Example 5.2.14.1, we note that Projv(u) = (u · v)\r\nv\r\nkvk\r\n2\r\nis par\u0002allel to v and u − Projv\r\n(u) is orthogonal to v. Thus,\r\n~z = Projv\r\n(u) = 1\r\n3\r\n(1, 1, −1, 0)tand w~ = (1, 1, 1, 1)t − ~z =\r\n1\r\n3\r\n(2, 2, 4, 3)t.\r\n2. Let u\r\nt = (1, 1, 1, 1), vt = (1, 1, −1, 0) and wt = (1, 1, 0, −1) be three vectors in R4\r\n.\r\nWrite v = v1 + v2 where v1 is parallel to u and v2 is orthogonal to u. Also, write\r\nw = w1 + w2 + w3 such that w1 is parallel to u, w2 is parallel to v2 and w3 is\r\northogonal to both u and v2.\r\nSolution : Note that\r\n(a) v1 = Proju(v) = hv, ui\r\nu\r\nkuk\r\n2 =\r\n1\r\n4\r\nu =\r\n1\r\n4\r\n(1, 1, 1, 1)tis parallel to u and\r\n(b) v2 = v −\r\n1\r\n4\r\nu =\r\n1\r\n4\r\n(3, 3, −5, −1)tis orthogonal to u.\r\nNote that Proju(w) is parallel to u and Projv2(w) is parallel to v2. Hence, we have\r\n(a) w1 = Proju(w) = hw, ui\r\nu\r\nkuk\r\n2 =\r\n1\r\n4\r\nu =\r\n1\r\n4\r\n(1, 1, 1, 1)tis parallel to u,\r\n(b) w2 = Projv2(w) = hw, v2i\r\nv2\r\nkv2k\r\n2 =\r\n7\r\n44 (3, 3, −5, −1)t\r\nis parallel to v2 and\r\n(c) w3 = w − w1 − w2 =\r\n3\r\n11 (1, 1, 2, −4)t\r\nis orthogonal to both u and v2.\r\nThat is, from the given vector subtract all the orthogonal components that are obtained\r\nas orthogonal projections. If this new vector is non-zero then this vector is orthogonal\r\nto the previous ones.\r\nTheorem 5.3.2 (Gram-Schmidt Orthogonalization Process). Let V be an inner product\r\nspace. Suppose {u1, u2, . . . , un} is a set of linearly independent vectors in V. Then there\r\nexists a set {v1, v2, . . . , vn} of vectors in V satisfying the following:\r\n1. kvik = 1 for 1 ≤ i ≤ n,\r\n2. hvi, vj i = 0 for 1 ≤ i 6= j ≤ n and\r\n3. L(v1, v2, . . . , vi) = L(u1, u2, . . . , ui) for 1 ≤ i ≤ n.\r\nProof. We successively define the vectors v1, v2, . . . , vn as follows.\r\nStep 1: v1 =\r\nu1\r\nku1k\r\n.\r\nStep 2: Calculate w2 = u2 − hu2, v1iv1, and let v2 =\r\nw2\r\nkw2k\r\n.\r\nStep 3: Obtain w3 = u3 − hu3, v1iv1 − hu3, v2iv2, and let v3 =\r\nw3\r\nkw3k\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5c6d0a6f-ba09-431a-8e4e-c9ee93082726.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=64f329fd8cf70a7a9e3e1d02c0e283336dd5ea26c287cd18ef00c76e5870c660",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 510
      },
      {
        "segments": [
          {
            "segment_id": "49c1f873-80a0-4159-8a97-e1e408cbe433",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 125,
            "page_width": 612,
            "page_height": 792,
            "content": "5.3. GRAM-SCHMIDT ORTHOGONALIZATION PROCESS 125\r\nStep i: In general, if v1, v2, . . . , vi−1 are already obtained, we compute\r\nwi = ui − hui, v1iv1 − hui, v2iv2 − · · · − hui, vi−1ivi−1. (5.3.2)\r\nAs the set {u1, u2, . . . , un} is linearly independent, it can be verified that kwik 6= 0\r\nand hence we define vi =\r\nwi\r\nkwik\r\n.\r\nWe prove this by induction on n, the number of linearly independent vectors. For n = 1,\r\nv1 =\r\nu1\r\nku1k\r\n. As u is an element of a linearly independent set, u1 6= 0 and thus v1 6= 0 and\r\nkv1k\r\n2 = hv1, v1i = h\r\nu1\r\nku1k\r\n,\r\nu1\r\nku1k\r\ni =\r\nhu1, u1i\r\nku1k\r\n2\r\n= 1.\r\nHence, the result holds for n = 1.\r\nLet the result hold for all k ≤ n−1. That is, suppose we are given any set of k, 1 ≤ k ≤\r\nn−1 linearly independent vectors {u1, u2, . . . , uk} of V. Then by the inductive assumption,\r\nthere exists a set {v1, v2, . . . , vk} of vectors satisfying the following:\r\n1. kvik = 1 for 1 ≤ i ≤ k,\r\n2. hvi\r\n, vj i = 0 for 1 ≤ i 6= j ≤ k, and\r\n3. L(v1, v2, . . . , vi) = L(u1, u2, . . . , ui) for 1 ≤ i ≤ k.\r\nNow, let us assume that {u1, u2, . . . , un} is a linearly independent subset of V . Then\r\nby the inductive assumption, we already have vectors v1, v2, . . . , vn−1 satisfying\r\n1. kvik = 1 for 1 ≤ i ≤ n − 1,\r\n2. hvi, vj i = 0 for 1 ≤ i 6= j ≤ n − 1, and\r\n3. L(v1, v2, . . . , vi) = L(u1, u2, . . . , ui) for 1 ≤ i ≤ n − 1.\r\nUsing (5.3.2), we define\r\nwn = un − hun, v1iv1 − hun, v2iv2 − · · · − hun, vn−1ivn−1. (5.3.3)\r\nWe first show that wn 6∈ L(v1, v2, . . . , vn−1). This will imply that wn 6= 0 and hence\r\nvn =\r\nwn\r\nkwnk\r\nis well defined. Also, kvnk = 1.\r\nOn the contrary, assume that wn ∈ L(v1, v2, . . . , vn−1). Then, by definition, there exist\r\nscalars α1, α2, . . . , αn−1, not all zero, such that\r\nwn = α1v1 + α2v2 + · · · + αn−1vn−1.\r\nSo, substituting α1v1 + α2v2 + · · · + αn−1vn−1 for wn in (5.3.3), we get\r\nun =\r\n\r\nα1 + hun, v1i\r\n\u0001\r\nv1 +\r\n\r\nα2 + hun, v2i\r\n\u0001\r\nv2 + · · · + (αn−1 + hun, vn−1i\r\n\u0001\r\nvn−1.\r\nThat is, un ∈ L(v1, v2, . . . , vn−1). But L(v1, . . . , vn−1) = L(u1, . . . , un−1) using the third\r\ninduction assumption. Hence un ∈ L(u1, . . . , un−1). A contradiction to the given assump\u0002tion that the set of vectors {u1, . . . , un} is linearly independent.\r\nAlso, it can be easily verified that hvn, vii = 0 for 1 ≤ i ≤ n−1. Hence, by the principle\r\nof mathematical induction, the proof of the theorem is complete.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/49c1f873-80a0-4159-8a97-e1e408cbe433.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=124f5270bab9a5a89ba0ec2ef910fc437ed29aa454813dcadc6f3c3082a2f530",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 563
      },
      {
        "segments": [
          {
            "segment_id": "49c1f873-80a0-4159-8a97-e1e408cbe433",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 125,
            "page_width": 612,
            "page_height": 792,
            "content": "5.3. GRAM-SCHMIDT ORTHOGONALIZATION PROCESS 125\r\nStep i: In general, if v1, v2, . . . , vi−1 are already obtained, we compute\r\nwi = ui − hui, v1iv1 − hui, v2iv2 − · · · − hui, vi−1ivi−1. (5.3.2)\r\nAs the set {u1, u2, . . . , un} is linearly independent, it can be verified that kwik 6= 0\r\nand hence we define vi =\r\nwi\r\nkwik\r\n.\r\nWe prove this by induction on n, the number of linearly independent vectors. For n = 1,\r\nv1 =\r\nu1\r\nku1k\r\n. As u is an element of a linearly independent set, u1 6= 0 and thus v1 6= 0 and\r\nkv1k\r\n2 = hv1, v1i = h\r\nu1\r\nku1k\r\n,\r\nu1\r\nku1k\r\ni =\r\nhu1, u1i\r\nku1k\r\n2\r\n= 1.\r\nHence, the result holds for n = 1.\r\nLet the result hold for all k ≤ n−1. That is, suppose we are given any set of k, 1 ≤ k ≤\r\nn−1 linearly independent vectors {u1, u2, . . . , uk} of V. Then by the inductive assumption,\r\nthere exists a set {v1, v2, . . . , vk} of vectors satisfying the following:\r\n1. kvik = 1 for 1 ≤ i ≤ k,\r\n2. hvi\r\n, vj i = 0 for 1 ≤ i 6= j ≤ k, and\r\n3. L(v1, v2, . . . , vi) = L(u1, u2, . . . , ui) for 1 ≤ i ≤ k.\r\nNow, let us assume that {u1, u2, . . . , un} is a linearly independent subset of V . Then\r\nby the inductive assumption, we already have vectors v1, v2, . . . , vn−1 satisfying\r\n1. kvik = 1 for 1 ≤ i ≤ n − 1,\r\n2. hvi, vj i = 0 for 1 ≤ i 6= j ≤ n − 1, and\r\n3. L(v1, v2, . . . , vi) = L(u1, u2, . . . , ui) for 1 ≤ i ≤ n − 1.\r\nUsing (5.3.2), we define\r\nwn = un − hun, v1iv1 − hun, v2iv2 − · · · − hun, vn−1ivn−1. (5.3.3)\r\nWe first show that wn 6∈ L(v1, v2, . . . , vn−1). This will imply that wn 6= 0 and hence\r\nvn =\r\nwn\r\nkwnk\r\nis well defined. Also, kvnk = 1.\r\nOn the contrary, assume that wn ∈ L(v1, v2, . . . , vn−1). Then, by definition, there exist\r\nscalars α1, α2, . . . , αn−1, not all zero, such that\r\nwn = α1v1 + α2v2 + · · · + αn−1vn−1.\r\nSo, substituting α1v1 + α2v2 + · · · + αn−1vn−1 for wn in (5.3.3), we get\r\nun =\r\n\r\nα1 + hun, v1i\r\n\u0001\r\nv1 +\r\n\r\nα2 + hun, v2i\r\n\u0001\r\nv2 + · · · + (αn−1 + hun, vn−1i\r\n\u0001\r\nvn−1.\r\nThat is, un ∈ L(v1, v2, . . . , vn−1). But L(v1, . . . , vn−1) = L(u1, . . . , un−1) using the third\r\ninduction assumption. Hence un ∈ L(u1, . . . , un−1). A contradiction to the given assump\u0002tion that the set of vectors {u1, . . . , un} is linearly independent.\r\nAlso, it can be easily verified that hvn, vii = 0 for 1 ≤ i ≤ n−1. Hence, by the principle\r\nof mathematical induction, the proof of the theorem is complete.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/49c1f873-80a0-4159-8a97-e1e408cbe433.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=124f5270bab9a5a89ba0ec2ef910fc437ed29aa454813dcadc6f3c3082a2f530",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 563
      },
      {
        "segments": [
          {
            "segment_id": "fbd09cff-787f-4feb-91b8-ac5472b015b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 126,
            "page_width": 612,
            "page_height": 792,
            "content": "126 CHAPTER 5. INNER PRODUCT SPACES\r\nWe illustrate the Gram-Schmidt process by the following example.\r\nExample 5.3.3. 1. Let {(1, −1, 1, 1),(1, 0, 1, 0),(0, 1, 0, 1)} ⊂ R\r\n4\r\n. Find a set {v1, v2, v3}\r\nthat is orthonormal and L( (1, −1, 1, 1),(1, 0, 1, 0),(0, 1, 0, 1) ) = L(v\r\nt\r\n1\r\n, v\r\nt\r\n2\r\n, v\r\nt\r\n3\r\n).\r\nSolution: Let u\r\nt\r\n1 = (1, 0, 1, 0), u\r\nt\r\n2 = (0, 1, 0, 1) and u\r\nt\r\n3 = (1, −1, 1, 1). Then v\r\nt\r\n1 =\r\n√\r\n1\r\n2\r\n(1, 0, 1, 0). Also, hu2, v1i = 0 and hence w2 = u2. Thus, v\r\nt\r\n2 = √\r\n1\r\n2\r\n(0, 1, 0, 1) and\r\nw3 = u3 − hu3, v1iv1 − hu3, v2iv2 = (0, −1, 0, 1)t\r\n.\r\nTherefore, v\r\nt\r\n3 = √\r\n1\r\n2\r\n(0, −1, 0, 1).\r\n2. Find an orthonormal set in R\r\n3\r\ncontaining (1, 2, 1).\r\nSolution: Let (x, y, z) ∈ R\r\n3 with \n\r\n(1, 2, 1),(x, y, z)\r\n\u000b\r\n= 0. Then x + 2y + z = 0 or\r\nequivalently, x = −2y − z. Thus,\r\n(x, y, z) = (−2y − z, y, z) = y(−2, 1, 0) + z(−1, 0, 1).\r\nObserve that the vectors (−2, 1, 0) and (−1, 0, 1) are both orthogonal to (1, 2, 1) but\r\nare not orthogonal to each other.\r\nMethod 1: Consider {√\r\n1\r\n6\r\n(1, 2, 1),(−2, 1, 0),(−1, 0, 1)} ⊂ R\r\n3 and apply the Gram\u0002Schmidt process to get the result.\r\nMethod 2: This method can be used only if the vectors are from R\r\n3\r\n. Recall that\r\nin R\r\n3\r\n, the cross product of two vectors u and v, denoted u × v, is a vector that is\r\northogonal to both the vectors u and v. Hence, the vector\r\n(1, 2, 1) × (−2, 1, 0) = (0 − 1, −2 − 0, 1 + 4) = (−1, −2, 5)\r\nis orthogonal to the vectors (1, 2, 1) and (−2, 1, 0) and hence the required orthonormal\r\nset is {√\r\n1\r\n6\r\n(1, 2, 1), √−1\r\n5\r\n(2, −1, 0), √−1\r\n30 (1, 2, −5)}.\r\nRemark 5.3.4. 1. Let V be a vector space. Then the following holds.\r\n(a) Let {u1, u2, . . . , uk} be a linearly independent subset of V. Then Gram-Schmidt\r\northogonalization process gives an orthonormal set {v1, v2, . . . , vk} of V with\r\nL(v1, v2, . . . , vi) = L(u1, u2, . . . , ui) for 1 ≤ i ≤ k.\r\n(b) Let W be a subspace of V with a basis {u1, u2, . . . , uk}. Then {v1, v2, . . . , vk}\r\nis also a basis of W.\r\n(c) Suppose {u1, u2, . . . , un} is a linearly dependent subset of V . Then there exists\r\na smallest k, 2 ≤ k ≤ n such that wk = 0.\r\nIdea of the proof: Linear dependence (see Corollary 3.2.5) implies that there\r\nexists a smallest k, 2 ≤ k ≤ n such that\r\nL(u1, u2, . . . , uk) = L(u1, u2, . . . , uk−1).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fbd09cff-787f-4feb-91b8-ac5472b015b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=83da89300a1cee99b716522ada39e674401ec05c8ce8533b08748a1af0d77735",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "fbd09cff-787f-4feb-91b8-ac5472b015b5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 126,
            "page_width": 612,
            "page_height": 792,
            "content": "126 CHAPTER 5. INNER PRODUCT SPACES\r\nWe illustrate the Gram-Schmidt process by the following example.\r\nExample 5.3.3. 1. Let {(1, −1, 1, 1),(1, 0, 1, 0),(0, 1, 0, 1)} ⊂ R\r\n4\r\n. Find a set {v1, v2, v3}\r\nthat is orthonormal and L( (1, −1, 1, 1),(1, 0, 1, 0),(0, 1, 0, 1) ) = L(v\r\nt\r\n1\r\n, v\r\nt\r\n2\r\n, v\r\nt\r\n3\r\n).\r\nSolution: Let u\r\nt\r\n1 = (1, 0, 1, 0), u\r\nt\r\n2 = (0, 1, 0, 1) and u\r\nt\r\n3 = (1, −1, 1, 1). Then v\r\nt\r\n1 =\r\n√\r\n1\r\n2\r\n(1, 0, 1, 0). Also, hu2, v1i = 0 and hence w2 = u2. Thus, v\r\nt\r\n2 = √\r\n1\r\n2\r\n(0, 1, 0, 1) and\r\nw3 = u3 − hu3, v1iv1 − hu3, v2iv2 = (0, −1, 0, 1)t\r\n.\r\nTherefore, v\r\nt\r\n3 = √\r\n1\r\n2\r\n(0, −1, 0, 1).\r\n2. Find an orthonormal set in R\r\n3\r\ncontaining (1, 2, 1).\r\nSolution: Let (x, y, z) ∈ R\r\n3 with \n\r\n(1, 2, 1),(x, y, z)\r\n\u000b\r\n= 0. Then x + 2y + z = 0 or\r\nequivalently, x = −2y − z. Thus,\r\n(x, y, z) = (−2y − z, y, z) = y(−2, 1, 0) + z(−1, 0, 1).\r\nObserve that the vectors (−2, 1, 0) and (−1, 0, 1) are both orthogonal to (1, 2, 1) but\r\nare not orthogonal to each other.\r\nMethod 1: Consider {√\r\n1\r\n6\r\n(1, 2, 1),(−2, 1, 0),(−1, 0, 1)} ⊂ R\r\n3 and apply the Gram\u0002Schmidt process to get the result.\r\nMethod 2: This method can be used only if the vectors are from R\r\n3\r\n. Recall that\r\nin R\r\n3\r\n, the cross product of two vectors u and v, denoted u × v, is a vector that is\r\northogonal to both the vectors u and v. Hence, the vector\r\n(1, 2, 1) × (−2, 1, 0) = (0 − 1, −2 − 0, 1 + 4) = (−1, −2, 5)\r\nis orthogonal to the vectors (1, 2, 1) and (−2, 1, 0) and hence the required orthonormal\r\nset is {√\r\n1\r\n6\r\n(1, 2, 1), √−1\r\n5\r\n(2, −1, 0), √−1\r\n30 (1, 2, −5)}.\r\nRemark 5.3.4. 1. Let V be a vector space. Then the following holds.\r\n(a) Let {u1, u2, . . . , uk} be a linearly independent subset of V. Then Gram-Schmidt\r\northogonalization process gives an orthonormal set {v1, v2, . . . , vk} of V with\r\nL(v1, v2, . . . , vi) = L(u1, u2, . . . , ui) for 1 ≤ i ≤ k.\r\n(b) Let W be a subspace of V with a basis {u1, u2, . . . , uk}. Then {v1, v2, . . . , vk}\r\nis also a basis of W.\r\n(c) Suppose {u1, u2, . . . , un} is a linearly dependent subset of V . Then there exists\r\na smallest k, 2 ≤ k ≤ n such that wk = 0.\r\nIdea of the proof: Linear dependence (see Corollary 3.2.5) implies that there\r\nexists a smallest k, 2 ≤ k ≤ n such that\r\nL(u1, u2, . . . , uk) = L(u1, u2, . . . , uk−1).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fbd09cff-787f-4feb-91b8-ac5472b015b5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=83da89300a1cee99b716522ada39e674401ec05c8ce8533b08748a1af0d77735",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "9a015647-c3ac-48be-9317-474a4117955d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 127,
            "page_width": 612,
            "page_height": 792,
            "content": "5.3. GRAM-SCHMIDT ORTHOGONALIZATION PROCESS 127\r\nAlso, by Gram-Schmidt orthogonalization process\r\nL(u1, u2, . . . , uk−1) = L(v1, v2, . . . , vk−1).\r\nThus, uk ∈ L(v1, v2, . . . , vk−1) and hence by Remark 5.2.20\r\nuk = huk, v1iv1 + huk, v2iv2 + · · · + huk, vk−1ivk−1.\r\nSo, by definition wk = 0.\r\n2. Let S be a countably infinite set of linearly independent vectors. Then one can apply\r\nthe Gram-Schmidt process to get a countably infinite orthonormal set.\r\n3. Consider R\r\nn with the standard inner product and let {v1, v2, . . . , vn} be an orthonor\u0002mal set. Then, we see that\r\n(a) kvik = 1 is equivalent to v\r\nt\r\nivi = 1, for 1 ≤ i ≤ n,\r\n(b) hvi\r\n, vj i = 0 is equivalent to v\r\nt\r\nivj = 0, for 1 ≤ i 6= j ≤ n.\r\nHence, we see that\r\nA\r\ntA =\r\n\r\n\r\n\r\n\r\n\r\n\r\nv\r\nt\r\n1\r\nv\r\nt\r\n2\r\n.\r\n.\r\n.\r\nv\r\nt\r\nn\r\n\r\n\r\n\r\n\r\n\r\n\r\n[v1, v2, . . . , vn]\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\nv\r\nt\r\n1v1 v\r\nt\r\n1v2 · · · v\r\nt\r\n1vn\r\nv\r\nt\r\n2v1 v\r\nt\r\n2v2 · · · v\r\nt\r\n2vn\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nv\r\nt\r\nnv1 v\r\nt\r\nnv2i · · · v\r\nt\r\nnvn\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\n1 0 · · · 0\r\n0 1 · · · 0\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 · · · 1\r\n\r\n\r\n\r\n\r\n\r\n\r\n= In.\r\nSince AtA = In, it follows that AAt = In. But,\r\nAAt = [v1, v2, . . . , vn]\r\n\r\n\r\n\r\n\r\n\r\n\r\nv\r\nt\r\n1\r\nv\r\nt\r\n2\r\n.\r\n.\r\n.\r\nv\r\nt\r\nn\r\n\r\n\r\n\r\n\r\n\r\n\r\n= v1v\r\nt\r\n1 + v2v\r\nt\r\n2 + · · · + vnv\r\nt\r\nn\r\n.\r\nNow, for each i, 1 ≤ i ≤ n, the matrix viv\r\nt\r\ni\r\nhas the following properties:\r\n(a) it is symmetric;\r\n(b) it is idempotent; and\r\n(c) it has rank one.\r\n4. The first two properties imply that the matrix viv\r\nt\r\ni\r\n, for each i, 1 ≤ i ≤ n is a projection\r\noperator. That is, the identity matrix is the sum of projection operators, each of rank\r\n1.\r\n5. Now define a linear transformation T : R\r\nn−→Rn\r\nby T(x) = (viv\r\nt\r\ni\r\n)x = (v\r\nt\r\nix)vi\r\nis a\r\nprojection operator on the subspace L(vi).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9a015647-c3ac-48be-9317-474a4117955d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5c8811949b3edeb9b41021dbf5dd304281fc7f6f50bc088add43f0c366a33d09",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 459
      },
      {
        "segments": [
          {
            "segment_id": "7ed2f8d1-5ecc-40cc-838c-c4cc39544119",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 128,
            "page_width": 612,
            "page_height": 792,
            "content": "128 CHAPTER 5. INNER PRODUCT SPACES\r\n6. Now, let us fix k, 1 ≤ k ≤ n. Then, it can be observed that the linear transformation\r\nT : R\r\nn−→Rn defined by T(x) = (Pk\r\ni=1 viv\r\nt\r\ni\r\n)x =\r\nPk\r\ni=1(v\r\nt\r\nix)vi\r\nis a projection operator\r\non the subspace L(v1, . . . , vk). We will use this idea in Subsection 5.4.1 .\r\nDefinition 5.2.12 started with a subspace of an inner product space V and looked at its\r\ncomplement. We now look at the orthogonal complement of a subset of an inner product\r\nspace V and the results associated with it.\r\nDefinition 5.3.5 (Orthogonal Subspace of a Set). Let V be an inner product space. Let\r\nS be a non-empty subset of V . We define\r\nS\r\n⊥ = {v ∈ V : hv, si = 0 for all s ∈ S}.\r\nExample 5.3.6. Let V = R.\r\n1. S = {0}. Then S\r\n⊥ = R.\r\n2. S = R, Then S\r\n⊥ = {0}.\r\n3. Let S be any subset of R containing a non-zero real number. Then S\r\n⊥ = {0}.\r\n4. Let S = {(1, 2, 1)} ⊂ R\r\n3\r\n. Then using Example 5.3.3.2, S\r\n⊥ = L({(−2, 1, 0),(−1, 0, 1)}).\r\nWe now state the result which gives the existence of an orthogonal subspace of a finite\r\ndimensional inner product space.\r\nTheorem 5.3.7. Let S be a subset of a finite dimensional inner product space V, with\r\ninner product h , i. Then\r\n1. S\r\n⊥ is a subspace of V.\r\n2. Let W = L(S). Then the subspaces W and S\r\n⊥ = W⊥ are complementary. That is,\r\nV = W + S\r\n⊥ = W + W⊥.\r\n3. Moreover, hu, wi = 0 for all w ∈ W and u ∈ S\r\n⊥.\r\nProof. We leave the prove of the first part to the reader. The prove of the second part is\r\nas follows:\r\nLet dim(V ) = n and dim(W) = k. Let {w1, w2, . . . , wk} be a basis of W. By Gram-Schmidt\r\northogonalization process, we get an orthonormal basis, say {v1, v2, . . . , vk} of W. Then,\r\nfor any v ∈ V,\r\nv −\r\nX\r\nk\r\ni=1\r\nhv, viivi ∈ S\r\n⊥.\r\nSo, V ⊂ W + S\r\n⊥. Hence, V = W + S⊥. We now need to show that W ∩ S⊥ = {0}.\r\nTo do this, let v ∈ W ∩ S\r\n⊥. Then v ∈ W and v ∈ S⊥. Hence, be definition, hv, vi = 0.\r\nThat is, kvk\r\n2 = hv, vi = 0 implying v = 0 and hence W ∩ S⊥ = {0}.\r\nThe third part is a direct consequence of the definition of S\r\n⊥.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/7ed2f8d1-5ecc-40cc-838c-c4cc39544119.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=13bcae7bba265c091a73333c77a520e16b0d5fb0ada25d7ad422686ba07ba4fa",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 466
      },
      {
        "segments": [
          {
            "segment_id": "c0924e80-ba92-4efe-a53e-fc82b97d0b0c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 129,
            "page_width": 612,
            "page_height": 792,
            "content": "5.3. GRAM-SCHMIDT ORTHOGONALIZATION PROCESS 129\r\nExercise 5.3.8. 1. Let A be an n × n orthogonal matrix. Then prove that\r\n(a) the rows of A form an orthonormal basis of R\r\nn\r\n.\r\n(b) the columns of A form an orthonormal basis of R\r\nn\r\n.\r\n(c) for any two vectors x, y ∈ R\r\nn×1\r\n, hAx, Ayi = hx, yi.\r\n(d) for any vector x ∈ R\r\nn×1\r\n, kAxk = kxk.\r\n2. Let A be an n × n unitary matrix. Then prove that\r\n(a) the rows/columns of A form an orthonormal basis of the complex vector space\r\nC\r\nn\r\n.\r\n(b) for any two vectors x, y ∈ C\r\nn×1\r\n, hAx, Ayi = hx, yi.\r\n(c) for any vector x ∈ C\r\nn×1\r\n, kAxk = kxk.\r\n3. Let A and B be two n × n orthogonal matrices. Then prove that AB and BA are\r\nboth orthogonal matrices. Prove a similar result for unitary matrices.\r\n4. Prove the statements made in Remark 5.3.4.3 about orthogonal matrices. State and\r\nprove a similar result for unitary matrices.\r\n5. Let A be an n × n upper triangular matrix. If A is also an orthogonal matrix then A\r\nis a diagonal matrix with diagonal entries ±1.\r\n6. Determine an orthonormal basis of R\r\n4\r\ncontaining the vectors (1, −2, 1, 3) and (2, 1, −3, 1).\r\n7. Consider the real inner product space C[−1, 1] with hf, gi =\r\nR\r\n1\r\n−1\r\nf(t)g(t)dt. Prove that\r\nthe polynomials 1, x, 3\r\n2\r\nx\r\n2 − 1\r\n2\r\n,\r\n5\r\n2\r\nx\r\n3 − 3\r\n2\r\nx form an orthogonal set in C[−1, 1]. Find the\r\ncorresponding functions f(x) with kf(x)k = 1.\r\n8. Consider the real inner product space C[−π, π] with hf, gi =\r\nRπ\r\n−π\r\nf(t)g(t)dt. Find an\r\northonormal basis for L(x,sin x,sin(x + 1)).\r\n9. Let M be a subspace of R\r\nn and dim M = m. A vector x ∈ Rn\r\nis said to be orthogonal\r\nto M if hx, yi = 0 for every y ∈ M.\r\n(a) How many linearly independent vectors can be orthogonal to M?\r\n(b) If M = {(x1, x2, x3) ∈ R\r\n3\r\n: x1 + x2 + x3 = 0}, determine a maximal set of\r\nlinearly independent vectors orthogonal to M in R\r\n3\r\n.\r\n10. Determine an orthogonal basis of L({(1, 1, 0, 1),(−1, 1, 1, −1),(0, 2, 1, 0),(1, 0, 0, 0)})\r\nin R\r\n4\r\n.\r\n11. Let R\r\nn\r\nbe endowed with the standard inner product. Suppose we have a vector x\r\nt =\r\n(x1, x2, . . . , xn) ∈ R\r\nn with kxk = 1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c0924e80-ba92-4efe-a53e-fc82b97d0b0c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=66d5b8b37ab26bf2570f7b5a12a10bf4387670eab44b7b519518e5029d23abcb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 445
      },
      {
        "segments": [
          {
            "segment_id": "c84f3c05-90e8-4206-b3ff-5c75b766af31",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 130,
            "page_width": 612,
            "page_height": 792,
            "content": "130 CHAPTER 5. INNER PRODUCT SPACES\r\n(a) Then prove that the set {x} can always be extended to form an orthonormal\r\nbasis of R\r\nn\r\n.\r\n(b) Let this basis be {x, x2, . . . , xn}. Suppose B = (e1, e2, . . . , en) is the standard\r\nbasis of R\r\nn and let A =\r\n\u0014\r\n[x]B, [x2]B, . . . , [xn]B\r\n\u0015\r\n. Then prove that A is an\r\northogonal matrix.\r\n12. Let v, w ∈ R\r\nn\r\n, n ≥ 1 with kuk = kwk = 1. Prove that there exists an orthogonal\r\nmatrix A such that Av = w. Prove also that A can be chosen such that det(A) = 1.\r\n5.4 Orthogonal Projections and Applications\r\nRecall that given a k-dimensional vector subspace of a vector space V of dimension n, one\r\ncan always find an (n − k)-dimensional vector subspace W0 of V (see Exercise 3.3.13.5)\r\nsatisfying\r\nW + W0 = V and W ∩ W0 = {0}.\r\nThe subspace W0 is called the complementary subspace of W in V. We first use Theo\u0002rem 5.3.7 to get the complementary subspace in such a way that the vectors in different\r\nsubspaces are orthogonal. That is, hw, vi = 0 for all w ∈ W and v ∈ W0. We then use this\r\nto define an important class of linear transformations on an inner product space, called\r\northogonal projections.\r\nDefinition 5.4.1 (Orthogonal Complement and Orthogonal Projection). Let W be a sub\u0002space of a finite dimensional inner product space V .\r\n1. Then W⊥ is called the orthogonal complement of W in V. We represent it by writing\r\nV = W ⊕ W⊥ in place of V = W + W⊥.\r\n2. Also, for each v ∈ V there exist unique vectors w ∈ W and u ∈ W⊥ such that\r\nv = w + u. We use this to define\r\nPW : V −→ V by PW (v) = w.\r\nThen PW is called the orthogonal projection of V onto W.\r\nExercise 5.4.2. Let W be a subspace of a finite dimensional inner product space V . Use\r\nV = W ⊕ W⊥ to define the orthogonal projection operator PW⊥ of V onto W⊥. Prove\r\nthat the maps PW and PW⊥ are indeed linear transformations. What can you say about\r\nPW + PW⊥ ?\r\nExample 5.4.3. 1. Let V = R\r\n3 and W = {(x, y, z) ∈ R3\r\n: x + y − z = 0}. Then it can\r\nbe easily verified that {(1, 1, −1)} is a basis of W⊥ as for each (x, y, z) ∈ W, we have\r\nx + y − z = 0 and hence\r\nh(x, y, z),(1, 1, −1)i = x + y − z = 0 for each (x, y, z) ∈ W.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c84f3c05-90e8-4206-b3ff-5c75b766af31.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=958f4a67adce7b9426b4497689449c1d2ea4118d489c2cd179edc62fa7e8e614",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 469
      },
      {
        "segments": [
          {
            "segment_id": "c12ad599-a39e-439e-8212-b7e72ab7b4df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 131,
            "page_width": 612,
            "page_height": 792,
            "content": "5.4. ORTHOGONAL PROJECTIONS AND APPLICATIONS 131\r\nAlso, using Equation (5.3.1), for every x\r\nt = (x, y, z) ∈ R3\r\n, we have u =\r\nx+y−z\r\n3\r\n(1, 1, −1),\r\nw = ( 2x−y+z\r\n3\r\n,\r\n−x+2y+z\r\n3\r\n,\r\nx+y+2z\r\n3\r\n) and x = w + u. Let\r\nA =\r\n1\r\n3\r\n\r\n\r\n\r\n2 −1 1\r\n−1 2 1\r\n1 1 2\r\n\r\n\r\n\r\nand B =\r\n1\r\n3\r\n\r\n\r\n\r\n1 1 −1\r\n1 1 −1\r\n−1 −1 1\r\n\r\n\r\n.\r\nThen by definition, PW (x) = w = Ax and PW⊥ (x) = u = Bx. Observe that\r\nA2 = A, B2 = B, At = A, Bt = B, A · B = 03, B · A = 03 and A + B = I3, where\r\n03 is the zero matrix of size 3 × 3 and I3 is the identity matrix of size 3. Also, verify\r\nthat rank(A) = 2 and rank(B) = 1.\r\n2. Let W = L( (1, 2, 1) ) ⊂ R\r\n3\r\n. Then using Example 5.3.3.2, and Equation (5.3.1), we\r\nget\r\nW⊥ = L({(−2, 1, 0),(−1, 0, 1)}) = L({(−2, 1, 0),(1, 2, −5)}),\r\nu = ( 5x−2y−z\r\n6\r\n,\r\n−2x+2y−2z\r\n6\r\n,\r\n−x−2y+5z\r\n6\r\n) and w =\r\nx+2y+z\r\n6\r\n(1, 2, 1) with (x, y, z) = w + u.\r\nHence, for\r\nA =\r\n1\r\n6\r\n\r\n\r\n\r\n1 2 1\r\n2 4 2\r\n1 2 1\r\n\r\n\r\n\r\nand B =\r\n1\r\n6\r\n\r\n\r\n\r\n5 −2 −1\r\n−2 2 −2\r\n−1 −2 5\r\n\r\n\r\n ,\r\nwe have PW (x) = w = Ax and PW⊥ (x) = u = Bx. Observe that A2 = A, B2 = B,\r\nAt = A and Bt = B, A · B = 03, B · A = 03 and A + B = I3, where 03 is the\r\nzero matrix of size 3 × 3 and I3 is the identity matrix of size 3. Also, verify that\r\nrank(A) = 1 and rank(B) = 2.\r\nWe now prove some basic properties related to orthogonal projections. We also need\r\nthe following definition.\r\nDefinition 5.4.4 (Self-Adjoint Transformation/Operator). Let V be an inner product\r\nspace with inner product h , i. A linear transformation T : V −→ V is called a self-adjoint\r\noperator if hT(v), ui = hv, T(u)i for every u, v ∈ V.\r\nThe example below gives an indication that the self-adjoint operators and Hermitian\r\nmatrices are related. It also shows that the vector spaces C\r\nn and Rn\r\ncan be decomposed in\r\nterms of the null space and range space of Hermitian matrices. These examples also follow\r\ndirectly from the fundamental theorem of linear algebra.\r\nExample 5.4.5. 1. Let A be an n×n real symmetric matrix and define TA : R\r\nn −→ Rn\r\nby TA(x) = Ax for every x\r\nt ∈ Rn\r\n.\r\n(a) TA is a self adjoint operator.\r\nAs A = At, for every x\r\nt\r\n, y\r\nt ∈ Rn\r\n,\r\nhTA(x), yi = (y\r\nt\r\n)Ax = (y\r\nt\r\n)A\r\ntx = (Ay)tx = hx, Ayi = hx, TA(y)i.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c12ad599-a39e-439e-8212-b7e72ab7b4df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f2019ca7ef3d2afdd8d66f0771cbb201913dc7b5e9fce107a46089a12e2aedf4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "c12ad599-a39e-439e-8212-b7e72ab7b4df",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 131,
            "page_width": 612,
            "page_height": 792,
            "content": "5.4. ORTHOGONAL PROJECTIONS AND APPLICATIONS 131\r\nAlso, using Equation (5.3.1), for every x\r\nt = (x, y, z) ∈ R3\r\n, we have u =\r\nx+y−z\r\n3\r\n(1, 1, −1),\r\nw = ( 2x−y+z\r\n3\r\n,\r\n−x+2y+z\r\n3\r\n,\r\nx+y+2z\r\n3\r\n) and x = w + u. Let\r\nA =\r\n1\r\n3\r\n\r\n\r\n\r\n2 −1 1\r\n−1 2 1\r\n1 1 2\r\n\r\n\r\n\r\nand B =\r\n1\r\n3\r\n\r\n\r\n\r\n1 1 −1\r\n1 1 −1\r\n−1 −1 1\r\n\r\n\r\n.\r\nThen by definition, PW (x) = w = Ax and PW⊥ (x) = u = Bx. Observe that\r\nA2 = A, B2 = B, At = A, Bt = B, A · B = 03, B · A = 03 and A + B = I3, where\r\n03 is the zero matrix of size 3 × 3 and I3 is the identity matrix of size 3. Also, verify\r\nthat rank(A) = 2 and rank(B) = 1.\r\n2. Let W = L( (1, 2, 1) ) ⊂ R\r\n3\r\n. Then using Example 5.3.3.2, and Equation (5.3.1), we\r\nget\r\nW⊥ = L({(−2, 1, 0),(−1, 0, 1)}) = L({(−2, 1, 0),(1, 2, −5)}),\r\nu = ( 5x−2y−z\r\n6\r\n,\r\n−2x+2y−2z\r\n6\r\n,\r\n−x−2y+5z\r\n6\r\n) and w =\r\nx+2y+z\r\n6\r\n(1, 2, 1) with (x, y, z) = w + u.\r\nHence, for\r\nA =\r\n1\r\n6\r\n\r\n\r\n\r\n1 2 1\r\n2 4 2\r\n1 2 1\r\n\r\n\r\n\r\nand B =\r\n1\r\n6\r\n\r\n\r\n\r\n5 −2 −1\r\n−2 2 −2\r\n−1 −2 5\r\n\r\n\r\n ,\r\nwe have PW (x) = w = Ax and PW⊥ (x) = u = Bx. Observe that A2 = A, B2 = B,\r\nAt = A and Bt = B, A · B = 03, B · A = 03 and A + B = I3, where 03 is the\r\nzero matrix of size 3 × 3 and I3 is the identity matrix of size 3. Also, verify that\r\nrank(A) = 1 and rank(B) = 2.\r\nWe now prove some basic properties related to orthogonal projections. We also need\r\nthe following definition.\r\nDefinition 5.4.4 (Self-Adjoint Transformation/Operator). Let V be an inner product\r\nspace with inner product h , i. A linear transformation T : V −→ V is called a self-adjoint\r\noperator if hT(v), ui = hv, T(u)i for every u, v ∈ V.\r\nThe example below gives an indication that the self-adjoint operators and Hermitian\r\nmatrices are related. It also shows that the vector spaces C\r\nn and Rn\r\ncan be decomposed in\r\nterms of the null space and range space of Hermitian matrices. These examples also follow\r\ndirectly from the fundamental theorem of linear algebra.\r\nExample 5.4.5. 1. Let A be an n×n real symmetric matrix and define TA : R\r\nn −→ Rn\r\nby TA(x) = Ax for every x\r\nt ∈ Rn\r\n.\r\n(a) TA is a self adjoint operator.\r\nAs A = At, for every x\r\nt\r\n, y\r\nt ∈ Rn\r\n,\r\nhTA(x), yi = (y\r\nt\r\n)Ax = (y\r\nt\r\n)A\r\ntx = (Ay)tx = hx, Ayi = hx, TA(y)i.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c12ad599-a39e-439e-8212-b7e72ab7b4df.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f2019ca7ef3d2afdd8d66f0771cbb201913dc7b5e9fce107a46089a12e2aedf4",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "056b2d46-5cac-49c9-804f-45b1d7cad65f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 132,
            "page_width": 612,
            "page_height": 792,
            "content": "132 CHAPTER 5. INNER PRODUCT SPACES\r\n(b) N (TA) = R(TA)\r\n⊥ follows from Theorem 5.2.15 as A = At\r\n. But we do give a\r\nproof for completeness.\r\nLet x ∈ N (TA). Then TA(x) = 0 and hx, TA(u)i = hTA(x), ui = 0. Thus,\r\nx ∈ R(TA)\r\n⊥ and hence N (TA) ⊂ R(TA)⊥.\r\nLet x ∈ R(TA)\r\n⊥. Then 0 = hx, TA(y)i = hTA(x), yi for every y ∈ Rn\r\n. Hence,\r\nby Exercise 2 TA(x) = 0. That is, x ∈ N (A) and hence R(TA)\r\n⊥ ⊂ N (TA).\r\n(c) R\r\nn = N (TA) ⊕ R(TA) as N (TA) = R(TA)⊥.\r\n(d) Thus N (A) = Im(A)\r\n⊥, or equivalently, Rn = N (A) ⊕ Im(A).\r\n2. Let A be an n × n Hermitian matrix. Define TA : C\r\nn −→ Cn defined by TA(z) = Az\r\nfor all z\r\nt ∈ Cn\r\n. Then using arguments similar to the arguments in Example 5.4.5.1,\r\nprove the following:\r\n(a) TA is a self-adjoint operator.\r\n(b) N (TA) = R(TA)\r\n⊥ and Cn = N (TA) ⊕ R(TA).\r\n(c) N (A) = Im(A)\r\n⊥ and Cn = N (A) ⊕ Im(A).\r\nWe now state and prove the main result related with orthogonal projection operators.\r\nTheorem 5.4.6. Let W be a vector subspace of a finite dimensional inner product space\r\nV and let PW : V −→ V be the orthogonal projection operator of V onto W.\r\n1. Then N (PW ) = {v ∈ V : PW (v) = 0} = W⊥ = R(PW⊥ ).\r\n2. Then R(PW ) = {PW (v) : v ∈ V } = W = N (PW⊥ ).\r\n3. Then PW ◦ PW = PW , PW⊥ ◦ PW⊥ = PW⊥ .\r\n4. Let 0V denote the zero operator on V defined by 0V (v) = 0 for all v ∈ V . Then\r\nPW⊥ ◦ PW = 0V and PW ◦ PW⊥ = 0V .\r\n5. Let IV denote the identity operator on V defined by IV (v) = v for all v ∈ V . Then\r\nIV = PW ⊕ PW⊥ , where we have written ⊕ instead of + to indicate the relationship\r\nPW⊥ ◦ PW = 0V and PW ◦ PW⊥ = 0V .\r\n6. The operators PW and PW⊥ are self-adjoint.\r\nProof. Part 1: Let u ∈ W⊥. As V = W ⊕ W⊥, we have u = 0 + u for 0 ∈ W and\r\nu ∈ W⊥. Hence by definition, PW (u) = 0 and PW⊥ (u) = u. Thus, W⊥ ⊂ N (PW ) and\r\nW⊥ ⊂ R(PW⊥ ).\r\nAlso, suppose that v ∈ N (PW ) for some v ∈ V . As v has a unique expression as\r\nv = w + u for some w ∈ W and some u ∈ W⊥, by definition of PW , we have PW (v) = w.\r\nAs v ∈ N (PW ), by definition, PW (v) = 0 and hence w = 0. That is, v = u ∈ W⊥. Thus,\r\nN (PW ) ⊂ W⊥.\r\nOne can similarly show that R(PW⊥ ) ⊂ W⊥. Thus, the proof of the first part is\r\ncomplete.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/056b2d46-5cac-49c9-804f-45b1d7cad65f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63482f13cccc1cc24ec9bdb423bea6610a7dfa6c2e5542ee313cb4293fd2a08a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 534
      },
      {
        "segments": [
          {
            "segment_id": "056b2d46-5cac-49c9-804f-45b1d7cad65f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 132,
            "page_width": 612,
            "page_height": 792,
            "content": "132 CHAPTER 5. INNER PRODUCT SPACES\r\n(b) N (TA) = R(TA)\r\n⊥ follows from Theorem 5.2.15 as A = At\r\n. But we do give a\r\nproof for completeness.\r\nLet x ∈ N (TA). Then TA(x) = 0 and hx, TA(u)i = hTA(x), ui = 0. Thus,\r\nx ∈ R(TA)\r\n⊥ and hence N (TA) ⊂ R(TA)⊥.\r\nLet x ∈ R(TA)\r\n⊥. Then 0 = hx, TA(y)i = hTA(x), yi for every y ∈ Rn\r\n. Hence,\r\nby Exercise 2 TA(x) = 0. That is, x ∈ N (A) and hence R(TA)\r\n⊥ ⊂ N (TA).\r\n(c) R\r\nn = N (TA) ⊕ R(TA) as N (TA) = R(TA)⊥.\r\n(d) Thus N (A) = Im(A)\r\n⊥, or equivalently, Rn = N (A) ⊕ Im(A).\r\n2. Let A be an n × n Hermitian matrix. Define TA : C\r\nn −→ Cn defined by TA(z) = Az\r\nfor all z\r\nt ∈ Cn\r\n. Then using arguments similar to the arguments in Example 5.4.5.1,\r\nprove the following:\r\n(a) TA is a self-adjoint operator.\r\n(b) N (TA) = R(TA)\r\n⊥ and Cn = N (TA) ⊕ R(TA).\r\n(c) N (A) = Im(A)\r\n⊥ and Cn = N (A) ⊕ Im(A).\r\nWe now state and prove the main result related with orthogonal projection operators.\r\nTheorem 5.4.6. Let W be a vector subspace of a finite dimensional inner product space\r\nV and let PW : V −→ V be the orthogonal projection operator of V onto W.\r\n1. Then N (PW ) = {v ∈ V : PW (v) = 0} = W⊥ = R(PW⊥ ).\r\n2. Then R(PW ) = {PW (v) : v ∈ V } = W = N (PW⊥ ).\r\n3. Then PW ◦ PW = PW , PW⊥ ◦ PW⊥ = PW⊥ .\r\n4. Let 0V denote the zero operator on V defined by 0V (v) = 0 for all v ∈ V . Then\r\nPW⊥ ◦ PW = 0V and PW ◦ PW⊥ = 0V .\r\n5. Let IV denote the identity operator on V defined by IV (v) = v for all v ∈ V . Then\r\nIV = PW ⊕ PW⊥ , where we have written ⊕ instead of + to indicate the relationship\r\nPW⊥ ◦ PW = 0V and PW ◦ PW⊥ = 0V .\r\n6. The operators PW and PW⊥ are self-adjoint.\r\nProof. Part 1: Let u ∈ W⊥. As V = W ⊕ W⊥, we have u = 0 + u for 0 ∈ W and\r\nu ∈ W⊥. Hence by definition, PW (u) = 0 and PW⊥ (u) = u. Thus, W⊥ ⊂ N (PW ) and\r\nW⊥ ⊂ R(PW⊥ ).\r\nAlso, suppose that v ∈ N (PW ) for some v ∈ V . As v has a unique expression as\r\nv = w + u for some w ∈ W and some u ∈ W⊥, by definition of PW , we have PW (v) = w.\r\nAs v ∈ N (PW ), by definition, PW (v) = 0 and hence w = 0. That is, v = u ∈ W⊥. Thus,\r\nN (PW ) ⊂ W⊥.\r\nOne can similarly show that R(PW⊥ ) ⊂ W⊥. Thus, the proof of the first part is\r\ncomplete.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/056b2d46-5cac-49c9-804f-45b1d7cad65f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=63482f13cccc1cc24ec9bdb423bea6610a7dfa6c2e5542ee313cb4293fd2a08a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 534
      },
      {
        "segments": [
          {
            "segment_id": "681c4846-eefa-424c-832e-f1dc23dd4c0b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 133,
            "page_width": 612,
            "page_height": 792,
            "content": "5.4. ORTHOGONAL PROJECTIONS AND APPLICATIONS 133\r\nPart 2: Similar argument as in the proof of Part 1.\r\nPart 3, Part 4 and Part 5: Let v ∈ V and let v = w + u for some w ∈ W and\r\nu ∈ W⊥. Then by definition,\r\n(PW ◦ PW )(v) = PW\r\n\r\nPW (v)\r\n\u0001\r\n= PW (w) = w & PW (v) = w, (5.4.1)\r\n(PW⊥ ◦ PW )(v) = PW⊥\r\n\r\nPW (v)\r\n\u0001\r\n= PW⊥ (w) = 0 and (5.4.2)\r\n(PW ⊕ PW⊥ )(v) = PW (v) + PW⊥ (v) = w + u = v = IV (v). (5.4.3)\r\nHence, applying Exercise 2 to Equations (5.4.1), (5.4.2) and (5.4.3), respectively, we get\r\nPW ◦ PW = PW , PW⊥ ◦ PW = 0V and IV = PW ⊕ PW⊥ .\r\nPart 6: Let u = w1 + x1 and v = w2 + x2, where w1, w2 ∈ W and x1, x2 ∈ W⊥.\r\nThen, by definition hwi, xj i = 0 for 1 ≤ i, j ≤ 2. Thus,\r\nhPW (u), vi = hw1, vi = hw1, w2i = hu, w2i = hu, PW (v)i\r\nand the proof of the theorem is complete.\r\nThe next theorem is a generalization of Theorem 5.4.6 when a finite dimensional inner\r\nproduct space V can be written as V = W1⊕W2⊕· · ·⊕Wk, where Wi’s are vector subspaces\r\nof V . That is, for each v ∈ V there exist unique vectors v1, v2, . . . , vk such that\r\n1. vi ∈ Wifor 1 ≤ i ≤ k,\r\n2. hvi\r\n, vj i = 0 for each vi ∈ Wi, vj ∈ Wj , 1 ≤ i 6= j ≤ k and\r\n3. v = v1 + v2 + · · · + vk.\r\nWe omit the proof as it basically uses arguments that are similar to the arguments used in\r\nthe proof of Theorem 5.4.6.\r\nTheorem 5.4.7. Let V be a finite dimensional inner product space and let W1, W2, . . . , Wk\r\nbe vector subspaces of V such that V = W1 ⊕ W2 ⊕ · · · ⊕ Wk. Then for each i, j, 1 ≤ i 6=\r\nj ≤ k, there exist orthogonal projection operators PWi: V −→ V of V onto Wi satisfying\r\nthe following:\r\n1. N (PWi\r\n) = W⊥\r\ni = W1 ⊕ W2 ⊕ · · · ⊕ Wi−1 ⊕ Wi+1 ⊕ · · · ⊕ Wk.\r\n2. R(PWi) = Wi.\r\n3. PWi◦ PWi = PWi.\r\n4. PWi\r\n◦ PWj = 0V .\r\n5. PWiis a self-adjoint operator, and\r\n6. IV = PW1 ⊕ PW2 ⊕ · · · ⊕ PWk.\r\nRemark 5.4.8. 1. By Exercise 5.4.2, PW is a linear transformation.\r\n2. By Theorem 5.4.6, we observe the following:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/681c4846-eefa-424c-832e-f1dc23dd4c0b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6038de09817b1b89f10cf976f65f31106f133170976e381111113d8acd9c1ea9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 463
      },
      {
        "segments": [
          {
            "segment_id": "78baf8b9-9270-49a4-a6b4-6eb1ac7af06f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 134,
            "page_width": 612,
            "page_height": 792,
            "content": "134 CHAPTER 5. INNER PRODUCT SPACES\r\n(a) The orthogonal projection operators PW and PW⊥ are idempotent operators.\r\n(b) The orthogonal projection operators PW and PW⊥ are also self-adjoint operators.\r\n(c) Let v ∈ V . Then v − PW (v) = (IV − PW )(v) = PW⊥ (v) ∈ W⊥. Thus,\r\nhv − PW (v), wi = 0 for every v ∈ V and w ∈ W.\r\n(d) Using Remark 5.4.8.2c, PW (v) − w ∈ W for each v ∈ V and w ∈ W. Thus,\r\nkv − wk\r\n2 = kv − PW (v) + PW (v) − wk2\r\n= kv − PW (v)k\r\n2 + kPW (v) − wk2\r\n+2hv − PW (v), PW (v) − wi\r\n= kv − PW (v)k\r\n2 + kPW (v) − wk2\r\n.\r\nTherefore,\r\nkv − wk ≥ kv − PW (v)k\r\nand equality holds if and only if w = PW (v). Since PW (v) ∈ W, we see that\r\nd(v, W) = inf {kv − wk : w ∈ W} = kv − PW (v)k.\r\nThat is, PW (v) is the vector nearest to v ∈ W. This can also be stated as: the\r\nvector PW (v) solves the following minimization problem:\r\ninf\r\nw∈W\r\nkv − wk = kv − PW (v)k.\r\nExercise 5.4.9. 1. Let A ∈ Mn(R) be an idempotent matrix and define TA : R\r\nn −→ Rn\r\nby TA(v) = Av for all v\r\nt ∈ Rn\r\n. Recall the following results from Exercise 4.3.12.5.\r\n(a) TA ◦ TA = TA\r\n(b) N (TA) ∩ R(TA) = {0}.\r\n(c) R\r\nn = R(TA) + N (TA).\r\nThe linear map TA need not be an orthogonal projection operator as R(TA)\r\n⊥\r\nneed not be equal to N (TA). Here TA is called a projection operator of R\r\nn onto\r\nR(TA) along N (TA).\r\n(d) If A is also symmetric then prove that TA is an orthogonal projection operator.\r\n(e) Which of the above results can be generalized to an n × n complex idempotent\r\nmatrix A? Give reasons for your answer.\r\n2. Find all 2 × 2 real matrices A such that A2 = A. Hence or otherwise, determine all\r\nprojection operators of R\r\n2\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/78baf8b9-9270-49a4-a6b4-6eb1ac7af06f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=49b76cb433c0b3b1493203d6b18e9120cdeee91dc4ca8ee09aedad01977c6e41",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 370
      },
      {
        "segments": [
          {
            "segment_id": "305f979a-6d5e-44e0-a490-51f4852dabe9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 135,
            "page_width": 612,
            "page_height": 792,
            "content": "5.4. ORTHOGONAL PROJECTIONS AND APPLICATIONS 135\r\n5.4.1 Matrix of the Orthogonal Projection\r\nThe minimization problem stated above arises in lot of applications. So, it is very helpful\r\nif the matrix of the orthogonal projection can be obtained under a given basis.\r\nTo this end, let W be a k-dimensional subspace of R\r\nn with W⊥ as its orthogonal\r\ncomplement. Let PW : R\r\nn −→ Rn be the orthogonal projection of Rn onto W. Then\r\nRemark 5.3.4.6 implies that we just need to know an orthonormal basis of W. So, let B =\r\n(v1, v2, . . . , vk) be an orthonormal basis of W. Thus, the matrix of PW equals Pk\r\ni=1 viv\r\nt\r\ni\r\n.\r\nHence, we have proved the following theorem.\r\nTheorem 5.4.10. Let W be a k-dimensional subspace of R\r\nn and let PW be the corre\u0002sponding orthogonal projection of R\r\nn onto W. Also assume that B = (v1, v2, . . . , vk) is\r\nan orthonormal ordered basis of W. Define A = [v1, v2, . . . , vk], an n × k matrix. Then\r\nthe matrix of PW in the standard ordered basis of R\r\nn\r\nis AAt =\r\nPk\r\ni=1 viv\r\nt\r\ni\r\n(a symmetric\r\nmatrix).\r\nWe illustrate the above theorem with the help of an example. One can also see Exam\u0002ple 5.4.3.\r\nExample 5.4.11. Let W = {(x, y, z, w) ∈ R\r\n4\r\n: x = y, z = w} be a subspace of W.\r\nThen an orthonormal ordered basis of W and W⊥ is \r\n√\r\n1\r\n2\r\n(1, 1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, 1)\u0001and\r\n\r\n√\r\n1\r\n2\r\n(1, −1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, −1)\u0001, respectively. Let PW : R\r\n4 −→ R4\r\nbe an orthogonal projec\u0002tion of R\r\n4 onto W. Then\r\nA =\r\n\r\n\r\n\r\n\r\n\r\n\r\n√\r\n1\r\n2\r\n0\r\n√\r\n1\r\n2\r\n0\r\n0 √\r\n1\r\n2\r\n0 √\r\n1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n\r\nand PW [B,B] = AAt =\r\n\r\n\r\n\r\n\r\n\r\n1\r\n2\r\n1\r\n2\r\n0 0\r\n1\r\n2\r\n1\r\n2\r\n0 0\r\n0 0 1\r\n2\r\n1\r\n2\r\n0 0 1\r\n2\r\n1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n,\r\nwhere B =\r\n\u0010\r\n√\r\n1\r\n2\r\n(1, 1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, 1), √\r\n1\r\n2\r\n(1, −1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, −1)\u0011. Verify that\r\n1. PW [B,B] is symmetric,\r\n2. (PW [B,B])2 = PW [B,B] and\r\n3. I4 − PW [B,B]\r\n\u0001\r\nPW [B,B] = 0 = PW [B,B]\r\n\r\nI4 − PW [B,B]\r\n\u0001\r\n.\r\nAlso, [(x, y, z, w)]B =\r\n\u0010\r\nx+y\r\n√\r\n2\r\n,\r\nz√+w\r\n2\r\n,\r\nx−y\r\n√\r\n2\r\n,\r\nz√−w\r\n2\r\n\u0011t\r\nand hence\r\nPW\r\n\r\n(x, y, z, w)\r\n\u0001\r\n=\r\nx + y\r\n2\r\n(1, 1, 0, 0) + z + w\r\n2\r\n(0, 0, 1, 1)\r\nis the closest vector to the subspace W for any vector (x, y, z, w) ∈ R\r\n4\r\n.\r\nExercise 5.4.12. 1. Show that for any non-zero vector v\r\nt ∈ Rn\r\n, rank(vvt) = 1.\r\n2. Let W be a subspace of an inner product space V and let P : V −→ V be the\r\northogonal projection of V onto W. Let B be an orthonormal ordered basis of V.\r\nThen prove that (P[B,B])t = P[B,B].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/305f979a-6d5e-44e0-a490-51f4852dabe9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=05d54c30334113922d824927e482b7a5cfb4680945651816fa0099b87ff204d3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 565
      },
      {
        "segments": [
          {
            "segment_id": "305f979a-6d5e-44e0-a490-51f4852dabe9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 135,
            "page_width": 612,
            "page_height": 792,
            "content": "5.4. ORTHOGONAL PROJECTIONS AND APPLICATIONS 135\r\n5.4.1 Matrix of the Orthogonal Projection\r\nThe minimization problem stated above arises in lot of applications. So, it is very helpful\r\nif the matrix of the orthogonal projection can be obtained under a given basis.\r\nTo this end, let W be a k-dimensional subspace of R\r\nn with W⊥ as its orthogonal\r\ncomplement. Let PW : R\r\nn −→ Rn be the orthogonal projection of Rn onto W. Then\r\nRemark 5.3.4.6 implies that we just need to know an orthonormal basis of W. So, let B =\r\n(v1, v2, . . . , vk) be an orthonormal basis of W. Thus, the matrix of PW equals Pk\r\ni=1 viv\r\nt\r\ni\r\n.\r\nHence, we have proved the following theorem.\r\nTheorem 5.4.10. Let W be a k-dimensional subspace of R\r\nn and let PW be the corre\u0002sponding orthogonal projection of R\r\nn onto W. Also assume that B = (v1, v2, . . . , vk) is\r\nan orthonormal ordered basis of W. Define A = [v1, v2, . . . , vk], an n × k matrix. Then\r\nthe matrix of PW in the standard ordered basis of R\r\nn\r\nis AAt =\r\nPk\r\ni=1 viv\r\nt\r\ni\r\n(a symmetric\r\nmatrix).\r\nWe illustrate the above theorem with the help of an example. One can also see Exam\u0002ple 5.4.3.\r\nExample 5.4.11. Let W = {(x, y, z, w) ∈ R\r\n4\r\n: x = y, z = w} be a subspace of W.\r\nThen an orthonormal ordered basis of W and W⊥ is \r\n√\r\n1\r\n2\r\n(1, 1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, 1)\u0001and\r\n\r\n√\r\n1\r\n2\r\n(1, −1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, −1)\u0001, respectively. Let PW : R\r\n4 −→ R4\r\nbe an orthogonal projec\u0002tion of R\r\n4 onto W. Then\r\nA =\r\n\r\n\r\n\r\n\r\n\r\n\r\n√\r\n1\r\n2\r\n0\r\n√\r\n1\r\n2\r\n0\r\n0 √\r\n1\r\n2\r\n0 √\r\n1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n\r\nand PW [B,B] = AAt =\r\n\r\n\r\n\r\n\r\n\r\n1\r\n2\r\n1\r\n2\r\n0 0\r\n1\r\n2\r\n1\r\n2\r\n0 0\r\n0 0 1\r\n2\r\n1\r\n2\r\n0 0 1\r\n2\r\n1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n,\r\nwhere B =\r\n\u0010\r\n√\r\n1\r\n2\r\n(1, 1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, 1), √\r\n1\r\n2\r\n(1, −1, 0, 0), √\r\n1\r\n2\r\n(0, 0, 1, −1)\u0011. Verify that\r\n1. PW [B,B] is symmetric,\r\n2. (PW [B,B])2 = PW [B,B] and\r\n3. I4 − PW [B,B]\r\n\u0001\r\nPW [B,B] = 0 = PW [B,B]\r\n\r\nI4 − PW [B,B]\r\n\u0001\r\n.\r\nAlso, [(x, y, z, w)]B =\r\n\u0010\r\nx+y\r\n√\r\n2\r\n,\r\nz√+w\r\n2\r\n,\r\nx−y\r\n√\r\n2\r\n,\r\nz√−w\r\n2\r\n\u0011t\r\nand hence\r\nPW\r\n\r\n(x, y, z, w)\r\n\u0001\r\n=\r\nx + y\r\n2\r\n(1, 1, 0, 0) + z + w\r\n2\r\n(0, 0, 1, 1)\r\nis the closest vector to the subspace W for any vector (x, y, z, w) ∈ R\r\n4\r\n.\r\nExercise 5.4.12. 1. Show that for any non-zero vector v\r\nt ∈ Rn\r\n, rank(vvt) = 1.\r\n2. Let W be a subspace of an inner product space V and let P : V −→ V be the\r\northogonal projection of V onto W. Let B be an orthonormal ordered basis of V.\r\nThen prove that (P[B,B])t = P[B,B].",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/305f979a-6d5e-44e0-a490-51f4852dabe9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=05d54c30334113922d824927e482b7a5cfb4680945651816fa0099b87ff204d3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 565
      },
      {
        "segments": [
          {
            "segment_id": "ccb18921-67d5-4e20-8333-6b0dff350831",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 136,
            "page_width": 612,
            "page_height": 792,
            "content": "136 CHAPTER 5. INNER PRODUCT SPACES\r\n3. Let W1 = {(x, 0) : x ∈ R} and W2 = {(x, x) : x ∈ R} be two subspaces of R\r\n2\r\n. Let\r\nPW1 and PW2be the corresponding orthogonal projection operators of R\r\n2 onto W1\r\nand W2, respectively. Compute PW1◦ PW2 and conclude that the composition of two\r\northogonal projections need not be an orthogonal projection?\r\n4. Let W be an (n−1)-dimensional subspace of R\r\nn\r\n. Suppose B is an orthogonal ordered\r\nbasis of R\r\nn obtained by extending an orthogonal ordered basis of W. Define\r\nT : R\r\nn −→ Rn\r\nby T(v) = w0 − w\r\nwhenever v = w + w0 for some w ∈ W and w0 ∈ W⊥. Then\r\n(a) prove that T is a linear transformation,\r\n(b) find T[B,B] and\r\n(c) prove that T[B,B] is an orthogonal matrix.\r\nT is called the reflection operator along W⊥.\r\n5.5 QR Decomposition∗\r\nThe next result gives the proof of the QR decomposition for real matrices. A similar\r\nresult holds for matrices with complex entries. The readers are advised to prove that\r\nfor themselves. This decomposition and its generalizations are helpful in the numerical\r\ncalculations related with eigenvalue problems (see Chapter 6).\r\nTheorem 5.5.1 (QR Decomposition). Let A be a square matrix of order n with real\r\nentries. Then there exist matrices Q and R such that Q is orthogonal and R is upper\r\ntriangular with A = QR.\r\nIn case, A is non-singular, the diagonal entries of R can be chosen to be positive. Also,\r\nin this case, the decomposition is unique.\r\nProof. We prove the theorem when A is non-singular. The proof for the singular case is\r\nleft as an exercise.\r\nLet the columns of A be x1, x2, . . . , xn. Then {x1, x2, . . . , xn} is a basis of R\r\nn and hence\r\nthe Gram-Schmidt orthogonalization process gives an ordered basis (see Remark 5.3.4), say\r\nB = (v1, v2, . . . , vn) of R\r\nn\r\nsatisfying\r\nL(v1, v2, . . . , vi) = L(x1, x2, . . . , xi),\r\nkvik = 1, hvi, vj i = 0,\r\n)\r\nfor 1 ≤ i 6= j ≤ n. (5.5.4)\r\nAs xi ∈ R\r\nn and xi ∈ L(v1, v2, . . . , vi), we can find αji, 1 ≤ j ≤ i such that\r\nxi = α1iv1 + α2iv2 + · · · + αiivi =\r\n\u0002\r\n(α1i, . . . , αii, 0 . . . , 0)t\r\n\u0003\r\nB\r\n. (5.5.5)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ccb18921-67d5-4e20-8333-6b0dff350831.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=136395dd2580340f1abb4708d13ce065a23f17a037d148d8952f1616a511c6e8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 429
      },
      {
        "segments": [
          {
            "segment_id": "b2bbe4be-53c5-4eed-8a8d-80c390cdbf3f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 137,
            "page_width": 612,
            "page_height": 792,
            "content": "5.5. QR DECOMPOSITION∗ 137\r\nNow define Q = [v1, v2, . . . , vn] and R =\r\n\r\n\r\n\r\n\r\n\r\n\r\nα11 α12 · · · α1n\r\n0 α22 · · · α2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 · · · αnn\r\n\r\n\r\n\r\n\r\n\r\n\r\n. Then by Exercise 5.3.8.4,\r\nQ is an orthogonal matrix and using (5.5.5), we get\r\nQR = [v1, v2, . . . , vn]\r\n\r\n\r\n\r\n\r\n\r\n\r\nα11 α12 · · · α1n\r\n0 α22 · · · α2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 · · · αnn\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\u0014\r\nα11v1, α12v1 + α22v2, . . . ,Xn\r\ni=1\r\nαinvi\r\n\u0015\r\n= [x1, x2, . . . , xn] = A.\r\nThus, we see that A = QR, where Q is an orthogonal matrix (see Remark 5.3.4.1) and R\r\nis an upper triangular matrix.\r\nThe proof doesn’t guarantee that for 1 ≤ i ≤ n, αii is positive. But this can be achieved\r\nby replacing the vector vi by −vi whenever αii is negative.\r\nUniqueness: suppose Q1R1 = Q2R2 then Q\r\n−1\r\n2 Q1 = R2R\r\n−1\r\n1\r\n. Observe the following\r\nproperties of upper triangular matrices.\r\n1. The inverse of an upper triangular matrix is also an upper triangular matrix, and\r\n2. product of upper triangular matrices is also upper triangular.\r\nThus the matrix R2R\r\n−1\r\n1\r\nis an upper triangular matrix. Also, by Exercise 5.3.8.3, the matrix\r\nQ\r\n−1\r\n2 Q1 is an orthogonal matrix. Hence, by Exercise 5.3.8.5, R2R\r\n−1\r\n1 = In. So, R2 = R1 and\r\ntherefore Q2 = Q1.\r\nLet A = [x1, x2, . . . , xk] be an n × k matrix with rank (A) = r. Then by Remark\r\n5.3.4.1c , the Gram-Schmidt orthogonalization process applied to {x1, x2, . . . , xk} yields a\r\nset {v1, v2, . . . , vr} of orthonormal vectors of R\r\nn and for each i, 1 ≤ i ≤ r, we have\r\nL(v1, v2, . . . , vi) = L(x1, x2, . . . , xj ), for some j, i ≤ j ≤ k.\r\nHence, proceeding on the lines of the above theorem, we have the following result.\r\nTheorem 5.5.2 (Generalized QR Decomposition). Let A be an n × k matrix of rank r.\r\nThen A = QR, where\r\n1. Q = [v1, v2, . . . , vr] is an n × r matrix with QtQ = Ir,\r\n2. L(v1, v2, . . . , vr) = L(x1, x2, . . . , xk), and\r\n3. R is an r × k matrix with rank (R) = r.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/b2bbe4be-53c5-4eed-8a8d-80c390cdbf3f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a33dcc01b3e01d38165f3e9c47cd20d5bbf86461ebd274ce31c4cba9fc0f91ec",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 468
      },
      {
        "segments": [
          {
            "segment_id": "6bd4bb4b-e932-46e5-b535-30e3fc8dada3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 138,
            "page_width": 612,
            "page_height": 792,
            "content": "138 CHAPTER 5. INNER PRODUCT SPACES\r\nExample 5.5.3. 1. Let A =\r\n\r\n\r\n\r\n\r\n\r\n1 0 1 2\r\n0 1 −1 1\r\n1 0 1 1\r\n0 1 1 1\r\n\r\n\r\n\r\n\r\n\r\n. Find an orthogonal matrix Q and an\r\nupper triangular matrix R such that A = QR.\r\nSolution: From Example 5.3.3, we know that\r\nv1 =\r\n1\r\n√\r\n2\r\n(1, 0, 1, 0), v2 =\r\n1\r\n√\r\n2\r\n(0, 1, 0, 1) and v3 =\r\n1\r\n√\r\n2\r\n(0, −1, 0, 1). (5.5.6)\r\nWe now compute w4. If we denote u4 = (2, 1, 1, 1)tthen\r\nw4 = u4 − hu4, v1iv1 − hu4, v2iv2 − hu4, v3iv3 =\r\n1\r\n2\r\n(1, 0, −1, 0)t. (5.5.7)\r\nThus, using Equations (5.5.6), (5.5.7) and Q =\r\n\u0002\r\nv1, v2, v3, v4\r\n\u0003\r\n, we get\r\nQ =\r\n\r\n\r\n\r\n\r\n\r\n\r\n√\r\n1\r\n2\r\n0 0 √\r\n1\r\n2\r\n0 √\r\n1\r\n2\r\n√−1\r\n2\r\n0\r\n√\r\n1\r\n2\r\n0 0 √−1\r\n2\r\n0 √\r\n1\r\n2\r\n√\r\n1\r\n2\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\nand R =\r\n\r\n\r\n\r\n\r\n\r\n√\r\n2 0 √2 √\r\n3\r\n2\r\n0\r\n√\r\n2 0 √2\r\n0 0 √2 0\r\n0 0 0 √−1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n. The readers are advised\r\nto check that A = QR is indeed correct.\r\n2. Let A =\r\n\r\n\r\n\r\n\r\n\r\n1 1 1 0\r\n−1 0 −2 1\r\n1 1 1 0\r\n1 0 2 1\r\n\r\n\r\n\r\n\r\n\r\n. Find a 4 × 3 matrix Q satisfying QtQ = I3 and an upper\r\ntriangular matrix R such that A = QR.\r\nSolution: Let us apply the Gram Schmidt orthogonalization process to the columns of\r\nA. That is, apply the process to the subset {(1, −1, 1, 1),(1, 0, 1, 0),(1, −2, 1, 2),(0, 1, 0, 1)}\r\nof R\r\n4\r\n.\r\nLet u1 = (1, −1, 1, 1). Define v1 = 1\r\n2\r\nu1. Let u2 = (1, 0, 1, 0). Then\r\nw2 = (1, 0, 1, 0) − hu2, v1iv1 = (1, 0, 1, 0) − v1 =\r\n1\r\n2\r\n(1, 1, 1, −1).\r\nHence, v2 =\r\n1\r\n2\r\n(1, 1, 1, −1). Let u3 = (1, −2, 1, 2). Then\r\nw3 = u3 − hu3, v1iv1 − hu3, v2iv2 = u3 − 3v1 + v2 = 0.\r\nSo, we again take u3 = (0, 1, 0, 1). Then\r\nw3 = u3 − hu3, v1iv1 − hu3, v2iv2 = u3 − 0v1 − 0v2 = u3.\r\nSo, v3 = √\r\n1\r\n2\r\n(0, 1, 0, 1). Hence,\r\nQ = [v1, v2, v3] =\r\n\r\n\r\n\r\n\r\n\r\n1\r\n2\r\n1\r\n2\r\n0\r\n−1\r\n2\r\n1\r\n2\r\n√\r\n1\r\n2\r\n1\r\n2\r\n1\r\n2\r\n0\r\n1\r\n2\r\n−1\r\n2\r\n√\r\n1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n, and R =\r\n\r\n\r\n\r\n2 1 3 0\r\n0 1 −1 0\r\n0 0 0 √\r\n2\r\n\r\n\r\n .\r\nThe readers are advised to check the following:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6bd4bb4b-e932-46e5-b535-30e3fc8dada3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=26d83370e2a06812aab456b516327db8c50ea10a6fe0f6d695f0c123adbac29d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "6bd4bb4b-e932-46e5-b535-30e3fc8dada3",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 138,
            "page_width": 612,
            "page_height": 792,
            "content": "138 CHAPTER 5. INNER PRODUCT SPACES\r\nExample 5.5.3. 1. Let A =\r\n\r\n\r\n\r\n\r\n\r\n1 0 1 2\r\n0 1 −1 1\r\n1 0 1 1\r\n0 1 1 1\r\n\r\n\r\n\r\n\r\n\r\n. Find an orthogonal matrix Q and an\r\nupper triangular matrix R such that A = QR.\r\nSolution: From Example 5.3.3, we know that\r\nv1 =\r\n1\r\n√\r\n2\r\n(1, 0, 1, 0), v2 =\r\n1\r\n√\r\n2\r\n(0, 1, 0, 1) and v3 =\r\n1\r\n√\r\n2\r\n(0, −1, 0, 1). (5.5.6)\r\nWe now compute w4. If we denote u4 = (2, 1, 1, 1)tthen\r\nw4 = u4 − hu4, v1iv1 − hu4, v2iv2 − hu4, v3iv3 =\r\n1\r\n2\r\n(1, 0, −1, 0)t. (5.5.7)\r\nThus, using Equations (5.5.6), (5.5.7) and Q =\r\n\u0002\r\nv1, v2, v3, v4\r\n\u0003\r\n, we get\r\nQ =\r\n\r\n\r\n\r\n\r\n\r\n\r\n√\r\n1\r\n2\r\n0 0 √\r\n1\r\n2\r\n0 √\r\n1\r\n2\r\n√−1\r\n2\r\n0\r\n√\r\n1\r\n2\r\n0 0 √−1\r\n2\r\n0 √\r\n1\r\n2\r\n√\r\n1\r\n2\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\nand R =\r\n\r\n\r\n\r\n\r\n\r\n√\r\n2 0 √2 √\r\n3\r\n2\r\n0\r\n√\r\n2 0 √2\r\n0 0 √2 0\r\n0 0 0 √−1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n. The readers are advised\r\nto check that A = QR is indeed correct.\r\n2. Let A =\r\n\r\n\r\n\r\n\r\n\r\n1 1 1 0\r\n−1 0 −2 1\r\n1 1 1 0\r\n1 0 2 1\r\n\r\n\r\n\r\n\r\n\r\n. Find a 4 × 3 matrix Q satisfying QtQ = I3 and an upper\r\ntriangular matrix R such that A = QR.\r\nSolution: Let us apply the Gram Schmidt orthogonalization process to the columns of\r\nA. That is, apply the process to the subset {(1, −1, 1, 1),(1, 0, 1, 0),(1, −2, 1, 2),(0, 1, 0, 1)}\r\nof R\r\n4\r\n.\r\nLet u1 = (1, −1, 1, 1). Define v1 = 1\r\n2\r\nu1. Let u2 = (1, 0, 1, 0). Then\r\nw2 = (1, 0, 1, 0) − hu2, v1iv1 = (1, 0, 1, 0) − v1 =\r\n1\r\n2\r\n(1, 1, 1, −1).\r\nHence, v2 =\r\n1\r\n2\r\n(1, 1, 1, −1). Let u3 = (1, −2, 1, 2). Then\r\nw3 = u3 − hu3, v1iv1 − hu3, v2iv2 = u3 − 3v1 + v2 = 0.\r\nSo, we again take u3 = (0, 1, 0, 1). Then\r\nw3 = u3 − hu3, v1iv1 − hu3, v2iv2 = u3 − 0v1 − 0v2 = u3.\r\nSo, v3 = √\r\n1\r\n2\r\n(0, 1, 0, 1). Hence,\r\nQ = [v1, v2, v3] =\r\n\r\n\r\n\r\n\r\n\r\n1\r\n2\r\n1\r\n2\r\n0\r\n−1\r\n2\r\n1\r\n2\r\n√\r\n1\r\n2\r\n1\r\n2\r\n1\r\n2\r\n0\r\n1\r\n2\r\n−1\r\n2\r\n√\r\n1\r\n2\r\n\r\n\r\n\r\n\r\n\r\n, and R =\r\n\r\n\r\n\r\n2 1 3 0\r\n0 1 −1 0\r\n0 0 0 √\r\n2\r\n\r\n\r\n .\r\nThe readers are advised to check the following:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6bd4bb4b-e932-46e5-b535-30e3fc8dada3.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=26d83370e2a06812aab456b516327db8c50ea10a6fe0f6d695f0c123adbac29d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 514
      },
      {
        "segments": [
          {
            "segment_id": "e8910507-0e25-44e6-8b94-f667abef96cb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 139,
            "page_width": 612,
            "page_height": 792,
            "content": "5.6. SUMMARY 139\r\n(a) rank (A) = 3,\r\n(b) A = QR with QtQ = I3, and\r\n(c) R a 3 × 4 upper triangular matrix with rank (R) = 3.\r\n5.6 Summary",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/e8910507-0e25-44e6-8b94-f667abef96cb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=759c6f7a0ad8d00fad92846e6b354d650c22d2fca517b049f77c3fc8885b5c8a",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "dc68bc6e-adfc-45d0-9946-c706c8aa072b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 140,
            "page_width": 612,
            "page_height": 792,
            "content": "140 CHAPTER 5. INNER PRODUCT SPACES",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/dc68bc6e-adfc-45d0-9946-c706c8aa072b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=218f7a74960a3a4e9662d699ad7f4eb9cce1e2378f8b1bd5316fc18dd228fd6b",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "f97fcdee-f238-4cba-b883-444ed4ada79e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 141,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 6\r\nEigenvalues, Eigenvectors and\r\nDiagonalization\r\n6.1 Introduction and Definitions\r\nIn this chapter, the linear transformations are from the complex vector space C\r\nn\r\nto itself.\r\nObserve that in this case, the matrix of the linear transformation is an n×n matrix. So, in\r\nthis chapter, all the matrices are square matrices and a vector x means x = (x1, x2, . . . , xn)\r\nt\r\nfor some positive integer n.\r\nExample 6.1.1. Let A be a real symmetric matrix. Consider the following problem:\r\nMaximize (Minimize) x\r\ntAx such that x ∈ Rn\r\nand x\r\ntx = 1.\r\nTo solve this, consider the Lagrangian\r\nL(x, λ) = x\r\ntAx − λ(xtx − 1) = Xn\r\ni=1\r\nXn\r\nj=1\r\naijxixj − λ(\r\nXn\r\ni=1\r\nx\r\n2\r\ni − 1).\r\nPartially differentiating L(x, λ) with respect to xifor 1 ≤ i ≤ n, we get\r\n∂L\r\n∂x1\r\n= 2a11x1 + 2a12x2 + · · · + 2a1nxn − 2λx1,\r\n∂L\r\n∂x2\r\n= 2a21x1 + 2a22x2 + · · · + 2a2nxn − 2λx2,\r\nand so on, till\r\n∂L\r\n∂xn\r\n= 2an1x1 + 2an2x2 + · · · + 2annxn − 2λxn.\r\nTherefore, to get the points of extremum, we solve for\r\n(0, 0, . . . , 0)t = ( ∂L\r\n∂x1\r\n,\r\n∂L\r\n∂x2\r\n, . . . ,\r\n∂L\r\n∂xn\r\n)\r\nt =\r\n∂L\r\n∂x\r\n= 2(Ax − λx).\r\nWe therefore need to find a λ ∈ R and 0 6= x ∈ R\r\nn\r\nsuch that Ax = λx for the extremal\r\nproblem.\r\n141",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/f97fcdee-f238-4cba-b883-444ed4ada79e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e71ebc4d4355182b85ebc99ca376f3413f6140b0b2ebdb5c94341b735ba203f2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 294
      },
      {
        "segments": [
          {
            "segment_id": "e23148c5-0089-467a-8a29-b69595134623",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 142,
            "page_width": 612,
            "page_height": 792,
            "content": "142 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nLet A be a matrix of order n. In general, we ask the question:\r\nFor what values of λ ∈ F, there exist a non-zero vector x ∈ F\r\nn\r\nsuch that\r\nAx = λx? (6.1.1)\r\nHere, F\r\nn\r\nstands for either the vector space R\r\nn over R or Cn over C. Equation (6.1.1) is\r\nequivalent to the equation\r\n(A − λI)x = 0.\r\nBy Theorem 2.4.1, this system of linear equations has a non-zero solution, if\r\nrank (A − λI) < n, or equivalently det(A − λI) = 0.\r\nSo, to solve (6.1.1), we are forced to choose those values of λ ∈ F for which det(A−λI) = 0.\r\nObserve that det(A − λI) is a polynomial in λ of degree n. We are therefore lead to the\r\nfollowing definition.\r\nDefinition 6.1.2 (Characteristic Polynomial, Characteristic Equation). Let A be a square\r\nmatrix of order n. The polynomial det(A − λI) is called the characteristic polynomial of A\r\nand is denoted by pA(λ) (in short, p(λ), if the matrix A is clear from the context). The\r\nequation p(λ) = 0 is called the characteristic equation of A. If λ ∈ F is a solution of the\r\ncharacteristic equation p(λ) = 0, then λ is called a characteristic value of A.\r\nSome books use the term eigenvalue in place of characteristic value.\r\nTheorem 6.1.3. Let A ∈ Mn(F). Suppose λ = λ0 ∈ F is a root of the characteristic\r\nequation. Then there exists a non-zero v ∈ F\r\nn\r\nsuch that Av = λ0v.\r\nProof. Since λ0 is a root of the characteristic equation, det(A − λ0I) = 0. This shows that\r\nthe matrix A − λ0I is singular and therefore by Theorem 2.4.1 the linear system\r\n(A − λ0In)x = 0\r\nhas a non-zero solution.\r\nRemark 6.1.4. Observe that the linear system Ax = λx has a solution x = 0 for every\r\nλ ∈ F. So, we consider only those x ∈ F\r\nn\r\nthat are non-zero and are also solutions of the\r\nlinear system Ax = λx.\r\nDefinition 6.1.5 (Eigenvalue and Eigenvector). Let A ∈ Mn(F) and let the linear system\r\nAx = λx has a non-zero solution x ∈ F\r\nn\r\nfor some λ ∈ F. Then\r\n1. λ ∈ F is called an eigenvalue of A,\r\n2. x ∈ F\r\nn\r\nis called an eigenvector corresponding to the eigenvalue λ of A, and\r\n3. the tuple (λ, x) is called an eigen-pair.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/e23148c5-0089-467a-8a29-b69595134623.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4fd72793aba98e2d0dfab2611aed5546234ce6937a3585f9985bc8e11bef6c42",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 415
      },
      {
        "segments": [
          {
            "segment_id": "fbb4bbcc-108a-4ed6-ae48-4a980220110b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 143,
            "page_width": 612,
            "page_height": 792,
            "content": "6.1. INTRODUCTION AND DEFINITIONS 143\r\nRemark 6.1.6. To understand the difference between a characteristic value and an eigen\u0002value, we give the following example.\r\nLet A =\r\n\"\r\n0 1\r\n−1 0#\r\n. Then pA(λ) = λ\r\n2 + 1. Also, define the linear operator TA : F2−→F2\r\nby TA(x) = Ax for every x ∈ F\r\n2\r\n.\r\n1. Suppose F = C, i.e., A ∈ M2(C). Then the roots of p(λ) = 0 in C are ±i. So, A has\r\n(i,(1, i)\r\nt\r\n) and (−i,(i, 1)t) as eigen-pairs.\r\n2. If A ∈ M2(R), then p(λ) = 0 has no solution in R. Therefore, if F = R, then A has\r\nno eigenvalue but it has ±i as characteristic values.\r\nRemark 6.1.7. 1. Let A ∈ Mn(F). Suppose (λ, x) is an eigen-pair of A. Then for\r\nany c ∈ F, c 6= 0, (λ, cx) is also an eigen-pair for A. Similarly, if x1, x2, . . . , xr are\r\nlinearly independent eigenvectors of A corresponding to the eigenvalue λ, then Pr\r\ni=1\r\ncixi\r\nis also an eigenvector of A corresponding to λ if at least one ci 6= 0. Hence, if S\r\nis a collection of eigenvectors, it is implicitly understood that the set S is linearly\r\nindependent.\r\n2. Suppose pA(λ0) = 0 for some λ0 ∈ F. Then A−λ0I is singular. If rank (A−λ0I) = r\r\nthen r < n. Hence, by Theorem 2.4.1 on page 48, the system (A−λ0I)x = 0 has n−r\r\nlinearly independent solutions. That is, A has n−r linearly independent eigenvectors\r\ncorresponding to λ0 whenever rank (A − λ0I) = r.\r\nExample 6.1.8. 1. Let A = diag(d1, d2, . . . , dn) with di ∈ R for 1 ≤ i ≤ n. Then\r\np(λ) = Qn\r\ni=1\r\n(λ − di) and the eigen-pairs are (d1, e1),(d2, e2), . . . ,(dn, en).\r\n2. Let A =\r\n\"\r\n1 1\r\n0 1#\r\n. Then p(λ) = (1 − λ)\r\n2\r\n. Hence, the characteristic equation has roots\r\n1, 1. That is, 1 is a repeated eigenvalue. But the system (A−I2)x = 0 for x = (x1, x2)\r\nt\r\nimplies that x2 = 0. Thus, x = (x1, 0)tis a solution of (A − I2)x = 0. Hence using\r\nRemark 6.1.7.1, (1, 0)tis an eigenvector. Therefore, note that 1 is a repeated\r\neigenvalue whereas there is only one eigenvector.\r\n3. Let A =\r\n\"\r\n1 0\r\n0 1#\r\n. Then p(λ) = (1 − λ)\r\n2\r\n. Again, 1 is a repeated root of p(λ) = 0.\r\nBut in this case, the system (A − I2)x = 0 has a solution for every x\r\nt ∈ R2\r\n. Hence,\r\nwe can choose any two linearly independent vectors x\r\nt\r\n, y\r\nt\r\nfrom R\r\n2\r\nto get\r\n(1, x) and (1, y) as the two eigen-pairs. In general, if x1, x2, . . . , xn ∈ R\r\nn are linearly\r\nindependent vectors then (1, x1), (1, x2), . . . ,(1, xn) are eigen-pairs of the identity\r\nmatrix, In.\r\n4. Let A =\r\n\"\r\n1 2\r\n2 1#\r\n. Then p(λ) = (λ − 3)(λ + 1) and its roots are 3, −1. Verify that\r\nthe eigen-pairs are (3,(1, 1)t) and (−1,(1, −1)t). The readers are advised to prove the\r\nlinear independence of the two eigenvectors.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fbb4bbcc-108a-4ed6-ae48-4a980220110b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bbb8d26c548003a8236e1147539c3f1db22eb85f278a32edbed59a660e620a73",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 556
      },
      {
        "segments": [
          {
            "segment_id": "fbb4bbcc-108a-4ed6-ae48-4a980220110b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 143,
            "page_width": 612,
            "page_height": 792,
            "content": "6.1. INTRODUCTION AND DEFINITIONS 143\r\nRemark 6.1.6. To understand the difference between a characteristic value and an eigen\u0002value, we give the following example.\r\nLet A =\r\n\"\r\n0 1\r\n−1 0#\r\n. Then pA(λ) = λ\r\n2 + 1. Also, define the linear operator TA : F2−→F2\r\nby TA(x) = Ax for every x ∈ F\r\n2\r\n.\r\n1. Suppose F = C, i.e., A ∈ M2(C). Then the roots of p(λ) = 0 in C are ±i. So, A has\r\n(i,(1, i)\r\nt\r\n) and (−i,(i, 1)t) as eigen-pairs.\r\n2. If A ∈ M2(R), then p(λ) = 0 has no solution in R. Therefore, if F = R, then A has\r\nno eigenvalue but it has ±i as characteristic values.\r\nRemark 6.1.7. 1. Let A ∈ Mn(F). Suppose (λ, x) is an eigen-pair of A. Then for\r\nany c ∈ F, c 6= 0, (λ, cx) is also an eigen-pair for A. Similarly, if x1, x2, . . . , xr are\r\nlinearly independent eigenvectors of A corresponding to the eigenvalue λ, then Pr\r\ni=1\r\ncixi\r\nis also an eigenvector of A corresponding to λ if at least one ci 6= 0. Hence, if S\r\nis a collection of eigenvectors, it is implicitly understood that the set S is linearly\r\nindependent.\r\n2. Suppose pA(λ0) = 0 for some λ0 ∈ F. Then A−λ0I is singular. If rank (A−λ0I) = r\r\nthen r < n. Hence, by Theorem 2.4.1 on page 48, the system (A−λ0I)x = 0 has n−r\r\nlinearly independent solutions. That is, A has n−r linearly independent eigenvectors\r\ncorresponding to λ0 whenever rank (A − λ0I) = r.\r\nExample 6.1.8. 1. Let A = diag(d1, d2, . . . , dn) with di ∈ R for 1 ≤ i ≤ n. Then\r\np(λ) = Qn\r\ni=1\r\n(λ − di) and the eigen-pairs are (d1, e1),(d2, e2), . . . ,(dn, en).\r\n2. Let A =\r\n\"\r\n1 1\r\n0 1#\r\n. Then p(λ) = (1 − λ)\r\n2\r\n. Hence, the characteristic equation has roots\r\n1, 1. That is, 1 is a repeated eigenvalue. But the system (A−I2)x = 0 for x = (x1, x2)\r\nt\r\nimplies that x2 = 0. Thus, x = (x1, 0)tis a solution of (A − I2)x = 0. Hence using\r\nRemark 6.1.7.1, (1, 0)tis an eigenvector. Therefore, note that 1 is a repeated\r\neigenvalue whereas there is only one eigenvector.\r\n3. Let A =\r\n\"\r\n1 0\r\n0 1#\r\n. Then p(λ) = (1 − λ)\r\n2\r\n. Again, 1 is a repeated root of p(λ) = 0.\r\nBut in this case, the system (A − I2)x = 0 has a solution for every x\r\nt ∈ R2\r\n. Hence,\r\nwe can choose any two linearly independent vectors x\r\nt\r\n, y\r\nt\r\nfrom R\r\n2\r\nto get\r\n(1, x) and (1, y) as the two eigen-pairs. In general, if x1, x2, . . . , xn ∈ R\r\nn are linearly\r\nindependent vectors then (1, x1), (1, x2), . . . ,(1, xn) are eigen-pairs of the identity\r\nmatrix, In.\r\n4. Let A =\r\n\"\r\n1 2\r\n2 1#\r\n. Then p(λ) = (λ − 3)(λ + 1) and its roots are 3, −1. Verify that\r\nthe eigen-pairs are (3,(1, 1)t) and (−1,(1, −1)t). The readers are advised to prove the\r\nlinear independence of the two eigenvectors.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/fbb4bbcc-108a-4ed6-ae48-4a980220110b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bbb8d26c548003a8236e1147539c3f1db22eb85f278a32edbed59a660e620a73",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 556
      },
      {
        "segments": [
          {
            "segment_id": "812b19ee-00a1-4dec-8725-68e2fae20f74",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 144,
            "page_width": 612,
            "page_height": 792,
            "content": "144 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\n5. Let A =\r\n\"\r\n1 −1\r\n1 1 #\r\n. Then p(λ) = λ\r\n2 − 2λ + 2 and its roots are 1 + i, 1 − i. Hence, over\r\nR, the matrix A has no eigenvalue. Over C, the reader is required to show that the\r\neigen-pairs are (1 + i,(i, 1)t) and (1 − i,(1, i)\r\nt\r\n).\r\nExercise 6.1.9. 1. Find the eigenvalues of a triangular matrix.\r\n2. Find eigen-pairs over C, for each of the following matrices:\r\n\"\r\n1 1 + i\r\n1 − i 1\r\n#\r\n,\r\n\"\r\ni 1 + i\r\n−1 + i i #\r\n,\r\n\"\r\ncos θ − sin θ\r\nsin θ cos θ\r\n#\r\nand \"\r\ncos θ sin θ\r\nsin θ − cos θ\r\n#\r\n.\r\n3. Let A and B be similar matrices.\r\n(a) Then prove that A and B have the same set of eigenvalues.\r\n(b) If B = P AP −1\r\nfor some invertible matrix P then prove that Px is an eigenvector\r\nof B if and only if x is an eigenvector of A.\r\n4. Let A = (aij ) be an n × n matrix. Suppose that for all i, 1 ≤ i ≤ n, Pn\r\nj=1\r\naij = a.\r\nThen prove that a is an eigenvalue of A. What is the corresponding eigenvector?\r\n5. Prove that the matrices A and At have the same set of eigenvalues. Construct a 2×2\r\nmatrix A such that the eigenvectors of A and At are different.\r\n6. Let A be a matrix such that A2 = A (A is called an idempotent matrix). Then prove\r\nthat its eigenvalues are either 0 or 1 or both.\r\n7. Let A be a matrix such that Ak = 0 (A is called a nilpotent matrix) for some positive\r\ninteger k ≥ 1. Then prove that its eigenvalues are all 0.\r\n8. Compute the eigen-pairs of the matrices \"\r\n2 1\r\n−1 0#\r\nand \"\r\n2 i\r\ni 0\r\n#\r\n.\r\nTheorem 6.1.10. Let A = [aij ] be an n × n matrix with eigenvalues λ1, λ2, . . . , λn, not\r\nnecessarily distinct. Then det(A) = Qn\r\ni=1\r\nλi and tr(A) = Pn\r\ni=1\r\naii =\r\nPn\r\ni=1\r\nλi.\r\nProof. Since λ1, λ2, . . . , λn are the n eigenvalues of A, by definition,\r\ndet(A − λIn) = p(λ) = (−1)n\r\n(λ − λ1)(λ − λ2)· · ·(λ − λn). (6.1.2)\r\n(6.1.2) is an identity in λ as polynomials. Therefore, by substituting λ = 0 in (6.1.2), we\r\nget\r\ndet(A) = (−1)n\r\n(−1)n Yn\r\ni=1\r\nλi =\r\nYn\r\ni=1\r\nλi.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/812b19ee-00a1-4dec-8725-68e2fae20f74.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=55af549705e1ed10ee3304e40a407538e5b44d8e9847ad000953131eaf60019f",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 443
      },
      {
        "segments": [
          {
            "segment_id": "c842661a-a64b-4f16-8182-99490171d4f9",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 145,
            "page_width": 612,
            "page_height": 792,
            "content": "6.1. INTRODUCTION AND DEFINITIONS 145\r\nAlso,\r\ndet(A − λIn) =\r\n\r\n\r\n\r\n\r\n\r\n\r\na11 − λ a12 · · · a1n\r\na21 a22 − λ · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 an2 · · · ann − λ\r\n\r\n\r\n\r\n\r\n\r\n\r\n(6.1.3)\r\n= a0 − λa1 + λ\r\n2\r\na2 + · · ·\r\n+(−1)n−1λ\r\nn−1\r\nan−1 + (−1)nλ\r\nn\r\n(6.1.4)\r\nfor some a0, a1, . . . , an−1 ∈ F. Note that an−1, the coefficient of (−1)n−1λ\r\nn−1\r\n, comes from\r\nthe product\r\n(a11 − λ)(a22 − λ)· · ·(ann − λ).\r\nSo, an−1 =\r\nPn\r\ni=1\r\naii = tr(A) by definition of trace.\r\nBut , from (6.1.2) and (6.1.4), we get\r\na0 − λa1 + λ\r\n2\r\na2 + · · · + (−1)n−1λ\r\nn−1\r\nan−1 + (−1)nλ\r\nn\r\n= (−1)n(λ − λ1)(λ − λ2)· · ·(λ − λn). (6.1.5)\r\nTherefore, comparing the coefficient of (−1)n−1λ\r\nn−1\r\n, we have\r\ntr(A) = an−1 = (−1){(−1)Xn\r\ni=1\r\nλi} =\r\nXn\r\ni=1\r\nλi.\r\nHence, we get the required result.\r\nExercise 6.1.11. 1. Let A be a skew symmetric matrix of order 2n + 1. Then prove\r\nthat 0 is an eigenvalue of A.\r\n2. Let A be a 3 × 3 orthogonal matrix (AAt = I). If det(A) = 1, then prove that there\r\nexists a non-zero vector v ∈ R\r\n3\r\nsuch that Av = v.\r\nLet A be an n × n matrix. Then in the proof of the above theorem, we observed that\r\nthe characteristic equation det(A−λI) = 0 is a polynomial equation of degree n in λ. Also,\r\nfor some numbers a0, a1, . . . , an−1 ∈ F, it has the form\r\nλ\r\nn + an−1λn−1 + an−2λ2 + · · · a1λ + a0 = 0.\r\nNote that, in the expression det(A − λI) = 0, λ is an element of F. Thus, we can only\r\nsubstitute λ by elements of F.\r\nIt turns out that the expression\r\nA\r\nn + an−1An−1 + an−2A2 + · · · a1A + a0I = 0\r\nholds true as a matrix identity. This is a celebrated theorem called the Cayley Hamilton\r\nTheorem. We state this theorem without proof and give some implications.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c842661a-a64b-4f16-8182-99490171d4f9.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2b874f965a8f20bf2b8a4b63cef4a9bbadca94c769a946f48df39fd28e224d5d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 384
      },
      {
        "segments": [
          {
            "segment_id": "af6c346f-2e69-4000-8302-a9e088e2424b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 146,
            "page_width": 612,
            "page_height": 792,
            "content": "146 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nTheorem 6.1.12 (Cayley Hamilton Theorem). Let A be a square matrix of order n. Then\r\nA satisfies its characteristic equation. That is,\r\nA\r\nn + an−1An−1 + an−2A2 + · · · a1A + a0I = 0\r\nholds true as a matrix identity.\r\nSome of the implications of Cayley Hamilton Theorem are as follows.\r\nRemark 6.1.13. 1. Let A =\r\n\"\r\n0 1\r\n0 0 #\r\n. Then its characteristic polynomial is p(λ) =\r\nλ\r\n2\r\n. Also, for the function, f(x) = x, f(0) = 0, and f(A) = A 6= 0. This shows that\r\nthe condition f(λ) = 0 for each eigenvalue λ of A does not imply that f(A) = 0.\r\n2. let A be a square matrix of order n with characteristic polynomial p(λ) = λ\r\nn +\r\nan−1λ\r\nn−1 + an−2λ2 + · · · a1λ + a0.\r\n(a) Then for any positive integer ℓ, we can use the division algorithm to find numbers\r\nα0, α1, . . . , αn−1 and a polynomial f(λ) such that\r\nλ\r\nℓ = f(λ)\r\n\r\nλ\r\nn + an−1λn−1 + an−2λ2 + · · · a1λ + a0\r\n\u0001\r\n+α0 + λα1 + · · · + λ\r\nn−1αn−1.\r\nHence, by the Cayley Hamilton Theorem,\r\nA\r\nℓ = α0I + α1A + · · · + αn−1An−1\r\n.\r\nThat is, we just need to compute the powers of A till n − 1.\r\nIn the language of graph theory, it says the following:\r\n“Let G be a graph on n vertices. Suppose there is no path of length n − 1 or less from\r\na vertex v to a vertex u of G. Then there is no path from v to u of any length. That is,\r\nthe graph G is disconnected and v and u are in different components.”\r\n(b) If A is non-singular then an = det(A) 6= 0 and hence\r\nA\r\n−1 =\r\n−1\r\nan\r\n[A\r\nn−1 + an−1An−2 + · · · + a1I].\r\nThis matrix identity can be used to calculate the inverse.\r\nNote that the vector A−1(as an element of the vector space of all n × n matrices) is a\r\nlinear combination of the vectors I, A, . . . , An−1.\r\nExercise 6.1.14. Find inverse of the following matrices by using the Cayley Hamilton\r\nTheorem\r\ni)\r\n\r\n\r\n\r\n2 3 4\r\n5 6 7\r\n1 1 2\r\n\r\n\r\n\r\nii)\r\n\r\n\r\n\r\n−1 −1 1\r\n1 −1 1\r\n0 1 1\r\n\r\n\r\n\r\niii)\r\n\r\n\r\n\r\n1 −2 −1\r\n−2 1 −1\r\n0 −1 2\r\n\r\n\r\n.\r\nTheorem 6.1.15. If λ1, λ2, . . . , λk are distinct eigenvalues of a matrix A with correspond\u0002ing eigenvectors x1, x2, . . . , xk, then the set {x1, x2, . . . , xk} is linearly independent.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/af6c346f-2e69-4000-8302-a9e088e2424b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b9d3949eb1ec3c7090b1efd363c1f234793dab356ea9747568c37bdcf905282a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 479
      },
      {
        "segments": [
          {
            "segment_id": "febf5b6c-c997-42a8-93db-36c35e85731b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 147,
            "page_width": 612,
            "page_height": 792,
            "content": "6.1. INTRODUCTION AND DEFINITIONS 147\r\nProof. The proof is by induction on the number m of eigenvalues. The result is obviously\r\ntrue if m = 1 as the corresponding eigenvector is non-zero and we know that any set\r\ncontaining exactly one non-zero vector is linearly independent.\r\nLet the result be true for m, 1 ≤ m < k. We prove the result for m + 1. We consider\r\nthe equation\r\nc1x1 + c2x2 + · · · + cm+1xm+1 = 0 (6.1.6)\r\nfor the unknowns c1, c2, . . . , cm+1. We have\r\n0 = A0 = A(c1x1 + c2x2 + · · · + cm+1xm+1)\r\n= c1Ax1 + c2Ax2 + · · · + cm+1Axm+1\r\n= c1λ1x1 + c2λ2x2 + · · · + cm+1λm+1xm+1. (6.1.7)\r\nFrom Equations (6.1.6) and (6.1.7), we get\r\nc2(λ2 − λ1)x2 + c3(λ3 − λ1)x3 + · · · + cm+1(λm+1 − λ1)xm+1 = 0.\r\nThis is an equation in m eigenvectors. So, by the induction hypothesis, we have\r\nci(λi − λ1) = 0 for 2 ≤ i ≤ m + 1.\r\nBut the eigenvalues are distinct implies λi − λ1 6= 0 for 2 ≤ i ≤ m + 1. We therefore get\r\nci = 0 for 2 ≤ i ≤ m + 1. Also, x1 6= 0 and therefore (6.1.6) gives c1 = 0.\r\nThus, we have the required result.\r\nWe are thus lead to the following important corollary.\r\nCorollary 6.1.16. The eigenvectors corresponding to distinct eigenvalues are linearly in\u0002dependent.\r\nExercise 6.1.17. 1. Let A, B ∈ Mn(R). Prove that\r\n(a) if λ is an eigenvalue of A then λ\r\nk\r\nis an eigenvalue of Akfor all k ∈ Z\r\n+.\r\n(b) if A is invertible and λ is an eigenvalue of A then 1\r\nλ\r\nis an eigenvalue of A−1.\r\n(c) if A is nonsingular then BA−1 and A−1B have the same set of eigenvalues.\r\n(d) AB and BA have the same non-zero eigenvalues.\r\nIn each case, what can you say about the eigenvectors?\r\n2. Let A ∈ Mn(R) be an invertible matrix and let x\r\nt\r\n, y\r\nt ∈ Rn with x 6= 0 and ytA−1x 6=\r\n0. Define B = xytA−1\r\n. Then prove that\r\n(a) λ0 = y\r\ntA−1x is an eigenvalue of B of multiplicity 1.\r\n(b) 0 is an eigenvalue of B of multiplicity n − 1 [Hint: Use Exercise 6.1.17.1d].\r\n(c) 1 + αλ0 is an eigenvalue of I + αB of multiplicity 1, for any α ∈ R, α 6= 0.\r\n(d) 1 is an eigenvalue of I + αB of multiplicity n − 1, for any α ∈ R.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/febf5b6c-c997-42a8-93db-36c35e85731b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=bf5ee6cafbd82bd41ab4888c1ac7eb334e1def195426752de3b2557bdba00569",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 438
      },
      {
        "segments": [
          {
            "segment_id": "6752542c-f1e3-4c21-962c-0099278abe35",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 148,
            "page_width": 612,
            "page_height": 792,
            "content": "148 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\n(e) det(A + αxyt\r\n) equals (1 + αλ0) det(A) for any α ∈ R. This result is known as\r\nthe Shermon-Morrison formula for determinant.\r\n3. Let A, B ∈ M2(R) such that det(A) = det(B) and tr(A) = tr(B).\r\n(a) Do A and B have the same set of eigenvalues?\r\n(b) Give examples to show that the matrices A and B need not be similar.\r\n4. Let A, B ∈ Mn(R). Also, let (λ1, u) be an eigen-pair for A and (λ2, v) be an eigen\u0002pair for B.\r\n(a) If u = αv for some α ∈ R then (λ1 + λ2, u) is an eigen-pair for A + B.\r\n(b) Give an example to show that if u and v are linearly independent then λ1 + λ2\r\nneed not be an eigenvalue of A + B.\r\n5. Let A ∈ Mn(R) be an invertible matrix with eigen-pairs (λ1, u1),(λ2, u2), . . . ,(λn, un).\r\nThen prove that B = {u1, u2, . . . , un} forms a basis of R\r\nn\r\n(R). If [b]B = (c1, c2, . . . , cn)\r\nt\r\nthen the system Ax = b has the unique solution\r\nx =\r\nc1\r\nλ1\r\nu1 +\r\nc2\r\nλ2\r\nu2 + · · · +\r\ncn\r\nλn\r\nun.\r\n6.2 Diagonalization\r\nLet A ∈ Mn(F) and let TA : F\r\nn−→Fn be the corresponding linear operator. In this section,\r\nwe ask the question “does there exist a basis B of F\r\nn\r\nsuch that TA[B,B], the matrix of the\r\nlinear operator TA with respect to the ordered basis B, is a diagonal matrix.” it will be\r\nshown that for a certain class of matrices, the answer to the above question is in affirmative.\r\nTo start with, we have the following definition.\r\nDefinition 6.2.1 (Matrix Digitalization). A matrix A is said to be diagonalizable if there\r\nexists a non-singular matrix P such that P\r\n−1AP is a diagonal matrix.\r\nRemark 6.2.2. Let A ∈ Mn(F) be a diagonalizable matrix with eigenvalues λ1, λ2, . . . , λn.\r\nBy definition, A is similar to a diagonal matrix D = diag(λ1, λ2, . . . , λn) as similar matrices\r\nhave the same set of eigenvalues and the eigenvalues of a diagonal matrix are its diagonal\r\nentries.\r\nExample 6.2.3. Let A =\r\n\"\r\n0 1\r\n−1 0 #\r\n. Then we have the following:\r\n1. Let V = R\r\n2\r\n. Then A has no real eigenvalue (see Example 6.1.7 and hence A doesn’t\r\nhave eigenvectors that are vectors in R\r\n2\r\n. Hence, there does not exist any non-singular\r\n2 × 2 real matrix P such that P\r\n−1AP is a diagonal matrix.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/6752542c-f1e3-4c21-962c-0099278abe35.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=6e0016506d00ddbf9f23c4e66d917a0b16786103d4640c8e3436d265bb304ef8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 454
      },
      {
        "segments": [
          {
            "segment_id": "5a625075-febc-4ec9-acc1-3535a669c874",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 149,
            "page_width": 612,
            "page_height": 792,
            "content": "6.2. DIAGONALIZATION 149\r\n2. In case, V = C\r\n2\r\n(C), the two complex eigenvalues of A are −i, i and the corresponding\r\neigenvectors are (i, 1)t and (−i, 1)t, respectively. Also, (i, 1)t and (−i, 1)tcan be taken\r\nas a basis of C\r\n2\r\n(C). Define U = √\r\n1\r\n2\r\n\"\r\ni −i\r\n1 1 #\r\n. Then\r\nU\r\n∗AU =\r\n\"\r\n−i 0\r\n0 i\r\n#\r\n.\r\nTheorem 6.2.4. Let A ∈ Mn(R). Then A is diagonalizable if and only if A has n linearly\r\nindependent eigenvectors.\r\nProof. Let A be diagonalizable. Then there exist matrices P and D such that\r\nP\r\n−1AP = D = diag(λ1, λ2, . . . , λn).\r\nOr equivalently, AP = P D. Let P = [u1, u2, . . . , un]. Then AP = P D implies that\r\nAui = diuifor 1 ≤ i ≤ n.\r\nSince ui\r\n’s are the columns of a non-singular matrix P, using Corollary 4.3.10, they form\r\na linearly independent set. Thus, we have shown that if A is diagonalizable then A has n\r\nlinearly independent eigenvectors.\r\nConversely, suppose A has n linearly independent eigenvectors ui\r\n, 1 ≤ i ≤ n with\r\neigenvalues λi. Then Aui = λiui. Let P = [u1, u2, . . . , un]. Since u1, u2, . . . , un are linearly\r\nindependent, by Corollary 4.3.10, P is non-singular. Also,\r\nAP = [Au1, Au2, . . . , Aun] = [λ1u1, λ2u2, . . . , λnun]\r\n= [u1, u2, . . . , un]\r\n\r\n\r\n\r\n\r\n\r\n\r\nλ1 0 0\r\n0 λ2 0\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n0 0 λn\r\n\r\n\r\n\r\n\r\n\r\n\r\n= P D.\r\nTherefore, the matrix A is diagonalizable.\r\nCorollary 6.2.5. If the eigenvalues of a A ∈ Mn(R) are distinct then A is diagonalizable.\r\nProof. As A ∈ Mn(R), it has n eigenvalues. Since all the eigenvalues of A are distinct, by\r\nCorollary 6.1.16, the n eigenvectors are linearly independent. Hence, by Theorem 6.2.4, A\r\nis diagonalizable.\r\nCorollary 6.2.6. Let λ1, λ2, . . . , λk be distinct eigenvalues of A ∈ Mn(R) and let p(λ) be\r\nits characteristic polynomial. Suppose that for each i, 1 ≤ i ≤ k, (x − λi)\r\nmi divides p(λ)\r\nbut (x − λi)\r\nmi+1 does not divides p(λ) for some positive integers mi\r\n. Then prove that A is\r\ndiagonalizable if and only if dimker(A − λiI)\r\n\u0001\r\n= mifor each i, 1 ≤ i ≤ k. Or equivalently\r\nA is diagonalizable if and only if rank(A − λiI) = n − mifor each i, 1 ≤ i ≤ k.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5a625075-febc-4ec9-acc1-3535a669c874.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=84af243348c482856e838a1afe190cea1612f33a4e3c4732a8473e1b7a8fcc4d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 447
      },
      {
        "segments": [
          {
            "segment_id": "5379bb33-23f0-4537-8231-c909206077f7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 150,
            "page_width": 612,
            "page_height": 792,
            "content": "150 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nProof. As A is diagonalizable, by Theorem 6.2.4, A has n linearly independent eigenvalues.\r\nAlso, by assumption, P\r\nk\r\ni=1\r\nmi = n as deg(p(λ)) = n. Hence, for each eigenvalue λi, 1 ≤ i ≤ k,\r\nA has exactly mi\r\nlinearly independent eigenvectors. Thus, for each i, 1 ≤ i ≤ k, the\r\nhomogeneous linear system (A − λiI)x = 0 has exactly mi\r\nlinearly independent vectors in\r\nits solution set. Therefore, dimker(A − λiI)\r\n\u0001\r\n≥ mi. Indeed dimker(A − λiI)\r\n\u0001\r\n= mifor\r\n1 ≤ i ≤ k follows from a simple counting argument.\r\nNow suppose that for each i, 1 ≤ i ≤ k, dimker(A−λiI)\r\n\u0001\r\n= mi. Then for each i, 1 ≤\r\ni ≤ k, we can choose milinearly independent eigenvectors. Also by Corollary 6.1.16, the\r\neigenvectors corresponding to distinct eigenvalues are linearly independent. Hence A has\r\nn =\r\nP\r\nk\r\ni=1\r\nmilinearly independent eigenvectors. Hence by Theorem 6.2.4, A is diagonalizable.\r\nExample 6.2.7. 1. Let A =\r\n\r\n\r\n\r\n2 1 1\r\n1 2 1\r\n0 −1 1\r\n\r\n\r\n . Then pA(λ) = (2 − λ)\r\n2\r\n(1 − λ). Hence,\r\nthe eigenvalues of A are 1, 2, 2. Verify that 1,(1, 0, −1)t\r\n\u0001\r\nand (\r\n\r\n2,(1, 1, −1)t\r\n\u0001\r\nare the\r\nonly eigen-pairs. That is, the matrix A has exactly one eigenvector corresponding to\r\nthe repeated eigenvalue 2. Hence, by Theorem 6.2.4, A is not diagonalizable.\r\n2. Let A =\r\n\r\n\r\n\r\n2 1 1\r\n1 2 1\r\n1 1 2\r\n\r\n\r\n . Then pA(λ) = (4−λ)(1−λ)\r\n2\r\n. Hence, A has eigenvalues 1, 1, 4.\r\nVerify that u1 = (1, −1, 0)t and u2 = (1, 0, −1)t are eigenvectors corresponding to 1\r\nand u3 = (1, 1, 1)tis an eigenvector corresponding to the eigenvalue 4. As u1, u2, u3\r\nare linearly independent, by Theorem 6.2.4, A is diagonalizable.\r\nNote that the vectors u1 and u2 (corresponding to the eigenvalue 1) are not orthogo\u0002nal. So, in place of u1, u2, we will take the orthogonal vectors u2 and w = 2u1 − u2\r\nas eigenvectors. Now define U = [ √\r\n1\r\n3\r\nu3, √\r\n1\r\n2\r\nu2, √\r\n1\r\n6\r\nw] =\r\n\r\n\r\n\r\n√\r\n1\r\n3\r\n√\r\n1\r\n2\r\n√\r\n1\r\n6\r\n√\r\n1\r\n3\r\n0 √−2\r\n6\r\n√\r\n1\r\n3\r\n− √\r\n1\r\n2\r\n√\r\n1\r\n6\r\n\r\n\r\n. Then U\r\nis an orthogonal matrix and U\r\n∗AU = diag(4, 1, 1).\r\nObserve that A is a symmetric matrix. In this case, we chose our eigenvectors to be\r\nmutually orthogonal. This result is true for any real symmetric matrix A. This result\r\nwill be proved later.\r\nExercise 6.2.8. 1. Are the matrices A =\r\n\"\r\ncos θ sin θ\r\n− sin θ cos θ\r\n#\r\nand B =\r\n\"\r\ncos θ sin θ\r\nsin θ − cos θ\r\n#\r\nfor some θ, 0 ≤ θ ≤ 2π, diagonalizable?\r\n2. Find the eigen-pairs of A = [aij ]n×n, where aij = a if i = j and b, otherwise.\r\n3. Let A ∈ Mn(R) and B ∈ Mm(R). Suppose C =\r\n\"\r\nA 0\r\n0 B\r\n#\r\n. Then prove that C is\r\ndiagonalizable if and only if both A and B are diagonalizable.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5379bb33-23f0-4537-8231-c909206077f7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ffde4754534694e2b6e5d05d57b6e515e70198a95da888b6049488d5b575db3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 551
      },
      {
        "segments": [
          {
            "segment_id": "5379bb33-23f0-4537-8231-c909206077f7",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 150,
            "page_width": 612,
            "page_height": 792,
            "content": "150 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nProof. As A is diagonalizable, by Theorem 6.2.4, A has n linearly independent eigenvalues.\r\nAlso, by assumption, P\r\nk\r\ni=1\r\nmi = n as deg(p(λ)) = n. Hence, for each eigenvalue λi, 1 ≤ i ≤ k,\r\nA has exactly mi\r\nlinearly independent eigenvectors. Thus, for each i, 1 ≤ i ≤ k, the\r\nhomogeneous linear system (A − λiI)x = 0 has exactly mi\r\nlinearly independent vectors in\r\nits solution set. Therefore, dimker(A − λiI)\r\n\u0001\r\n≥ mi. Indeed dimker(A − λiI)\r\n\u0001\r\n= mifor\r\n1 ≤ i ≤ k follows from a simple counting argument.\r\nNow suppose that for each i, 1 ≤ i ≤ k, dimker(A−λiI)\r\n\u0001\r\n= mi. Then for each i, 1 ≤\r\ni ≤ k, we can choose milinearly independent eigenvectors. Also by Corollary 6.1.16, the\r\neigenvectors corresponding to distinct eigenvalues are linearly independent. Hence A has\r\nn =\r\nP\r\nk\r\ni=1\r\nmilinearly independent eigenvectors. Hence by Theorem 6.2.4, A is diagonalizable.\r\nExample 6.2.7. 1. Let A =\r\n\r\n\r\n\r\n2 1 1\r\n1 2 1\r\n0 −1 1\r\n\r\n\r\n . Then pA(λ) = (2 − λ)\r\n2\r\n(1 − λ). Hence,\r\nthe eigenvalues of A are 1, 2, 2. Verify that 1,(1, 0, −1)t\r\n\u0001\r\nand (\r\n\r\n2,(1, 1, −1)t\r\n\u0001\r\nare the\r\nonly eigen-pairs. That is, the matrix A has exactly one eigenvector corresponding to\r\nthe repeated eigenvalue 2. Hence, by Theorem 6.2.4, A is not diagonalizable.\r\n2. Let A =\r\n\r\n\r\n\r\n2 1 1\r\n1 2 1\r\n1 1 2\r\n\r\n\r\n . Then pA(λ) = (4−λ)(1−λ)\r\n2\r\n. Hence, A has eigenvalues 1, 1, 4.\r\nVerify that u1 = (1, −1, 0)t and u2 = (1, 0, −1)t are eigenvectors corresponding to 1\r\nand u3 = (1, 1, 1)tis an eigenvector corresponding to the eigenvalue 4. As u1, u2, u3\r\nare linearly independent, by Theorem 6.2.4, A is diagonalizable.\r\nNote that the vectors u1 and u2 (corresponding to the eigenvalue 1) are not orthogo\u0002nal. So, in place of u1, u2, we will take the orthogonal vectors u2 and w = 2u1 − u2\r\nas eigenvectors. Now define U = [ √\r\n1\r\n3\r\nu3, √\r\n1\r\n2\r\nu2, √\r\n1\r\n6\r\nw] =\r\n\r\n\r\n\r\n√\r\n1\r\n3\r\n√\r\n1\r\n2\r\n√\r\n1\r\n6\r\n√\r\n1\r\n3\r\n0 √−2\r\n6\r\n√\r\n1\r\n3\r\n− √\r\n1\r\n2\r\n√\r\n1\r\n6\r\n\r\n\r\n. Then U\r\nis an orthogonal matrix and U\r\n∗AU = diag(4, 1, 1).\r\nObserve that A is a symmetric matrix. In this case, we chose our eigenvectors to be\r\nmutually orthogonal. This result is true for any real symmetric matrix A. This result\r\nwill be proved later.\r\nExercise 6.2.8. 1. Are the matrices A =\r\n\"\r\ncos θ sin θ\r\n− sin θ cos θ\r\n#\r\nand B =\r\n\"\r\ncos θ sin θ\r\nsin θ − cos θ\r\n#\r\nfor some θ, 0 ≤ θ ≤ 2π, diagonalizable?\r\n2. Find the eigen-pairs of A = [aij ]n×n, where aij = a if i = j and b, otherwise.\r\n3. Let A ∈ Mn(R) and B ∈ Mm(R). Suppose C =\r\n\"\r\nA 0\r\n0 B\r\n#\r\n. Then prove that C is\r\ndiagonalizable if and only if both A and B are diagonalizable.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/5379bb33-23f0-4537-8231-c909206077f7.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ffde4754534694e2b6e5d05d57b6e515e70198a95da888b6049488d5b575db3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 551
      },
      {
        "segments": [
          {
            "segment_id": "2416d55f-9a75-4863-af7d-b74e18602636",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 151,
            "page_width": 612,
            "page_height": 792,
            "content": "6.3. DIAGONALIZABLE MATRICES 151\r\n4. Let T : R\r\n5 −→ R5\r\nbe a linear operator with rank (T − I) = 3 and\r\nN (T) = {(x1, x2, x3, x4, x5) ∈ R\r\n5\r\n| x1 + x4 + x5 = 0, x2 + x3 = 0}.\r\n(a) Determine the eigenvalues of T?\r\n(b) Find the number of linearly independent eigenvectors corresponding to each\r\neigenvalue?\r\n(c) Is T diagonalizable? Justify your answer.\r\n5. Let A be a non-zero square matrix such that A2 = 0. Prove that A cannot be diago\u0002nalized. [Hint: Use Remark 6.2.2.]\r\n6. Are the following matrices diagonalizable?\r\ni)\r\n\r\n\r\n\r\n\r\n\r\n1 3 2 1\r\n0 2 3 1\r\n0 0 −1 1\r\n0 0 0 4\r\n\r\n\r\n\r\n\r\n\r\n, ii)\r\n\r\n\r\n\r\n1 0 −1\r\n0 0 1\r\n0 2 0\r\n\r\n\r\n , iii)\r\n\r\n\r\n\r\n1 −3 3\r\n0 −5 6\r\n0 −3 4\r\n\r\n\r\n\r\nand iv)\r\n\"\r\n2 i\r\ni 0\r\n#\r\n.\r\n6.3 Diagonalizable Matrices\r\nIn this section, we will look at some special classes of square matrices that are diagonal\u0002izable. Recall that for a matrix A = [aij ], A∗ = [aji] = At = A\r\nt\r\n, is called the conjugate\r\ntranspose of A. We also recall the following definitions.\r\nDefinition 6.3.1 (Special Matrices). 1. A matrix A ∈ Mn(C) is called\r\n(a) a Hermitian matrix if A∗ = A.\r\n(b) a unitary matrix if A A∗ = A∗A = In.\r\n(c) a skew-Hermitian matrix if A∗ = −A.\r\n(d) a normal matrix if A∗A = AA∗.\r\n2. A matrix A ∈ Mn(R) is called\r\n(a) a symmetric matrix if At = A.\r\n(b) an orthogonal matrix if A At = AtA = In.\r\n(c) a skew-symmetric matrix if At = −A.\r\nNote that a symmetric matrix is always Hermitian, a skew-symmetric matrix is always\r\nskew-Hermitian and an orthogonal matrix is always unitary. Each of these matrices are\r\nnormal. If A is a unitary matrix then A∗ = A−1\r\n.\r\nExample 6.3.2. 1. Let B =\r\n\"\r\ni 1\r\n−1 i\r\n#\r\n. Then B is skew-Hermitian.\r\n2. Let A = √\r\n1\r\n2\r\n\"\r\n1 i\r\ni 1\r\n#\r\nand B =\r\n\"\r\n1 1\r\n−1 1#\r\n. Then A is a unitary matrix and B is a\r\nnormal matrix. Note that √\r\n2A is also a normal matrix.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/2416d55f-9a75-4863-af7d-b74e18602636.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=44224efd5144b07025c2ae65c8f13b30f08c63751fe523d0034b9147d8973b59",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 402
      },
      {
        "segments": [
          {
            "segment_id": "9db04347-2da4-4876-9b0b-b3691c33d236",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 152,
            "page_width": 612,
            "page_height": 792,
            "content": "152 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nDefinition 6.3.3 (Unitary Equivalence). Let A, B ∈ Mn(C). They are called unitarily\r\nequivalent if there exists a unitary matrix U such that A = U\r\n∗BU. As U is a unitary\r\nmatrix, U\r\n∗ = U−1\r\n. Hence, A is also unitarily similar to B.\r\nExercise 6.3.4. 1. Let A be a square matrix such that UAU∗is a diagonal matrix for\r\nsome unitary matrix U. Prove that A is a normal matrix.\r\n2. Let A ∈ Mn(C). Then A =\r\n1\r\n2\r\n(A+A∗)+ 1\r\n2\r\n(A−A∗), where 1\r\n2\r\n(A+A∗) is the Hermitian\r\npart of A and 1\r\n2\r\n(A− A∗) is the skew-Hermitian part of A. Recall that a similar result\r\nwas given in Exercise 1.3.3.1.\r\n3. Let A ∈ Mn(C). Prove that A − A∗is always skew-Hermitian.\r\n4. Every square matrix can be uniquely expressed as A = S + iT, where both S and T\r\nare Hermitian matrices.\r\n5. Does there exist a unitary matrix U such that U\r\n−1AU = B where\r\nA =\r\n\r\n\r\n\r\n1 1 4\r\n0 2 2\r\n0 0 3\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n2 −1 3√2\r\n0 1 √\r\n2\r\n0 0 3\r\n\r\n\r\n .\r\nTheorem 6.3.5. Let A ∈ Mn(C) be a Hermitian matrix. Then\r\n1. the eigenvalues, λi, 1 ≤ i ≤ n, of A are real.\r\n2. A is unitarily diagonalizable. That is, there exists a unitary matrix U such that\r\nU\r\n∗AU = D; where D = diag(λ1, . . . , λn). In other words, the eigenvectors of A\r\nform an orthonormal basis of C\r\nn\r\n.\r\nProof. For the proof of Part 1, let (λ, x) be an eigen-pair. Then Ax = λx and A∗ = A\r\nimplies that x\r\n∗A = x∗A∗ = (Ax)∗ = (λx)∗ = λx∗\r\n. Hence,\r\nλx\r\n∗x = x∗\r\n(λx) = x\r\n∗\r\n(Ax) = (x\r\n∗A)x = (λx∗\r\n)x = λx\r\n∗x.\r\nAs x is an eigenvector, x 6= 0 and therefore kxk\r\n2 = x∗x 6= 0. Thus λ = λ. That is, λ is a\r\nreal number.\r\nFor the proof of Part 2, we use induction on n, the size of the matrix. The result is\r\nclearly true for n = 1. Let the result be true for n = k − 1. we need to prove the result for\r\nn = k.\r\nLet (λ1, x) be an eigen-pair of a k × k matrix A with kxk = 1. Then by Part 1,\r\nλ1 ∈ R. As {x} is a linearly independent set, by Theorem 3.3.11 and the Gram-Schmidt\r\nOrthogonalization process, we get an orthonormal basis {x, u2, . . . , uk} of C\r\nk\r\n. Let U1 =\r\n[x, u2, . . . , uk] (the vectors x, u2, . . . , uk are columns of the matrix U1). Then U1 is a\r\nunitary matrix. In particular, u\r\n∗\r\ni x = 0, for 2 ≤ i ≤ k. Therefore, for 2 ≤ i ≤ k,\r\nx\r\n∗\r\n(Aui) = (Aui)\r\n∗x = (u∗\r\ni A\r\n∗\r\n)x = u\r\n∗\r\ni\r\n(A\r\n∗x) = u∗\r\ni\r\n(Ax) = u\r\n∗\r\ni\r\n(λ1x) = λ1(u\r\n∗\r\ni x) = 0 and",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9db04347-2da4-4876-9b0b-b3691c33d236.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5035f6fe4bfc072a3f4765721b1287a5c1ad88d4bcc38b45635d7e057f419e1a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "9db04347-2da4-4876-9b0b-b3691c33d236",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 152,
            "page_width": 612,
            "page_height": 792,
            "content": "152 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nDefinition 6.3.3 (Unitary Equivalence). Let A, B ∈ Mn(C). They are called unitarily\r\nequivalent if there exists a unitary matrix U such that A = U\r\n∗BU. As U is a unitary\r\nmatrix, U\r\n∗ = U−1\r\n. Hence, A is also unitarily similar to B.\r\nExercise 6.3.4. 1. Let A be a square matrix such that UAU∗is a diagonal matrix for\r\nsome unitary matrix U. Prove that A is a normal matrix.\r\n2. Let A ∈ Mn(C). Then A =\r\n1\r\n2\r\n(A+A∗)+ 1\r\n2\r\n(A−A∗), where 1\r\n2\r\n(A+A∗) is the Hermitian\r\npart of A and 1\r\n2\r\n(A− A∗) is the skew-Hermitian part of A. Recall that a similar result\r\nwas given in Exercise 1.3.3.1.\r\n3. Let A ∈ Mn(C). Prove that A − A∗is always skew-Hermitian.\r\n4. Every square matrix can be uniquely expressed as A = S + iT, where both S and T\r\nare Hermitian matrices.\r\n5. Does there exist a unitary matrix U such that U\r\n−1AU = B where\r\nA =\r\n\r\n\r\n\r\n1 1 4\r\n0 2 2\r\n0 0 3\r\n\r\n\r\n\r\nand B =\r\n\r\n\r\n\r\n2 −1 3√2\r\n0 1 √\r\n2\r\n0 0 3\r\n\r\n\r\n .\r\nTheorem 6.3.5. Let A ∈ Mn(C) be a Hermitian matrix. Then\r\n1. the eigenvalues, λi, 1 ≤ i ≤ n, of A are real.\r\n2. A is unitarily diagonalizable. That is, there exists a unitary matrix U such that\r\nU\r\n∗AU = D; where D = diag(λ1, . . . , λn). In other words, the eigenvectors of A\r\nform an orthonormal basis of C\r\nn\r\n.\r\nProof. For the proof of Part 1, let (λ, x) be an eigen-pair. Then Ax = λx and A∗ = A\r\nimplies that x\r\n∗A = x∗A∗ = (Ax)∗ = (λx)∗ = λx∗\r\n. Hence,\r\nλx\r\n∗x = x∗\r\n(λx) = x\r\n∗\r\n(Ax) = (x\r\n∗A)x = (λx∗\r\n)x = λx\r\n∗x.\r\nAs x is an eigenvector, x 6= 0 and therefore kxk\r\n2 = x∗x 6= 0. Thus λ = λ. That is, λ is a\r\nreal number.\r\nFor the proof of Part 2, we use induction on n, the size of the matrix. The result is\r\nclearly true for n = 1. Let the result be true for n = k − 1. we need to prove the result for\r\nn = k.\r\nLet (λ1, x) be an eigen-pair of a k × k matrix A with kxk = 1. Then by Part 1,\r\nλ1 ∈ R. As {x} is a linearly independent set, by Theorem 3.3.11 and the Gram-Schmidt\r\nOrthogonalization process, we get an orthonormal basis {x, u2, . . . , uk} of C\r\nk\r\n. Let U1 =\r\n[x, u2, . . . , uk] (the vectors x, u2, . . . , uk are columns of the matrix U1). Then U1 is a\r\nunitary matrix. In particular, u\r\n∗\r\ni x = 0, for 2 ≤ i ≤ k. Therefore, for 2 ≤ i ≤ k,\r\nx\r\n∗\r\n(Aui) = (Aui)\r\n∗x = (u∗\r\ni A\r\n∗\r\n)x = u\r\n∗\r\ni\r\n(A\r\n∗x) = u∗\r\ni\r\n(Ax) = u\r\n∗\r\ni\r\n(λ1x) = λ1(u\r\n∗\r\ni x) = 0 and",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9db04347-2da4-4876-9b0b-b3691c33d236.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5035f6fe4bfc072a3f4765721b1287a5c1ad88d4bcc38b45635d7e057f419e1a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 546
      },
      {
        "segments": [
          {
            "segment_id": "a1e15e91-205a-4ee2-8eda-065bf206ce33",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 153,
            "page_width": 612,
            "page_height": 792,
            "content": "6.3. DIAGONALIZABLE MATRICES 153\r\nU\r\n∗\r\n1 AU1 = U\r\n∗\r\n1\r\n[Ax, Au2, · · · , Auk] =\r\n\r\n\r\n\r\n\r\n\r\n\r\nx\r\n∗\r\nu\r\n∗\r\n2\r\n.\r\n.\r\n.\r\nu\r\n∗\r\nk\r\n\r\n\r\n\r\n\r\n\r\n\r\n[λ1x, Au2, · · · , Auk]\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\nλ1x\r\n∗x · · · x∗Auk\r\nu\r\n∗\r\n2\r\n(λ1x) · · · u\r\n∗\r\n2\r\n(Auk)\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nu\r\n∗\r\nk\r\n(λ1x) · · · u\r\n∗\r\nk\r\n(Auk)\r\n\r\n\r\n\r\n\r\n\r\n\r\n=\r\n\r\n\r\n\r\n\r\n\r\n\r\nλ1 0\r\n0\r\n.\r\n.\r\n. B\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n,\r\nwhere B is a (k − 1) × (k − 1) matrix. As (U\r\n∗\r\n1 AU1)\r\n∗ = U∗\r\n1 AU1 and λ1 ∈ R, the matrix B is\r\nalso Hermitian. Therefore, by induction hypothesis there exists a (k − 1) × (k − 1) unitary\r\nmatrix U2 such that U\r\n∗\r\n2BU2 = D2 = diag(λ2, . . . , λk), where λi ∈ R, for 2 ≤ i ≤ k are the\r\neigenvalues of B. Define U = U1\r\n\"\r\n1 0\r\n0 U2\r\n#\r\n. Then U is a unitary matrix and\r\nU\r\n∗AU =\r\n \r\nU1\r\n\"\r\n1 0\r\n0 U2\r\n#!∗\r\nA\r\n \r\nU1\r\n\"\r\n1 0\r\n0 U2\r\n#!\r\n=\r\n \"1 0\r\n0 U\r\n∗\r\n2\r\n#\r\nU\r\n∗\r\n1\r\n!\r\nA\r\n \r\nU1\r\n\"\r\n1 0\r\n0 U2\r\n#! =\"\r\n1 0\r\n0 U\r\n∗\r\n2\r\n#\r\n\r\nU\r\n∗\r\n1 AU1\r\n\u0001\r\n\"\r\n1 0\r\n0 U2\r\n#\r\n=\r\n\"\r\n1 0\r\n0 U\r\n∗\r\n2\r\n# \"λ1 0\r\n0 B\r\n# \"1 0\r\n0 U2\r\n#\r\n=\r\n\"\r\nλ1 0\r\n0 U\r\n∗\r\n2 BU2\r\n#\r\n=\r\n\"\r\nλ1 0\r\n0 D2\r\n#\r\n.\r\nObserve that λ2, . . . , λn are also the eigenvalues of A. Thus, U\r\n∗AU is a diagonal matrix\r\nwith diagonal entries λ1, λ2, . . . , λk, the eigenvalues of A. Hence, the result follows.\r\nCorollary 6.3.6. Let A ∈ Mn(R) be a symmetric matrix. Then\r\n1. the eigenvalues of A are all real,\r\n2. the eigenvectors can be chosen to have real entries and\r\n3. the eigenvectors also form an orthonormal basis of R\r\nn\r\n.\r\nProof. As A is symmetric, A is also a Hermitian matrix. Hence, by Theorem 6.3.5, the\r\neigenvalues of A are all real. Let (λ, x) be an eigen-pair of A. Suppose x\r\nt ∈ Cn\r\n. Then\r\nthere exist y\r\nt\r\n, z\r\nt ∈ Rn\r\nsuch that x = y + iz. So,\r\nAx = λx =⇒ A(y + iz) = λ(y + iz).\r\nComparing the real and imaginary parts, we get Ay = λy and Az = λz. Thus, we can\r\nchoose the eigenvectors to have real entries.\r\nThe readers are advised to prove the orthonormality of the eigenvectors (see the proof\r\nof Theorem 6.3.5).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a1e15e91-205a-4ee2-8eda-065bf206ce33.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=346e203a0bd917335cf7bab39488997332d02360ac5ad2790a2220c6c2eeb276",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 500
      },
      {
        "segments": [
          {
            "segment_id": "ae397e75-509a-4b4a-ac6d-8f13a491cf1a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 154,
            "page_width": 612,
            "page_height": 792,
            "content": "154 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nExercise 6.3.7. 1. Let A be a skew-Hermitian matrix. Then the eigenvalues of A are\r\neither zero or purely imaginary. Also, the eigenvectors corresponding to distinct\r\neigenvalues are mutually orthogonal. [Hint: Carefully see the proof of Theorem 6.3.5.]\r\n2. Let A be a normal matrix with (λ, x) as an eigen-pair. Then\r\n(a) (A∗)\r\nkx for k ∈ Z+ is also an eigenvector corresponding to λ.\r\n(b) (λ, x) is an eigen-pair for A∗. [Hint: Verify kA∗x − λxk\r\n2 = kAx − λxk2\r\n.]\r\n3. Let A be an n × n unitary matrix. Then\r\n(a) the rows of A form an orthonormal basis of C\r\nn\r\n.\r\n(b) the columns of A form an orthonormal basis of C\r\nn\r\n.\r\n(c) for any two vectors x, y ∈ C\r\nn×1\r\n, hAx, Ayi = hx, yi.\r\n(d) for any vector x ∈ C\r\nn×1\r\n, kAxk = kxk.\r\n(e) |λ| = 1 for any eigenvalue λ of A.\r\n(f) the eigenvectors x, y corresponding to distinct eigenvalues λ and µ satisfy hx, yi =\r\n0. That is, if (λ, x) and (µ, y) are eigen-pairs with λ 6= µ, then x and y are mu\u0002tually orthogonal.\r\n4. Show that the matrices A =\r\n\"\r\n4 4\r\n0 4#\r\nand B =\r\n\"\r\n10 9\r\n−4 −2\r\n#\r\nare similar. Is it possible to\r\nfind a unitary matrix U such that A = U\r\n∗BU?\r\n5. Let A be a 2 × 2 orthogonal matrix. Then prove the following:\r\n(a) if det(A) = 1, then A =\r\n\"\r\ncos θ − sin θ\r\nsin θ cos θ\r\n#\r\nfor some θ, 0 ≤ θ < 2π. That is, A\r\ncounterclockwise rotates every point in R\r\n2\r\nby an angle θ.\r\n(b) if det A = −1, then A =\r\n\"\r\ncos θ sin θ\r\nsin θ − cos θ\r\n#\r\nfor some θ, 0 ≤ θ < 2π. That is, A\r\nreflects every point in R\r\n2 about a line passing through origin. Determine this\r\nline. Or equivalently, there exists a non-singular matrix P such that P\r\n\"\r\n−1AP =\r\n1 0\r\n0 −1\r\n#\r\n.\r\n6. Let A be a 3 × 3 orthogonal matrix. Then prove the following:\r\n(a) if det(A) = 1, then A is a rotation about a fixed axis, in the sense that A\r\nhas an eigen-pair (1, x) such that the restriction of A to the plane x\r\n⊥ is a two\r\ndimensional rotation in x\r\n⊥.\r\n(b) if det A = −1, then A corresponds to a reflection through a plane P, followed by\r\na rotation about the line through origin that is orthogonal to P.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ae397e75-509a-4b4a-ac6d-8f13a491cf1a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f3fe02cda7fe76c3584550cd1004408f2d34d380a99b124867a915f2c3c143e2",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 451
      },
      {
        "segments": [
          {
            "segment_id": "103b8bbb-b521-4f40-bc9a-afebec0fa8eb",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 155,
            "page_width": 612,
            "page_height": 792,
            "content": "6.3. DIAGONALIZABLE MATRICES 155\r\n7. Let A =\r\n\r\n\r\n\r\n2 1 1\r\n1 2 1\r\n1 1 2\r\n\r\n\r\n . Find a non-singular matrix P such that P\r\n−1AP = diag (4, 1, 1).\r\nUse this to compute A301.\r\n8. Let A be a Hermitian matrix. Then prove that rank(A) equals the number of non-zero\r\neigenvalues of A.\r\nRemark 6.3.8. Let A and B be the 2 × 2 matrices in Exercise 6.3.7.4. Then A and B\r\nwere similar matrices but they were not unitarily equivalent. In numerical calculations,\r\nunitary transformations are preferred as compared to similarity transformations due to the\r\nfollowing main reasons:\r\n1. Exercise 6.3.7.3d implies that an orthonormal change of basis does not alter the sum\r\nof squares of the absolute values of the entries. This need not be true under a non\u0002singularity change of basis.\r\n2. For a unitary matrix U, U −1 = U\r\n∗ and hence unitary equivalence is computationally\r\nsimpler.\r\n3. Also there is no round-off error in the operation of “conjugate transpose”.\r\nWe next prove the Schur’s Lemma and use it to show that normal matrices are unitarily\r\ndiagonalizable. The proof is similar to the proof of Theorem 6.3.5. We give it again so\r\nthat the readers have a better understanding of unitary transformations.\r\nLemma 6.3.9. (Schur’s Lemma) Let A ∈ Mn(C). Then A is unitarily similar to an upper\r\ntriangular matrix.\r\nProof. We will prove the result by induction on n. The result is clearly true for n = 1. Let\r\nthe result be true for n = k − 1. we need to prove the result for n = k.\r\nLet (λ1, x) be an eigen-pair of a k × k matrix A with kxk = 1. Let us extend the set\r\n{x}, a linearly independent set, to form an orthonormal basis {x, u2, u3, . . . , uk} (using\r\nGram-Schmidt Orthogonalization) of C\r\nk\r\n. Then U1 = [x u2 · · · uk] is a unitary matrix and\r\nU\r\n∗\r\n1 AU1 = U\r\n∗\r\n1\r\n[Ax Au2 · · · Auk] =\r\n\r\n\r\n\r\n\r\n\r\n\r\nx\r\n∗\r\nu\r\n∗\r\n2\r\n.\r\n.\r\n.\r\nu\r\n∗\r\nk\r\n\r\n\r\n\r\n\r\n\r\n\r\n[λ1x Au2 · · · Auk] =\r\n\r\n\r\n\r\n\r\n\r\n\r\nλ1 ∗\r\n0\r\n.\r\n.\r\n. B\r\n0\r\n\r\n\r\n\r\n\r\n\r\n\r\n,\r\nwhere B is a (k−1)×(k−1) matrix. By induction hypothesis there exists a (k−1)×(k−1)\r\nunitary matrix U2 such that U\r\n∗\r\n2BU2 is an upper triangular matrix with diagonal entries\r\nλ2, . . . , λk, the eigenvalues of B. Define U = U1\r\n\"\r\n1 0\r\n0 U2\r\n#\r\n. Then check that U is a unitary\r\nmatrix and U\r\n∗AU is an upper triangular matrix with diagonal entries λ1, λ2, . . . , λk, the\r\neigenvalues of the matrix A. Hence, the result follows.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/103b8bbb-b521-4f40-bc9a-afebec0fa8eb.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4fa7d8445d26699ea12b4a9fc9669e50d3b866e4156e13240b87f49e3fca2ee1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 487
      },
      {
        "segments": [
          {
            "segment_id": "1f0d5f76-4793-4ea1-816a-f0c93bb77629",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 156,
            "page_width": 612,
            "page_height": 792,
            "content": "156 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nIn Lemma 6.3.9, it can be observed that whenever A is a normal matrix then the matrix\r\nB is also a normal matrix. It is also known that if T is an upper triangular matrix that\r\nsatisfies T T∗ = T\r\n∗T then T is a diagonal matrix (see Exercise 16). Thus, it follows that\r\nnormal matrices are diagonalizable. We state it as a remark.\r\nRemark 6.3.10 (The Spectral Theorem for Normal Matrices). Let A be an n × n normal\r\nmatrix. Then there exists an orthonormal basis {x1, x2, . . . , xn} of C\r\nn\r\n(C) such that Axi =\r\nλixifor 1 ≤ i ≤ n. In particular, if U − [x1, x2, . . . , xn] then U\r\n∗AU is a diagonal matrix.\r\nExercise 6.3.11. 1. Let A ∈ Mn(R) be an invertible matrix. Prove that AAt = P DPt\r\n,\r\nwhere P is an orthogonal and D is a diagonal matrix with positive diagonal entries.\r\n2. Let A =\r\n\r\n\r\n\r\n1 1 1\r\n0 2 1\r\n0 0 3\r\n\r\n\r\n, B =\r\n\r\n\r\n\r\n2 −1\r\n√\r\n2\r\n0 1 0\r\n0 0 3\r\n\r\n\r\n\r\nand U = √\r\n1\r\n2\r\n\r\n\r\n\r\n1 1 0\r\n1 −1 0\r\n0 0 √2\r\n\r\n\r\n . Prove that A\r\nand B are unitarily equivalent via the unitary matrix U. Hence, conclude that the\r\nupper triangular matrix obtained in the ”Schur’s Lemma” need not be unique.\r\n3. Prove Remark 6.3.10.\r\n4. Let A be a normal matrix. If all the eigenvalues of A are 0 then prove that A = 0.\r\nWhat happens if all the eigenvalues of A are 1?\r\n5. Let A be an n × n matrix. Prove that if A is\r\n(a) Hermitian and xAx\r\n∗ = 0 for all x ∈ Cn\r\nthen A = 0.\r\n(b) a real, symmetric matrix and xAx\r\nt = 0 for all x ∈ Rn\r\nthen A = 0.\r\nDo these results hold for arbitrary matrices?\r\nWe end this chapter with an application of the theory of diagonalization to the study\r\nof conic sections in analytic geometry and the study of maxima and minima in analysis.\r\n6.4 Sylvester’s Law of Inertia and Applications\r\nDefinition 6.4.1 (Bilinear Form). Let A be an n × n real symmetric matrix. A bilinear\r\nform in x = (x1, x2, . . . , xn)\r\nt\r\n, y = (y1, y2, . . . , yn)\r\nt\r\nis an expression of the type\r\nQ(x, y) = y\r\ntAx =\r\nXn\r\ni,j=1\r\naijxiyj .\r\nDefinition 6.4.2 (Sesquilinear Form). Let A be an n×n Hermitian matrix. A sesquilinear\r\nform in x = (x1, x2, . . . , xn)\r\n∗\r\n, y = (y1, y2, . . . , yn)\r\n∗\r\nis given by\r\nH(x, y) = y\r\n∗Ax =\r\nXn\r\ni,j=1\r\naijxiyj .",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1f0d5f76-4793-4ea1-816a-f0c93bb77629.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=1cb33ee9b4c06ca034ab0fc9e1f4f9ed25ba0d6d24d09c4630d0b758b5bf1a4a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 488
      },
      {
        "segments": [
          {
            "segment_id": "a6e43ced-102c-452d-9ead-57d42b6b236d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 157,
            "page_width": 612,
            "page_height": 792,
            "content": "6.4. SYLVESTER’S LAW OF INERTIA AND APPLICATIONS 157\r\nObserve that if A = In then the bilinear (sesquilinear) form reduces to the standard\r\nreal (complex) inner product. Also, it can be easily seen that H(x, y) is ‘linear’ in x, the\r\nfirst component and ‘conjugate linear’ in y, the second component. The expression Q(x, x)\r\nis called the quadratic form and H(x, x) the Hermitian form. We generally write Q(x) and\r\nH(x) in place of Q(x, x) and H(x, x), respectively. It can be easily shown that for any\r\nchoice of x, the Hermitian form H(x) is a real number. Hence, for any real number α, the\r\nequation H(x) = α, represents a conic in C\r\nn\r\n.\r\nExample 6.4.3. Let A =\r\n\"\r\n1 2 − i\r\n2 + i 2\r\n#\r\n. Then A∗ = A and for x = (x1, x2)\r\n∗\r\n,\r\nH(x) = x\r\n∗Ax = (x1, x2)\r\n\"\r\n1 2 − i\r\n2 + i 2\r\n# x1\r\nx2\r\n!\r\n= x1x1 + 2x2x2 + (2 − i)x1x2 + (2 + i)x2x1\r\n= |x1|\r\n2 + 2|x2|2 + 2Re[(2 − i)x1x2]\r\nwhere ‘Re’ denotes the real part of a complex number. This shows that for every choice of\r\nx the Hermitian form is always real. Why?\r\nThe main idea of this section is to express H(x) as sum of squares and hence determine\r\nthe possible values that it can take. Note that if we replace x by cx, where c is any complex\r\nnumber, then H(x) simply gets multiplied by |c|\r\n2 and hence one needs to study only those\r\nx for which kxk = 1, i.e., x is a normalized vector.\r\nLet A∗ = A ∈ Mn(C). Then by Theorem 6.3.5, the eigenvalues λi\r\n, 1 ≤ i ≤ n, of A\r\nare real and there exists a unitary matrix U such that U\r\n∗AU = D ≡ diag(λ1, λ2, . . . , λn).\r\nNow define, z = (z1, z2, . . . , zn)\r\n∗ = U∗x. Then kzk = 1, x = Uz and\r\nH(x) = z\r\n∗U∗AUz = z∗Dz =\r\nXn\r\ni=1\r\nλi|zi|\r\n2 =\r\nX\r\np\r\ni=1\r\n\f\r\n\f\r\n\f\r\np\r\n|λi| zi\r\n\f\r\n\f\r\n\f\r\n2\r\n−\r\nXr\r\ni=p+1\r\n\f\r\n\f\r\n\f\r\np\r\n|λi| zi\r\n\f\r\n\f\r\n\f\r\n2\r\n. (6.4.1)\r\nThus, the possible values of H(x) depend only on the eigenvalues of A. Since U is an invert\u0002ible matrix, the components zi\r\n’s of z = U\r\n∗x are commonly known as linearly independent\r\nlinear forms. Also, note that in Equation (6.4.1), the number p (respectively r − p) seems\r\nto be related to the number of eigenvalues of A that are positive (respectively negative).\r\nThis is indeed true. That is, in any expression of H(x) as a sum of n absolute squares of\r\nlinearly independent linear forms, the number p (respectively r − p) gives the number of\r\npositive (respectively negative) eigenvalues of A. This is stated as the next lemma and it\r\npopularly known as the ‘Sylvester’s law of inertia’.\r\nLemma 6.4.4. Let A ∈ Mn(C) be a Hermitian matrix and let x = (x1, x2, . . . , xn)\r\n∗\r\n. Then\r\nevery Hermitian form H(x) = x\r\n∗Ax, in n variables can be written as\r\nH(x) = |y1|\r\n2 + |y2|2 + · · · + |yp|2 − |yp+1|2 − · · · − |yr|2\r\nwhere y1, y2, . . . , yr are linearly independent linear forms in x1, x2, . . . , xn, and the integers\r\np and r satisfying 0 ≤ p ≤ r ≤ n, depend only on A.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a6e43ced-102c-452d-9ead-57d42b6b236d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cd7baf8bb850b783e1e41ab7dbe6a0ee6c24363e02e52034d8aedbef125c7ab6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 591
      },
      {
        "segments": [
          {
            "segment_id": "a6e43ced-102c-452d-9ead-57d42b6b236d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 157,
            "page_width": 612,
            "page_height": 792,
            "content": "6.4. SYLVESTER’S LAW OF INERTIA AND APPLICATIONS 157\r\nObserve that if A = In then the bilinear (sesquilinear) form reduces to the standard\r\nreal (complex) inner product. Also, it can be easily seen that H(x, y) is ‘linear’ in x, the\r\nfirst component and ‘conjugate linear’ in y, the second component. The expression Q(x, x)\r\nis called the quadratic form and H(x, x) the Hermitian form. We generally write Q(x) and\r\nH(x) in place of Q(x, x) and H(x, x), respectively. It can be easily shown that for any\r\nchoice of x, the Hermitian form H(x) is a real number. Hence, for any real number α, the\r\nequation H(x) = α, represents a conic in C\r\nn\r\n.\r\nExample 6.4.3. Let A =\r\n\"\r\n1 2 − i\r\n2 + i 2\r\n#\r\n. Then A∗ = A and for x = (x1, x2)\r\n∗\r\n,\r\nH(x) = x\r\n∗Ax = (x1, x2)\r\n\"\r\n1 2 − i\r\n2 + i 2\r\n# x1\r\nx2\r\n!\r\n= x1x1 + 2x2x2 + (2 − i)x1x2 + (2 + i)x2x1\r\n= |x1|\r\n2 + 2|x2|2 + 2Re[(2 − i)x1x2]\r\nwhere ‘Re’ denotes the real part of a complex number. This shows that for every choice of\r\nx the Hermitian form is always real. Why?\r\nThe main idea of this section is to express H(x) as sum of squares and hence determine\r\nthe possible values that it can take. Note that if we replace x by cx, where c is any complex\r\nnumber, then H(x) simply gets multiplied by |c|\r\n2 and hence one needs to study only those\r\nx for which kxk = 1, i.e., x is a normalized vector.\r\nLet A∗ = A ∈ Mn(C). Then by Theorem 6.3.5, the eigenvalues λi\r\n, 1 ≤ i ≤ n, of A\r\nare real and there exists a unitary matrix U such that U\r\n∗AU = D ≡ diag(λ1, λ2, . . . , λn).\r\nNow define, z = (z1, z2, . . . , zn)\r\n∗ = U∗x. Then kzk = 1, x = Uz and\r\nH(x) = z\r\n∗U∗AUz = z∗Dz =\r\nXn\r\ni=1\r\nλi|zi|\r\n2 =\r\nX\r\np\r\ni=1\r\n\f\r\n\f\r\n\f\r\np\r\n|λi| zi\r\n\f\r\n\f\r\n\f\r\n2\r\n−\r\nXr\r\ni=p+1\r\n\f\r\n\f\r\n\f\r\np\r\n|λi| zi\r\n\f\r\n\f\r\n\f\r\n2\r\n. (6.4.1)\r\nThus, the possible values of H(x) depend only on the eigenvalues of A. Since U is an invert\u0002ible matrix, the components zi\r\n’s of z = U\r\n∗x are commonly known as linearly independent\r\nlinear forms. Also, note that in Equation (6.4.1), the number p (respectively r − p) seems\r\nto be related to the number of eigenvalues of A that are positive (respectively negative).\r\nThis is indeed true. That is, in any expression of H(x) as a sum of n absolute squares of\r\nlinearly independent linear forms, the number p (respectively r − p) gives the number of\r\npositive (respectively negative) eigenvalues of A. This is stated as the next lemma and it\r\npopularly known as the ‘Sylvester’s law of inertia’.\r\nLemma 6.4.4. Let A ∈ Mn(C) be a Hermitian matrix and let x = (x1, x2, . . . , xn)\r\n∗\r\n. Then\r\nevery Hermitian form H(x) = x\r\n∗Ax, in n variables can be written as\r\nH(x) = |y1|\r\n2 + |y2|2 + · · · + |yp|2 − |yp+1|2 − · · · − |yr|2\r\nwhere y1, y2, . . . , yr are linearly independent linear forms in x1, x2, . . . , xn, and the integers\r\np and r satisfying 0 ≤ p ≤ r ≤ n, depend only on A.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/a6e43ced-102c-452d-9ead-57d42b6b236d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cd7baf8bb850b783e1e41ab7dbe6a0ee6c24363e02e52034d8aedbef125c7ab6",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 591
      },
      {
        "segments": [
          {
            "segment_id": "17254bd1-9f37-46b8-a222-273ce6e21fe5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 158,
            "page_width": 612,
            "page_height": 792,
            "content": "158 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nProof. From Equation (6.4.1) it is easily seen that H(x) has the required form. We only\r\nneed to show that p and r are uniquely determined by A. Hence, let us assume on the\r\ncontrary that there exist positive integers p, q, r, s with p > q such that\r\nH(x) = |y1|\r\n2 + |y2|2 + · · · + |yp|2 − |yp+1|2 − · · · − |yr|2\r\n= |z1|\r\n2 + |z2|2 + · · · + |zq|2 − |zq+1|2 − · · · − |zs|2\r\n,\r\nwhere y = (y1, y2, . . . , yn)\r\n∗ = Mx and z = (z1, z2, . . . , zn)∗ = Nx for some invertible\r\nmatrices M and N. Hence, z = By for some invertible matrix B. Let us write Y1 =\r\n(y1, . . . , yp)\r\n∗\r\n, Z1 = (z1, . . . , zq)\r\n∗ and B =\r\n\"\r\nB1 B2\r\nB3 B4\r\n#\r\n, where B1 is a q × p matrix. As p > q,\r\nthe homogeneous linear system B1Y1 = 0 has a non-zero solution. Let Y˜\r\n1 = ( ˜y1, . . . , y˜p)\r\n∗\r\nbe a non-zero solution and let y˜\r\n∗ = (Y˜\r\n1\r\n∗\r\n, 0\r\n∗\r\n). Then\r\nH(y˜) = |y˜1|\r\n2 + |y˜2|2 + · · · + |y˜p|2 = −(|zq+1|2 + · · · + |zs|2\r\n).\r\nNow, this can hold only if ˜y1 = ˜y2 = · · · = ˜yp = 0, which gives a contradiction. Hence\r\np = q. Similarly, the case r > s can be resolved. Thus, the proof of the lemma is over.\r\nRemark 6.4.5. The integer r is the rank of the matrix A and the number r − 2p is\r\nsometimes called the inertial degree of A.\r\nWe complete this chapter by understanding the graph of\r\nax2 + 2hxy + by2 + 2f x + 2gy + c = 0\r\nfor a, b, c, f, g, h ∈ R. We first look at the following example.\r\nExample 6.4.6. Sketch the graph of 3x\r\n2 + 4xy + 3y2 = 5.\r\nSolution: Note that 3x\r\n2 + 4xy + 3y2 = [x, y]\r\n\"\r\n3 2\r\n2 3# \"xy\r\n#\r\nand the eigen-pairs of the\r\nmatrix \"\r\n3 2\r\n2 3#\r\nare (5,(1, 1)t\r\n), (1,(1, −1)t). Thus,\r\n\"\r\n3 2\r\n2 3#\r\n=\r\n\"\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n# \"5 0\r\n0 1# \" √\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n#\r\n.\r\nNow, let u =\r\nx+y\r\n√\r\n2\r\nand v =\r\nx−y\r\n√\r\n2\r\n. Then\r\n3x\r\n2 + 4xy + 3y2 = [x, y]\r\n\"\r\n3 2\r\n2 3# \"xy\r\n#\r\n= [x, y]\r\n\"\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n# \"5 0\r\n0 1# \" √\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n# \"x\r\ny\r\n#\r\n=\r\n\u0002\r\nu, v\u0003\r\n\"\r\n5 0\r\n0 1# \"uv\r\n#\r\n= 5u\r\n2 + v2\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/17254bd1-9f37-46b8-a222-273ce6e21fe5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ffa84fd8007504bbcbbda3b1511ef26ba11dd9cb6e7cbe5d89237456088e383",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 530
      },
      {
        "segments": [
          {
            "segment_id": "17254bd1-9f37-46b8-a222-273ce6e21fe5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 158,
            "page_width": 612,
            "page_height": 792,
            "content": "158 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\nProof. From Equation (6.4.1) it is easily seen that H(x) has the required form. We only\r\nneed to show that p and r are uniquely determined by A. Hence, let us assume on the\r\ncontrary that there exist positive integers p, q, r, s with p > q such that\r\nH(x) = |y1|\r\n2 + |y2|2 + · · · + |yp|2 − |yp+1|2 − · · · − |yr|2\r\n= |z1|\r\n2 + |z2|2 + · · · + |zq|2 − |zq+1|2 − · · · − |zs|2\r\n,\r\nwhere y = (y1, y2, . . . , yn)\r\n∗ = Mx and z = (z1, z2, . . . , zn)∗ = Nx for some invertible\r\nmatrices M and N. Hence, z = By for some invertible matrix B. Let us write Y1 =\r\n(y1, . . . , yp)\r\n∗\r\n, Z1 = (z1, . . . , zq)\r\n∗ and B =\r\n\"\r\nB1 B2\r\nB3 B4\r\n#\r\n, where B1 is a q × p matrix. As p > q,\r\nthe homogeneous linear system B1Y1 = 0 has a non-zero solution. Let Y˜\r\n1 = ( ˜y1, . . . , y˜p)\r\n∗\r\nbe a non-zero solution and let y˜\r\n∗ = (Y˜\r\n1\r\n∗\r\n, 0\r\n∗\r\n). Then\r\nH(y˜) = |y˜1|\r\n2 + |y˜2|2 + · · · + |y˜p|2 = −(|zq+1|2 + · · · + |zs|2\r\n).\r\nNow, this can hold only if ˜y1 = ˜y2 = · · · = ˜yp = 0, which gives a contradiction. Hence\r\np = q. Similarly, the case r > s can be resolved. Thus, the proof of the lemma is over.\r\nRemark 6.4.5. The integer r is the rank of the matrix A and the number r − 2p is\r\nsometimes called the inertial degree of A.\r\nWe complete this chapter by understanding the graph of\r\nax2 + 2hxy + by2 + 2f x + 2gy + c = 0\r\nfor a, b, c, f, g, h ∈ R. We first look at the following example.\r\nExample 6.4.6. Sketch the graph of 3x\r\n2 + 4xy + 3y2 = 5.\r\nSolution: Note that 3x\r\n2 + 4xy + 3y2 = [x, y]\r\n\"\r\n3 2\r\n2 3# \"xy\r\n#\r\nand the eigen-pairs of the\r\nmatrix \"\r\n3 2\r\n2 3#\r\nare (5,(1, 1)t\r\n), (1,(1, −1)t). Thus,\r\n\"\r\n3 2\r\n2 3#\r\n=\r\n\"\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n# \"5 0\r\n0 1# \" √\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n#\r\n.\r\nNow, let u =\r\nx+y\r\n√\r\n2\r\nand v =\r\nx−y\r\n√\r\n2\r\n. Then\r\n3x\r\n2 + 4xy + 3y2 = [x, y]\r\n\"\r\n3 2\r\n2 3# \"xy\r\n#\r\n= [x, y]\r\n\"\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n# \"5 0\r\n0 1# \" √\r\n1\r\n2\r\n√\r\n1\r\n2\r\n√\r\n1\r\n2\r\n− √\r\n1\r\n2\r\n# \"x\r\ny\r\n#\r\n=\r\n\u0002\r\nu, v\u0003\r\n\"\r\n5 0\r\n0 1# \"uv\r\n#\r\n= 5u\r\n2 + v2\r\n.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/17254bd1-9f37-46b8-a222-273ce6e21fe5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8ffa84fd8007504bbcbbda3b1511ef26ba11dd9cb6e7cbe5d89237456088e383",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 530
      },
      {
        "segments": [
          {
            "segment_id": "ca572c3e-4fe8-4313-af40-8475bb9c2835",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 159,
            "page_width": 612,
            "page_height": 792,
            "content": "6.4. SYLVESTER’S LAW OF INERTIA AND APPLICATIONS 159\r\nThus, the given graph reduces to 5u\r\n2 +v2 = 5 or equivalently to u2 +\r\nv\r\n2\r\n5 = 1. Therefore, the\r\ngiven graph represents an ellipse with the principal axes u = 0 and v = 0 (correspinding to\r\nthe line x + y = 0 and x − y = 0, respectively). See Figure 6.4.6.\r\ny = x\r\ny = −x\r\nFigure 1: The ellipse 3x\r\n2 + 4xy + 3y2 = 5.\r\nWe now consider the general conic. We obtain conditions on the eigenvalues of the\r\nassociated quadratic form, defined below, to characterize conic sections in R\r\n2\r\n(endowed\r\nwith the standard inner product).\r\nDefinition 6.4.7 (Quadratic Form). Let ax2 + 2hxy + by2 + 2gx + 2fy + c = 0 be the\r\nequation of a general conic. The quadratic expression\r\nax2 + 2hxy + by2 =\r\n\u0002\r\nx, y\u0003\r\n\"\r\na h\r\nh b# \"xy\r\n#\r\nis called the quadratic form associated with the given conic.\r\nProposition 6.4.8. For fixed real numbers a, b, c, g, f and h, consider the general conic\r\nax2 + 2hxy + by2 + 2gx + 2fy + c = 0.\r\nThen prove that this conic represents\r\n1. an ellipse if ab − h\r\n2 > 0,\r\n2. a parabola if ab − h\r\n2 = 0, and\r\n3. a hyperbola if ab − h\r\n2 < 0.\r\nProof. Let A =\r\n\"\r\na h\r\nh b#\r\n. Then ax2 + 2hxy + by2 =\r\n\u0002\r\nx y\u0003A\r\n\"\r\nx\r\ny\r\n#\r\nis the associated quadratic\r\nform. As A is a symmetric matrix, by Corollary 6.3.6, the eigenvalues λ1, λ2 of A are both\r\nreal, the corresponding eigenvectors u1, u2 are orthonormal and A is unitarily diagonaliz\u0002able with A =\r\n\u0002\r\nu1 u2\r\n\u0003\r\n\"\r\nλ1 0\r\n0 λ2\r\n# \"u\r\nt\r\n1\r\nu\r\nt\r\n2\r\n#\r\n. Let \"\r\nu\r\nv\r\n#\r\n=\r\n\"\r\nu\r\nt\r\n1\r\nu\r\nt\r\n2\r\n# \"x\r\ny\r\n#\r\n. Then ax2 + 2hxy + by2 =\r\nλ1u\r\n2 + λ2v2 and the equation of the conic section in the (u, v)-plane, reduces to\r\nλ1u\r\n2 + λ2v2 + 2g1u + 2f1v + c = 0. (6.4.2)\r\nNow, depending on the eigenvalues λ1, λ2, we consider different cases:",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/ca572c3e-4fe8-4313-af40-8475bb9c2835.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=228f457a2226fa3420413fce3e05b4d5bdeb01cc67cbce986dcb800fe6eaf31b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 383
      },
      {
        "segments": [
          {
            "segment_id": "9f4428c1-dc5b-4214-aeb0-927a654ff00a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 160,
            "page_width": 612,
            "page_height": 792,
            "content": "160 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\n1. λ1 = 0 = λ2. Substituting λ1 = λ2 = 0 in Equation (6.4.2) gives the straight line\r\n2g1u + 2f1v + c = 0 in the (u, v)-plane.\r\n2. λ1 = 0, λ2 > 0. As λ1 = 0, det(A) = 0. That is, ab − h\r\n2 = det(A) = 0. Also, in this\r\ncase, Equation (6.4.2) reduces to\r\nλ2(v + d1)\r\n2 = d2u + d3 for some d1, d2, d3 ∈ R.\r\nTo understand this case, we need to consider the following subcases:\r\n(a) Let d2 = d3 = 0. Then v + d1 = 0 is a pair of coincident lines.\r\n(b) Let d2 = 0, d3 6= 0.\r\ni. If d3 > 0, then we get a pair of parallel lines given by v = −d1 ±\r\nqd3\r\nλ2\r\n.\r\nii. If d3 < 0, the solution set of the corresponding conic is an empty set.\r\n(c) If d2 6= 0. Then the given equation is of the form Y\r\n2 = 4aX for some translates\r\nX = x + α and Y = y + β and thus represents a parabola.\r\n3. λ1 > 0 and λ2 < 0. In this case, ab − h\r\n2 = det(A) = λ1λ2 < 0. Let λ2 = −α2 with\r\nα2 > 0. Then Equation (6.4.2) can be rewritten as\r\nλ1(u + d1)\r\n2 − α2(v + d2)2 = d3 for some d1, d2, d3 ∈ R (6.4.3)\r\nwhose understanding requires the following subcases:\r\n(a) Let d3 = 0. Then Equation (6.4.3) reduces to\r\n\u0010p\r\nλ1(u + d1) + √α2(v + d2)\r\n\u0011\r\n·\r\n\u0010p\r\nλ1(u + d1) −\r\n√\r\nα2(v + d2)\r\n\u0011\r\n= 0\r\nor equivalently, a pair of intersecting straight lines in the (u, v)-plane.\r\n(b) Let d3 6= 0. In particular, let d3 > 0. Then Equation (6.4.3) reduces to\r\nλ1(u + d1)\r\n2\r\nd3\r\n−\r\nα2(v + d2)\r\n2\r\nd3\r\n= 1\r\nor equivalently, a hyperbola in the (u, v)-plane, with principal axes u + d1 = 0\r\nand v + d2 = 0.\r\n4. λ1, λ2 > 0. In this case, ab − h\r\n2 = det(A) = λ1λ2 > 0 and Equation (6.4.2) can be\r\nrewritten as\r\nλ1(u + d1)\r\n2 + λ2(v + d2)2 = d3 for some d1, d2, d3 ∈ R. (6.4.4)\r\nWe consider the following three subcases to understand this.\r\n(a) Let d3 = 0. Then Equation (6.4.4) reduces to a pair of perpendicular lines\r\nu + d1 = 0 and v + d2 = 0 in the (u, v)-plane.\r\n(b) Let d3 < 0. Then the solution set of Equation (6.4.4) is an empty set.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9f4428c1-dc5b-4214-aeb0-927a654ff00a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2befd847fed74c2bcefd3845d434e439613c918c5a31d9399ad90a7f6d2166b5",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 449
      },
      {
        "segments": [
          {
            "segment_id": "76da6559-47cf-4e11-9a4e-609b379a4fa5",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 161,
            "page_width": 612,
            "page_height": 792,
            "content": "6.4. SYLVESTER’S LAW OF INERTIA AND APPLICATIONS 161\r\n(c) Let d3 > 0. Then Equation (6.4.4) reduces to the ellipse\r\nλ1(u + d1)\r\n2\r\nd3\r\n+\r\nα2(v + d2)\r\n2\r\nd3\r\n= 1\r\nwhose principal axes are u + d1 = 0 and v + d2 = 0.\r\nRemark 6.4.9. Observe that the condition \"\r\nx\r\ny\r\n#\r\n=\r\n\u0002\r\nu1 u2\r\n\u0003\r\n\"\r\nu\r\nv\r\n#\r\nimplies that the principal\r\naxes of the conic are functions of the eigenvectors u1 and u2.\r\nExercise 6.4.10. Sketch the graph of the following surfaces:\r\n1. x\r\n2 + 2xy + y2 − 6x − 10y = 3.\r\n2. 2x\r\n2 + 6xy + 3y2 − 12x − 6y = 5.\r\n3. 4x\r\n2 − 4xy + 2y2 + 12x − 8y = 10.\r\n4. 2x\r\n2 − 6xy + 5y2 − 10x + 4y = 7.\r\nAs a last application, we consider the following problem that helps us in understanding\r\nthe quadrics. Let\r\nax2 + by2 + cz2 + 2dxy + 2exz + 2fyz + 2lx + 2my + 2nz + q = 0 (6.4.5)\r\nbe a general quadric. Then to get the geometrical idea of this quadric, do the following:\r\n1. Define A =\r\n\r\n\r\n\r\na d e\r\nd b f\r\ne f c\r\n\r\n\r\n, b =\r\n\r\n\r\n\r\n2l\r\n2m\r\n2n\r\n\r\n\r\n\r\nand x =\r\n\r\n\r\n\r\nx\r\ny\r\nz\r\n\r\n\r\n. Note that Equation (6.4.5) can\r\nbe rewritten as x\r\ntAx + btx + q = 0.\r\n2. As A is symmetric, find an orthogonal matrix P such that P\r\ntAP = diag(λ1, λ2, λ3).\r\n3. Let y = P\r\ntx = (y1, y2, y3)t\r\n. Then Equation (6.4.5) reduces to\r\nλ1y\r\n2\r\n1 + λ2y\r\n2\r\n2 + λ3y\r\n2\r\n3 + 2l1y1 + 2l2y2 + 2l3y3 + q\r\n′ = 0. (6.4.6)\r\n4. Depending on which λi 6= 0, rewrite Equation (6.4.6). That is, if λ1 6= 0 then rewrite\r\nλ1y\r\n2\r\n1 + 2l1y1 as λ1\r\n\u0010\r\ny1 +\r\nl1\r\nλ1\r\n\u00112\r\n−\r\n\u0010\r\nl1\r\nλ1\r\n\u00112\r\n.\r\n5. Use the condition x = Py to determine the center and the planes of symmetry of the\r\nquadric in terms of the original system.\r\nExample 6.4.11. Determine the following quadrics\r\n1. 2x\r\n2 + 2y2 + 2z2 + 2xy + 2xz + 2yz + 4x + 2y + 4z + 2 = 0.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/76da6559-47cf-4e11-9a4e-609b379a4fa5.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=69aaf06428f12069025b00448876fc48ef0d2e342e366dd458ddb3eb2a3bb219",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 405
      },
      {
        "segments": [
          {
            "segment_id": "920802fd-1ebf-4ccd-89dd-1c471bba24bc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 162,
            "page_width": 612,
            "page_height": 792,
            "content": "162 CHAPTER 6. EIGENVALUES, EIGENVECTORS AND DIAGONALIZATION\r\n2. 3x\r\n2 − y2 + z2 + 10 = 0.\r\nSolution: For Part 1, observe that A =\r\n\r\n\r\n\r\n2 1 1\r\n1 2 1\r\n1 1 2\r\n\r\n\r\n, b =\r\n\r\n\r\n\r\n4\r\n2\r\n4\r\n\r\n\r\n\r\nand q = 2. Also, the\r\northonormal matrix P =\r\n\r\n\r\n\r\n√\r\n1\r\n3\r\n√\r\n1\r\n2\r\n√\r\n1\r\n6\r\n√\r\n1\r\n3\r\n√−1\r\n2\r\n√\r\n1\r\n6\r\n√\r\n1\r\n3\r\n0 √−2\r\n6\r\n\r\n\r\n\r\nand P\r\ntAP =\r\n\r\n\r\n\r\n4 0 0\r\n0 1 0\r\n0 0 1\r\n\r\n\r\n . Hence, the quadric\r\nreduces to 4y\r\n2\r\n1 + y\r\n2\r\n2 + y\r\n2\r\n3 + √\r\n10\r\n3\r\ny1 + √\r\n2\r\n2\r\ny2 − √\r\n2\r\n6\r\ny3 + 2 = 0. Or equivalently to\r\n4(y1 +\r\n5\r\n4\r\n√\r\n3\r\n)\r\n2 + (y2 +\r\n1\r\n√\r\n2\r\n)\r\n2 + (y3 −\r\n1\r\n√\r\n6\r\n)\r\n2 =\r\n9\r\n12\r\n.\r\nSo, the standard form of the quadric is 4z\r\n2\r\n1 + z\r\n2\r\n2 + z\r\n2\r\n3 =\r\n9\r\n12 , where the center is given by\r\n(x, y, z)\r\nt = P(\r\n−5\r\n4\r\n√\r\n3\r\n, √−1\r\n2\r\n, √\r\n1\r\n6\r\n)\r\nt = ( −3\r\n4\r\n,\r\n1\r\n4\r\n,\r\n−3\r\n4\r\n)\r\nt\r\n.\r\nFor Part 2, observe that A =\r\n\r\n\r\n\r\n3 0 0\r\n0 −1 0\r\n0 0 1\r\n\r\n\r\n, b = 0 and q = 10. In this case, we can\r\nrewrite the quadric as\r\ny\r\n2\r\n10\r\n−\r\n3x\r\n2\r\n10\r\n−\r\nz\r\n2\r\n10\r\n= 1\r\nwhich is the equation of a hyperboloid consisting of two sheets.\r\nThe calculation of the planes of symmetry is left as an exercise to the reader.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/920802fd-1ebf-4ccd-89dd-1c471bba24bc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=920bf3d693444f59b90e85caef2edcf6e767ccb5d982ccfcaab2059ffb3ae59b",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 315
      },
      {
        "segments": [
          {
            "segment_id": "e659a901-8af8-49b7-94d6-cbecc50a039c",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 163,
            "page_width": 612,
            "page_height": 792,
            "content": "Chapter 7\r\nAppendix\r\n7.1 Permutation/Symmetric Groups\r\nIn this section, S denotes the set {1, 2, . . . , n}.\r\nDefinition 7.1.1. 1. A function σ : S−→S is called a permutation on n elements if σ\r\nis both one to one and onto.\r\n2. The set of all functions σ : S−→S that are both one to one and onto will be denoted\r\nby Sn. That is, Sn is the set of all permutations of the set {1, 2, . . . , n}.\r\nExample 7.1.2. 1. In general, we represent a permutation σ by σ =\r\n \r\n1 2 · · · n\r\nσ(1) σ(2) · · · σ(n)\r\n!\r\n.\r\nThis representation of a permutation is called a two row notation for σ.\r\n2. For each positive integer n, Sn has a special permutation called the identity per-\r\n \r\nmutation, denoted Idn, such that Idn(i) = i for 1 ≤ i ≤ n. That is, Idn =\r\n1 2 · · · n\r\n1 2 · · · n\r\n!\r\n.\r\n3. Let n = 3. Then\r\nS3 =\r\n(\r\nτ1 =\r\n \r\n1 2 3\r\n1 2 3 !\r\n, τ2 =\r\n \r\n1 2 3\r\n1 3 2 !\r\n, τ3 =\r\n \r\n1 2 3\r\n2 1 3 !\r\n,\r\nτ4 =\r\n \r\n1 2 3\r\n2 3 1 !\r\n, τ5 =\r\n \r\n1 2 3\r\n3 1 2 !\r\n, τ6 =\r\n \r\n1 2 3\r\n3 2 1 !)(7.1.1)\r\nRemark 7.1.3. 1. Let σ ∈ Sn. Then σ is determined if σ(i) is known for i = 1, 2, . . . , n.\r\nAs σ is both one to one and onto, {σ(1), σ(2), . . . , σ(n)} = S. So, there are n choices\r\nfor σ(1) (any element of S), n − 1 choices for σ(2) (any element of S different from\r\nσ(1)), and so on. Hence, there are n(n−1)(n−2)· · · 3·2·1 = n! possible permutations.\r\nThus, the number of elements in Sn is n!. That is, |Sn| = n!.\r\n163",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/e659a901-8af8-49b7-94d6-cbecc50a039c.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=23c7d25928b71e614eb710b41296655cee7b066704cc68eda14eb3b07dbe5404",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 336
      },
      {
        "segments": [
          {
            "segment_id": "f8c6ec89-3091-45c2-973e-235687b04f92",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 164,
            "page_width": 612,
            "page_height": 792,
            "content": "164 CHAPTER 7. APPENDIX\r\n2. Suppose that σ, τ ∈ Sn. Then both σ and τ are one to one and onto. So, their\r\ncomposition map σ ◦ τ , defined by (σ ◦ τ )(i) = σ\r\n\r\nτ (i)\r\n\u0001\r\n, is also both one to one and\r\nonto. Hence, σ ◦ τ is also a permutation. That is, σ ◦ τ ∈ Sn.\r\n3. Suppose σ ∈ Sn. Then σ is both one to one and onto. Hence, the function σ\r\n−1\r\n:\r\nS−→S defined by σ\r\n−1\r\n(m) = ℓ if and only if σ(ℓ) = m for 1 ≤ m ≤ n, is well\r\ndefined and indeed σ\r\n−1\r\nis also both one to one and onto. Hence, for every element\r\nσ ∈ Sn, σ−1 ∈ Sn and is the inverse of σ.\r\n4. Observe that for any σ ∈ Sn, the compositions σ ◦ σ\r\n−1 = σ−1 ◦ σ = Idn.\r\nProposition 7.1.4. Consider the set of all permutations Sn. Then the following holds:\r\n1. Fix an element τ ∈ Sn. Then the sets {σ ◦ τ : σ ∈ Sn} and {τ ◦ σ : σ ∈ Sn} have\r\nexactly n! elements. Or equivalently,\r\nSn = {τ ◦ σ : σ ∈ Sn} = {σ ◦ τ : σ ∈ Sn}.\r\n2. Sn = {σ\r\n−1\r\n: σ ∈ Sn}.\r\nProof. For the first part, we need to show that given any element α ∈ Sn, there exists\r\nelements β, γ ∈ Sn such that α = τ ◦ β = γ ◦ τ . It can easily be verified that β = τ\r\n−1 ◦ α\r\nand γ = α ◦ τ\r\n−1\r\n.\r\nFor the second part, note that for any σ ∈ Sn, (σ\r\n−1\r\n)\r\n−1 = σ. Hence the result holds.\r\nDefinition 7.1.5. Let σ ∈ Sn. Then the number of inversions of σ, denoted n(σ), equals\r\n|{(i, j) : i < j, σ(i) > σ(j) }|.\r\nNote that, for any σ ∈ Sn, n(σ) also equals\r\nXn\r\ni=1\r\n|{σ(j) < σ(i), for j = i + 1, i + 2, . . . , n}|.\r\nDefinition 7.1.6. A permutation σ ∈ Sn is called a transposition if there exists two positive\r\nintegers m, r ∈ {1, 2, . . . , n} such that σ(m) = r, σ(r) = m and σ(i) = i for 1 ≤ i 6= m, r ≤\r\nn.\r\nFor the sake of convenience, a transposition σ for which σ(m) = r, σ(r) = m and\r\nσ(i) = i for 1 ≤ i 6= m, r ≤ n will be denoted simply by σ = (m r) or (r m). Also, note\r\nthat for any transposition σ ∈ Sn, σ\r\n−1 = σ. That is, σ ◦ σ = Idn.\r\nExample 7.1.7. 1. The permutation τ =\r\n \r\n1 2 3 4\r\n3 2 1 4 !\r\nis a transposition as τ (1) =\r\n3, τ (3) = 1, τ (2) = 2 and τ (4) = 4. Here note that τ = (1 3) = (3 1). Also, check\r\nthat\r\nn(τ ) = |{(1, 2),(1, 3),(2, 3)}| = 3.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/f8c6ec89-3091-45c2-973e-235687b04f92.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f706c64682347152f8147f6d467f738ebf0b1080ae72b022c1427085ebce5022",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "f8c6ec89-3091-45c2-973e-235687b04f92",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 164,
            "page_width": 612,
            "page_height": 792,
            "content": "164 CHAPTER 7. APPENDIX\r\n2. Suppose that σ, τ ∈ Sn. Then both σ and τ are one to one and onto. So, their\r\ncomposition map σ ◦ τ , defined by (σ ◦ τ )(i) = σ\r\n\r\nτ (i)\r\n\u0001\r\n, is also both one to one and\r\nonto. Hence, σ ◦ τ is also a permutation. That is, σ ◦ τ ∈ Sn.\r\n3. Suppose σ ∈ Sn. Then σ is both one to one and onto. Hence, the function σ\r\n−1\r\n:\r\nS−→S defined by σ\r\n−1\r\n(m) = ℓ if and only if σ(ℓ) = m for 1 ≤ m ≤ n, is well\r\ndefined and indeed σ\r\n−1\r\nis also both one to one and onto. Hence, for every element\r\nσ ∈ Sn, σ−1 ∈ Sn and is the inverse of σ.\r\n4. Observe that for any σ ∈ Sn, the compositions σ ◦ σ\r\n−1 = σ−1 ◦ σ = Idn.\r\nProposition 7.1.4. Consider the set of all permutations Sn. Then the following holds:\r\n1. Fix an element τ ∈ Sn. Then the sets {σ ◦ τ : σ ∈ Sn} and {τ ◦ σ : σ ∈ Sn} have\r\nexactly n! elements. Or equivalently,\r\nSn = {τ ◦ σ : σ ∈ Sn} = {σ ◦ τ : σ ∈ Sn}.\r\n2. Sn = {σ\r\n−1\r\n: σ ∈ Sn}.\r\nProof. For the first part, we need to show that given any element α ∈ Sn, there exists\r\nelements β, γ ∈ Sn such that α = τ ◦ β = γ ◦ τ . It can easily be verified that β = τ\r\n−1 ◦ α\r\nand γ = α ◦ τ\r\n−1\r\n.\r\nFor the second part, note that for any σ ∈ Sn, (σ\r\n−1\r\n)\r\n−1 = σ. Hence the result holds.\r\nDefinition 7.1.5. Let σ ∈ Sn. Then the number of inversions of σ, denoted n(σ), equals\r\n|{(i, j) : i < j, σ(i) > σ(j) }|.\r\nNote that, for any σ ∈ Sn, n(σ) also equals\r\nXn\r\ni=1\r\n|{σ(j) < σ(i), for j = i + 1, i + 2, . . . , n}|.\r\nDefinition 7.1.6. A permutation σ ∈ Sn is called a transposition if there exists two positive\r\nintegers m, r ∈ {1, 2, . . . , n} such that σ(m) = r, σ(r) = m and σ(i) = i for 1 ≤ i 6= m, r ≤\r\nn.\r\nFor the sake of convenience, a transposition σ for which σ(m) = r, σ(r) = m and\r\nσ(i) = i for 1 ≤ i 6= m, r ≤ n will be denoted simply by σ = (m r) or (r m). Also, note\r\nthat for any transposition σ ∈ Sn, σ\r\n−1 = σ. That is, σ ◦ σ = Idn.\r\nExample 7.1.7. 1. The permutation τ =\r\n \r\n1 2 3 4\r\n3 2 1 4 !\r\nis a transposition as τ (1) =\r\n3, τ (3) = 1, τ (2) = 2 and τ (4) = 4. Here note that τ = (1 3) = (3 1). Also, check\r\nthat\r\nn(τ ) = |{(1, 2),(1, 3),(2, 3)}| = 3.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/f8c6ec89-3091-45c2-973e-235687b04f92.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f706c64682347152f8147f6d467f738ebf0b1080ae72b022c1427085ebce5022",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 526
      },
      {
        "segments": [
          {
            "segment_id": "9890b7ea-ca0d-4dda-b4d9-bd4a2812ff79",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 165,
            "page_width": 612,
            "page_height": 792,
            "content": "7.1. PERMUTATION/SYMMETRIC GROUPS 165\r\n2. Let τ =\r\n \r\n1 2 3 4 5 6 7 8 9\r\n4 2 3 5 1 9 8 7 6 !\r\n. Then check that\r\nn(τ ) = 3 + 1 + 1 + 1 + 0 + 3 + 2 + 1 = 12.\r\n3. Let ℓ, m and r be distinct element from {1, 2, . . . , n}. Suppose τ = (m r) and σ =\r\n(m ℓ). Then\r\n(τ ◦ σ)(ℓ) = τ\r\n\r\nσ(ℓ)\r\n\u0001\r\n= τ (m) = r, (τ ◦ σ)(m) = τ\r\n\r\nσ(m)\r\n\u0001\r\n= τ (ℓ) = ℓ\r\n(τ ◦ σ)(r) = τ\r\n\r\nσ(r)\r\n\u0001\r\n= τ (r) = m, and (τ ◦ σ)(i) = τ\r\n\r\nσ(i)\r\n\u0001\r\n= τ (i) = i if i 6= ℓ, m, r.\r\nTherefore,\r\nτ ◦ σ = (m r) ◦ (m ℓ) = \r\n1 2 · · · ℓ · · · m · · · r · · · n\r\n1 2 · · · r · · · ℓ · · · m · · · n\r\n!\r\n= (r l) ◦ (r m).\r\nSimilarly check that σ ◦ τ =\r\n \r\n1 2 · · · ℓ · · · m · · · r · · · n\r\n1 2 · · · m · · · r · · · ℓ · · · n\r\n!\r\n.\r\nWith the above definitions, we state and prove two important results.\r\nTheorem 7.1.8. For any σ ∈ Sn, σ can be written as composition (product) of transposi\u0002tions.\r\nProof. We will prove the result by induction on n(σ), the number of inversions of σ. If\r\nn(σ) = 0, then σ = Idn = (1 2) ◦ (1 2). So, let the result be true for all σ ∈ Sn with\r\nn(σ) ≤ k.\r\nFor the next step of the induction, suppose that τ ∈ Sn with n(τ ) = k + 1. Choose the\r\nsmallest positive number, say ℓ, such that\r\nτ (i) = i, for i = 1, 2, . . . , ℓ − 1 and τ (ℓ) 6= ℓ.\r\nAs τ is a permutation, there exists a positive number, say m, such that τ (ℓ) = m. Also,\r\nnote that m > ℓ. Define a transposition σ by σ = (ℓ m). Then note that\r\n(σ ◦ τ )(i) = i, for i = 1, 2, . . . , ℓ.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9890b7ea-ca0d-4dda-b4d9-bd4a2812ff79.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b58387e75b13bfdc72f92f032c04afcad48dfe1e332eebc96c730aff1bf9a2ea",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 406
      },
      {
        "segments": [
          {
            "segment_id": "9ba1237c-8f92-4e76-a63e-3a493798f936",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 166,
            "page_width": 612,
            "page_height": 792,
            "content": "166 CHAPTER 7. APPENDIX\r\nSo, the definition of “number of inversions” and m > ℓ implies that\r\nn(σ ◦ τ ) = Xn\r\ni=1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n=\r\nX\r\nℓ\r\ni=1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n+\r\nXn\r\ni=ℓ+1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n=\r\nXn\r\ni=ℓ+1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n≤\r\nXn\r\ni=ℓ+1\r\n|{τ (j) < τ (i), for j = i + 1, i + 2, . . . , n}| as m > ℓ,\r\n< (m − ℓ) + Xn\r\ni=ℓ+1\r\n|{τ (j) < τ (i), for j = i + 1, i + 2, . . . , n}|\r\n= n(τ ).\r\nThus, n(σ ◦ τ ) < k + 1. Hence, by the induction hypothesis, the permutation σ ◦ τ is a\r\ncomposition of transpositions. That is, there exist transpositions, say αi, 1 ≤ i ≤ t such\r\nthat\r\nσ ◦ τ = α1 ◦ α2 ◦ · · · ◦ αt\r\n.\r\nHence, τ = σ ◦ α1 ◦ α2 ◦ · · · ◦ αt as σ ◦ σ = Idn for any transposition σ ∈ Sn. Therefore, by\r\nmathematical induction, the proof of the theorem is complete.\r\nBefore coming to our next important result, we state and prove the following lemma.\r\nLemma 7.1.9. Suppose there exist transpositions αi, 1 ≤ i ≤ t such that\r\nIdn = α1 ◦ α2 ◦ · · · ◦ αt,\r\nthen t is even.\r\nProof. Observe that t 6= 1 as the identity permutation is not a transposition. Hence, t ≥ 2.\r\nIf t = 2, we are done. So, let us assume that t ≥ 3. We will prove the result by the method\r\nof mathematical induction. The result clearly holds for t = 2. Let the result be true for all\r\nexpressions in which the number of transpositions t ≤ k. Now, let t = k + 1.\r\nSuppose α1 = (m r). Note that the possible choices for the composition α1 ◦ α2 are\r\n(m r) ◦ (m r) = Idn, (m r) ◦ (m ℓ) = (r ℓ) ◦ (r m), (m r) ◦ (r ℓ) = (ℓ r) ◦ (ℓ m) and (m r) ◦\r\n(ℓ s) = (ℓ s) ◦ (m r), where ℓ and s are distinct elements of {1, 2, . . . , n} and are different\r\nfrom m, r. In the first case, we can remove α1 ◦ α2 and obtain Idn = α3 ◦ α4 ◦ · · · ◦ αt\r\n.\r\nIn this expression for identity, the number of transpositions is t − 2 = k − 1 < k. So, by\r\nmathematical induction, t − 2 is even and hence t is also even.\r\nIn the other three cases, we replace the original expression for α1 ◦ α2 by their coun\u0002terparts on the right to obtain another expression for identity in terms of t = k + 1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9ba1237c-8f92-4e76-a63e-3a493798f936.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=981f08db55b8ce549d1fb72568eb4fe6d824e8d0344a7b2ebd78a3dc70d796c3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "9ba1237c-8f92-4e76-a63e-3a493798f936",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 166,
            "page_width": 612,
            "page_height": 792,
            "content": "166 CHAPTER 7. APPENDIX\r\nSo, the definition of “number of inversions” and m > ℓ implies that\r\nn(σ ◦ τ ) = Xn\r\ni=1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n=\r\nX\r\nℓ\r\ni=1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n+\r\nXn\r\ni=ℓ+1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n=\r\nXn\r\ni=ℓ+1\r\n|{(σ ◦ τ )(j) < (σ ◦ τ )(i), for j = i + 1, i + 2, . . . , n}|\r\n≤\r\nXn\r\ni=ℓ+1\r\n|{τ (j) < τ (i), for j = i + 1, i + 2, . . . , n}| as m > ℓ,\r\n< (m − ℓ) + Xn\r\ni=ℓ+1\r\n|{τ (j) < τ (i), for j = i + 1, i + 2, . . . , n}|\r\n= n(τ ).\r\nThus, n(σ ◦ τ ) < k + 1. Hence, by the induction hypothesis, the permutation σ ◦ τ is a\r\ncomposition of transpositions. That is, there exist transpositions, say αi, 1 ≤ i ≤ t such\r\nthat\r\nσ ◦ τ = α1 ◦ α2 ◦ · · · ◦ αt\r\n.\r\nHence, τ = σ ◦ α1 ◦ α2 ◦ · · · ◦ αt as σ ◦ σ = Idn for any transposition σ ∈ Sn. Therefore, by\r\nmathematical induction, the proof of the theorem is complete.\r\nBefore coming to our next important result, we state and prove the following lemma.\r\nLemma 7.1.9. Suppose there exist transpositions αi, 1 ≤ i ≤ t such that\r\nIdn = α1 ◦ α2 ◦ · · · ◦ αt,\r\nthen t is even.\r\nProof. Observe that t 6= 1 as the identity permutation is not a transposition. Hence, t ≥ 2.\r\nIf t = 2, we are done. So, let us assume that t ≥ 3. We will prove the result by the method\r\nof mathematical induction. The result clearly holds for t = 2. Let the result be true for all\r\nexpressions in which the number of transpositions t ≤ k. Now, let t = k + 1.\r\nSuppose α1 = (m r). Note that the possible choices for the composition α1 ◦ α2 are\r\n(m r) ◦ (m r) = Idn, (m r) ◦ (m ℓ) = (r ℓ) ◦ (r m), (m r) ◦ (r ℓ) = (ℓ r) ◦ (ℓ m) and (m r) ◦\r\n(ℓ s) = (ℓ s) ◦ (m r), where ℓ and s are distinct elements of {1, 2, . . . , n} and are different\r\nfrom m, r. In the first case, we can remove α1 ◦ α2 and obtain Idn = α3 ◦ α4 ◦ · · · ◦ αt\r\n.\r\nIn this expression for identity, the number of transpositions is t − 2 = k − 1 < k. So, by\r\nmathematical induction, t − 2 is even and hence t is also even.\r\nIn the other three cases, we replace the original expression for α1 ◦ α2 by their coun\u0002terparts on the right to obtain another expression for identity in terms of t = k + 1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/9ba1237c-8f92-4e76-a63e-3a493798f936.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=981f08db55b8ce549d1fb72568eb4fe6d824e8d0344a7b2ebd78a3dc70d796c3",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 560
      },
      {
        "segments": [
          {
            "segment_id": "1e36180d-b585-486a-9221-3d2878ef774b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 167,
            "page_width": 612,
            "page_height": 792,
            "content": "7.1. PERMUTATION/SYMMETRIC GROUPS 167\r\ntranspositions. But note that in the new expression for identity, the positive integer m\r\ndoesn’t appear in the first transposition, but appears in the second transposition. We can\r\ncontinue the above process with the second and third transpositions. At this step, either\r\nthe number of transpositions will reduce by 2 (giving us the result by mathematical induc\u0002tion) or the positive number m will get shifted to the third transposition. The continuation\r\nof this process will at some stage lead to an expression for identity in which the number\r\nof transpositions is t − 2 = k − 1 (which will give us the desired result by mathematical\r\ninduction), or else we will have an expression in which the positive number m will get\r\nshifted to the right most transposition. In the later case, the positive integer m appears\r\nexactly once in the expression for identity and hence this expression does not fix m whereas\r\nfor the identity permutation Idn(m) = m. So the later case leads us to a contradiction.\r\nHence, the process will surely lead to an expression in which the number of transposi\u0002tions at some stage is t − 2 = k − 1. Therefore, by mathematical induction, the proof of\r\nthe lemma is complete.\r\nTheorem 7.1.10. Let α ∈ Sn. Suppose there exist transpositions τ1, τ2, . . . , τk and σ1, σ2, . . . , σℓ\r\nsuch that\r\nα = τ1 ◦ τ2 ◦ · · · ◦ τk = σ1 ◦ σ2 ◦ · · · ◦ σℓ\r\nthen either k and ℓ are both even or both odd.\r\nProof. Observe that the condition τ1 ◦ τ2 ◦ · · · ◦ τk = σ1 ◦ σ2 ◦ · · · ◦ σℓ and σ ◦ σ = Idn for\r\nany transposition σ ∈ Sn, implies that\r\nIdn = τ1 ◦ τ2 ◦ · · · ◦ τk ◦ σℓ ◦ σℓ−1 ◦ · · · ◦ σ1.\r\nHence by Lemma 7.1.9, k + ℓ is even. Hence, either k and ℓ are both even or both odd.\r\nThus the result follows.\r\nDefinition 7.1.11. A permutation σ ∈ Sn is called an even permutation if σ can be written\r\nas a composition (product) of an even number of transpositions. A permutation σ ∈ Sn is\r\ncalled an odd permutation if σ can be written as a composition (product) of an odd number\r\nof transpositions.\r\nRemark 7.1.12. Observe that if σ and τ are both even or both odd permutations, then the\r\npermutations σ ◦ τ and τ ◦ σ are both even. Whereas if one of them is odd and the other\r\neven then the permutations σ ◦ τ and τ ◦ σ are both odd. We use this to define a function\r\non Sn, called the sign of a permutation, as follows:\r\nDefinition 7.1.13. Let sgn : Sn−→{1, −1} be a function defined by\r\nsgn(σ) = (\r\n1 if σ is an even permutation\r\n−1 if σ is an odd permutation\r\n.\r\nExample 7.1.14. 1. The identity permutation, Idn is an even permutation whereas\r\nevery transposition is an odd permutation. Thus, sgn(Idn) = 1 and for any transpo\u0002sition σ ∈ Sn, sgn(σ) = −1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1e36180d-b585-486a-9221-3d2878ef774b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2d8b52690528a7cf4a539992bb6755f5eeaeb362727052a1f58d941d06a9544a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "1e36180d-b585-486a-9221-3d2878ef774b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 167,
            "page_width": 612,
            "page_height": 792,
            "content": "7.1. PERMUTATION/SYMMETRIC GROUPS 167\r\ntranspositions. But note that in the new expression for identity, the positive integer m\r\ndoesn’t appear in the first transposition, but appears in the second transposition. We can\r\ncontinue the above process with the second and third transpositions. At this step, either\r\nthe number of transpositions will reduce by 2 (giving us the result by mathematical induc\u0002tion) or the positive number m will get shifted to the third transposition. The continuation\r\nof this process will at some stage lead to an expression for identity in which the number\r\nof transpositions is t − 2 = k − 1 (which will give us the desired result by mathematical\r\ninduction), or else we will have an expression in which the positive number m will get\r\nshifted to the right most transposition. In the later case, the positive integer m appears\r\nexactly once in the expression for identity and hence this expression does not fix m whereas\r\nfor the identity permutation Idn(m) = m. So the later case leads us to a contradiction.\r\nHence, the process will surely lead to an expression in which the number of transposi\u0002tions at some stage is t − 2 = k − 1. Therefore, by mathematical induction, the proof of\r\nthe lemma is complete.\r\nTheorem 7.1.10. Let α ∈ Sn. Suppose there exist transpositions τ1, τ2, . . . , τk and σ1, σ2, . . . , σℓ\r\nsuch that\r\nα = τ1 ◦ τ2 ◦ · · · ◦ τk = σ1 ◦ σ2 ◦ · · · ◦ σℓ\r\nthen either k and ℓ are both even or both odd.\r\nProof. Observe that the condition τ1 ◦ τ2 ◦ · · · ◦ τk = σ1 ◦ σ2 ◦ · · · ◦ σℓ and σ ◦ σ = Idn for\r\nany transposition σ ∈ Sn, implies that\r\nIdn = τ1 ◦ τ2 ◦ · · · ◦ τk ◦ σℓ ◦ σℓ−1 ◦ · · · ◦ σ1.\r\nHence by Lemma 7.1.9, k + ℓ is even. Hence, either k and ℓ are both even or both odd.\r\nThus the result follows.\r\nDefinition 7.1.11. A permutation σ ∈ Sn is called an even permutation if σ can be written\r\nas a composition (product) of an even number of transpositions. A permutation σ ∈ Sn is\r\ncalled an odd permutation if σ can be written as a composition (product) of an odd number\r\nof transpositions.\r\nRemark 7.1.12. Observe that if σ and τ are both even or both odd permutations, then the\r\npermutations σ ◦ τ and τ ◦ σ are both even. Whereas if one of them is odd and the other\r\neven then the permutations σ ◦ τ and τ ◦ σ are both odd. We use this to define a function\r\non Sn, called the sign of a permutation, as follows:\r\nDefinition 7.1.13. Let sgn : Sn−→{1, −1} be a function defined by\r\nsgn(σ) = (\r\n1 if σ is an even permutation\r\n−1 if σ is an odd permutation\r\n.\r\nExample 7.1.14. 1. The identity permutation, Idn is an even permutation whereas\r\nevery transposition is an odd permutation. Thus, sgn(Idn) = 1 and for any transpo\u0002sition σ ∈ Sn, sgn(σ) = −1.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/1e36180d-b585-486a-9221-3d2878ef774b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=2d8b52690528a7cf4a539992bb6755f5eeaeb362727052a1f58d941d06a9544a",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "7a98461b-0ccc-4641-b670-45c72d47fb00",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 168,
            "page_width": 612,
            "page_height": 792,
            "content": "168 CHAPTER 7. APPENDIX\r\n2. Using Remark 7.1.12, sgn(σ◦τ ) = sgn(σ)·sgn(τ ) for any two permutations σ, τ ∈ Sn.\r\nWe are now ready to define determinant of a square matrix A.\r\nDefinition 7.1.15. Let A = [aij ] be an n×n matrix with entries from F. The determinant\r\nof A, denoted det(A), is defined as\r\ndet(A) = X\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) . . . anσ(n) =\r\nX\r\nσ∈Sn\r\nsgn(σ)\r\nYn\r\ni=1\r\naiσ(i).\r\nRemark 7.1.16. 1. Observe that det(A) is a scalar quantity. The expression for det(A)\r\nseems complicated at the first glance. But this expression is very helpful in proving\r\nthe results related with “properties of determinant”.\r\n2. If A = [aij ] is a 3 × 3 matrix, then using (7.1.1),\r\ndet(A) = X\r\nσ∈Sn\r\nsgn(σ)\r\nY\r\n3\r\ni=1\r\naiσ(i)\r\n= sgn(τ1)\r\nY\r\n3\r\ni=1\r\naiτ1(i) + sgn(τ2)\r\nY\r\n3\r\ni=1\r\naiτ2(i) + sgn(τ3)\r\nY\r\n3\r\ni=1\r\naiτ3(i) +\r\nsgn(τ4)\r\nY\r\n3\r\ni=1\r\naiτ4(i) + sgn(τ5)\r\nY\r\n3\r\ni=1\r\naiτ5(i) + sgn(τ6)\r\nY\r\n3\r\ni=1\r\naiτ6(i)\r\n= a11a22a33 − a11a23a32 − a12a21a33 + a12a23a31 + a13a21a32 − a13a22a31.\r\nObserve that this expression for det(A) for a 3 × 3 matrix A is same as that given in\r\n(2.5.1).\r\n7.2 Properties of Determinant\r\nTheorem 7.2.1 (Properties of Determinant). Let A = [aij ] be an n × n matrix. Then\r\n1. if B is obtained from A by interchanging two rows, then\r\ndet(B) = − det(A).\r\n2. if B is obtained from A by multiplying a row by c then\r\ndet(B) = c det(A).\r\n3. if all the elements of one row is 0 then det(A) = 0.\r\n4. if A is a square matrix having two rows equal then det(A) = 0.\r\n5. Let B = [bij ] and C = [cij ] be two matrices which differ from the matrix A = [aij ]\r\nonly in the mth row for some m. If cmj = amj + bmj for 1 ≤ j ≤ n then det(C) =\r\ndet(A) + det(B).\r\n6. if B is obtained from A by replacing the ℓth row by itself plus k times the mth row,\r\nfor ℓ 6= m then det(B) = det(A).",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/7a98461b-0ccc-4641-b670-45c72d47fb00.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8f073c0f362448d9ac17744ff10702a92858239c75f5d3033dbe76e8b15c5015",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 361
      },
      {
        "segments": [
          {
            "segment_id": "d9fa76c7-fb86-4109-87b3-ec7b2118a647",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 169,
            "page_width": 612,
            "page_height": 792,
            "content": "7.2. PROPERTIES OF DETERMINANT 169\r\n7. if A is a triangular matrix then det(A) = a11a22 · · · ann, the product of the diagonal\r\nelements.\r\n8. If E is an elementary matrix of order n then det(EA) = det(E) det(A).\r\n9. A is invertible if and only if det(A) 6= 0.\r\n10. If B is an n × n matrix then det(AB) = det(A) det(B).\r\n11. det(A) = det(At), where recall that Atis the transpose of the matrix A.\r\nProof. Proof of Part 1. Suppose B = [bij ] is obtained from A = [aij ] by the interchange\r\nof the ℓ\r\nth and mth row. Then bℓj = amj , bmj = aℓj for 1 ≤ j ≤ n and bij = aij for\r\n1 ≤ i 6= ℓ, m ≤ n, 1 ≤ j ≤ n.\r\nLet τ = (ℓ m) be a transposition. Then by Proposition 7.1.4, Sn = {σ ◦ τ : σ ∈ Sn}.\r\nHence by the definition of determinant and Example 7.1.14.2, we have\r\ndet(B) = X\r\nσ∈Sn\r\nsgn(σ)\r\nYn\r\ni=1\r\nbiσ(i) =\r\nX\r\nσ◦τ∈Sn\r\nsgn(σ ◦ τ )\r\nYn\r\ni=1\r\nbi(σ◦τ)(i)\r\n=\r\nX\r\nσ◦τ∈Sn\r\nsgn(τ ) · sgn(σ) b1(σ◦τ)(1)b2(σ◦τ)(2) · · · bℓ(σ◦τ)(ℓ)· · · bm(σ◦τ)(m)· · · bn(σ◦τ)(n)\r\n= sgn(τ )\r\nX\r\nσ∈Sn\r\nsgn(σ) b1σ(1) · b2σ(2) · · · bℓσ(m)· · · bmσ(ℓ)· · · bnσ(n)\r\n= −\r\n X\r\nσ∈Sn\r\nsgn(σ) a1σ(1) · a2σ(2) · · · amσ(m)· · · aℓσ(ℓ)· · · anσ(n)\r\n!\r\nas sgn(τ ) = −1\r\n= − det(A).\r\nProof of Part 2. Suppose that B = [bij ] is obtained by multiplying the mth row of A\r\nby c 6= 0. Then bmj = c amj and bij = aij for 1 ≤ i 6= m ≤ n, 1 ≤ j ≤ n. Then\r\ndet(B) = X\r\nσ∈Sn\r\nsgn(σ)b1σ(1)b2σ(2) · · · bmσ(m)· · · bnσ(n)\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · · camσ(m)· · · anσ(n)\r\n= c\r\nX\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · · amσ(m)· · · anσ(n)\r\n= c det(A).\r\nProof of Part 3. Note that det(A) = P\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) . . . anσ(n). So, each term\r\nin the expression for determinant, contains one entry from each row. Hence, from the\r\ncondition that A has a row consisting of all zeros, the value of each term is 0. Thus,\r\ndet(A) = 0.\r\nProof of Part 4. Suppose that the ℓ\r\nth and mth row of A are equal. Let B be the\r\nmatrix obtained from A by interchanging the ℓ\r\nth and mth rows. Then by the first part,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/d9fa76c7-fb86-4109-87b3-ec7b2118a647.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4923068926cff36278e02357b927bda52020f7e476f79a853646b18bae1b0132",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 427
      },
      {
        "segments": [
          {
            "segment_id": "c0a0f73c-7248-4e37-8b60-68deb7a20d35",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 170,
            "page_width": 612,
            "page_height": 792,
            "content": "170 CHAPTER 7. APPENDIX\r\ndet(B) = − det(A). But the assumption implies that B = A. Hence, det(B) = det(A). So,\r\nwe have det(B) = − det(A) = det(A). Hence, det(A) = 0.\r\nProof of Part 5. By definition and the given assumption, we have\r\ndet(C) = X\r\nσ∈Sn\r\nsgn(σ)c1σ(1)c2σ(2) · · · cmσ(m)· · · cnσ(n)\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ)c1σ(1)c2σ(2) · · ·(bmσ(m) + amσ(m))· · · cnσ(n)\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ)b1σ(1)b2σ(2) · · · bmσ(m)· · · bnσ(n)\r\n+\r\nX\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · · amσ(m)· · · anσ(n)\r\n= det(B) + det(A).\r\nProof of Part 6. Suppose that B = [bij ] is obtained from A by replacing the ℓth row\r\nby itself plus k times the mth row, for ℓ 6= m. Then bℓj = aℓj + k amj and bij = aij for\r\n1 ≤ i 6= m ≤ n, 1 ≤ j ≤ n. Then\r\ndet(B) = X\r\nσ∈Sn\r\nsgn(σ)b1σ(1)b2σ(2) · · · bℓσ(ℓ)· · · bmσ(m)· · · bnσ(n)\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · ·(aℓσ(ℓ) + kamσ(m))· · · amσ(m)· · · anσ(n)\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · · aℓσ(ℓ)· · · amσ(m)· · · anσ(n)\r\n+k\r\nX\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · · amσ(m)· · · amσ(m)· · · anσ(n)\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · · aℓσ(ℓ)· · · amσ(m)· · · anσ(n) use Part 4\r\n= det(A).\r\nProof of Part 7. First let us assume that A is an upper triangular matrix. Observe\r\nthat if σ ∈ Sn is different from the identity permutation then n(σ) ≥ 1. So, for every\r\nσ 6= Idn ∈ Sn, there exists a positive integer m, 1 ≤ m ≤ n − 1 (depending on σ) such\r\nthat m > σ(m). As A is an upper triangular matrix, amσ(m) = 0 for each σ(6= Idn) ∈ Sn.\r\nHence the result follows.\r\nA similar reasoning holds true, in case A is a lower triangular matrix.\r\nProof of Part 8. Let In be the identity matrix of order n. Then using Part 7, det(In) = 1.\r\nAlso, recalling the notations for the elementary matrices given in Remark 2.2.2, we have\r\ndet(Eij ) = −1, (using Part 1) det(Ei(c)) = c (using Part 2) and det(Eij (k) = 1 (using\r\nPart 6). Again using Parts 1, 2 and 6, we get det(EA) = det(E) det(A).\r\nProof of Part 9. Suppose A is invertible. Then by Theorem 2.2.5, A is a product\r\nof elementary matrices. That is, there exist elementary matrices E1, E2, . . . , Ek such\r\nthat A = E1E2 · · · Ek. Now a repeated application of Part 8 implies that det(A) =\r\ndet(E1) det(E2)· · · det(Ek). But det(Ei) 6= 0 for 1 ≤ i ≤ k. Hence, det(A) 6= 0.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/c0a0f73c-7248-4e37-8b60-68deb7a20d35.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f53994a51876a3f2b358665e9af1d9d50c7fd5a1c91a5c2aafa2923ea11acc86",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 460
      },
      {
        "segments": [
          {
            "segment_id": "377cf5d3-c98d-414e-84d4-dabcae7f4968",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 171,
            "page_width": 612,
            "page_height": 792,
            "content": "7.2. PROPERTIES OF DETERMINANT 171\r\nNow assume that det(A) 6= 0. We show that A is invertible. On the contrary, assume\r\nthat A is not invertible. Then by Theorem 2.2.5, the matrix A is not of full rank. That\r\nis there exists a positive integer r < n such that rank(A) = r. So, there exist elementary\r\nmatrices E1, E2, . . . , Ek such that E1E2 · · · EkA =\r\n\"\r\nB\r\n0\r\n#\r\n. Therefore, by Part 3 and a\r\nrepeated application of Part 8,\r\ndet(E1) det(E2)· · · det(Ek) det(A) = det(E1E2 · · · EkA) = det \" B\r\n0\r\n#! = 0.\r\nBut det(Ei) 6= 0 for 1 ≤ i ≤ k. Hence, det(A) = 0. This contradicts our assumption that\r\ndet(A) 6= 0. Hence our assumption is false and therefore A is invertible.\r\nProof of Part 10. Suppose A is not invertible. Then by Part 9, det(A) = 0. Also,\r\nthe product matrix AB is also not invertible. So, again by Part 9, det(AB) = 0. Thus,\r\ndet(AB) = det(A) det(B).\r\nNow suppose that A is invertible. Then by Theorem 2.2.5, A is a product of el\u0002ementary matrices. That is, there exist elementary matrices E1, E2, . . . , Ek such that\r\nA = E1E2 · · · Ek. Now a repeated application of Part 8 implies that\r\ndet(AB) = det(E1E2 · · · EkB) = det(E1) det(E2)· · · det(Ek) det(B)\r\n= det(E1E2 · · · Ek) det(B) = det(A) det(B).\r\nProof of Part 11. Let B = [bij ] = At\r\n. Then bij = aji for 1 ≤ i, j ≤ n. By Proposi\u0002tion 7.1.4, we know that Sn = {σ\r\n−1\r\n: σ ∈ Sn}. Also sgn(σ) = sgn(σ\r\n−1\r\n). Hence,\r\ndet(B) = X\r\nσ∈Sn\r\nsgn(σ)b1σ(1)b2σ(2) · · · bnσ(n)\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ\r\n−1\r\n)bσ−1(1) 1 bσ−1(2) 2 · · · bσ−1(n) n\r\n=\r\nX\r\nσ∈Sn\r\nsgn(σ\r\n−1\r\n)a1σ−1(1)b2σ−1(2) · · · bnσ−1(n)\r\n= det(A).\r\nRemark 7.2.2. 1. The result that det(A) = det(At) implies that in the statements\r\nmade in Theorem 7.2.1, where ever the word “row” appears it can be replaced by\r\n“column”.\r\n2. Let A = [aij ] be a matrix satisfying a11 = 1 and a1j = 0 for 2 ≤ j ≤ n. Let B be the\r\nsubmatrix of A obtained by removing the first row and the first column. Then it can\r\nbe easily shown that det(A) = det(B). The reason being is as follows:\r\nfor every σ ∈ Sn with σ(1) = 1 is equivalent to saying that σ is a permutation of the",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/377cf5d3-c98d-414e-84d4-dabcae7f4968.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=8fa127d99c4d7e1f177cdb3516aec0bf4be3219c87553561e6d53fc71cc77fdb",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 436
      },
      {
        "segments": [
          {
            "segment_id": "2f7d23c9-46c6-430e-af2f-0e349454781a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 172,
            "page_width": 612,
            "page_height": 792,
            "content": "172 CHAPTER 7. APPENDIX\r\nelements {2, 3, . . . , n}. That is, σ ∈ Sn−1. Hence,\r\ndet(A) = X\r\nσ∈Sn\r\nsgn(σ)a1σ(1)a2σ(2) · · · anσ(n) =\r\nX\r\nσ∈Sn,σ(1)=1\r\nsgn(σ)a2σ(2) · · · anσ(n)\r\n=\r\nX\r\nσ∈Sn−1\r\nsgn(σ)b1σ(1) · · · bnσ(n) = det(B).\r\nWe are now ready to relate this definition of determinant with the one given in Defini\u0002tion 2.5.2.\r\nTheorem 7.2.3. Let A be an n × n matrix. Then det(A) = Pn\r\nj=1\r\n(−1)1+ja1j detA(1|j)\r\n\u0001\r\n,\r\nwhere recall that A(1|j) is the submatrix of A obtained by removing the 1\r\nst row and the\r\nj\r\nth column.\r\nProof. For 1 ≤ j ≤ n, define two matrices\r\nBj =\r\n\r\n\r\n\r\n\r\n\r\n\r\n0 0 · · · a1j · · · 0\r\na21 a22 · · · a2j · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nan1 an2 · · · anj · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\nn×n\r\nand Cj =\r\n\r\n\r\n\r\n\r\n\r\n\r\na1j 0 0 · · · 0\r\na2j a21 a22 · · · a2n\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\nanj an1 an2 · · · ann\r\n\r\n\r\n\r\n\r\n\r\n\r\nn×n\r\n.\r\nThen by Theorem 7.2.1.5,\r\ndet(A) = Xn\r\nj=1\r\ndet(Bj ). (7.2.2)\r\nWe now compute det(Bj ) for 1 ≤ j ≤ n. Note that the matrix Bj can be transformed into\r\nCj by j − 1 interchanges of columns done in the following manner:\r\nfirst interchange the 1st and 2nd column, then interchange the 2nd and 3rd column and\r\nso on (the last process consists of interchanging the (j − 1)th column with the j\r\nth col\u0002umn. Then by Remark 7.2.2 and Parts 1 and 2 of Theorem 7.2.1, we have det(Bj ) =\r\na1j (−1)j−1 det(Cj ). Therefore by (7.2.2),\r\ndet(A) = Xn\r\nj=1\r\n(−1)j−1a1j detA(1|j)\r\n\u0001\r\n=\r\nXn\r\nj=1\r\n(−1)j+1a1j detA(1|j)\r\n\u0001\r\n.\r\n7.3 Dimension of M + N\r\nTheorem 7.3.1. Let V (F) be a finite dimensional vector space and let M and N be two\r\nsubspaces of V. Then\r\ndim(M) + dim(N) = dim(M + N) + dim(M ∩ N). (7.3.3)",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/2f7d23c9-46c6-430e-af2f-0e349454781a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5a0c8522813e42c8d69be970c539a36731f45cbb1baaffcc2dda7f5360d572e1",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 380
      },
      {
        "segments": [
          {
            "segment_id": "bf6a965c-4983-4d48-a1c8-1e19822acf7e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 173,
            "page_width": 612,
            "page_height": 792,
            "content": "7.3. DIMENSION OF M + N 173\r\nProof. Since M ∩ N is a vector subspace of V, consider a basis B1 = {u1, u2, . . . , uk}\r\nof M ∩ N. As, M ∩ N is a subspace of the vector spaces M and N, we extend the ba\u0002sis B1 to form a basis BM = {u1, u2, . . . , uk, v1, . . . , vr} of M and also a basis BN =\r\n{u1, u2, . . . , uk, w1, . . . , ws} of N.\r\nWe now proceed to prove that the set B2 = {u1, u2, . . . , uk, w1, . . . , ws, v1, v2, . . . , vr}\r\nis a basis of M + N.\r\nTo do this, we show that\r\n1. the set B2 is linearly independent subset of V, and\r\n2. L(B2) = M + N.\r\nThe second part can be easily verified. To prove the first part, we consider the linear\r\nsystem of equations\r\nα1u1 + · · · + αkuk + β1w1 + · · · + βsws + γ1v1 + · · · + γrvr = 0. (7.3.4)\r\nThis system can be rewritten as\r\nα1u1 + · · · + αkuk + β1w1 + · · · + βsws = −(γ1v1 + · · · + γrvr).\r\nThe vector v = −(γ1v1+· · ·+γrvr) ∈ M, as v1, . . . , vr ∈ BM. But we also have v = α1u1+\r\n· · · + αkuk + β1w1 + · · · + βsws ∈ N as the vectors u1, u2, . . . , uk, w1, . . . , ws ∈ BN . Hence,\r\nv ∈ M∩N and therefore, there exists scalars δ1, . . . , δk such that v = δ1u1+δ2u2+· · ·+δkuk.\r\nSubstituting this representation of v in Equation (7.3.4), we get\r\n(α1 − δ1)u1 + · · · + (αk − δk)uk + β1w1 + · · · + βsws = 0.\r\nBut then, the vectors u1, u2, . . . , uk, w1, . . . , ws are linearly independent as they form a\r\nbasis. Therefore, by the definition of linear independence, we get\r\nαi − δi = 0, for 1 ≤ i ≤ k and βj = 0 for 1 ≤ j ≤ s.\r\nThus the linear system of Equations (7.3.4) reduces to\r\nα1u1 + · · · + αkuk + γ1v1 + · · · + γrvr = 0.\r\nThe only solution for this linear system is\r\nαi = 0, for 1 ≤ i ≤ k and γj = 0 for 1 ≤ j ≤ r.\r\nThus we see that the linear system of Equations (7.3.4) has no non-zero solution. And\r\ntherefore, the vectors are linearly independent.\r\nHence, the set B2 is a basis of M + N. We now count the vectors in the sets B1,B2,BM\r\nand BN to get the required result.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/bf6a965c-4983-4d48-a1c8-1e19822acf7e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11774b25308cd34fc501176be5553801a9be34df7e23f99477f8dd100e12b00c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 497
      },
      {
        "segments": [
          {
            "segment_id": "2a77d23c-b7f7-4bed-a2a7-4366c6ca8d96",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 174,
            "page_width": 612,
            "page_height": 792,
            "content": "Index\r\nAdjoint of a Matrix, 53\r\nBack Substitution, 35\r\nBasic Variables, 30\r\nBasis of a Vector Space, 76\r\nBilinear Form, 156\r\nCauchy-Schwarz Inequality, 115\r\nCayley Hamilton Theorem, 146\r\nChange of Basis Theorem, 109\r\nCharacteristic Equation, 142\r\nCharacteristic Polynomial, 142\r\nCofactor Matrix, 52\r\nColumn Operations, 44\r\nColumn Rank of a Matrix, 44\r\nComplex Vector Space, 62\r\nCoordinates of a Vector, 90\r\nDefinition\r\nDiagonal Matrix, 6\r\nEquality of two Matrices, 5\r\nIdentity Matrix, 6\r\nLower Triangular Matrix, 6\r\nMatrix, 5\r\nPrincipal Diagonal, 6\r\nSquare Matrix, 6\r\nTranspose of a Matrix, 7\r\nTriangular Matrix, 6\r\nUpper Triangular Matrix, 6\r\nZero Matrix, 6\r\nDeterminant\r\nProperties, 168\r\nDeterminant of a Square Matrix, 49, 168\r\nDimension\r\nFinite Dimensional Vector Space, 79\r\nEigen-pair, 142\r\nEigenvalue, 142\r\nEigenvector, 142\r\nElementary Matrices, 37\r\nElementary Row Operations, 27\r\nElimination\r\nGauss, 28\r\nGauss-Jordan, 35\r\nEquality of Linear Operators, 96\r\nForward Elimination, 28\r\nFree Variables, 30\r\nFundamental Theorem of Linear Algebra, 117\r\nGauss Elimination Method, 28\r\nGauss-Jordan Elimination Method, 35\r\nGram-Schmidt Orthogonalization Process, 124\r\nIdempotent Matrix, 15\r\nIdentity Operator, 96\r\nInner Product, 113\r\nInner Product Space, 113\r\nInverse of a Linear Transformation, 105\r\nInverse of a Matrix, 13\r\nLeading Term, 30\r\nLinear Algebra\r\nFundamental Theorem, 117\r\nLinear Combination of Vectors, 69\r\nLinear Dependence, 73\r\nlinear Independence, 73\r\nLinear Operator, 95\r\nEquality, 96\r\nLinear Span of Vectors, 70\r\nLinear System, 24\r\nAssociated Homogeneous System, 25\r\nAugmented Matrix, 25\r\nCoefficient Matrix, 25\r\n174",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/2a77d23c-b7f7-4bed-a2a7-4366c6ca8d96.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=397c22674c37151723f5f2c54cf8f5e133af1a0a7eee3262f3dde92e05cde73a",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "d7bfe29a-0bbd-4f7a-a5c4-9f00c2300102",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 175,
            "page_width": 612,
            "page_height": 792,
            "content": "INDEX 175\r\nEquivalent Systems, 27\r\nHomogeneous, 25\r\nNon-Homogeneous, 25\r\nNon-trivial Solution, 25\r\nSolution, 25\r\nSolution Set, 25\r\nTrivial Solution, 25\r\nConsistent, 31\r\nInconsistent, 31\r\nLinear Transformation, 95\r\nMatrix, 100\r\nMatrix Product, 106\r\nNull Space, 102\r\nRange Space, 102\r\nComposition, 106\r\nInverse, 105, 108\r\nNullity, 102\r\nRank, 102\r\nMatrix, 5\r\nAdjoint, 53\r\nCofactor, 52\r\nColumn Rank, 44\r\nDeterminant, 49\r\nEigen-pair, 142\r\nEigenvalue, 142\r\nEigenvector, 142\r\nElementary, 37\r\nFull Rank, 46\r\nHermitian, 151\r\nNon-Singular, 50\r\nQuadratic Form, 159\r\nRank, 44\r\nRow Equivalence, 27\r\nRow-Reduced Echelon Form, 35\r\nScalar Multiplication, 7\r\nSingular, 50\r\nSkew-Hermitian, 151\r\nAddition, 7\r\nDiagonalisation, 148\r\nIdempotent, 15\r\nInverse, 13\r\nMinor, 52\r\nNilpotent, 15\r\nNormal, 151\r\nOrthogonal, 15\r\nProduct of Matrices, 8\r\nRow Echelon Form, 30\r\nRow Rank, 43\r\nSkew-Symmetric, 15\r\nSubmatrix, 16\r\nSymmetric, 15\r\nTrace, 18\r\nUnitary, 151\r\nMatrix Equality, 5\r\nMatrix Multiplication, 8\r\nMatrix of a Linear Transformation, 100\r\nMinor of a Matrix, 52\r\nNilpotent Matrix, 15\r\nNon-Singular Matrix, 50\r\nNormal Matrix\r\nSpectral Theorem, 156\r\nOperations\r\nColumn, 44\r\nOperator\r\nIdentity, 96\r\nSelf-Adjoint, 131\r\nOrder of Nilpotency, 15\r\nOrdered Basis, 90\r\nOrthogonal Complement, 130\r\nOrthogonal Projection, 130\r\nOrthogonal Subspace of a Set, 128\r\nOrthogonal Vectors, 116\r\nOrthonormal Basis, 121\r\nOrthonormal Set, 121\r\nOrthonormal Vectors, 121\r\nProperties of Determinant, 168\r\nQR Decomposition, 136\r\nGeneralized, 137\r\nQuadratic Form, 159\r\nRank Nullity Theorem, 104\r\nRank of a Matrix, 44",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/d7bfe29a-0bbd-4f7a-a5c4-9f00c2300102.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=76a7201329e859ffede19612d4b2cc325b54b3f6617acd9e53241f8729eec1f7",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 446
      },
      {
        "segments": [
          {
            "segment_id": "62b93963-52c6-410c-a1d5-aa88a9a1942a",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 176,
            "page_width": 612,
            "page_height": 792,
            "content": "176 INDEX\r\nReal Vector Space, 62\r\nRow Equivalent Matrices, 27\r\nRow Operations\r\nElementary, 27\r\nRow Rank of a Matrix, 43\r\nRow-Reduced Echelon Form, 35\r\nSelf-Adjoint Operator, 131\r\nSesquilinear Form, 156\r\nSimilar Matrices, 110\r\nSingular Matrix, 50\r\nSolution Set of a Linear System, 25\r\nSpectral Theorem for Normal Matrices, 156\r\nSquare Matrix\r\nBilinear Form, 156\r\nDeterminant, 168\r\nSesquilinear Form, 156\r\nSubmatrix of a Matrix, 16\r\nSubspace\r\nLinear Span, 72\r\nOrthogonal Complement, 117, 128\r\nSum of two Matrices, 7\r\nSystem of Linear Equations, 24\r\nTrace of a Matrix, 18\r\nTransformation\r\nZero, 96\r\nUnit Vector, 114\r\nUnitary Equivalence, 152\r\nVector Space, 61\r\nC\r\nn\r\n: Complex n-tuple, 63\r\nR\r\nn\r\n: Real n-tuple, 63\r\nBasis, 76\r\nDimension, 79\r\nDimension of M + N, 172\r\nInner Product, 113\r\nIsomorphism, 108\r\nReal, 62\r\nSubspace, 66\r\nComplex, 62\r\nFinite Dimensional, 71\r\nInfinite Dimensional, 71\r\nVector Subspace, 66\r\nVectors\r\nAngle, 116\r\nCoordinates, 90\r\nLength, 114\r\nLinear Combination, 69\r\nLinear Independence, 73\r\nLinear Span, 70\r\nNorm, 114\r\nOrthogonal, 116\r\nOrthonormal, 121\r\nLinear Dependence, 73\r\nZero Transformation, 96",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/34c0d054-82c1-453b-98db-1af675e31e57/images/62b93963-52c6-410c-a1d5-aa88a9a1942a.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T042125Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5e576042e97ed1ddb50559ff6e8cbb1bbfe463cf62c91ce1dc636378eec9e818",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 169
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "```json\n{\"title\": \"Linear Algebra\"}\n```"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "```json\n{\"author\": \"A K Lal S Pati\"}\n```"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "```json\n{\"date_published\": \"February 10, 2015\"}\n```"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "No response"
        }
      ]
    }
  }
}