{
  "file_name": "Integer Encoding - Chapter 9 (2013).pdf",
  "task_id": "1a40bfde-6003-48d1-8152-36eb7010f56a",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "bbeae95b-9d54-4ca3-aff4-60f199c79d9d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 1,
            "page_width": 595,
            "page_height": 842,
            "content": "9\r\nInteger encoding\r\nEverything should be made as\r\nsimple as possible, but no\r\nsimpler\r\nAlbert Einstein\r\n9.1 Elias codes: γ and δ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-3\r\n9.2 Rice code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-4\r\n9.3 PForDelta encoding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-5\r\n9.4 Variable-byte codes and (s, c)-dense codes . . . . . . . . . . . 9-5\r\n9.5 Interpolative coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-7\r\n9.6 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-9\r\nIn this chapter we will address a basic encoding problem which occurs in many contexts, and whose\r\nefficient dealing is frequently underestimated for the impact it may have on the total space occu\u0002pancy and speed of the underlying application [2, 6].\r\nProblem. Let S = s1, . . . , sn be a sequence of positive integers si, possibly repeated. The\r\ngoal is to represent the integers of S as binary sequences which are self-delimiting and use\r\nfew bits.\r\nWe note that the request about si of being positive integers can be relaxed by mapping a positive\r\ninteger x to 2x and a negative integer x to −2x+1, thus turning again the set S to a set of just positive\r\nintegers.\r\nLet us comment two exemplar applications. Search engines store for each term t the list of\r\ndocuments (i.e. Web pages, blog posts, tweets, etc. etc.) where t occurs. Answering a user query,\r\nformulated as sequence of keywords t1t2 . . . tk, then consists of finding the documents where all\r\ntis occur. This is implemented by intersecting the document lists for these k terms. Documents are\r\nusually represented via integer IDs, which are assigned during the crawling of those documents from\r\nthe Web. Storing these integers with a fixed-length binary encoding (i.e. 4 or 8 bytes) may require\r\nconsiderable space, and thus time for their retrieval, given that modern search engines index up to\r\n20 billion documents. In order to reduce disk-space occupancy, as well as increase the amount of\r\ncached lists in internal memory, two kinds of compression tricks are adopted: the first one consists\r\nof sorting the document IDs in each list, and then encode each of them with the difference between\r\nit and its preceding ID in the list, the so called d-gap1; the second trick consists of encoding each\r\nd-gap with a variable-length sequence of bits which is short for small integers.\r\nAnother example of occurrence for the above problem relates to data compression. We have seen\r\nin Chapter 8 that the LZ77-compressor turns input files into sequence of triples in which the first\r\ntwo components are integers. Other known compressors (such as MTF, MPEG, RLE, BWT, etc.)\r\nproduce as intermediate output one or more sets of integers, with smaller values most probable and\r\n1Of course, the first document ID of a list is stored explicitly.\r\n\rc Paolo Ferragina, 2009-2014 9-1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/bbeae95b-9d54-4ca3-aff4-60f199c79d9d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=33c351daf3dea7cfc32a397a41ea48a8904454f38db69d86c92f2f9a7c06691e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 663
      },
      {
        "segments": [
          {
            "segment_id": "bbeae95b-9d54-4ca3-aff4-60f199c79d9d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 1,
            "page_width": 595,
            "page_height": 842,
            "content": "9\r\nInteger encoding\r\nEverything should be made as\r\nsimple as possible, but no\r\nsimpler\r\nAlbert Einstein\r\n9.1 Elias codes: γ and δ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-3\r\n9.2 Rice code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-4\r\n9.3 PForDelta encoding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-5\r\n9.4 Variable-byte codes and (s, c)-dense codes . . . . . . . . . . . 9-5\r\n9.5 Interpolative coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-7\r\n9.6 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-9\r\nIn this chapter we will address a basic encoding problem which occurs in many contexts, and whose\r\nefficient dealing is frequently underestimated for the impact it may have on the total space occu\u0002pancy and speed of the underlying application [2, 6].\r\nProblem. Let S = s1, . . . , sn be a sequence of positive integers si, possibly repeated. The\r\ngoal is to represent the integers of S as binary sequences which are self-delimiting and use\r\nfew bits.\r\nWe note that the request about si of being positive integers can be relaxed by mapping a positive\r\ninteger x to 2x and a negative integer x to −2x+1, thus turning again the set S to a set of just positive\r\nintegers.\r\nLet us comment two exemplar applications. Search engines store for each term t the list of\r\ndocuments (i.e. Web pages, blog posts, tweets, etc. etc.) where t occurs. Answering a user query,\r\nformulated as sequence of keywords t1t2 . . . tk, then consists of finding the documents where all\r\ntis occur. This is implemented by intersecting the document lists for these k terms. Documents are\r\nusually represented via integer IDs, which are assigned during the crawling of those documents from\r\nthe Web. Storing these integers with a fixed-length binary encoding (i.e. 4 or 8 bytes) may require\r\nconsiderable space, and thus time for their retrieval, given that modern search engines index up to\r\n20 billion documents. In order to reduce disk-space occupancy, as well as increase the amount of\r\ncached lists in internal memory, two kinds of compression tricks are adopted: the first one consists\r\nof sorting the document IDs in each list, and then encode each of them with the difference between\r\nit and its preceding ID in the list, the so called d-gap1; the second trick consists of encoding each\r\nd-gap with a variable-length sequence of bits which is short for small integers.\r\nAnother example of occurrence for the above problem relates to data compression. We have seen\r\nin Chapter 8 that the LZ77-compressor turns input files into sequence of triples in which the first\r\ntwo components are integers. Other known compressors (such as MTF, MPEG, RLE, BWT, etc.)\r\nproduce as intermediate output one or more sets of integers, with smaller values most probable and\r\n1Of course, the first document ID of a list is stored explicitly.\r\n\rc Paolo Ferragina, 2009-2014 9-1",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/bbeae95b-9d54-4ca3-aff4-60f199c79d9d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=33c351daf3dea7cfc32a397a41ea48a8904454f38db69d86c92f2f9a7c06691e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 663
      },
      {
        "segments": [
          {
            "segment_id": "090be626-77c6-4747-9d75-04ac94189037",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 2,
            "page_width": 595,
            "page_height": 842,
            "content": "9-2 Paolo Ferragina\r\nlarger values increasingly less probable. The final coding stage of those compressors must therefore\r\nconvert these integers into a bit stream, such that the total number of bits is minimized.\r\nThe main question we address in this chapter is how we design a variable-length binary repre\u0002sentation for (unbounded) integers which takes as few bit as possible and is prefix-free, namely the\r\nencoding of sis can be concatenated to produce an output bit stream, which preserves decodability,\r\nin the sense that each individual integer encoding can be identified and decoded.\r\nThe first and simplest idea to solve this problem is surely that one to take m = maxj sj and then\r\nencode each integer si ∈ S by using 1+blog2 mc bits. This fixed-size encoding is efficient whenever\r\nthe set S is not much spread and concentrated around the value zero. But this is a very unusual\r\nsituation, in general, m \u001d si so that many bits are wasted in the output bit stream. So why not\r\nstoring each si by using its binary encoding with 1 + blog2sic bits. The subtle problem with this\r\napproach would be that this code is not self-delimiting, and in fact we cannot concatenate the binary\r\nencoding of all si and still be able to distinguish each codeword. As an example, take S = {1, 2, 3}\r\nand the output bit sequence 11011 which would be produced by using their binary encoding. It is\r\nevident that we could derive many compatible sequence of integers from 11011, such as S , but also\r\n{6, 1, 1}, as well as {1, 2, 1, 1}, and several others.\r\nIt is therefore clear that this simple encoding problem is challenging and deserves the attention\r\nthat we dedicate in this chapter. We start by introducing one of the simplest integer codes known,\r\nthe so called unary code. The unary code U(x) for an integer x ≥ 1 is given by a sequence of x − 1\r\nbits set to 0, ended by a (delimiting) bit set to 1. The correctness of the condition that x , 0 is easily\r\nestablished. U(x) requires x bits, which is exponentially longer than the length Θ(log x) of its binary\r\ncode, nonetheless this code is efficient for very small integers and soon becomes space inefficient as\r\nx increases.\r\nThis statement can be made more precise by recalling a basic fact coming from the Shannon’s\r\ncoding theory, which states that the ideal code length L(c) for a symbol c is equal to log2\r\n1\r\nPr[c]\r\nbits,\r\nwhere P[c] is the probability of occurrence of symbol c. This probability can be known in advance,\r\nif we have information about the source emitting c, or it can be estimated empirically by examining\r\nthe occurrences of integers siin S . The reader should be careful in recalling that, in the scenario\r\nconsidered in this chapter, symbols are positive integers so the ideal code for the integer x consists\r\nof log2\r\n1\r\nPr[x]\r\nbits. So, by solving the equation |U(x)| = log2\r\n1\r\nPr[x] with respect to P[x], we derive the\r\ndistribution of the sis for which the unary code is optimal. In this specific case it is P[x] = 2\r\n−x\r\n.\r\nAs far as efficiency is concerned, the unary code needs a lot of bit shifts which are slow to be\r\nimplemented in modern PCs; again another reason to favor small integers.\r\nFACT 9.1 The unary code of a positive integer x takes x bits, and thus it is optimal for the\r\ndistribution P[x] = 2\r\n−x\r\n.\r\nUsing this same argument we can also deduct that the fixed-length binary encoding, which uses\r\n1 + blog2 mc bits, is optimal whenever integers in S are distributed uniformly within the range\r\n{1, 2, . . . , m}.\r\nFACT 9.2 Given a set S of integers, of maximum value m, the fixed-length binary code repre\u0002sents each of them in 1+blog2 mc bits, and thus it is optimal for the uniform distribution P[x] = 1/m.\r\nIn general integers are not uniformly distributed, and in fact variable-length binary representations\r\nmust be considered which eventually improve the simple unary code. There are many proposals in\r\nthe literature, each offering a different trade-off between space occupancy of the binary-code and\r\ntime efficiency for its decoding. The following subsections will detail the most useful and the most",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/090be626-77c6-4747-9d75-04ac94189037.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a41380ff30bfb61e6ca8f91937270c26484aa03e068f9830dc623206e552aa70",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 727
      },
      {
        "segments": [
          {
            "segment_id": "090be626-77c6-4747-9d75-04ac94189037",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 2,
            "page_width": 595,
            "page_height": 842,
            "content": "9-2 Paolo Ferragina\r\nlarger values increasingly less probable. The final coding stage of those compressors must therefore\r\nconvert these integers into a bit stream, such that the total number of bits is minimized.\r\nThe main question we address in this chapter is how we design a variable-length binary repre\u0002sentation for (unbounded) integers which takes as few bit as possible and is prefix-free, namely the\r\nencoding of sis can be concatenated to produce an output bit stream, which preserves decodability,\r\nin the sense that each individual integer encoding can be identified and decoded.\r\nThe first and simplest idea to solve this problem is surely that one to take m = maxj sj and then\r\nencode each integer si ∈ S by using 1+blog2 mc bits. This fixed-size encoding is efficient whenever\r\nthe set S is not much spread and concentrated around the value zero. But this is a very unusual\r\nsituation, in general, m \u001d si so that many bits are wasted in the output bit stream. So why not\r\nstoring each si by using its binary encoding with 1 + blog2sic bits. The subtle problem with this\r\napproach would be that this code is not self-delimiting, and in fact we cannot concatenate the binary\r\nencoding of all si and still be able to distinguish each codeword. As an example, take S = {1, 2, 3}\r\nand the output bit sequence 11011 which would be produced by using their binary encoding. It is\r\nevident that we could derive many compatible sequence of integers from 11011, such as S , but also\r\n{6, 1, 1}, as well as {1, 2, 1, 1}, and several others.\r\nIt is therefore clear that this simple encoding problem is challenging and deserves the attention\r\nthat we dedicate in this chapter. We start by introducing one of the simplest integer codes known,\r\nthe so called unary code. The unary code U(x) for an integer x ≥ 1 is given by a sequence of x − 1\r\nbits set to 0, ended by a (delimiting) bit set to 1. The correctness of the condition that x , 0 is easily\r\nestablished. U(x) requires x bits, which is exponentially longer than the length Θ(log x) of its binary\r\ncode, nonetheless this code is efficient for very small integers and soon becomes space inefficient as\r\nx increases.\r\nThis statement can be made more precise by recalling a basic fact coming from the Shannon’s\r\ncoding theory, which states that the ideal code length L(c) for a symbol c is equal to log2\r\n1\r\nPr[c]\r\nbits,\r\nwhere P[c] is the probability of occurrence of symbol c. This probability can be known in advance,\r\nif we have information about the source emitting c, or it can be estimated empirically by examining\r\nthe occurrences of integers siin S . The reader should be careful in recalling that, in the scenario\r\nconsidered in this chapter, symbols are positive integers so the ideal code for the integer x consists\r\nof log2\r\n1\r\nPr[x]\r\nbits. So, by solving the equation |U(x)| = log2\r\n1\r\nPr[x] with respect to P[x], we derive the\r\ndistribution of the sis for which the unary code is optimal. In this specific case it is P[x] = 2\r\n−x\r\n.\r\nAs far as efficiency is concerned, the unary code needs a lot of bit shifts which are slow to be\r\nimplemented in modern PCs; again another reason to favor small integers.\r\nFACT 9.1 The unary code of a positive integer x takes x bits, and thus it is optimal for the\r\ndistribution P[x] = 2\r\n−x\r\n.\r\nUsing this same argument we can also deduct that the fixed-length binary encoding, which uses\r\n1 + blog2 mc bits, is optimal whenever integers in S are distributed uniformly within the range\r\n{1, 2, . . . , m}.\r\nFACT 9.2 Given a set S of integers, of maximum value m, the fixed-length binary code repre\u0002sents each of them in 1+blog2 mc bits, and thus it is optimal for the uniform distribution P[x] = 1/m.\r\nIn general integers are not uniformly distributed, and in fact variable-length binary representations\r\nmust be considered which eventually improve the simple unary code. There are many proposals in\r\nthe literature, each offering a different trade-off between space occupancy of the binary-code and\r\ntime efficiency for its decoding. The following subsections will detail the most useful and the most",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/090be626-77c6-4747-9d75-04ac94189037.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=a41380ff30bfb61e6ca8f91937270c26484aa03e068f9830dc623206e552aa70",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 727
      },
      {
        "segments": [
          {
            "segment_id": "d4e0b52a-851c-4c6c-a195-37ed87ba9044",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 3,
            "page_width": 595,
            "page_height": 842,
            "content": "Integer encoding 9-3\r\nused among these codes, starting from the most simplest ones which use fixed encoding models for\r\nthe integers (such as, e.g., γ and δ codes) and, then, moving to the more involved Huffman and Inter\u0002polative codes which use dynamic models that adapt themselves to the distribution of the integers in\r\nS . It is very well known that Huffman coding is optimal, but few times this optimality is dissected\r\nand made clear. In fact, this is crucial to explain some apparent contradictory statements about these\r\nmore involved codes: such as the fact that in some cases Interpolative coding is better than Huff\u0002man coding. The reason is that Huffman coding is optimal among the family of static prefix-free\r\ncodes, namely the ones that use a fixed model for encoding each single integer of S (specifically, the\r\nHuffman code of an integer x is defined according to P[x]). Vice versa, Interpolative coding uses\r\na dynamic model that encodes x according to the distribution of other integers in S , thus possibly\r\nadopting different codes for the occurrences of x. Depending on the distribution of the integers in\r\nS , this adaptivity might be useful and thus originate a shorter output bit stream.\r\n9.1 Elias codes: γ and δ\r\nThese are two very simple universal codes for integers which use a fixed model, they have been\r\nintroduced in the ’60s by Elias [3]. The adjective ”universal” here relates to the property that the\r\nlength of the code is O(log x) for any integer x. So it is just a constant factor more than the optimal\r\nbinary code B(x) having length 1+blog xc, with the additional wishful property of being prefix-free.\r\nγ-code represents the integer x as a binary sequence composed of two parts: a sequence of |B(x)|−\r\n1 zero, followed by the binary representation B(x). The initial sequence of zeros is delimited by the\r\n1 which starts the binary representation B(x). So γ(x) can be decoded easily: count the consecutive\r\nnumber of zeros up to the first 1, say they are c; then, fetch the following c + 1 bits (included the 1),\r\nand interpret the sequence as the integer x.\r\nFIGURE 9.1: Representation for γ(9).\r\nThe γ-code requires 2|B(x)|−1 bits, which is 2(1+blog2xc)−1 = 2blog2xc+1. In fact, the γ-code\r\nof the integer 9 needs 2blog29c + 1 = 7 bits. From Shannon’s condition on ideal codes, we derive\r\nthat the γ-code is optimal whenever the distribution of the values follows the formula Pr[x] ≈\r\n1\r\n2x\r\n2\r\n.\r\nFACT 9.3 The γ-code of a positive integer x takes 2blog2xc + 1 bits, and thus it is optimal for\r\nthe distribution P[x] ≈\r\n1\r\n2x\r\n2\r\n, and it is a factor of 2 from the length of the optimal binary code.\r\nThe inefficiency in the γ-code resides in the unary coding of the length |B(x)| which is really\r\ncostly as x becomes larger and larger. In order to mitigate this problem, Elias introduced the δ-code,\r\nwhich applies the γ-code in place of the unary code. So δ(x) consists of two parts: the first encodes\r\nγ(|B(x)|), the second encodes B(x). Notice that, since we are using the γ-code for B(x)’s length,\r\nthe first and the second parts do not share any bits; moreover we observe that γ is applied to |B(x)|\r\nwhich guarantees to be a number greater than zero. The decoding of δ(x) is easy, first we decode\r\nγ(|B(x)|) and then fetch B(x), so getting the value x in binary.\r\nAs far as the length in bits of δ(x) is concerned, we observe that it is (1+2blog2|B(x)|c)+|B(x)| ≈\r\n1 + log x + 2 log log x. This encoding is therefore a factor 1 + o(1) from the optimal binary code,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/d4e0b52a-851c-4c6c-a195-37ed87ba9044.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e3ce6797cfedcca3c45d87ca10e80c6566f4f92cf9a7306ea4164b03786fb831",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 622
      },
      {
        "segments": [
          {
            "segment_id": "d4e0b52a-851c-4c6c-a195-37ed87ba9044",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 3,
            "page_width": 595,
            "page_height": 842,
            "content": "Integer encoding 9-3\r\nused among these codes, starting from the most simplest ones which use fixed encoding models for\r\nthe integers (such as, e.g., γ and δ codes) and, then, moving to the more involved Huffman and Inter\u0002polative codes which use dynamic models that adapt themselves to the distribution of the integers in\r\nS . It is very well known that Huffman coding is optimal, but few times this optimality is dissected\r\nand made clear. In fact, this is crucial to explain some apparent contradictory statements about these\r\nmore involved codes: such as the fact that in some cases Interpolative coding is better than Huff\u0002man coding. The reason is that Huffman coding is optimal among the family of static prefix-free\r\ncodes, namely the ones that use a fixed model for encoding each single integer of S (specifically, the\r\nHuffman code of an integer x is defined according to P[x]). Vice versa, Interpolative coding uses\r\na dynamic model that encodes x according to the distribution of other integers in S , thus possibly\r\nadopting different codes for the occurrences of x. Depending on the distribution of the integers in\r\nS , this adaptivity might be useful and thus originate a shorter output bit stream.\r\n9.1 Elias codes: γ and δ\r\nThese are two very simple universal codes for integers which use a fixed model, they have been\r\nintroduced in the ’60s by Elias [3]. The adjective ”universal” here relates to the property that the\r\nlength of the code is O(log x) for any integer x. So it is just a constant factor more than the optimal\r\nbinary code B(x) having length 1+blog xc, with the additional wishful property of being prefix-free.\r\nγ-code represents the integer x as a binary sequence composed of two parts: a sequence of |B(x)|−\r\n1 zero, followed by the binary representation B(x). The initial sequence of zeros is delimited by the\r\n1 which starts the binary representation B(x). So γ(x) can be decoded easily: count the consecutive\r\nnumber of zeros up to the first 1, say they are c; then, fetch the following c + 1 bits (included the 1),\r\nand interpret the sequence as the integer x.\r\nFIGURE 9.1: Representation for γ(9).\r\nThe γ-code requires 2|B(x)|−1 bits, which is 2(1+blog2xc)−1 = 2blog2xc+1. In fact, the γ-code\r\nof the integer 9 needs 2blog29c + 1 = 7 bits. From Shannon’s condition on ideal codes, we derive\r\nthat the γ-code is optimal whenever the distribution of the values follows the formula Pr[x] ≈\r\n1\r\n2x\r\n2\r\n.\r\nFACT 9.3 The γ-code of a positive integer x takes 2blog2xc + 1 bits, and thus it is optimal for\r\nthe distribution P[x] ≈\r\n1\r\n2x\r\n2\r\n, and it is a factor of 2 from the length of the optimal binary code.\r\nThe inefficiency in the γ-code resides in the unary coding of the length |B(x)| which is really\r\ncostly as x becomes larger and larger. In order to mitigate this problem, Elias introduced the δ-code,\r\nwhich applies the γ-code in place of the unary code. So δ(x) consists of two parts: the first encodes\r\nγ(|B(x)|), the second encodes B(x). Notice that, since we are using the γ-code for B(x)’s length,\r\nthe first and the second parts do not share any bits; moreover we observe that γ is applied to |B(x)|\r\nwhich guarantees to be a number greater than zero. The decoding of δ(x) is easy, first we decode\r\nγ(|B(x)|) and then fetch B(x), so getting the value x in binary.\r\nAs far as the length in bits of δ(x) is concerned, we observe that it is (1+2blog2|B(x)|c)+|B(x)| ≈\r\n1 + log x + 2 log log x. This encoding is therefore a factor 1 + o(1) from the optimal binary code,",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/d4e0b52a-851c-4c6c-a195-37ed87ba9044.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e3ce6797cfedcca3c45d87ca10e80c6566f4f92cf9a7306ea4164b03786fb831",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 622
      },
      {
        "segments": [
          {
            "segment_id": "5f578dc6-361a-4ab2-a1a3-2b6b76697f6f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 4,
            "page_width": 595,
            "page_height": 842,
            "content": "9-4 Paolo Ferragina\r\nFIGURE 9.2: Representation for δ(14).\r\nhence it is universal.\r\nFACT 9.4 The δ-code of a positive integer x takes about 1 + log2x + 2 log2log2x bits, and thus\r\nit is optimal for the distribution P[x] ≈\r\n1\r\n2x(log x)\r\n2\r\n, and it is a factor of 1 + o(1) from the length of the\r\noptimal binary code.\r\nIn conclusion, γ- and δ-codes are universal and pretty efficient whenever the set S is concentrated\r\naround zero; however, it must be noted that these two codes need a lot of bit shifts to be decoded\r\nand this may be slow if numbers are larger and thus encoded in many bits. The following codes\r\ntrade space efficiency for decoding speed and, in fact, they are preferred in practical applications.\r\n9.2 Rice code\r\nThere are situations in which integers are concentrated around some value, different from zero;\r\nhere, Rice coding becomes advantageous both in compression ratio and decoding speed. Its special\r\nfeature is to be a parametric code, namely one which depends from a positive integer k, which\r\nmay be fixed according to the distribution of the integers in the set S . The Rice code Rk(x) of an\r\ninteger x, given the parameter k, consists of two parts: the quotient q = b\r\n(x−1)\r\n2\r\nk c and the remainder\r\nr = x − 2\r\nkq − 1. The quotient is stored in unary using q + 1 bits, the remainder r is stored in binary\r\nusing k bits. So the quotient is encoded in variable length, whereas the remainder is encoded in\r\nfixed length. The closer 2kis to the value of x, the shorter is the representation of q, and thus the\r\nfaster is its decoding. For this reason, k is chosen in such a way that 2kis concentrated around the\r\nmean of S ’s elements.\r\nFIGURE 9.3: Representation for R4(83)\r\nThe bit length of Rk(x) is q + k + 1. This code is a particular case of the Golomb Code [6], it is\r\noptimal when the values to be encoded follow a geometric distribution with parameter p, namely\r\nPr[x] = (1 − p)\r\nx−1 p. In this case, if 2k '\r\nln(2)\r\np\r\n' 0.69mean(S ), the Rice and all Golomb codes\r\ngenerate an optimal prefix-code [6].\r\nFACT 9.5 The Rice code of a positive integer x takes b\r\n(x−1)\r\n2\r\nk c + 1 + k bits, and it is optimal for\r\nthe geometric distribution Pr[x] = (1 − p)\r\nx−1 p.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/5f578dc6-361a-4ab2-a1a3-2b6b76697f6f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e6e9463b896e91ef6de5cda333566a1f6173ae5658f98e2346e11e4ab2733363",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 418
      },
      {
        "segments": [
          {
            "segment_id": "5099cf5e-361f-45de-a704-e6fd40e6c480",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 5,
            "page_width": 595,
            "page_height": 842,
            "content": "Integer encoding 9-5\r\n9.3 PForDelta encoding\r\nThis method for compressing integers supports extremely fast decompression and achieves a small\r\nsize in the compressed output whenever S ’s values follow a gaussian distribution. In detail, let us\r\nassume that most of S ’s values fall in an interval [base, base + 2\r\nb − 1], we translate the values in\r\nthe new interval [0, 2\r\nb − 1] in order to encode them in b bits; the other values outside this range\r\nare called exceptions and they are represented in the compressed list with an escape symbol and\r\nalso encoded explicitly in a separate list using a fixed-size representation of w bits (namely, a whole\r\nmemory word). The good property of this code is that all values in S are encoded in fixed length,\r\neither b bits or w + b bits, so that they can be decoded very fast and possibly in parallel by packing\r\nfew of them in a memory word.\r\nFIGURE 9.4: An example for PForDelta, with b = 3 and base = 0. The values in the range (blue\r\nbox) are encoded using 3 bits, while the out-of-range values (green box) are encoded separately and\r\nan escape symbol \u001c is used as a place-holder.\r\nFACT 9.6 The PForDelta code of a positive integer x takes either b bits or b+w bits, depending\r\non the fact that x ∈ [base, base + 2\r\nb − 1] or not, respectively. This code is proper for a gaussian\r\ndistribution of the integers to be encoded.\r\nThe design of a PForDelta code needs to deal with two problems:\r\n• How to choose b: in the original work, b was chosen such that about the 90% of the\r\nvalues in S are smaller than 2b. An alternative solution is to trade between space wasting\r\n(choosing a greater b) or space saving (more exceptions, smaller b). In [5] it has been\r\nproposed a method based on dynamic programming, that computes the optimal b for\r\na desired compression ratio. In particular, it returns the largest b that minimizes the\r\nnumber of exceptions and, thus, ensures a faster decompression.\r\n• How to encode the escape character: a possible solution is to assign a special bit se\u0002quence for it, thus leaving 2b − 1 configurations for the values in the range.\r\nIn conclusion PForDelta encodes blocks of k consecutive integers so that they can be stored in\r\na multi-word (i.e. multiple of 32 bits). Those integers that do not fit within b bits are treated as\r\nexceptions and stored in another array that is merged to the original sequence of codewords during\r\nthe decoding phase (thus paying w+b bits). PForDelta is surprisingly succinct in storing the integers\r\nwhich occur in search-engine indexes; but the actual positive feature which makes it very appealing\r\nfor developers is that it is incredibly fast in decoding because of the word-alignment and the fact that\r\nthere exist implementations which do not use if-statements, and thus avoid branch mispredictions.\r\n9.4 Variable-byte codes and (s, c)-dense codes",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/5099cf5e-361f-45de-a704-e6fd40e6c480.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=285f7f9460c368c18ebd6cd005757973d4bedfb98a0afda66a93228f0eaa5b78",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 504
      },
      {
        "segments": [
          {
            "segment_id": "2023c593-9968-49b2-9f2a-2fffa30c3d14",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 6,
            "page_width": 595,
            "page_height": 842,
            "content": "9-6 Paolo Ferragina\r\nAnother class of codes which trade speed by succinctness is the one of the so called (s, c)-dense\r\ncodes. Their simplest instantiation, originally used in the Altavista search engine, is the variable\u0002byte code which uses a sequence of bytes to represent an integer x. This byte-aligned coding is\r\nuseful to achieve a significant decoding speed. It is constructed as follows: the binary representation\r\nB(x) is partitioned into groups of 7-bits, possibly the first group is padded by appending 0s to its\r\nfront; a flag-bit is appended to each group to indicate whether that group is the last one (bit set to 0)\r\nor not (bit set to 1) of the representation. The decoding is simple, we scan the byte sequence until\r\nwe find a byte whose value is smaller than 128.\r\nFIGURE 9.5: Variable-byte representation for the integer 216\r\nThe minimum amount of bits necessary to encode x is 8, and on average 4 bits are wasted because\r\nof the padding. Hence this method is proper for large values x.\r\nFACT 9.7 The Variable-byte code of a positive integer x takes d\r\n|B(x)|\r\n7\r\ne bytes. This code is optimal\r\nfor the distribution P[x] ≈\r\n√7\r\n1/x.\r\nThe use of the status bit induces a subtle issue, in that it partitions the configurations of each\r\nbyte into two sets: the values smaller than 128 (status bit equal to 0, called stoppers) and the values\r\nlarger or equal than 128 (status bit equal to 1, called continuers). For the sake of presentation we\r\ndenote the cardinalities of the two sets by s and c, respectively. Of course, we have that s + c = 256\r\nbecause they represent all possibly byte-configurations. During the decoding phase, whenever we\r\nencounter a continuer byte, we go on reading, otherwise we stop.\r\nThe drawback of this approach is that for any x < 128 we use always 1 byte. Therefore if the set\r\nS consists of very-small integers, we are wasting bits. Vice versa, if S consists of integers larger\r\nthan 128, then it could be better to enlarge the set of stoppers. Indeed nobody prevents us to change\r\nthe distribution of stoppers and continuers, provided that s + c = 256. Let us analyze how changes\r\nthe number of integers which can be encoded with one of more bytes, depending on the choice of s\r\nand c:\r\n• One byte can encode the first s integers;\r\n• Two bytes can encode the subsequent sc integers.\r\n• Three bytes can encode the subsequent sc2integers.\r\n• k bytes can encode sck−1integers.\r\nIt is evident, at this point, that the choice of s and c depends on the distribution of the integers\r\nto be encoded. For example, assume that we want to encode the values 1, . . . , 15 and they have\r\ndecreasing frequency; moreover, assume that the word-length is 3 bits (instead of 8 bits), so that\r\ns + c = 2\r\n3 = 8 (instead of 256).\r\nTable 9.1 shows how the integers smaller than 15 are encoded by using two different choices for s\r\nand c: in the first case, the number of stoppers and continuers is 4; in the second case, the number of",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/2023c593-9968-49b2-9f2a-2fffa30c3d14.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=850b9dc64f7dd85e668405d8fa6b97252d9a548d9aee39cac6b782e045d43852",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "2023c593-9968-49b2-9f2a-2fffa30c3d14",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 6,
            "page_width": 595,
            "page_height": 842,
            "content": "9-6 Paolo Ferragina\r\nAnother class of codes which trade speed by succinctness is the one of the so called (s, c)-dense\r\ncodes. Their simplest instantiation, originally used in the Altavista search engine, is the variable\u0002byte code which uses a sequence of bytes to represent an integer x. This byte-aligned coding is\r\nuseful to achieve a significant decoding speed. It is constructed as follows: the binary representation\r\nB(x) is partitioned into groups of 7-bits, possibly the first group is padded by appending 0s to its\r\nfront; a flag-bit is appended to each group to indicate whether that group is the last one (bit set to 0)\r\nor not (bit set to 1) of the representation. The decoding is simple, we scan the byte sequence until\r\nwe find a byte whose value is smaller than 128.\r\nFIGURE 9.5: Variable-byte representation for the integer 216\r\nThe minimum amount of bits necessary to encode x is 8, and on average 4 bits are wasted because\r\nof the padding. Hence this method is proper for large values x.\r\nFACT 9.7 The Variable-byte code of a positive integer x takes d\r\n|B(x)|\r\n7\r\ne bytes. This code is optimal\r\nfor the distribution P[x] ≈\r\n√7\r\n1/x.\r\nThe use of the status bit induces a subtle issue, in that it partitions the configurations of each\r\nbyte into two sets: the values smaller than 128 (status bit equal to 0, called stoppers) and the values\r\nlarger or equal than 128 (status bit equal to 1, called continuers). For the sake of presentation we\r\ndenote the cardinalities of the two sets by s and c, respectively. Of course, we have that s + c = 256\r\nbecause they represent all possibly byte-configurations. During the decoding phase, whenever we\r\nencounter a continuer byte, we go on reading, otherwise we stop.\r\nThe drawback of this approach is that for any x < 128 we use always 1 byte. Therefore if the set\r\nS consists of very-small integers, we are wasting bits. Vice versa, if S consists of integers larger\r\nthan 128, then it could be better to enlarge the set of stoppers. Indeed nobody prevents us to change\r\nthe distribution of stoppers and continuers, provided that s + c = 256. Let us analyze how changes\r\nthe number of integers which can be encoded with one of more bytes, depending on the choice of s\r\nand c:\r\n• One byte can encode the first s integers;\r\n• Two bytes can encode the subsequent sc integers.\r\n• Three bytes can encode the subsequent sc2integers.\r\n• k bytes can encode sck−1integers.\r\nIt is evident, at this point, that the choice of s and c depends on the distribution of the integers\r\nto be encoded. For example, assume that we want to encode the values 1, . . . , 15 and they have\r\ndecreasing frequency; moreover, assume that the word-length is 3 bits (instead of 8 bits), so that\r\ns + c = 2\r\n3 = 8 (instead of 256).\r\nTable 9.1 shows how the integers smaller than 15 are encoded by using two different choices for s\r\nand c: in the first case, the number of stoppers and continuers is 4; in the second case, the number of",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/2023c593-9968-49b2-9f2a-2fffa30c3d14.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=850b9dc64f7dd85e668405d8fa6b97252d9a548d9aee39cac6b782e045d43852",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 536
      },
      {
        "segments": [
          {
            "segment_id": "110ebe0e-625e-4819-b1b6-a02c646a998e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 7,
            "page_width": 595,
            "page_height": 842,
            "content": "Integer encoding 9-7\r\nValues s = c = 4 s = 6, c = 2\r\n1 001 001\r\n2 010 010\r\n3 011 011\r\n4 100 000 100\r\n5 100 001 101\r\n6 100 010 110 000\r\n7 100 011 110 001\r\n8 101 000 110 010\r\n9 101 001 110 011\r\n10 101 010 110 100\r\n11 101 011 110 101\r\n12 110 000 111 000\r\n13 110 001 111 001\r\n14 110 010 111 010\r\n15 110 011 111 011\r\nTABLE 9.1 Example of (s, c)-encoding using two different values for s and c.\r\nstoppers is 6 and the number of continuers is 2. Notice that in both cases we correctly have s+c = 8.\r\nWe point out that in both cases, two words of 3 bits (i.e. 6 bits) are enough to encode all the 15\r\nintegers; but, while in the former case we can encode only the first four values with one word, in\r\nthe latter the values encoded using one word are six. This can lead to a more compressed sequence\r\naccording to the skewness of the distribution of {1, . . . , 15}.\r\nThis shows, surprisingly, that can be advantageous to adapt the number of stoppers and continuers\r\nto the probability distribution of S ’s values. Figure 9.6 further details this observation, by showing\r\nthe compression ratio as a function of s, for two different distributions ZIFF and AP, the former is\r\nthe classic Zipfian distribution (i.e. P[x] ≈ 1/x), the latter is the distribution derived from the words\r\nof the Associated-Press collection (i.e. P[x] is the frequency of occurrence of the x-th most frequent\r\nword). When s is very small, the number of high frequency values encoded with one byte is also\r\nvery small, but in this case c is large and therefore many words with low frequency will be encoded\r\nwith few bytes. As s grows, we gain compression in more frequent values and loose compression\r\nin less frequent values. At some later point, the compression lost in the last values is larger than the\r\ncompression gained in values at the beginning, and therefore the global compression ratio worsens.\r\nThat point give us the optimal s value. In [1] it is shown that the minimum is unique and the authors\r\npropose an efficient algorithm to calculate that optimal s.\r\n9.5 Interpolative coding\r\nThis is an integer-encoding technique that is ideal whenever the sequence S shows clustered occur\u0002rences of integers, namely subsequences which are concentrated in small ranges. This is a typical\r\nsituation which arises in the storage of posting lists of search engines [5]. Interpolative code is\r\ndesigned in a recursive way by assuming that the integer sequence to be compressed consists of\r\nincreasing values: namely S\r\n0 = s0\r\n1\r\n, . . . , s\r\n0\r\nn with s\r\n0\r\ni\r\n< s\r\n0\r\ni+1\r\n. We can turn the original problem to this\r\none, by just setting s\r\n0\r\ni\r\n=\r\nPi\r\nj=1\r\nsj.\r\nAt each iteration we know, for the current subsequence S\r\n0\r\nl,r\r\nto be encoded, the following 5 pa\u0002rameters:\r\n• the left index l and the right index r delimiting the subsequence S\r\n0\r\nl,r\r\n= {s\r\n0\r\nl\r\n, s\r\n0\r\nl+1\r\n, . . . , s\r\n0\r\nr\r\n};",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/110ebe0e-625e-4819-b1b6-a02c646a998e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fc60aca2e7fbd7278839b79509fa47159232ebde651699679e45197f4a5594a8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 548
      },
      {
        "segments": [
          {
            "segment_id": "110ebe0e-625e-4819-b1b6-a02c646a998e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 7,
            "page_width": 595,
            "page_height": 842,
            "content": "Integer encoding 9-7\r\nValues s = c = 4 s = 6, c = 2\r\n1 001 001\r\n2 010 010\r\n3 011 011\r\n4 100 000 100\r\n5 100 001 101\r\n6 100 010 110 000\r\n7 100 011 110 001\r\n8 101 000 110 010\r\n9 101 001 110 011\r\n10 101 010 110 100\r\n11 101 011 110 101\r\n12 110 000 111 000\r\n13 110 001 111 001\r\n14 110 010 111 010\r\n15 110 011 111 011\r\nTABLE 9.1 Example of (s, c)-encoding using two different values for s and c.\r\nstoppers is 6 and the number of continuers is 2. Notice that in both cases we correctly have s+c = 8.\r\nWe point out that in both cases, two words of 3 bits (i.e. 6 bits) are enough to encode all the 15\r\nintegers; but, while in the former case we can encode only the first four values with one word, in\r\nthe latter the values encoded using one word are six. This can lead to a more compressed sequence\r\naccording to the skewness of the distribution of {1, . . . , 15}.\r\nThis shows, surprisingly, that can be advantageous to adapt the number of stoppers and continuers\r\nto the probability distribution of S ’s values. Figure 9.6 further details this observation, by showing\r\nthe compression ratio as a function of s, for two different distributions ZIFF and AP, the former is\r\nthe classic Zipfian distribution (i.e. P[x] ≈ 1/x), the latter is the distribution derived from the words\r\nof the Associated-Press collection (i.e. P[x] is the frequency of occurrence of the x-th most frequent\r\nword). When s is very small, the number of high frequency values encoded with one byte is also\r\nvery small, but in this case c is large and therefore many words with low frequency will be encoded\r\nwith few bytes. As s grows, we gain compression in more frequent values and loose compression\r\nin less frequent values. At some later point, the compression lost in the last values is larger than the\r\ncompression gained in values at the beginning, and therefore the global compression ratio worsens.\r\nThat point give us the optimal s value. In [1] it is shown that the minimum is unique and the authors\r\npropose an efficient algorithm to calculate that optimal s.\r\n9.5 Interpolative coding\r\nThis is an integer-encoding technique that is ideal whenever the sequence S shows clustered occur\u0002rences of integers, namely subsequences which are concentrated in small ranges. This is a typical\r\nsituation which arises in the storage of posting lists of search engines [5]. Interpolative code is\r\ndesigned in a recursive way by assuming that the integer sequence to be compressed consists of\r\nincreasing values: namely S\r\n0 = s0\r\n1\r\n, . . . , s\r\n0\r\nn with s\r\n0\r\ni\r\n< s\r\n0\r\ni+1\r\n. We can turn the original problem to this\r\none, by just setting s\r\n0\r\ni\r\n=\r\nPi\r\nj=1\r\nsj.\r\nAt each iteration we know, for the current subsequence S\r\n0\r\nl,r\r\nto be encoded, the following 5 pa\u0002rameters:\r\n• the left index l and the right index r delimiting the subsequence S\r\n0\r\nl,r\r\n= {s\r\n0\r\nl\r\n, s\r\n0\r\nl+1\r\n, . . . , s\r\n0\r\nr\r\n};",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/110ebe0e-625e-4819-b1b6-a02c646a998e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fc60aca2e7fbd7278839b79509fa47159232ebde651699679e45197f4a5594a8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 548
      },
      {
        "segments": [
          {
            "segment_id": "7c88c975-8928-4532-bd84-13db1135316d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 8,
            "page_width": 595,
            "page_height": 842,
            "content": "9-8 Paolo Ferragina\r\nFIGURE 9.6: An example of how compression rate varies according to the choice of s, given that\r\nc = 256 − s.\r\n• the number n of elements in subsequence S\r\n0\r\nl,r\r\n;\r\n• a lower-bound low to the lowest value in S\r\n0\r\nl,r\r\n, and an upper-bound hi to the highest value\r\nin S\r\n0\r\nl,r\r\n, hence low ≤ s\r\n0\r\nl\r\nand hi ≥ s\r\n0\r\nr\r\n.\r\nInitially we have n = |S |, l = 1, r = n, low = s\r\n0\r\n1\r\nand hi = s\r\n0\r\nn\r\n. At each step we first encode the mid\u0002dle element s\r\n0\r\nm, where m = b\r\nl+r\r\n2\r\nc, given the information available for the quintuple hn, l,r, low, hii,\r\nand then recursively encode the two subsequences s\r\n0\r\nl\r\n, . . . , s\r\n0\r\nm−1\r\nand s\r\n0\r\nm+1\r\n, . . . , s\r\n0\r\nr\r\n, by using a properly\r\nrecomputed parameters hn, l,r, low, hii for each of them.\r\nIn order to succinctly encode s\r\n0\r\nm we deploy as much information as possible we can derive from\r\nhn, l,r, low, hii. Specifically, we observe that it is s\r\n0\r\nm ≥ low + m − l (in the first half of S\r\n0\r\nl,r we\r\nhave m − l distinct values and the smallest one is larger than low) and s\r\n0\r\nm ≤ hi − (r − m) (via a\r\nsimilar argument). Thus s\r\n0\r\nm lies in the range [low + m − l, hi − r + m] so we can encode the value\r\ns\r\n0\r\nm − (low + m − l) by using dlog2\r\nle bits, where l = hi − low − r + l is the size of that interval. In this\r\nway, interpolative coding can use very few bits per value whenever the sequence S\r\n0\r\nl,r\r\nis dense.\r\nWith the exception of the values of the first iteration, which must be known to both the encoder\r\nand the decoder, all values for the subsequent iterations can be easily derived from the previous\r\nones. In particular,\r\n• for the subsequence s\r\n0\r\nl\r\n, . . . , s\r\n0\r\nm−1\r\n, the parameter low is the same of the previous step,\r\nsince s\r\n0\r\nl\r\nhas not changed; and we can set hi = s\r\n0\r\nm − 1, since s\r\n0\r\nm−1\r\n< s\r\n0\r\nm given that we\r\nassumed the integers to be distinct and increasing;\r\n• for the subsequence s\r\n0\r\nm+1\r\n, . . . , s\r\n0\r\nr\r\n, the parameter hi is the same as before, since s\r\n0\r\nr has not\r\nchanged; and we can set low = s\r\n0\r\nm + 1, since s\r\n0\r\nm+1\r\n> s\r\n0\r\nm;\r\n• the parameters l, r and n are recomputed accordingly.\r\nThe following figure shows a running example of the behavior of the algorithm:\r\nWe conclude the description of Interpolative coding by noticing that the encoding of an integer s\r\n0\r\ni\r\nis not fixed but depends on the distribution of the other integers in S\r\n0\r\n. This reflects onto the original\r\nsequence S in such a way that the same integer x may be encoded differently in its occurrences.\r\nThis code is therefore adaptive and, additionally, it is not prefix-free; these two specialties may turn\r\nit better than Huffman code, which is optimal among the class of static prefix-free codes.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/7c88c975-8928-4532-bd84-13db1135316d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=166b0bc117d1beefb7858744c23842e7fb8fa90fce2bb1555b78ab6b6b194e54",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 586
      },
      {
        "segments": [
          {
            "segment_id": "7c88c975-8928-4532-bd84-13db1135316d",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 8,
            "page_width": 595,
            "page_height": 842,
            "content": "9-8 Paolo Ferragina\r\nFIGURE 9.6: An example of how compression rate varies according to the choice of s, given that\r\nc = 256 − s.\r\n• the number n of elements in subsequence S\r\n0\r\nl,r\r\n;\r\n• a lower-bound low to the lowest value in S\r\n0\r\nl,r\r\n, and an upper-bound hi to the highest value\r\nin S\r\n0\r\nl,r\r\n, hence low ≤ s\r\n0\r\nl\r\nand hi ≥ s\r\n0\r\nr\r\n.\r\nInitially we have n = |S |, l = 1, r = n, low = s\r\n0\r\n1\r\nand hi = s\r\n0\r\nn\r\n. At each step we first encode the mid\u0002dle element s\r\n0\r\nm, where m = b\r\nl+r\r\n2\r\nc, given the information available for the quintuple hn, l,r, low, hii,\r\nand then recursively encode the two subsequences s\r\n0\r\nl\r\n, . . . , s\r\n0\r\nm−1\r\nand s\r\n0\r\nm+1\r\n, . . . , s\r\n0\r\nr\r\n, by using a properly\r\nrecomputed parameters hn, l,r, low, hii for each of them.\r\nIn order to succinctly encode s\r\n0\r\nm we deploy as much information as possible we can derive from\r\nhn, l,r, low, hii. Specifically, we observe that it is s\r\n0\r\nm ≥ low + m − l (in the first half of S\r\n0\r\nl,r we\r\nhave m − l distinct values and the smallest one is larger than low) and s\r\n0\r\nm ≤ hi − (r − m) (via a\r\nsimilar argument). Thus s\r\n0\r\nm lies in the range [low + m − l, hi − r + m] so we can encode the value\r\ns\r\n0\r\nm − (low + m − l) by using dlog2\r\nle bits, where l = hi − low − r + l is the size of that interval. In this\r\nway, interpolative coding can use very few bits per value whenever the sequence S\r\n0\r\nl,r\r\nis dense.\r\nWith the exception of the values of the first iteration, which must be known to both the encoder\r\nand the decoder, all values for the subsequent iterations can be easily derived from the previous\r\nones. In particular,\r\n• for the subsequence s\r\n0\r\nl\r\n, . . . , s\r\n0\r\nm−1\r\n, the parameter low is the same of the previous step,\r\nsince s\r\n0\r\nl\r\nhas not changed; and we can set hi = s\r\n0\r\nm − 1, since s\r\n0\r\nm−1\r\n< s\r\n0\r\nm given that we\r\nassumed the integers to be distinct and increasing;\r\n• for the subsequence s\r\n0\r\nm+1\r\n, . . . , s\r\n0\r\nr\r\n, the parameter hi is the same as before, since s\r\n0\r\nr has not\r\nchanged; and we can set low = s\r\n0\r\nm + 1, since s\r\n0\r\nm+1\r\n> s\r\n0\r\nm;\r\n• the parameters l, r and n are recomputed accordingly.\r\nThe following figure shows a running example of the behavior of the algorithm:\r\nWe conclude the description of Interpolative coding by noticing that the encoding of an integer s\r\n0\r\ni\r\nis not fixed but depends on the distribution of the other integers in S\r\n0\r\n. This reflects onto the original\r\nsequence S in such a way that the same integer x may be encoded differently in its occurrences.\r\nThis code is therefore adaptive and, additionally, it is not prefix-free; these two specialties may turn\r\nit better than Huffman code, which is optimal among the class of static prefix-free codes.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/7c88c975-8928-4532-bd84-13db1135316d.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=166b0bc117d1beefb7858744c23842e7fb8fa90fce2bb1555b78ab6b6b194e54",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 586
      },
      {
        "segments": [
          {
            "segment_id": "4e73f517-86c9-4626-8598-ea38d5b23b9e",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 9,
            "page_width": 595,
            "page_height": 842,
            "content": "Integer encoding 9-9\r\nFIGURE 9.7: The blue and the red boxes are, respectively, the left and the right subsequence of each\r\niteration. In the green boxes is indicated the integer s\r\n0\r\nm to be encoded. The procedure performs, in\r\npractice, a preorder traversal of a balanced binary tree whose leaves are the integers in S . When it\r\nencounters a subsequence of the form [low, low+1, . . . , low+n−1], it doesn’t emit anything. Thus,\r\nthe items are encoded in the following order (in brackets the actual number encoded): 9 (3), 3 (3), 5\r\n(1), 7 (1), 18 (6), 11 (1), 15 (4).\r\n9.6 Concluding remarks\r\nWe wish to convince the reader about the generality of the Integer Compression problem, because\r\nmore and more frequently other compression problems, such as the classic Text Compression, boil\r\ndown to compressing sequences of integers. An example was given by the LZ77-compressor in\r\nChapter 8. Another example can be obtained by looking at any text T as a sequence of tokens,\r\nbeing them words or single characters; each token can be represented with an integer (aka token\u0002ID), so that the problem of compressing T can be solved by compressing the sequence of token-IDs.\r\nIn order to better deploy one of the previous integer-encoding schemes, one can adopt an interesting\r\nstrategy which consists of sorting the tokens by decreasing frequency of occurrence in T, and then\r\nassign as token-ID their rank in the ordered sequence. This way, the more frequent is the occurrence\r\nof the token in T, the smaller is the token-ID, and thus the shorter will be the codeword assigned to it\r\nby anyone of the previous integer-encoding schemes. Therefore this simple strategy implements the\r\ngolden rule of data compression which consists of assigning short codewords to frequent tokens. If\r\nthe distribution of the tokens follows one of the distributions indicated in the previous sections, those\r\ncodewords have optimal length; otherwise, the codewords may be sub-optimal. In [4] it is shown\r\nthat, if the i-th word follows a Zipfian distribution, such as P[i] = c(1/i)\r\nα where c is a normalization\r\nconstant and α is a parameter depending on the input text, then the previous algorithm using δ\u0002coding achieves a performance close to the entropy of the input text.\r\nReferences",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/4e73f517-86c9-4626-8598-ea38d5b23b9e.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=64b60dc0cb4c40584b9970c887c5e0a80873f26f79969ac52351c0ca9a831f0c",
            "html": null,
            "markdown": null
          },
          {
            "segment_id": "31418bfb-e43a-497f-b477-d9323e885361",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 595,
              "height": 842
            },
            "page_number": 10,
            "page_width": 595,
            "page_height": 842,
            "content": "9-10 Paolo Ferragina\r\n[1] Nieves R. Brisaboa, Antonio Farina, Gonzalo Navarro, Jos´e R. Param´a. Lightweight\r\nnatural language text compression. Information Retrieval, 10:1-33, 2007.\r\n[2] Alistair Moffat. Compressing Integer Sequences and Sets. In Encyclopedia of Algorithms.\r\nSpringer, 2009.\r\n[3] Peter Fenwick. Universal Codes. In Lossless Data Compression Handbook. Academic\r\nPress, 2003.\r\n[4] Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch¨utze. Introduction to\r\nInformation Retrieval. Cambridge University Press, 2008.\r\n[5] Hao Yan, Shuai Ding, Torsten Suel. Inverted Index Compression and Query Processing\r\nwith Optimized Document Ordering. In Procs of WWW, pp. 401-410, 2009.\r\n[6] Ian H. Witten, Alistair Moffat, Timoty C. Bell. Managing Gigabytes. Morgan Kauffman,\r\nsecond edition, 1999.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/1a40bfde-6003-48d1-8152-36eb7010f56a/images/31418bfb-e43a-497f-b477-d9323e885361.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041241Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=057247aa70df06b316f8d6cdf848224afbd97a4e94f97699452a2c431698d66d",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 486
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "\"Integer encoding\"\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Paolo Ferragina\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "I am unable to answer the question about the format of the `date_published` field based on the context provided.  The text discusses integer encoding and compression techniques, and does not contain any information related to dates or date formats.\n"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "```json\n{\"location\": null}\n```"
        }
      ]
    }
  }
}