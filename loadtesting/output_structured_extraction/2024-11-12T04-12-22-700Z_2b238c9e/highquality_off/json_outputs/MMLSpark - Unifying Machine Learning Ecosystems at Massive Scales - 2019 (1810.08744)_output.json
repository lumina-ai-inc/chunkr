{
  "file_name": "MMLSpark - Unifying Machine Learning Ecosystems at Massive Scales - 2019 (1810.08744).pdf",
  "task_id": "403a9cbe-7a70-4381-b8e3-dfcbbf02e77f",
  "output": {
    "chunks": [
      {
        "segments": [
          {
            "segment_id": "398dbce5-dee6-4b3f-b16f-9ee0c41d016f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nMark Hamilton 1 Sudarshan Raghunathan 2Ilya Matiach 3 Andrew Schonhoffer 3 Anand Raman 2\r\nEli Barzilay 1 Karthik Rajendran 4 5 Dalitso Banda 4 5 Casey Jisoo Hong 4 5 Manon Knoertzer 4 5 Ben Brodsky 2\r\nMinsoo Thigpen 4 Janhavi Suresh Mahajan 4 Courtney Cochrane 4 Abhiram Eswaran 4 Ari Green 4\r\nAbstract\r\nWe introduce Microsoft Machine Learning for\r\nApache Spark (MMLSpark), an open-source li\u0002brary that expands the Apache Spark distributed\r\ncomputing library to tackle problems in deep\r\nlearning, micro-service orchestration, gradient\r\nboosting, model interpretability, and other areas\r\nof modern machine learning. We also present a\r\nnovel machine learning deployment system called\r\nSpark Serving that can deploy Apache Spark pro\u0002grams as distributed, sub-millisecond latency web\r\nservices with significantly greater flexibility and\r\nlower latencies than existing frameworks. Spark\r\nServing generalizes beyond just map-style compu\u0002tations and allows distributed aggregations, joins,\r\nand shuffles and allows users to leverage the\r\nsame cluster for both training and deployment.\r\nOur contributions allow easy composition across\r\nmachine learning frameworks, compute modes\r\n(batch, streaming, and RESTful web serving) and\r\ncluster types (static, elastic, and serverless). We\r\ndemonstrate the value of MMLSpark by creating a\r\nmethod for deep object detection capable of learn\u0002ing without human labeled data and demonstrate\r\nits effectiveness for Snow Leopard conservation.\r\nWe also demonstrate its ability to create large\u0002scale image search engines.\r\n1. Introduction\r\nAs the field of machine learning has advanced, frameworks\r\nfor using, authoring, and training machine learning systems\r\n*Equal contribution 1Microsoft Applied AI, Cambridge, Mas\u0002sachusetts, USA 2Microsoft Applied AI, Redmond, Washing\u0002ton, USA 3Microsoft Azure Machine Learning, Cambridge, Mas\u0002sachusetts, USA 4Microsoft AI Development Acceleration Pro\u0002gram, Cambridge, Massachusetts, USA 5AUTHORERR: Miss\u0002ing \\icmlaffiliation. . Correspondence to: Mark Hamilton\r\n<marhamil@microsoft.com>.\r\nProceedings of the 36 th International Conference on Machine\r\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\r\n2019 by the author(s).\r\nhave proliferated. These different frameworks often have\r\ndramatically different APIs, data models, usage patterns,\r\nand scalability considerations. This heterogeneity makes\r\nit difficult to combine systems and complicates production\r\ndeployments. In this work, we present Microsoft Machine\r\nLearning for Apache Spark (MMLSpark), an ecosystem that\r\naims to unify major machine learning workloads into a sin\u0002gle API for execution in a variety of distributed production\r\ngrade environments and languages. We describe the tech\u0002niques and principles used to unify a representative sample\r\nof machine learning technologies, each with its own soft\u0002ware stack, communication requirements, and paradigms.\r\nWe also introduce tools for deploying these technologies\r\nas distributed real-time web services. Code and documen\u0002tation for MMLSpark can be found through our website,\r\nhttps://aka.ms/spark.\r\n2. Background\r\nThroughout this work we build upon the distributed comput\u0002ing framework Apache Spark (Zaharia et al., 2016). Spark\r\nis capable of a broad range of workloads and applications\r\nsuch as fault-tolerant and distributed map, reduce, filter, and\r\naggregation style programs. Spark improves on its prede\u0002cessors MapReduce and Hadoop by reducing disk IO with\r\nin memory computing, and whole program optimization\r\n(Dean & Ghemawat, 2008; Shvachko et al., 2010). Spark\r\nclusters can adaptively resize to compute a workload effi\u0002ciently (elasticity) and can run on resource managers such\r\nas Yarn, Mesos, Kubernetes, or manually created clusters.\r\nFurthermore, Spark has language bindings in several popu\u0002lar languages like Scala, Java, Python, R, Julia, C# and F#,\r\nmaking it usable from almost any project.\r\nIn recent years, Spark has expanded its scope to support\r\nSQL, streaming, machine learning, and graph style compu\u0002tations (Armbrust et al., 2015; Meng et al., 2016; Xin et al.,\r\n2013). This broad set of APIs allows a rich space of compu\u0002tations that we can leverage for our work. More specifically,\r\nwe build upon the SparkML API, which is similar to the\r\npopular Python machine learning library, scikit-learn (Buit\u0002inck et al., 2013). Like scikit-learn, all SparkML models\r\nhave the same API, which makes it easy to create, substitute,\r\narXiv:1810.08744v2 [cs.LG] 21 Jun 2019",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/398dbce5-dee6-4b3f-b16f-9ee0c41d016f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5364f1ea4514c0ab204e344398ccb07dbc8552b8dda8aa5a05a0d91c0f25bb28",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 634
      },
      {
        "segments": [
          {
            "segment_id": "398dbce5-dee6-4b3f-b16f-9ee0c41d016f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 1,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nMark Hamilton 1 Sudarshan Raghunathan 2Ilya Matiach 3 Andrew Schonhoffer 3 Anand Raman 2\r\nEli Barzilay 1 Karthik Rajendran 4 5 Dalitso Banda 4 5 Casey Jisoo Hong 4 5 Manon Knoertzer 4 5 Ben Brodsky 2\r\nMinsoo Thigpen 4 Janhavi Suresh Mahajan 4 Courtney Cochrane 4 Abhiram Eswaran 4 Ari Green 4\r\nAbstract\r\nWe introduce Microsoft Machine Learning for\r\nApache Spark (MMLSpark), an open-source li\u0002brary that expands the Apache Spark distributed\r\ncomputing library to tackle problems in deep\r\nlearning, micro-service orchestration, gradient\r\nboosting, model interpretability, and other areas\r\nof modern machine learning. We also present a\r\nnovel machine learning deployment system called\r\nSpark Serving that can deploy Apache Spark pro\u0002grams as distributed, sub-millisecond latency web\r\nservices with significantly greater flexibility and\r\nlower latencies than existing frameworks. Spark\r\nServing generalizes beyond just map-style compu\u0002tations and allows distributed aggregations, joins,\r\nand shuffles and allows users to leverage the\r\nsame cluster for both training and deployment.\r\nOur contributions allow easy composition across\r\nmachine learning frameworks, compute modes\r\n(batch, streaming, and RESTful web serving) and\r\ncluster types (static, elastic, and serverless). We\r\ndemonstrate the value of MMLSpark by creating a\r\nmethod for deep object detection capable of learn\u0002ing without human labeled data and demonstrate\r\nits effectiveness for Snow Leopard conservation.\r\nWe also demonstrate its ability to create large\u0002scale image search engines.\r\n1. Introduction\r\nAs the field of machine learning has advanced, frameworks\r\nfor using, authoring, and training machine learning systems\r\n*Equal contribution 1Microsoft Applied AI, Cambridge, Mas\u0002sachusetts, USA 2Microsoft Applied AI, Redmond, Washing\u0002ton, USA 3Microsoft Azure Machine Learning, Cambridge, Mas\u0002sachusetts, USA 4Microsoft AI Development Acceleration Pro\u0002gram, Cambridge, Massachusetts, USA 5AUTHORERR: Miss\u0002ing \\icmlaffiliation. . Correspondence to: Mark Hamilton\r\n<marhamil@microsoft.com>.\r\nProceedings of the 36 th International Conference on Machine\r\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\r\n2019 by the author(s).\r\nhave proliferated. These different frameworks often have\r\ndramatically different APIs, data models, usage patterns,\r\nand scalability considerations. This heterogeneity makes\r\nit difficult to combine systems and complicates production\r\ndeployments. In this work, we present Microsoft Machine\r\nLearning for Apache Spark (MMLSpark), an ecosystem that\r\naims to unify major machine learning workloads into a sin\u0002gle API for execution in a variety of distributed production\r\ngrade environments and languages. We describe the tech\u0002niques and principles used to unify a representative sample\r\nof machine learning technologies, each with its own soft\u0002ware stack, communication requirements, and paradigms.\r\nWe also introduce tools for deploying these technologies\r\nas distributed real-time web services. Code and documen\u0002tation for MMLSpark can be found through our website,\r\nhttps://aka.ms/spark.\r\n2. Background\r\nThroughout this work we build upon the distributed comput\u0002ing framework Apache Spark (Zaharia et al., 2016). Spark\r\nis capable of a broad range of workloads and applications\r\nsuch as fault-tolerant and distributed map, reduce, filter, and\r\naggregation style programs. Spark improves on its prede\u0002cessors MapReduce and Hadoop by reducing disk IO with\r\nin memory computing, and whole program optimization\r\n(Dean & Ghemawat, 2008; Shvachko et al., 2010). Spark\r\nclusters can adaptively resize to compute a workload effi\u0002ciently (elasticity) and can run on resource managers such\r\nas Yarn, Mesos, Kubernetes, or manually created clusters.\r\nFurthermore, Spark has language bindings in several popu\u0002lar languages like Scala, Java, Python, R, Julia, C# and F#,\r\nmaking it usable from almost any project.\r\nIn recent years, Spark has expanded its scope to support\r\nSQL, streaming, machine learning, and graph style compu\u0002tations (Armbrust et al., 2015; Meng et al., 2016; Xin et al.,\r\n2013). This broad set of APIs allows a rich space of compu\u0002tations that we can leverage for our work. More specifically,\r\nwe build upon the SparkML API, which is similar to the\r\npopular Python machine learning library, scikit-learn (Buit\u0002inck et al., 2013). Like scikit-learn, all SparkML models\r\nhave the same API, which makes it easy to create, substitute,\r\narXiv:1810.08744v2 [cs.LG] 21 Jun 2019",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/398dbce5-dee6-4b3f-b16f-9ee0c41d016f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5364f1ea4514c0ab204e344398ccb07dbc8552b8dda8aa5a05a0d91c0f25bb28",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 634
      },
      {
        "segments": [
          {
            "segment_id": "32d951b0-9daa-420d-9fc2-f9b38a664503",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nand compose machine learning algorithms into “pipelines”.\r\nHowever, SparkML has several key advantages such as\r\nlimitless scalability, streaming compatibility, support for\r\nstructured datasets, broad language support, a fluent API\r\nfor initializing complex algorithms, and a type system that\r\ndifferentiates computations based on whether they extract\r\nstate (learn) from data. In addition, Spark clusters can use a\r\nwide variety of hardware SKUs making it possible to lever\u0002age modern advances in GPU accelerated frameworks like\r\nTensorflow, CNTK, and PyTorch (Abadi et al., 2016; Seide\r\n& Agarwal, 2016; Paszke et al., 2017). These properties\r\nmake the SparkML API a natural and principled choice to\r\nunify the APIs of other machine learning frameworks.\r\nAcross the broader computing literature, many have turned\r\nto intermediate languages to “unify” and integrate disparate\r\nforms of computation. One of the most popular of these lan\u0002guages is the Hypertext Transfer Protocol (HTTP) used\r\nwidely throughout internet communications. To enable\r\nbroad adoption and integration of code, one simply needs to\r\ncreate a web-hosted HTTP endpoint or “service”. Putting\r\ncompute behind an intermediate language allows different\r\nsystem components to scale independently to minimize bot\u0002tlenecks. If services reside on the same machine, one can\r\nuse local networking capabilities to bypass internet data\r\ntransfer costs and come closer to the latency of normal func\u0002tion dispatch. This pattern is referred to as a “micro-service”\r\narchitecture, and powers many of today’s large-scale appli\u0002cations (Sill, 2016).\r\nMany machine learning workflows rely on deploying\r\nlearned models as web endpoints for use in front-end appli\u0002cations. In the Spark ecosystem, there are several ways to\r\ndeploy applications as web services such as Azure Machine\r\nLearning Services (AML), Clipper, and MLeap. However,\r\nthese frameworks all compromise on the breadth of models\r\nthey export, or the latency of their deployed services. AML\r\ndeploys PySpark code in a dockerized Flask application that\r\nuses Spark’s Batch API on a single node standalone cluster\r\n(Microsoft, a; Grinberg, 2018). Clipper uses on an interme\u0002diate RPC communication service to invoke a Spark batch\r\njob for each request (Crankshaw et al., 2017). Both method\r\nuse Spark’s Batch API which adds large overheads. Further\u0002more, if back-end containers are isolated, this precludes ser\u0002vices with inter-node communication like shuffles and joins.\r\nMLeap achieves millisecond latencies by re-implementing\r\nSparkML models in single threaded Scala, and exporting\r\nSparkML pipelines to this alternate implementation (Com\u0002bust). This incurs a twofold development cost, a lag behind\r\nthe SparkML library, and a export limitation to models in\r\nSpark’s core, which is a small subset of the ecosystem. In\r\nsection 3.3, we present a novel method, Spark Serving, that\r\nachieves millisecond latencies without compromising the\r\nbreadth or latency of models and does not rely on model\r\nexport. This makes the transition from distributed training\r\nto distributed serving seamless and instant.\r\nMany companies such as Microsoft, Amazon, IBM, and\r\nGoogle have embraced model deployment with web ser\u0002vices to provide pre-built intelligent algorithms for a wide\r\nrange of applications (Jackson et al., 2010; Microsoft, c;\r\nHigh, 2012). This standardization enables easy use of cloud\r\nintelligence and abstracts away implementation details, en\u0002vironment setup, and compute requirements. Furthermore,\r\nintelligent services allow application developers to quickly\r\nuse existing state of the art models to prototype ideas. In the\r\nAzure ecosystem, the Cognitive Services provide intelligent\r\nservices in domains such as text, vision, speech, search,\r\ntime series, and geospatial workloads.\r\n3. Contributions\r\nIn this section we describe our contributions in three key ar\u0002eas: 1) Unifying several Machine Learning ecosystems with\r\nSpark. 2) Integrating Spark with the networking protocol\r\nHTTP and several intelligent web services. 3) Deploying\r\nSpark computations as distributed web services with Spark\r\nServing.\r\nThese contributions allow users to create scalable machine\r\nlearning systems that draw from a wide variety of libraries\r\nand expose these contributions as web services for others\r\nto use. All of these contributions carry the common theme\r\nof building a single distributed API that can easily and\r\nelegantly create a variety of different intelligent applications.\r\nIn Section 4 we show how to combine these contributions\r\nto solve problems in unsupervised object detection, wildlife\r\necology, and visual search engine creation.\r\n3.1. Algorithms and Frameworks Unified in\r\nMMLSpark\r\n3.1.1. DEEP LEARNING\r\nTo enable GPU accelerated deep learning on Spark, we have\r\npreviously parallelized Microsoft’s deep learning frame\u0002work, the Cognitive Toolkit (CNTK) (Seide & Agarwal,\r\n2016; Hamilton et al., 2018). This framework powers\r\nroughly 80% of Microsoft’s internal deep learning work\u0002loads and is flexible enough to create most models described\r\nin the deep learning literature. CNTK is similar to other\r\nautomatic differentiation systems like Tensorflow, PyTorch,\r\nand MxNet as they all create symbolic computation graphs\r\nthat automatically differentiate and compile to machine code.\r\nThese tools liberate developers and researchers from the dif\u0002ficult task of deriving training algorithms and writing GPU\r\naccelerated code.\r\nCNTK’s core functionality is written in C++ but exposed to\r\nC# and Python through bindings. To integrate this frame\u0002work into Spark, we used the Simple Wrapper and Interface",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/32d951b0-9daa-420d-9fc2-f9b38a664503.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=994beb4074e05c9f0ee50cb468c8428c3368e60a0ead000f0b0e395db21c40f8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 816
      },
      {
        "segments": [
          {
            "segment_id": "32d951b0-9daa-420d-9fc2-f9b38a664503",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 2,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nand compose machine learning algorithms into “pipelines”.\r\nHowever, SparkML has several key advantages such as\r\nlimitless scalability, streaming compatibility, support for\r\nstructured datasets, broad language support, a fluent API\r\nfor initializing complex algorithms, and a type system that\r\ndifferentiates computations based on whether they extract\r\nstate (learn) from data. In addition, Spark clusters can use a\r\nwide variety of hardware SKUs making it possible to lever\u0002age modern advances in GPU accelerated frameworks like\r\nTensorflow, CNTK, and PyTorch (Abadi et al., 2016; Seide\r\n& Agarwal, 2016; Paszke et al., 2017). These properties\r\nmake the SparkML API a natural and principled choice to\r\nunify the APIs of other machine learning frameworks.\r\nAcross the broader computing literature, many have turned\r\nto intermediate languages to “unify” and integrate disparate\r\nforms of computation. One of the most popular of these lan\u0002guages is the Hypertext Transfer Protocol (HTTP) used\r\nwidely throughout internet communications. To enable\r\nbroad adoption and integration of code, one simply needs to\r\ncreate a web-hosted HTTP endpoint or “service”. Putting\r\ncompute behind an intermediate language allows different\r\nsystem components to scale independently to minimize bot\u0002tlenecks. If services reside on the same machine, one can\r\nuse local networking capabilities to bypass internet data\r\ntransfer costs and come closer to the latency of normal func\u0002tion dispatch. This pattern is referred to as a “micro-service”\r\narchitecture, and powers many of today’s large-scale appli\u0002cations (Sill, 2016).\r\nMany machine learning workflows rely on deploying\r\nlearned models as web endpoints for use in front-end appli\u0002cations. In the Spark ecosystem, there are several ways to\r\ndeploy applications as web services such as Azure Machine\r\nLearning Services (AML), Clipper, and MLeap. However,\r\nthese frameworks all compromise on the breadth of models\r\nthey export, or the latency of their deployed services. AML\r\ndeploys PySpark code in a dockerized Flask application that\r\nuses Spark’s Batch API on a single node standalone cluster\r\n(Microsoft, a; Grinberg, 2018). Clipper uses on an interme\u0002diate RPC communication service to invoke a Spark batch\r\njob for each request (Crankshaw et al., 2017). Both method\r\nuse Spark’s Batch API which adds large overheads. Further\u0002more, if back-end containers are isolated, this precludes ser\u0002vices with inter-node communication like shuffles and joins.\r\nMLeap achieves millisecond latencies by re-implementing\r\nSparkML models in single threaded Scala, and exporting\r\nSparkML pipelines to this alternate implementation (Com\u0002bust). This incurs a twofold development cost, a lag behind\r\nthe SparkML library, and a export limitation to models in\r\nSpark’s core, which is a small subset of the ecosystem. In\r\nsection 3.3, we present a novel method, Spark Serving, that\r\nachieves millisecond latencies without compromising the\r\nbreadth or latency of models and does not rely on model\r\nexport. This makes the transition from distributed training\r\nto distributed serving seamless and instant.\r\nMany companies such as Microsoft, Amazon, IBM, and\r\nGoogle have embraced model deployment with web ser\u0002vices to provide pre-built intelligent algorithms for a wide\r\nrange of applications (Jackson et al., 2010; Microsoft, c;\r\nHigh, 2012). This standardization enables easy use of cloud\r\nintelligence and abstracts away implementation details, en\u0002vironment setup, and compute requirements. Furthermore,\r\nintelligent services allow application developers to quickly\r\nuse existing state of the art models to prototype ideas. In the\r\nAzure ecosystem, the Cognitive Services provide intelligent\r\nservices in domains such as text, vision, speech, search,\r\ntime series, and geospatial workloads.\r\n3. Contributions\r\nIn this section we describe our contributions in three key ar\u0002eas: 1) Unifying several Machine Learning ecosystems with\r\nSpark. 2) Integrating Spark with the networking protocol\r\nHTTP and several intelligent web services. 3) Deploying\r\nSpark computations as distributed web services with Spark\r\nServing.\r\nThese contributions allow users to create scalable machine\r\nlearning systems that draw from a wide variety of libraries\r\nand expose these contributions as web services for others\r\nto use. All of these contributions carry the common theme\r\nof building a single distributed API that can easily and\r\nelegantly create a variety of different intelligent applications.\r\nIn Section 4 we show how to combine these contributions\r\nto solve problems in unsupervised object detection, wildlife\r\necology, and visual search engine creation.\r\n3.1. Algorithms and Frameworks Unified in\r\nMMLSpark\r\n3.1.1. DEEP LEARNING\r\nTo enable GPU accelerated deep learning on Spark, we have\r\npreviously parallelized Microsoft’s deep learning frame\u0002work, the Cognitive Toolkit (CNTK) (Seide & Agarwal,\r\n2016; Hamilton et al., 2018). This framework powers\r\nroughly 80% of Microsoft’s internal deep learning work\u0002loads and is flexible enough to create most models described\r\nin the deep learning literature. CNTK is similar to other\r\nautomatic differentiation systems like Tensorflow, PyTorch,\r\nand MxNet as they all create symbolic computation graphs\r\nthat automatically differentiate and compile to machine code.\r\nThese tools liberate developers and researchers from the dif\u0002ficult task of deriving training algorithms and writing GPU\r\naccelerated code.\r\nCNTK’s core functionality is written in C++ but exposed to\r\nC# and Python through bindings. To integrate this frame\u0002work into Spark, we used the Simple Wrapper and Interface",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/32d951b0-9daa-420d-9fc2-f9b38a664503.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=994beb4074e05c9f0ee50cb468c8428c3368e60a0ead000f0b0e395db21c40f8",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 816
      },
      {
        "segments": [
          {
            "segment_id": "da6b5bd1-16e2-4bd2-844d-9b8ede0a136f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nGenerator (SWIG) to contribute a set of Java Bindings to\r\nCNTK (Beazley, 1996). These bindings allow users to\r\ncall and train CNTK models from Java, Scala and other\r\nJVM based languages. We used these bindings to create\r\na SparkML transformer to distribute CNTK in Scala. Ad\u0002ditionally we automatically generate PySpark and Spark\u0002lyR bindings for all of MMLSpark’s Scala transformers,\r\nso all contributions in this work are usable across differ\u0002ent languages. To improve our implementation’s perfor\u0002mance we broadcast the model to each worker using Bit\u0002Torrent broadcasting, re-use C++ objects to reduce garbage\r\ncollection overhead, asynchronously mini-batch data, and\r\nshare weights between local threads to reduce memory over\u0002head. With CNTK on Spark, users can embed any deep\r\nnetwork into parallel maps, SQL queries, and streaming\r\npipelines. We also contribute and host a large cloud reposi\u0002tory of trained models and tools to perform image classifi\u0002cation with transfer learning. We have utilized this work for\r\nwildlife recognition, bio-medical entity extraction, and gas\r\nstation fire detection (Hamilton et al., 2018).\r\nWe released our implementation concurrently with\r\nDatabrick’s “Deep Learning Pipelines” that provides an\r\nanalogous integration of Spark and Tensorflow (Databricks).\r\nOur two integrations share the same API making it easy to\r\nuse CNTK and/or Tensorflow inside of SparkML pipelines\r\nwithout code changes.\r\n3.1.2. GRADIENT BOOSTING AND DECISION TREES\r\nThough Tensorflow and CNTK provide GPU enabled deep\r\nnetworks, these frameworks are optimized for differentiable\r\nmodels. To efficiently learn tree/forest-based models, many\r\nturn to GPU enabled gradient boosting libraries such as\r\nXGBoost or LightGBM (Chen & Guestrin, 2016; Ke et al.,\r\n2017). We contribute an integration of LightGBM into\r\nSpark to enable large scale optimized gradient boosting\r\nwithin SparkML pipelines. LightGBM is one of the most\r\nperformant decision tree frameworks and can use socket or\r\nMessage Passing Interface (MPI) communication schemes\r\nthat communicate much more efficiently than SparkML’s\r\nGradient Boosted Tree implementation. As a result, Light\u0002GBM trains up to 30% faster than SparkML’s gradient\r\nboosted tree implementation and exposes many more fea\u0002tures, optimization metrics, and growing/pruning parame\u0002ters. Like CNTK, LightGBM is written in C++ and pub\u0002lishes bindings for use in other languages. For this work,\r\nwe used SWIG to contribute a set of Java bindings to Light\u0002GBM for use in Spark. Unlike our work with CNTK on\r\nSpark, LightGBM training involves nontrivial MPI com\u0002munication between workers. To unify Spark’s API with\r\nLightGBM’s communication scheme, we transfer control to\r\nLightGBM with a Spark “MapPartitions” operation. More\r\nspecifically, we communicate the hostnames of all workers\r\nto the driver node of the Spark cluster and use this informa\u0002tion to launch an MPI ring. The Spark worker processes\r\nuse the Java Native Interface to transfer control and data to\r\nthe LightGBM processes. This integration allows users to\r\ncreate performant models for classification, quantile regres\u0002sion, and other applications that excel in discrete feature\r\ndomains.\r\n3.1.3. MODEL INTERPRETABILITY\r\nIn addition to integrating frameworks into Spark through\r\ntransfers of control, we have also expanded SparkML’s na\u0002tive library of algorithms. One example is our distributed\r\nimplementation of Local Interpretable Model Agnostic Ex\u0002planations (LIME) (Ribeiro et al., 2016). LIME provides\r\na way to “interpret” the predictions of any model without\r\nreference to that model’s functional form. More concretely,\r\nLIME interprets black box functions though a locally lin\u0002ear approximation constructed from a sampling procedure.\r\nLIME, and its generalization SHAP, rely solely on function\r\nevaluations and can and apply to any black box algorithm\r\n(Lundberg & Lee, 2017). We consider other methods such\r\nas DeepLIFT and feature gradients outside the scope of this\r\nwork because SparkML models are not necessarily differen\u0002tiable (Shrikumar et al., 2017).\r\nIntuitively speaking, if “turning off” a part of the model\r\ninput dramatically changes a model’s output, that part is\r\n“important”. More concretely, LIME for image classifiers\r\ncreates thousands of perturbed images by setting random\r\nchunks or “superpixels” of the image to a neutral color. Next,\r\nit feeds each of these perturbed images through the model to\r\nsee how the perturbations affect the model’s output. Finally,\r\nit uses a locally weighted Lasso model to learn a linear\r\nmapping between a Boolean vector representing the “states”\r\nof the superpixels to the model’s outputs. Text and tabular\r\nLIME employ similar featurization schemes, and we refer\r\nreaders to (Ribeiro et al., 2016) for detailed descriptions.\r\nTo interpret a classifier’s predictions for an image, one must\r\nevaluate the classifier on thousands of perturbed images to\r\nsufficiently sample the superpixel state space. Practically\r\nspeaking, if it takes 1 hour to score a model on your dataset,\r\nit would take ≈ 50 days to interpret this dataset with LIME.\r\nWe have created a distributed implementation to reduce this\r\nmassive computation time. LIME affords several possible\r\ndistributed implementations, and we have chosen a paral\u0002lelization scheme that speeds each individual interpretation.\r\nMore specifically, we parallelize the superpixel decompo\u0002sitions over the input images. Next, we iterate through the\r\nsuperpixel decompositions and create a new parallel col\u0002lection of “state samples” for each input image. We then\r\nperturb these images and apply the model in parallel. Fi\u0002nally, we fit a distributed linear model to the inner collection\r\nand add its weights to the original parallel collection. Be\u0002cause of this nontrivial parallelization scheme, this kind of",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/da6b5bd1-16e2-4bd2-844d-9b8ede0a136f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ef0d57fec0f20a56d4f553af990ed1139c38eeee77a1f53f6450ba0a45402f55",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 858
      },
      {
        "segments": [
          {
            "segment_id": "da6b5bd1-16e2-4bd2-844d-9b8ede0a136f",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 3,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nGenerator (SWIG) to contribute a set of Java Bindings to\r\nCNTK (Beazley, 1996). These bindings allow users to\r\ncall and train CNTK models from Java, Scala and other\r\nJVM based languages. We used these bindings to create\r\na SparkML transformer to distribute CNTK in Scala. Ad\u0002ditionally we automatically generate PySpark and Spark\u0002lyR bindings for all of MMLSpark’s Scala transformers,\r\nso all contributions in this work are usable across differ\u0002ent languages. To improve our implementation’s perfor\u0002mance we broadcast the model to each worker using Bit\u0002Torrent broadcasting, re-use C++ objects to reduce garbage\r\ncollection overhead, asynchronously mini-batch data, and\r\nshare weights between local threads to reduce memory over\u0002head. With CNTK on Spark, users can embed any deep\r\nnetwork into parallel maps, SQL queries, and streaming\r\npipelines. We also contribute and host a large cloud reposi\u0002tory of trained models and tools to perform image classifi\u0002cation with transfer learning. We have utilized this work for\r\nwildlife recognition, bio-medical entity extraction, and gas\r\nstation fire detection (Hamilton et al., 2018).\r\nWe released our implementation concurrently with\r\nDatabrick’s “Deep Learning Pipelines” that provides an\r\nanalogous integration of Spark and Tensorflow (Databricks).\r\nOur two integrations share the same API making it easy to\r\nuse CNTK and/or Tensorflow inside of SparkML pipelines\r\nwithout code changes.\r\n3.1.2. GRADIENT BOOSTING AND DECISION TREES\r\nThough Tensorflow and CNTK provide GPU enabled deep\r\nnetworks, these frameworks are optimized for differentiable\r\nmodels. To efficiently learn tree/forest-based models, many\r\nturn to GPU enabled gradient boosting libraries such as\r\nXGBoost or LightGBM (Chen & Guestrin, 2016; Ke et al.,\r\n2017). We contribute an integration of LightGBM into\r\nSpark to enable large scale optimized gradient boosting\r\nwithin SparkML pipelines. LightGBM is one of the most\r\nperformant decision tree frameworks and can use socket or\r\nMessage Passing Interface (MPI) communication schemes\r\nthat communicate much more efficiently than SparkML’s\r\nGradient Boosted Tree implementation. As a result, Light\u0002GBM trains up to 30% faster than SparkML’s gradient\r\nboosted tree implementation and exposes many more fea\u0002tures, optimization metrics, and growing/pruning parame\u0002ters. Like CNTK, LightGBM is written in C++ and pub\u0002lishes bindings for use in other languages. For this work,\r\nwe used SWIG to contribute a set of Java bindings to Light\u0002GBM for use in Spark. Unlike our work with CNTK on\r\nSpark, LightGBM training involves nontrivial MPI com\u0002munication between workers. To unify Spark’s API with\r\nLightGBM’s communication scheme, we transfer control to\r\nLightGBM with a Spark “MapPartitions” operation. More\r\nspecifically, we communicate the hostnames of all workers\r\nto the driver node of the Spark cluster and use this informa\u0002tion to launch an MPI ring. The Spark worker processes\r\nuse the Java Native Interface to transfer control and data to\r\nthe LightGBM processes. This integration allows users to\r\ncreate performant models for classification, quantile regres\u0002sion, and other applications that excel in discrete feature\r\ndomains.\r\n3.1.3. MODEL INTERPRETABILITY\r\nIn addition to integrating frameworks into Spark through\r\ntransfers of control, we have also expanded SparkML’s na\u0002tive library of algorithms. One example is our distributed\r\nimplementation of Local Interpretable Model Agnostic Ex\u0002planations (LIME) (Ribeiro et al., 2016). LIME provides\r\na way to “interpret” the predictions of any model without\r\nreference to that model’s functional form. More concretely,\r\nLIME interprets black box functions though a locally lin\u0002ear approximation constructed from a sampling procedure.\r\nLIME, and its generalization SHAP, rely solely on function\r\nevaluations and can and apply to any black box algorithm\r\n(Lundberg & Lee, 2017). We consider other methods such\r\nas DeepLIFT and feature gradients outside the scope of this\r\nwork because SparkML models are not necessarily differen\u0002tiable (Shrikumar et al., 2017).\r\nIntuitively speaking, if “turning off” a part of the model\r\ninput dramatically changes a model’s output, that part is\r\n“important”. More concretely, LIME for image classifiers\r\ncreates thousands of perturbed images by setting random\r\nchunks or “superpixels” of the image to a neutral color. Next,\r\nit feeds each of these perturbed images through the model to\r\nsee how the perturbations affect the model’s output. Finally,\r\nit uses a locally weighted Lasso model to learn a linear\r\nmapping between a Boolean vector representing the “states”\r\nof the superpixels to the model’s outputs. Text and tabular\r\nLIME employ similar featurization schemes, and we refer\r\nreaders to (Ribeiro et al., 2016) for detailed descriptions.\r\nTo interpret a classifier’s predictions for an image, one must\r\nevaluate the classifier on thousands of perturbed images to\r\nsufficiently sample the superpixel state space. Practically\r\nspeaking, if it takes 1 hour to score a model on your dataset,\r\nit would take ≈ 50 days to interpret this dataset with LIME.\r\nWe have created a distributed implementation to reduce this\r\nmassive computation time. LIME affords several possible\r\ndistributed implementations, and we have chosen a paral\u0002lelization scheme that speeds each individual interpretation.\r\nMore specifically, we parallelize the superpixel decompo\u0002sitions over the input images. Next, we iterate through the\r\nsuperpixel decompositions and create a new parallel col\u0002lection of “state samples” for each input image. We then\r\nperturb these images and apply the model in parallel. Fi\u0002nally, we fit a distributed linear model to the inner collection\r\nand add its weights to the original parallel collection. Be\u0002cause of this nontrivial parallelization scheme, this kind of",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/da6b5bd1-16e2-4bd2-844d-9b8ede0a136f.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=ef0d57fec0f20a56d4f553af990ed1139c38eeee77a1f53f6450ba0a45402f55",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 858
      },
      {
        "segments": [
          {
            "segment_id": "e8a042be-5206-44b4-b368-26c9048bd1d2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nintegration benefited from a complete re-write in fast com\u0002piled Scala and Spark SQL, as opposed to using a tool like\r\nPy4J to integrate the existing LIME repository into Spark.\r\n3.2. Unifying Microservices with Spark\r\nIn Section 3.1 we explored three contributions that unify\r\nSpark with other Machine Learning tools using the Java\r\nNative Interface (JNI) and function dispatch. These meth\u0002ods are efficient, but require re-implementing code in Scala\r\nor auto-generating wrappers from existing code. For many\r\nframeworks, these dispatch-based integrations are impos\u0002sible due to differences in language, operating system, or\r\ncomputational architecture. For these cases, we can utilize\r\ninter-process communication protocols like HTTP to bridge\r\nthe gap between systems.\r\n3.2.1. HTTP ON SPARK\r\nWe present HTTP on Spark, an integration between the en\u0002tire HTTP communication protocol and Spark SQL. HTTP\r\non Spark allows Spark users to leverage the parallel network\u0002ing capabilities of their cluster to integrate any local, docker,\r\nor web service. At a high level, HTTP on Spark provides a\r\nsimple and principled way to integrate any framework into\r\nthe Spark ecosystem. The contribution adds HTTP Request\r\nand Response types to the Spark SQL schema so that users\r\ncan create and manipulate their requests and responses using\r\nSQL operations, maps, reduces, and filters. When combined\r\nwith SparkML, users can chain services together, allowing\r\nSpark to function as a distributed micro-service orchestrator.\r\nHTTP on Spark also automatically provides asynchronous\r\nparallelism, batching, throttling, and exponential back-offs\r\nfor failed requests.\r\n3.2.2. THE COGNITIVE SERVICES ON SPARK\r\nWe have built on HTTP on Spark to create a simple and\r\npowerful integration between the Microsoft Cognitive Ser\u0002vices and Spark. The Cognitive Services on Spark allows\r\nusers to embed general purpose and continuously improv\u0002ing intelligent models directly into their Spark and SQL\r\ncomputations. This contribution aims to liberate users from\r\nlow level networking details, so they can focus on creating\r\nintelligent distributed applications. Each Cognitive Service\r\nis a SparkML transformer, so users can add services to ex\u0002isting SparkML pipelines. We introduce a new class of\r\nmodel parameters to the SparkML framework that allow\r\nusers to parameterize models by either a single scalar value\r\nor vectorize the requests with columns of the DataFrame.\r\nThis syntax yields a succinct yet powerful fluent query lan\u0002guage that offers a full distributed parameterization without\r\nclutter. For example, by vectorizing the “subscription key”\r\nparameter, users can distribute requests across several ac\u0002counts, regions, or deployments to maximize throughput\r\nand resiliency to error.\r\nOnce can combine HTTP on Spark with Kubernetes or\r\nother container orchestrators to deploy services directly onto\r\nSpark worker machines (Rensin, 2015). This enables near\r\nnative integration speeds as requests do not have to travel\r\nacross machines. The cognitive services on Spark can also\r\ncall the newly released containerized cognitive services,\r\nwhich dramatically reduces the latency of cognitive service\r\npipelines. We have contributed a helm chart for deploying\r\na Spark based microservice architecture with containerized\r\ncognitive services, load balancers for Spark Serving, and\r\nintegration with the newly released second generation of\r\nAzure Storage with a single command (Norm Estabrook;\r\nSayfan, 2017). Figure 1 shows a diagram of the aforemen\u0002tioned architecture.\r\nFigure 1. Architecture diagram of our integration of cloud and\r\ncontainerized Cognitive Services. Architecture depicted on Kuber\u0002netes, but any container orchestrator could deploy the same. Load\r\nbalancers expose deployed Spark Serving models, job submission\r\nendpoints, and monitoring frontends. This helm chart can also\r\nleverage our integration between Azure Search and Spark. Note\r\nthat we omit Kubernetes and Spark head nodes for simplicity.\r\n3.2.3. AZURE SEARCH SINK FOR SPARK\r\nWe demonstrate the flexibility and robustness of the HTTP\r\non Spark framework by contributing an integration between\r\nSpark and Azure Search. Azure Search is a cloud database\r\nthat supports rapid information retrieval and query execution\r\non heterogeneous, unstructured data (Microsoft, b). Azure\r\nSearch leverages elastic search to index documents and pro\u0002vide REST APIs for document search on linguistic similarity\r\nand a variety of other filters and logical constraints. With\r\nthis integration, users can leverage the frameworks men\u0002tioned in 3.1 to enrich their documents in parallel prior to\r\nindexing.\r\n3.3. Spark Serving: Scalable Real-Time Web Serving\r\nThrough HTTP on Spark, we have enabled Spark as a dis\u0002tributed web client. In this work we also contribute Spark",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/e8a042be-5206-44b4-b368-26c9048bd1d2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5b76d3aa44f3084a639feece1f6ce130dff406c5b4e3678f1dcea10dc8cdf479",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 701
      },
      {
        "segments": [
          {
            "segment_id": "e8a042be-5206-44b4-b368-26c9048bd1d2",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 4,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nintegration benefited from a complete re-write in fast com\u0002piled Scala and Spark SQL, as opposed to using a tool like\r\nPy4J to integrate the existing LIME repository into Spark.\r\n3.2. Unifying Microservices with Spark\r\nIn Section 3.1 we explored three contributions that unify\r\nSpark with other Machine Learning tools using the Java\r\nNative Interface (JNI) and function dispatch. These meth\u0002ods are efficient, but require re-implementing code in Scala\r\nor auto-generating wrappers from existing code. For many\r\nframeworks, these dispatch-based integrations are impos\u0002sible due to differences in language, operating system, or\r\ncomputational architecture. For these cases, we can utilize\r\ninter-process communication protocols like HTTP to bridge\r\nthe gap between systems.\r\n3.2.1. HTTP ON SPARK\r\nWe present HTTP on Spark, an integration between the en\u0002tire HTTP communication protocol and Spark SQL. HTTP\r\non Spark allows Spark users to leverage the parallel network\u0002ing capabilities of their cluster to integrate any local, docker,\r\nor web service. At a high level, HTTP on Spark provides a\r\nsimple and principled way to integrate any framework into\r\nthe Spark ecosystem. The contribution adds HTTP Request\r\nand Response types to the Spark SQL schema so that users\r\ncan create and manipulate their requests and responses using\r\nSQL operations, maps, reduces, and filters. When combined\r\nwith SparkML, users can chain services together, allowing\r\nSpark to function as a distributed micro-service orchestrator.\r\nHTTP on Spark also automatically provides asynchronous\r\nparallelism, batching, throttling, and exponential back-offs\r\nfor failed requests.\r\n3.2.2. THE COGNITIVE SERVICES ON SPARK\r\nWe have built on HTTP on Spark to create a simple and\r\npowerful integration between the Microsoft Cognitive Ser\u0002vices and Spark. The Cognitive Services on Spark allows\r\nusers to embed general purpose and continuously improv\u0002ing intelligent models directly into their Spark and SQL\r\ncomputations. This contribution aims to liberate users from\r\nlow level networking details, so they can focus on creating\r\nintelligent distributed applications. Each Cognitive Service\r\nis a SparkML transformer, so users can add services to ex\u0002isting SparkML pipelines. We introduce a new class of\r\nmodel parameters to the SparkML framework that allow\r\nusers to parameterize models by either a single scalar value\r\nor vectorize the requests with columns of the DataFrame.\r\nThis syntax yields a succinct yet powerful fluent query lan\u0002guage that offers a full distributed parameterization without\r\nclutter. For example, by vectorizing the “subscription key”\r\nparameter, users can distribute requests across several ac\u0002counts, regions, or deployments to maximize throughput\r\nand resiliency to error.\r\nOnce can combine HTTP on Spark with Kubernetes or\r\nother container orchestrators to deploy services directly onto\r\nSpark worker machines (Rensin, 2015). This enables near\r\nnative integration speeds as requests do not have to travel\r\nacross machines. The cognitive services on Spark can also\r\ncall the newly released containerized cognitive services,\r\nwhich dramatically reduces the latency of cognitive service\r\npipelines. We have contributed a helm chart for deploying\r\na Spark based microservice architecture with containerized\r\ncognitive services, load balancers for Spark Serving, and\r\nintegration with the newly released second generation of\r\nAzure Storage with a single command (Norm Estabrook;\r\nSayfan, 2017). Figure 1 shows a diagram of the aforemen\u0002tioned architecture.\r\nFigure 1. Architecture diagram of our integration of cloud and\r\ncontainerized Cognitive Services. Architecture depicted on Kuber\u0002netes, but any container orchestrator could deploy the same. Load\r\nbalancers expose deployed Spark Serving models, job submission\r\nendpoints, and monitoring frontends. This helm chart can also\r\nleverage our integration between Azure Search and Spark. Note\r\nthat we omit Kubernetes and Spark head nodes for simplicity.\r\n3.2.3. AZURE SEARCH SINK FOR SPARK\r\nWe demonstrate the flexibility and robustness of the HTTP\r\non Spark framework by contributing an integration between\r\nSpark and Azure Search. Azure Search is a cloud database\r\nthat supports rapid information retrieval and query execution\r\non heterogeneous, unstructured data (Microsoft, b). Azure\r\nSearch leverages elastic search to index documents and pro\u0002vide REST APIs for document search on linguistic similarity\r\nand a variety of other filters and logical constraints. With\r\nthis integration, users can leverage the frameworks men\u0002tioned in 3.1 to enrich their documents in parallel prior to\r\nindexing.\r\n3.3. Spark Serving: Scalable Real-Time Web Serving\r\nThrough HTTP on Spark, we have enabled Spark as a dis\u0002tributed web client. In this work we also contribute Spark",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/e8a042be-5206-44b4-b368-26c9048bd1d2.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=5b76d3aa44f3084a639feece1f6ce130dff406c5b4e3678f1dcea10dc8cdf479",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 701
      },
      {
        "segments": [
          {
            "segment_id": "5acb1503-f8dc-4d53-b41c-5e542a9166fd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nServing, a framework that allows Spark clusters to oper\u0002ate as distributed web servers. Spark Serving builds upon\r\nSpark’s Structured Streaming library that transforms ex\u0002isting Spark SQL computations into continuously running\r\nstreaming queries. Structured Streaming supports a large\r\nmajority of Spark primitives including maps, filters, aggre\u0002gations, and joins. To convert a batch query to a streaming\r\nquery, users only need to change a single line of dataset\r\nreading/writing code and can keep all other computational\r\nlogic in place. We extend this easy to use API to web serv\u0002ing by creating novel paired sources and sinks that manage a\r\nservice layer. Intuitively speaking, a web service is a stream\u0002ing pipeline where the data source and the data sink are the\r\nsame HTTP request.\r\nSpark Serving can deploy any Spark computation as a web\r\nservice including all of our contributions (CNTK, Light\u0002GBM, SparkML, Cognitive Services, HTTP Services), arbi\u0002trary Python, R, Scala, Java, and all compositions and com\u0002binations therein. Through other open source contributions\r\nin the Spark ecosystem, frameworks such as Tensorflow,\r\nXGBoost, and Scikit-learn models join this list.\r\n3.3.1. IMPLEMENTATION AND ARCHITECTURE\r\nUnder the hood, each Spark worker/executor manages a web\r\nservice that en-queues incoming data in an efficient parallel\r\ndata structure that supports constant time routing, addition,\r\ndeletion, and load balancing across the multiple threads.\r\nEach worker node manages a public service for accepting\r\nincoming requests, and an internal routing service to send\r\nresponse data to the originating request (on a potentially\r\ndifferent node after a shuffle). The worker services commu\u0002nicate their locations and statuses to a monitor service on\r\nthe driver node. This lets future developers create hooks for\r\ntheir own load balancers, and lets users understand the state\r\nand locations of their servers.\r\nIf one uses an external load balancer, each request is routed\r\ndirectly to a worker, skipping the costly hop from head\r\nnode to worker node that is common in other frameworks.\r\nThe worker converts each request to our Spark SQL type\r\nfor an HTTP Request (The same types used in HTTP on\r\nSpark), and a unique routing address to ensure the reply\r\ncan route to the originating request. Once the data is con\u0002verted to Spark’s SQL representation it flows through the\r\ncomputational pipeline like any other Spark data.\r\nTo reply to incoming requests, Spark Serving leverages a\r\nnew data sink that uses routing IDs and SQL objects for\r\nHTTP Responses to reply to the stored request. If the re\u0002sponse ends up on a different machine then the originating\r\nrequest, a routing service sends the request to the appropri\u0002ate machine through an internal network. If the request and\r\nresponse are on the same machine, Spark Serving uses faster\r\nfunction dispatch to reply. In the future, we hope to explore\r\nwhether the routing service could leverage Spark’s under\u0002lying shuffle capabilities to speed data transfer throughput.\r\nOur choice of HTTP Request and Response Data types en\u0002ables users to work with the entire breadth of the HTTP\r\nprotocol for full generality and customizability. Figure 2\r\ndepicts a schematic overview of the architecture of Spark\r\nServing, and the hotpath during request and response time.\r\nFigure 2. Overview of Spark Serving architecture during request\r\ntime (a) and response time (b). Requests are routed to each worker\r\nthrough a load balancer where they can flow through the pipeline\r\nwhen partitions are formed. At reply time, workers use paired\r\nID rows to route information to the correct machines that can\r\nreply to the open request. If the request resides on the same\r\nmachine, Spark Serving uses function dispatch instead of inter\u0002process communication\r\nAs of MMLSpark v0.14, we have integrated Spark Serving\r\nwith Spark’s new Continuous Processing feature. Continu\u0002ous Processing dramatically reduces the latency of stream\u0002ing pipelines from ≈ 100ms to 1ms. This acceleration\r\nenables real-time web services and machine learning appli\u0002cations. Minibatch processing can still be used to maximize\r\nthroughput, and MMLSpark also provides a batching API\r\nfor use in continuous processing.\r\nTo the authors’ knowledge, Spark Serving is the only serving\r\nframework that leverages an existing Spark cluster’s work\u0002ers to serve models. As a result, developers do not need to\r\nre-implement and export their models into other languages,\r\nsuch as MLeap, to create web services (Combust). Fur\u0002thermore, frameworks that require additional run-times add",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/5acb1503-f8dc-4d53-b41c-5e542a9166fd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=255832e3df7fc133ace21c1aad2d691a8c0db4e4b514c13060785e02064aa464",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 707
      },
      {
        "segments": [
          {
            "segment_id": "5acb1503-f8dc-4d53-b41c-5e542a9166fd",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 5,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nServing, a framework that allows Spark clusters to oper\u0002ate as distributed web servers. Spark Serving builds upon\r\nSpark’s Structured Streaming library that transforms ex\u0002isting Spark SQL computations into continuously running\r\nstreaming queries. Structured Streaming supports a large\r\nmajority of Spark primitives including maps, filters, aggre\u0002gations, and joins. To convert a batch query to a streaming\r\nquery, users only need to change a single line of dataset\r\nreading/writing code and can keep all other computational\r\nlogic in place. We extend this easy to use API to web serv\u0002ing by creating novel paired sources and sinks that manage a\r\nservice layer. Intuitively speaking, a web service is a stream\u0002ing pipeline where the data source and the data sink are the\r\nsame HTTP request.\r\nSpark Serving can deploy any Spark computation as a web\r\nservice including all of our contributions (CNTK, Light\u0002GBM, SparkML, Cognitive Services, HTTP Services), arbi\u0002trary Python, R, Scala, Java, and all compositions and com\u0002binations therein. Through other open source contributions\r\nin the Spark ecosystem, frameworks such as Tensorflow,\r\nXGBoost, and Scikit-learn models join this list.\r\n3.3.1. IMPLEMENTATION AND ARCHITECTURE\r\nUnder the hood, each Spark worker/executor manages a web\r\nservice that en-queues incoming data in an efficient parallel\r\ndata structure that supports constant time routing, addition,\r\ndeletion, and load balancing across the multiple threads.\r\nEach worker node manages a public service for accepting\r\nincoming requests, and an internal routing service to send\r\nresponse data to the originating request (on a potentially\r\ndifferent node after a shuffle). The worker services commu\u0002nicate their locations and statuses to a monitor service on\r\nthe driver node. This lets future developers create hooks for\r\ntheir own load balancers, and lets users understand the state\r\nand locations of their servers.\r\nIf one uses an external load balancer, each request is routed\r\ndirectly to a worker, skipping the costly hop from head\r\nnode to worker node that is common in other frameworks.\r\nThe worker converts each request to our Spark SQL type\r\nfor an HTTP Request (The same types used in HTTP on\r\nSpark), and a unique routing address to ensure the reply\r\ncan route to the originating request. Once the data is con\u0002verted to Spark’s SQL representation it flows through the\r\ncomputational pipeline like any other Spark data.\r\nTo reply to incoming requests, Spark Serving leverages a\r\nnew data sink that uses routing IDs and SQL objects for\r\nHTTP Responses to reply to the stored request. If the re\u0002sponse ends up on a different machine then the originating\r\nrequest, a routing service sends the request to the appropri\u0002ate machine through an internal network. If the request and\r\nresponse are on the same machine, Spark Serving uses faster\r\nfunction dispatch to reply. In the future, we hope to explore\r\nwhether the routing service could leverage Spark’s under\u0002lying shuffle capabilities to speed data transfer throughput.\r\nOur choice of HTTP Request and Response Data types en\u0002ables users to work with the entire breadth of the HTTP\r\nprotocol for full generality and customizability. Figure 2\r\ndepicts a schematic overview of the architecture of Spark\r\nServing, and the hotpath during request and response time.\r\nFigure 2. Overview of Spark Serving architecture during request\r\ntime (a) and response time (b). Requests are routed to each worker\r\nthrough a load balancer where they can flow through the pipeline\r\nwhen partitions are formed. At reply time, workers use paired\r\nID rows to route information to the correct machines that can\r\nreply to the open request. If the request resides on the same\r\nmachine, Spark Serving uses function dispatch instead of inter\u0002process communication\r\nAs of MMLSpark v0.14, we have integrated Spark Serving\r\nwith Spark’s new Continuous Processing feature. Continu\u0002ous Processing dramatically reduces the latency of stream\u0002ing pipelines from ≈ 100ms to 1ms. This acceleration\r\nenables real-time web services and machine learning appli\u0002cations. Minibatch processing can still be used to maximize\r\nthroughput, and MMLSpark also provides a batching API\r\nfor use in continuous processing.\r\nTo the authors’ knowledge, Spark Serving is the only serving\r\nframework that leverages an existing Spark cluster’s work\u0002ers to serve models. As a result, developers do not need to\r\nre-implement and export their models into other languages,\r\nsuch as MLeap, to create web services (Combust). Fur\u0002thermore, frameworks that require additional run-times add",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/5acb1503-f8dc-4d53-b41c-5e542a9166fd.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=255832e3df7fc133ace21c1aad2d691a8c0db4e4b514c13060785e02064aa464",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 707
      },
      {
        "segments": [
          {
            "segment_id": "724508ce-acac-40f5-a9be-f598c38f33cf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nTable 1. Latencies of various Spark deployment methods in mil\u0002liseconds. We compare Azure Machine Learning Services (AML),\r\nClipper, and MLeap against Spark Serving with local services on\r\nan azure virtual machine for reliability. We explore the latency\r\nwith a pipeline of string indexing, one hot encoding and logistic\r\nregression (LR), and a SQLTransformer select statement (SQL).\r\nNote that MLeap cannot export SQLTransformer SparkML mod\u0002els.\r\nMETHOD LR SELECT\r\nAML 530.3 ± 32.1 179.5 ± 19.5\r\nCLIPPER 626.8 ± 332.1 403.6 ± 280.0\r\nMLEAP 3.18 ± 1.84 N/A\r\nSPARK SERVING 2.13 ± .96 1.81 ± .73\r\nTable 2. Comparison of features across different Spark Deploy\u0002ment systems\r\nFEATURE AML CLIPPER MLEAP SPARK\r\nSERVING\r\nANY\r\nSPARKML\r\n√ √ ×√\r\nANY\r\nSPARK\r\n(NARROW)\r\n√ √ ×√\r\nJOINS × × × √\r\nAGGREGATES × × × √\r\nSHUFFLES × × × √\r\nMILLISECOND\r\nLATENCY\r\n× × √ √\r\ncomplexity and incur costs from additional deployment sys\u0002tems. Other serving frameworks, such as Clipper and Azure\r\nMachine Learning Services (AML), rely on using Spark’s\r\nbatch processing API to evaluate data. This approach results\r\nin large latencies, as each call re-builds the computation\r\ngraph on the head node, generates code for the workers on\r\nthe head node, sends this generated code (and all data in the\r\nclosures) to the workers, sends all the request data to the\r\nworkers, and collects the response data from the workers.\r\nBecause we leverage Structured Streaming and continuous\r\nprocessing, all code generation and graph building occurs\r\nonly in initialization, and the hotpath of the request stays\r\nclear of unnecessary computations. Table 1 demonstrates\r\nour improvements in latency over other systems in the liter\u0002ature. Furthermore Spark Serving inherits desire-able fault\r\ntolerance behavior, management APIs, and ability to handle\r\nshuffles and joins from Spark Streaming. This flexibility\r\nmakes Spark Serving significantly more general than other\r\nframeworks whose workers cannot communicate with each\r\nother and do not have access to continuously updated dis\u0002tributed tables. Table 2 shows a comparison of functionality\r\nacross different serving frameworks.\r\n4. Applications\r\nWe have used MMLSpark to power engagements in a wide\r\nvariety of machine learning domains, such as text, image,\r\nand speech domains. In this section, we highlight the\r\naforementioned contributions in our ongoing work to use\r\nMMLSpark for wildlife conservation and custom search\r\nengine creation.\r\n4.1. Snow Leopard Conservation\r\nSnow leopards are dwindling due to poaching, mining, and\r\nretribution killing yet we know little about how to best\r\nprotect them. Currently, researchers estimate that there\r\nare only about four thousand to seven thousand individual\r\nanimals within a potential 2 million square kilometer range\r\nof rugged central Asian mountains (Taubmann et al., 2016).\r\nOur collaborators, the Snow Leopard Trust, have deployed\r\nhundreds of motion sensitive cameras across large areas\r\nof Snow Leopard territory to help monitor this population\r\n(Taubmann et al., 2016). Over the years, these cameras have\r\nproduced over 1 million images, but most of these images\r\nare of goats, sheep, foxes, or other moving objects and\r\nmanual sorting takes thousands of hours. Using tools from\r\nthe MMLSpark ecosystem, we have created an automated\r\nsystem to classify and localize Snow Leopards in camera\r\ntrap images without human labeled data. This method saves\r\nthe Trust hours of labor and provides data to help identify\r\nindividual leopards by their spot patterns.\r\nFigure 3. End to end architecture of unsupervised object detection\r\non Spark\r\n4.1.1. UNSUPERVISED CLASSIFICATION\r\nIn our previous work, we used deep transfer learning with\r\nCNTK on Spark to create a system that could classify Snow",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/724508ce-acac-40f5-a9be-f598c38f33cf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e4011f520863c2a81667a042969763740f01d4db03208f9a207d0a747c8a711c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 583
      },
      {
        "segments": [
          {
            "segment_id": "724508ce-acac-40f5-a9be-f598c38f33cf",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 6,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nTable 1. Latencies of various Spark deployment methods in mil\u0002liseconds. We compare Azure Machine Learning Services (AML),\r\nClipper, and MLeap against Spark Serving with local services on\r\nan azure virtual machine for reliability. We explore the latency\r\nwith a pipeline of string indexing, one hot encoding and logistic\r\nregression (LR), and a SQLTransformer select statement (SQL).\r\nNote that MLeap cannot export SQLTransformer SparkML mod\u0002els.\r\nMETHOD LR SELECT\r\nAML 530.3 ± 32.1 179.5 ± 19.5\r\nCLIPPER 626.8 ± 332.1 403.6 ± 280.0\r\nMLEAP 3.18 ± 1.84 N/A\r\nSPARK SERVING 2.13 ± .96 1.81 ± .73\r\nTable 2. Comparison of features across different Spark Deploy\u0002ment systems\r\nFEATURE AML CLIPPER MLEAP SPARK\r\nSERVING\r\nANY\r\nSPARKML\r\n√ √ ×√\r\nANY\r\nSPARK\r\n(NARROW)\r\n√ √ ×√\r\nJOINS × × × √\r\nAGGREGATES × × × √\r\nSHUFFLES × × × √\r\nMILLISECOND\r\nLATENCY\r\n× × √ √\r\ncomplexity and incur costs from additional deployment sys\u0002tems. Other serving frameworks, such as Clipper and Azure\r\nMachine Learning Services (AML), rely on using Spark’s\r\nbatch processing API to evaluate data. This approach results\r\nin large latencies, as each call re-builds the computation\r\ngraph on the head node, generates code for the workers on\r\nthe head node, sends this generated code (and all data in the\r\nclosures) to the workers, sends all the request data to the\r\nworkers, and collects the response data from the workers.\r\nBecause we leverage Structured Streaming and continuous\r\nprocessing, all code generation and graph building occurs\r\nonly in initialization, and the hotpath of the request stays\r\nclear of unnecessary computations. Table 1 demonstrates\r\nour improvements in latency over other systems in the liter\u0002ature. Furthermore Spark Serving inherits desire-able fault\r\ntolerance behavior, management APIs, and ability to handle\r\nshuffles and joins from Spark Streaming. This flexibility\r\nmakes Spark Serving significantly more general than other\r\nframeworks whose workers cannot communicate with each\r\nother and do not have access to continuously updated dis\u0002tributed tables. Table 2 shows a comparison of functionality\r\nacross different serving frameworks.\r\n4. Applications\r\nWe have used MMLSpark to power engagements in a wide\r\nvariety of machine learning domains, such as text, image,\r\nand speech domains. In this section, we highlight the\r\naforementioned contributions in our ongoing work to use\r\nMMLSpark for wildlife conservation and custom search\r\nengine creation.\r\n4.1. Snow Leopard Conservation\r\nSnow leopards are dwindling due to poaching, mining, and\r\nretribution killing yet we know little about how to best\r\nprotect them. Currently, researchers estimate that there\r\nare only about four thousand to seven thousand individual\r\nanimals within a potential 2 million square kilometer range\r\nof rugged central Asian mountains (Taubmann et al., 2016).\r\nOur collaborators, the Snow Leopard Trust, have deployed\r\nhundreds of motion sensitive cameras across large areas\r\nof Snow Leopard territory to help monitor this population\r\n(Taubmann et al., 2016). Over the years, these cameras have\r\nproduced over 1 million images, but most of these images\r\nare of goats, sheep, foxes, or other moving objects and\r\nmanual sorting takes thousands of hours. Using tools from\r\nthe MMLSpark ecosystem, we have created an automated\r\nsystem to classify and localize Snow Leopards in camera\r\ntrap images without human labeled data. This method saves\r\nthe Trust hours of labor and provides data to help identify\r\nindividual leopards by their spot patterns.\r\nFigure 3. End to end architecture of unsupervised object detection\r\non Spark\r\n4.1.1. UNSUPERVISED CLASSIFICATION\r\nIn our previous work, we used deep transfer learning with\r\nCNTK on Spark to create a system that could classify Snow",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/724508ce-acac-40f5-a9be-f598c38f33cf.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e4011f520863c2a81667a042969763740f01d4db03208f9a207d0a747c8a711c",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 583
      },
      {
        "segments": [
          {
            "segment_id": "934ef51f-38e7-4138-8062-8e8872b93f35",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nFigure 4. Human labeled images (top left) and unsupervised Faster\u0002RCNN outputs (top right). Difficult human labeled images (bottom\r\nleft) and unsupervised Faster-RCNN outputs (bottom right).\r\nLeopards in Camera trap images (Hamilton et al., 2018).\r\nThis work leveraged a large dataset of manually labeled\r\nimages accumulated through years of intensive labelling\r\nby the Snow Leopard Trust. In this work, we show that\r\nwe can avoid all dependence on human labels by using\r\nBing Image Search to automatically curate a labeled Snow\r\nLeopard dataset. More specifically, we used our SparkML\r\nbindings for Bing Image Search to make this process easy\r\nand scalable. To create a binary classification dataset, we\r\nfirst create a dataset of leopards by pulling the first 80 pages\r\nof the results for the “Snow Leopard” query. To create\r\na dataset of negative images, we drew inspiration from\r\nNoise Contrastive Estimation, a mathematical technique\r\nused frequently in the Word Embedding literature (Gut\u0002mann & Hyvarinen ¨ , 2010). More specifically, we generated\r\na large and diverse dataset of random images, by using ran\u0002dom queries as a surrogate for random image sampling. We\r\nused an existing online random word generator to create a\r\ndataframe of thousands of random queries. We used Bing\r\nImages on Spark to pull the first 10 images for each ran\u0002dom query. After generating two datasets, we used Spark\r\nSQL to add labels, stitch them together, drop duplicates, and\r\nTable 3. Object detector performance (mAP@[.5:.95]) evaluated\r\non human curated test images\r\nTRAINING METHOD MAP\r\nUNSUPERVISED + PRE-TRAINING 49.8\r\nHUMAN 30.9\r\nHUMAN + PRE-TRAINING 79.3\r\nTable 4. Deep classifier performance on synthetic data, human\r\nlabelled data, and a logistic regression model on human data\r\nALGORITHM ACCURACY\r\nUNSUPERVISED 77.6%\r\nHUMAN 86.8%\r\nHUMAN + LR 65.6%\r\ndownload the images to the cluster in only a few minutes.\r\nNext, we used CNTK on Spark to train a deep classification\r\nnetwork using transfer learning on our automatically gener\u0002ated dataset. Though we illustrated this process with Snow\r\nLeopard classification, the method applies to any domain\r\nindexed by Bing Images.\r\n4.1.2. UNSUPERVISED OBJECT DETECTION\r\nMany animal identification systems, such as HotSpotter,\r\nrequire more than just classification probabilities to identify\r\nindividual animals by their patterns (Crall et al., 2013). In\r\nthis work, we introduce a refinement method capable of\r\nextracting a deep object detector from any image classifier.\r\nWhen combined with our unsupervised dataset generation\r\ntechnique in Section 4.1.1, we can create a custom object\r\ndetector for any object found on Bing Image Search. This\r\nmethod leverages our LIME on Spark contribution to “inter\u0002pret” our trained leopard classifier. These classifier interpre\u0002tations often directly highlight leopard pixels, allowing us to\r\nrefine our input dataset with localization information. How\u0002ever, this refinement operation incurs the 1000x computa\u0002tion cost associated with LIME, making even the distributed\r\nversion untenable for real-time applications. However, we\r\ncan use this localized dataset to train a real-time object de\u0002tection algorithm like Faster-RCNN (Girshick, 2015). We\r\ntrain Faster-RCNN to quickly reproduce the computation\u0002ally expensive LIME outputs, and this network serves as\r\na fast leopard localization algorithm that does not require\r\nhuman labels at any step of the training process. Because\r\nLIME is a model agnostic interpretation engine, this refine\u0002ment technique can apply to any image classification from\r\nany domain. Figure 3 shows a diagram of the end-to-end\r\narchitecture.\r\n4.1.3. RESULTS FOR UNSUPERVISED LEOPARD\r\nDETECTION\r\nWe discovered that our completely unsupervised object de\u0002tector closely matched human drawn bounding boxes on",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/934ef51f-38e7-4138-8062-8e8872b93f35.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0d8341f23d330ea19415e31ce1934f9ea3d0c05a6f08f40424ca9dfd132df4d9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 570
      },
      {
        "segments": [
          {
            "segment_id": "934ef51f-38e7-4138-8062-8e8872b93f35",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 7,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nFigure 4. Human labeled images (top left) and unsupervised Faster\u0002RCNN outputs (top right). Difficult human labeled images (bottom\r\nleft) and unsupervised Faster-RCNN outputs (bottom right).\r\nLeopards in Camera trap images (Hamilton et al., 2018).\r\nThis work leveraged a large dataset of manually labeled\r\nimages accumulated through years of intensive labelling\r\nby the Snow Leopard Trust. In this work, we show that\r\nwe can avoid all dependence on human labels by using\r\nBing Image Search to automatically curate a labeled Snow\r\nLeopard dataset. More specifically, we used our SparkML\r\nbindings for Bing Image Search to make this process easy\r\nand scalable. To create a binary classification dataset, we\r\nfirst create a dataset of leopards by pulling the first 80 pages\r\nof the results for the “Snow Leopard” query. To create\r\na dataset of negative images, we drew inspiration from\r\nNoise Contrastive Estimation, a mathematical technique\r\nused frequently in the Word Embedding literature (Gut\u0002mann & Hyvarinen ¨ , 2010). More specifically, we generated\r\na large and diverse dataset of random images, by using ran\u0002dom queries as a surrogate for random image sampling. We\r\nused an existing online random word generator to create a\r\ndataframe of thousands of random queries. We used Bing\r\nImages on Spark to pull the first 10 images for each ran\u0002dom query. After generating two datasets, we used Spark\r\nSQL to add labels, stitch them together, drop duplicates, and\r\nTable 3. Object detector performance (mAP@[.5:.95]) evaluated\r\non human curated test images\r\nTRAINING METHOD MAP\r\nUNSUPERVISED + PRE-TRAINING 49.8\r\nHUMAN 30.9\r\nHUMAN + PRE-TRAINING 79.3\r\nTable 4. Deep classifier performance on synthetic data, human\r\nlabelled data, and a logistic regression model on human data\r\nALGORITHM ACCURACY\r\nUNSUPERVISED 77.6%\r\nHUMAN 86.8%\r\nHUMAN + LR 65.6%\r\ndownload the images to the cluster in only a few minutes.\r\nNext, we used CNTK on Spark to train a deep classification\r\nnetwork using transfer learning on our automatically gener\u0002ated dataset. Though we illustrated this process with Snow\r\nLeopard classification, the method applies to any domain\r\nindexed by Bing Images.\r\n4.1.2. UNSUPERVISED OBJECT DETECTION\r\nMany animal identification systems, such as HotSpotter,\r\nrequire more than just classification probabilities to identify\r\nindividual animals by their patterns (Crall et al., 2013). In\r\nthis work, we introduce a refinement method capable of\r\nextracting a deep object detector from any image classifier.\r\nWhen combined with our unsupervised dataset generation\r\ntechnique in Section 4.1.1, we can create a custom object\r\ndetector for any object found on Bing Image Search. This\r\nmethod leverages our LIME on Spark contribution to “inter\u0002pret” our trained leopard classifier. These classifier interpre\u0002tations often directly highlight leopard pixels, allowing us to\r\nrefine our input dataset with localization information. How\u0002ever, this refinement operation incurs the 1000x computa\u0002tion cost associated with LIME, making even the distributed\r\nversion untenable for real-time applications. However, we\r\ncan use this localized dataset to train a real-time object de\u0002tection algorithm like Faster-RCNN (Girshick, 2015). We\r\ntrain Faster-RCNN to quickly reproduce the computation\u0002ally expensive LIME outputs, and this network serves as\r\na fast leopard localization algorithm that does not require\r\nhuman labels at any step of the training process. Because\r\nLIME is a model agnostic interpretation engine, this refine\u0002ment technique can apply to any image classification from\r\nany domain. Figure 3 shows a diagram of the end-to-end\r\narchitecture.\r\n4.1.3. RESULTS FOR UNSUPERVISED LEOPARD\r\nDETECTION\r\nWe discovered that our completely unsupervised object de\u0002tector closely matched human drawn bounding boxes on",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/934ef51f-38e7-4138-8062-8e8872b93f35.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0d8341f23d330ea19415e31ce1934f9ea3d0c05a6f08f40424ca9dfd132df4d9",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 570
      },
      {
        "segments": [
          {
            "segment_id": "74fff71f-741c-4b0e-bb2a-4776dc40597b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nmost images. Table 3 and 4 show that our method can ap\u0002proach that of a classifiers and object detectors trained on\r\nhuman labelled images. However, certain types of images\r\nposed problems for our method. Our network tended to\r\nonly highlight the visually dominant leopards in images\r\nwith more than one leopard, such as those in Figure 4. We\r\nhypothesize that this arises from our simple method of con\u0002verting LIME outputs to bounding boxes. Because we only\r\ndraw a single box around highlighted pixels, our algorithm\r\nhas only seen examples with a single bounding box. In the\r\nfuture, we plan to cluster LIME pixels to identify images\r\nwith bi-modal interpretations. Furthermore, the method also\r\nmissed several camouflaged leopards, as in Figure 4. We\r\nhypothesize that this is an anthropic effect, as Bing only re\u0002turns clear images of leopards. We plan to explore this effect\r\nby combining this Bing generated data with active learn\u0002ing on a “real” dataset to help humans target the toughest\r\nexamples quickly.\r\nFigure 5. Overview of distributed image analysis pipeline that\r\nleverages containerized microservices on Spark, deep-learning\r\ncomputation graphs, nontrivial joins for KNN and locally sensitive\r\nhashing, and distributed index creation with Azure Search.\r\n4.2. Visual Search Engine Creation\r\nWhen many different frameworks unify within the same API\r\nand ecosystem, it becomes possible to create high-quality\r\ndistributed applications with very few lines of code. We\r\ndemonstrate this by using MMLSpark and its surrounding\r\necosystem to create a visual search engine. As is shown\r\nin Figure 5, we use MMLSpark’s binary reading extension\r\nto ingest raw files with high throughput using all nodes\r\nof the cluster. We can then pump these images through\r\na variety of Computer Vision services with MMLSpark’s\r\ncognitive service integration. These services add image\r\ncontent, descriptions, faces, celebrities, and other useful in\u0002telligence to the dataframe of images. We can then featurize\r\nimages with headless networks such as ResNet50 or Incep\u0002tionV3 with either CNTK on Spark (ours), or Tensorflow\r\non Spark (Databrick’s). We can then pass these high-level\r\nfeatures through SparkML’s locally sensitive hashing imple\u0002mentation, K-means clustering, or a third-party K-nearest\r\nneighbor SparkML package (Liu et al., 2007). We can then\r\ncreate and write the dataframe to an Azure Search index\r\nin a distributed fashion in a single line with MMLSpark’s\r\nAzure Search integration. The resulting index can be quickly\r\nqueried for fuzzy matches on image information, content,\r\nand visually similarity to other images.\r\n5. Conclusion\r\nIn this work we have introduced Microsoft Machine Learn\u0002ing for Apache Spark, a framework that integrates a wide\r\nvariety of computing technologies into a single distributed\r\nAPI. We have contributed CNTK, LightGBM, and LIME on\r\nSpark, and have added a foundational integration between\r\nSpark and the HTTP Protocol. We built on this to integrate\r\nthe Microsoft Cognitive Services with Spark and create a\r\nnovel real-time serving framework for Spark models. We\r\nhave also shown that through combining these technologies,\r\none can quickly create and deploy intelligent applications.\r\nOur first application used Bing, CNTK, LIME, and Spark\r\nServing to create a deep Snow Leopard detector that did not\r\nrely on costly human labeled data. This application made\r\nno assumptions regarding the application domain and could\r\nextract custom object detectors for anything searchable on\r\nBing Images. Our second application leveraged container\u0002ized cognitive services, HTTP on Spark, CNTK, Tensorflow,\r\nand other tools from the SparkML ecosystem to create an\r\nintelligent image search engine. The contributions we have\r\nprovided in MMLSpark allow users to draw from and com\u0002bine a wide variety machine learning frameworks in only a\r\nfew lines of Spark code. This work dramatically expands\r\nthe Spark framework into several new areas of modern com\u0002puting and has helped us rapidly create several distributed\r\nmachine learning applications.\r\n6. Future Work\r\nWe hope to continue adding computation frameworks and\r\naim to extend the techniques of HTTP on Spark to other\r\nprotocols like GRPC. We are continuously expanding our\r\ncollection of HTTP-based Spark models, and aim to add ser\u0002vices from Google, IBM, and AWS in addition to Microsoft.\r\nWe also hope to explore Spark Clusters of accelerated hard-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/74fff71f-741c-4b0e-bb2a-4776dc40597b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b03d120dac59d0fc8a6768a67203ac3780a2ca6fd1b0ba1142ac28ab819745a0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 680
      },
      {
        "segments": [
          {
            "segment_id": "74fff71f-741c-4b0e-bb2a-4776dc40597b",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 8,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nmost images. Table 3 and 4 show that our method can ap\u0002proach that of a classifiers and object detectors trained on\r\nhuman labelled images. However, certain types of images\r\nposed problems for our method. Our network tended to\r\nonly highlight the visually dominant leopards in images\r\nwith more than one leopard, such as those in Figure 4. We\r\nhypothesize that this arises from our simple method of con\u0002verting LIME outputs to bounding boxes. Because we only\r\ndraw a single box around highlighted pixels, our algorithm\r\nhas only seen examples with a single bounding box. In the\r\nfuture, we plan to cluster LIME pixels to identify images\r\nwith bi-modal interpretations. Furthermore, the method also\r\nmissed several camouflaged leopards, as in Figure 4. We\r\nhypothesize that this is an anthropic effect, as Bing only re\u0002turns clear images of leopards. We plan to explore this effect\r\nby combining this Bing generated data with active learn\u0002ing on a “real” dataset to help humans target the toughest\r\nexamples quickly.\r\nFigure 5. Overview of distributed image analysis pipeline that\r\nleverages containerized microservices on Spark, deep-learning\r\ncomputation graphs, nontrivial joins for KNN and locally sensitive\r\nhashing, and distributed index creation with Azure Search.\r\n4.2. Visual Search Engine Creation\r\nWhen many different frameworks unify within the same API\r\nand ecosystem, it becomes possible to create high-quality\r\ndistributed applications with very few lines of code. We\r\ndemonstrate this by using MMLSpark and its surrounding\r\necosystem to create a visual search engine. As is shown\r\nin Figure 5, we use MMLSpark’s binary reading extension\r\nto ingest raw files with high throughput using all nodes\r\nof the cluster. We can then pump these images through\r\na variety of Computer Vision services with MMLSpark’s\r\ncognitive service integration. These services add image\r\ncontent, descriptions, faces, celebrities, and other useful in\u0002telligence to the dataframe of images. We can then featurize\r\nimages with headless networks such as ResNet50 or Incep\u0002tionV3 with either CNTK on Spark (ours), or Tensorflow\r\non Spark (Databrick’s). We can then pass these high-level\r\nfeatures through SparkML’s locally sensitive hashing imple\u0002mentation, K-means clustering, or a third-party K-nearest\r\nneighbor SparkML package (Liu et al., 2007). We can then\r\ncreate and write the dataframe to an Azure Search index\r\nin a distributed fashion in a single line with MMLSpark’s\r\nAzure Search integration. The resulting index can be quickly\r\nqueried for fuzzy matches on image information, content,\r\nand visually similarity to other images.\r\n5. Conclusion\r\nIn this work we have introduced Microsoft Machine Learn\u0002ing for Apache Spark, a framework that integrates a wide\r\nvariety of computing technologies into a single distributed\r\nAPI. We have contributed CNTK, LightGBM, and LIME on\r\nSpark, and have added a foundational integration between\r\nSpark and the HTTP Protocol. We built on this to integrate\r\nthe Microsoft Cognitive Services with Spark and create a\r\nnovel real-time serving framework for Spark models. We\r\nhave also shown that through combining these technologies,\r\none can quickly create and deploy intelligent applications.\r\nOur first application used Bing, CNTK, LIME, and Spark\r\nServing to create a deep Snow Leopard detector that did not\r\nrely on costly human labeled data. This application made\r\nno assumptions regarding the application domain and could\r\nextract custom object detectors for anything searchable on\r\nBing Images. Our second application leveraged container\u0002ized cognitive services, HTTP on Spark, CNTK, Tensorflow,\r\nand other tools from the SparkML ecosystem to create an\r\nintelligent image search engine. The contributions we have\r\nprovided in MMLSpark allow users to draw from and com\u0002bine a wide variety machine learning frameworks in only a\r\nfew lines of Spark code. This work dramatically expands\r\nthe Spark framework into several new areas of modern com\u0002puting and has helped us rapidly create several distributed\r\nmachine learning applications.\r\n6. Future Work\r\nWe hope to continue adding computation frameworks and\r\naim to extend the techniques of HTTP on Spark to other\r\nprotocols like GRPC. We are continuously expanding our\r\ncollection of HTTP-based Spark models, and aim to add ser\u0002vices from Google, IBM, and AWS in addition to Microsoft.\r\nWe also hope to explore Spark Clusters of accelerated hard-",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/74fff71f-741c-4b0e-bb2a-4776dc40597b.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=b03d120dac59d0fc8a6768a67203ac3780a2ca6fd1b0ba1142ac28ab819745a0",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 680
      },
      {
        "segments": [
          {
            "segment_id": "bdf8c3fc-1449-44f1-9dc2-7d9bbdfe7acc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nware SKUs like cloud TPUs and FPGAs to accelerate com\u0002putations. Additionally, we aim to leverage advancements in\r\nthe Spark Ecosystem such as Barrier Execution to improve\r\nthe fault tolerance of LightGBM (Xin). Additionally, we\r\nhope to integrate Spark Serving as a deployment flavor for\r\nthe popular machine learning management tool, MLFlow\r\n(Zaharia et al., 2018).\r\nAcknowledgements\r\nWe would like to acknowledge the generous support from\r\nour collaborators, Dr. Koustubh Sharma, Rhetick Sengupta,\r\nMichael Despines, and the rest of the Snow Leopard Trust.\r\nWe would also like to acknowledge those at Microsoft who\r\nhelped fund and share this work: the Microsoft AI for Earth\r\nProgram, Lucas Joppa, Joseph Sirosh, Pablo Castro, Brian\r\nSmith, Arvind Krishnaa Jagannathan, and Wee Hyong Tok.\r\nReferences\r\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\r\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\r\nTensorflow: a system for large-scale machine learning.\r\nIn OSDI, volume 16, pp. 265–283, 2016.\r\nArmbrust, M., Xin, R. S., Lian, C., Huai, Y., Liu, D.,\r\nBradley, J. K., Meng, X., Kaftan, T., Franklin, M. J.,\r\nGhodsi, A., and Zaharia, M. Spark sql: Relational\r\ndata processing in spark. In Proceedings of the 2015\r\nACM SIGMOD International Conference on Manage\u0002ment of Data, SIGMOD ’15, pp. 1383–1394, New York,\r\nNY, USA, 2015. ACM. ISBN 978-1-4503-2758-9. doi:\r\n10.1145/2723372.2742797. URL http://doi.acm.\r\norg/10.1145/2723372.2742797.\r\nBeazley, D. M. Swig: An easy to use tool for integrating\r\nscripting languages with c and c++. In Proceedings of\r\nthe 4th Conference on USENIX Tcl/Tk Workshop, 1996\r\n- Volume 4, TCLTK’96, pp. 15–15, Berkeley, CA, USA,\r\n1996. USENIX Association. URL http://dl.acm.\r\norg/citation.cfm?id=1267498.1267513.\r\nBuitinck, L., Louppe, G., Blondel, M., Pedregosa, F.,\r\nMueller, A., Grisel, O., Niculae, V., Prettenhofer, P.,\r\nGramfort, A., Grobler, J., Layton, R., VanderPlas, J.,\r\nJoly, A., Holt, B., and Varoquaux, G. API design for ma\u0002chine learning software: experiences from the scikit-learn\r\nproject. In ECML PKDD Workshop: Languages for Data\r\nMining and Machine Learning, pp. 108–122, 2013.\r\nChen, T. and Guestrin, C. Xgboost: A scalable tree boosting\r\nsystem. In Proceedings of the 22nd acm sigkdd inter\u0002national conference on knowledge discovery and data\r\nmining, pp. 785–794. ACM, 2016.\r\nCombust, I. MLeap. http://mleap-docs.combust.\r\nml/.\r\nCrall, J. P., Stewart, C. V., Berger-Wolf, T. Y., Rubenstein,\r\nD. I., and Sundaresan, S. R. Hotspotterpatterned species\r\ninstance recognition. In Applications of Computer Vision\r\n(WACV), 2013 IEEE Workshop on, pp. 230–237. IEEE,\r\n2013.\r\nCrankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gon\u0002zalez, J. E., and Stoica, I. Clipper: A low-latency online\r\nprediction serving system. In NSDI, pp. 613–627, 2017.\r\nDatabricks. Deep learning pipelines for apache\r\nspark. https://github.com/databricks/\r\nspark-deep-learning. Accessed: 2019-01-20.\r\nDean, J. and Ghemawat, S. Mapreduce: simplified data\r\nprocessing on large clusters. Communications of the\r\nACM, 51(1):107–113, 2008.\r\nGirshick, R. Fast r-cnn. In Proceedings of the IEEE inter\u0002national conference on computer vision, pp. 1440–1448,\r\n2015.\r\nGrinberg, M. Flask web development: developing web\r\napplications with python. ” O’Reilly Media, Inc.”, 2018.\r\nGutmann, M. and Hyvarinen, A. Noise-contrastive esti- ¨\r\nmation: A new estimation principle for unnormalized\r\nstatistical models. In Proceedings of the Thirteenth Inter\u0002national Conference on Artificial Intelligence and Statis\u0002tics, pp. 297–304, 2010.\r\nHamilton, M., Raghunathan, S., Annavajhala, A., Kirsanov,\r\nD., Leon, E., Barzilay, E., Matiach, I., Davison, J.,\r\nBusch, M., Oprescu, M., Sur, R., Astala, R., Wen, T.,\r\nand Park, C. Flexible and scalable deep learning with\r\nmmlspark. In Hardgrove, C., Dorard, L., and Thomp\u0002son, K. (eds.), Proceedings of The 4th International\r\nConference on Predictive Applications and APIs, vol\u0002ume 82 of Proceedings of Machine Learning Research,\r\npp. 11–22, Microsoft NERD, Boston, USA, 24–25 Oct\r\n2018. PMLR. URL http://proceedings.mlr.\r\npress/v82/hamilton18a.html.\r\nHigh, R. The era of cognitive systems: An inside look at ibm\r\nwatson and how it works. IBM Corporation, Redbooks,\r\n2012.\r\nJackson, K. R., Ramakrishnan, L., Muriki, K., Canon, S.,\r\nCholia, S., Shalf, J., Wasserman, H. J., and Wright, N. J.\r\nPerformance analysis of high performance computing ap\u0002plications on the amazon web services cloud. In 2nd IEEE\r\ninternational conference on cloud computing technology\r\nand science, pp. 159–168. IEEE, 2010.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/bdf8c3fc-1449-44f1-9dc2-7d9bbdfe7acc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4bef2653c038f584cb37c2e720ce0fbcb9aa8210850436228298e09a935d97dc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 661
      },
      {
        "segments": [
          {
            "segment_id": "bdf8c3fc-1449-44f1-9dc2-7d9bbdfe7acc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 9,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nware SKUs like cloud TPUs and FPGAs to accelerate com\u0002putations. Additionally, we aim to leverage advancements in\r\nthe Spark Ecosystem such as Barrier Execution to improve\r\nthe fault tolerance of LightGBM (Xin). Additionally, we\r\nhope to integrate Spark Serving as a deployment flavor for\r\nthe popular machine learning management tool, MLFlow\r\n(Zaharia et al., 2018).\r\nAcknowledgements\r\nWe would like to acknowledge the generous support from\r\nour collaborators, Dr. Koustubh Sharma, Rhetick Sengupta,\r\nMichael Despines, and the rest of the Snow Leopard Trust.\r\nWe would also like to acknowledge those at Microsoft who\r\nhelped fund and share this work: the Microsoft AI for Earth\r\nProgram, Lucas Joppa, Joseph Sirosh, Pablo Castro, Brian\r\nSmith, Arvind Krishnaa Jagannathan, and Wee Hyong Tok.\r\nReferences\r\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\r\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\r\nTensorflow: a system for large-scale machine learning.\r\nIn OSDI, volume 16, pp. 265–283, 2016.\r\nArmbrust, M., Xin, R. S., Lian, C., Huai, Y., Liu, D.,\r\nBradley, J. K., Meng, X., Kaftan, T., Franklin, M. J.,\r\nGhodsi, A., and Zaharia, M. Spark sql: Relational\r\ndata processing in spark. In Proceedings of the 2015\r\nACM SIGMOD International Conference on Manage\u0002ment of Data, SIGMOD ’15, pp. 1383–1394, New York,\r\nNY, USA, 2015. ACM. ISBN 978-1-4503-2758-9. doi:\r\n10.1145/2723372.2742797. URL http://doi.acm.\r\norg/10.1145/2723372.2742797.\r\nBeazley, D. M. Swig: An easy to use tool for integrating\r\nscripting languages with c and c++. In Proceedings of\r\nthe 4th Conference on USENIX Tcl/Tk Workshop, 1996\r\n- Volume 4, TCLTK’96, pp. 15–15, Berkeley, CA, USA,\r\n1996. USENIX Association. URL http://dl.acm.\r\norg/citation.cfm?id=1267498.1267513.\r\nBuitinck, L., Louppe, G., Blondel, M., Pedregosa, F.,\r\nMueller, A., Grisel, O., Niculae, V., Prettenhofer, P.,\r\nGramfort, A., Grobler, J., Layton, R., VanderPlas, J.,\r\nJoly, A., Holt, B., and Varoquaux, G. API design for ma\u0002chine learning software: experiences from the scikit-learn\r\nproject. In ECML PKDD Workshop: Languages for Data\r\nMining and Machine Learning, pp. 108–122, 2013.\r\nChen, T. and Guestrin, C. Xgboost: A scalable tree boosting\r\nsystem. In Proceedings of the 22nd acm sigkdd inter\u0002national conference on knowledge discovery and data\r\nmining, pp. 785–794. ACM, 2016.\r\nCombust, I. MLeap. http://mleap-docs.combust.\r\nml/.\r\nCrall, J. P., Stewart, C. V., Berger-Wolf, T. Y., Rubenstein,\r\nD. I., and Sundaresan, S. R. Hotspotterpatterned species\r\ninstance recognition. In Applications of Computer Vision\r\n(WACV), 2013 IEEE Workshop on, pp. 230–237. IEEE,\r\n2013.\r\nCrankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gon\u0002zalez, J. E., and Stoica, I. Clipper: A low-latency online\r\nprediction serving system. In NSDI, pp. 613–627, 2017.\r\nDatabricks. Deep learning pipelines for apache\r\nspark. https://github.com/databricks/\r\nspark-deep-learning. Accessed: 2019-01-20.\r\nDean, J. and Ghemawat, S. Mapreduce: simplified data\r\nprocessing on large clusters. Communications of the\r\nACM, 51(1):107–113, 2008.\r\nGirshick, R. Fast r-cnn. In Proceedings of the IEEE inter\u0002national conference on computer vision, pp. 1440–1448,\r\n2015.\r\nGrinberg, M. Flask web development: developing web\r\napplications with python. ” O’Reilly Media, Inc.”, 2018.\r\nGutmann, M. and Hyvarinen, A. Noise-contrastive esti- ¨\r\nmation: A new estimation principle for unnormalized\r\nstatistical models. In Proceedings of the Thirteenth Inter\u0002national Conference on Artificial Intelligence and Statis\u0002tics, pp. 297–304, 2010.\r\nHamilton, M., Raghunathan, S., Annavajhala, A., Kirsanov,\r\nD., Leon, E., Barzilay, E., Matiach, I., Davison, J.,\r\nBusch, M., Oprescu, M., Sur, R., Astala, R., Wen, T.,\r\nand Park, C. Flexible and scalable deep learning with\r\nmmlspark. In Hardgrove, C., Dorard, L., and Thomp\u0002son, K. (eds.), Proceedings of The 4th International\r\nConference on Predictive Applications and APIs, vol\u0002ume 82 of Proceedings of Machine Learning Research,\r\npp. 11–22, Microsoft NERD, Boston, USA, 24–25 Oct\r\n2018. PMLR. URL http://proceedings.mlr.\r\npress/v82/hamilton18a.html.\r\nHigh, R. The era of cognitive systems: An inside look at ibm\r\nwatson and how it works. IBM Corporation, Redbooks,\r\n2012.\r\nJackson, K. R., Ramakrishnan, L., Muriki, K., Canon, S.,\r\nCholia, S., Shalf, J., Wasserman, H. J., and Wright, N. J.\r\nPerformance analysis of high performance computing ap\u0002plications on the amazon web services cloud. In 2nd IEEE\r\ninternational conference on cloud computing technology\r\nand science, pp. 159–168. IEEE, 2010.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/bdf8c3fc-1449-44f1-9dc2-7d9bbdfe7acc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=4bef2653c038f584cb37c2e720ce0fbcb9aa8210850436228298e09a935d97dc",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 661
      },
      {
        "segments": [
          {
            "segment_id": "2d2ce734-5b73-44d0-975b-d94bcb6b1ddc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nKe, G., Meng, Q., Finely, T., Wang, T., Chen, W., Ma,\r\nW., Ye, Q., and Liu, T.-Y. Lightgbm: A highly efficient\r\ngradient boosting decision tree. In Advances in Neural\r\nInformation Processing Systems 30, December 2017.\r\nLiu, T., Rosenberg, C., and Rowley, H. A. Clustering bil\u0002lions of images with large scale nearest neighbor search.\r\nIn null, pp. 28. IEEE, 2007.\r\nLundberg, S. M. and Lee, S.-I. A unified approach to in\u0002terpreting model predictions. In Advances in Neural\r\nInformation Processing Systems, pp. 4765–4774, 2017.\r\nMeng, X., Bradley, J., Yavuz, B., Sparks, E., Venkataraman,\r\nS., Liu, D., Freeman, J., Tsai, D., Amde, M., Owen,\r\nS., et al. Mllib: Machine learning in apache spark. The\r\nJournal of Machine Learning Research, 17(1):1235–1241,\r\n2016.\r\nMicrosoft. Azure machine learning service documenta\u0002tion. https://docs.microsoft.com/en-us/\r\nazure/machine-learning/service/, a. Ac\u0002cessed: 2019-01-20.\r\nMicrosoft. Azure search. https://azure.\r\nmicrosoft.com/en-us/services/search/,\r\nb. Accessed: 2019-01-20.\r\nMicrosoft. Cognitive services. https://\r\nazure.microsoft.com/en-us/services/\r\ncognitive-services/, c. Accessed: 2019-01-20.\r\nNorm Estabrook, James Baker, T. M. T. W.\r\nN. S. Introduction to azure data lake storage\r\ngen2 preview. https://docs.microsoft.\r\ncom/en-us/azure/storage/blobs/\r\ndata-lake-storage-introduction. Ac\u0002cessed: 2019-01-20.\r\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\r\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\r\nA. Automatic differentiation in pytorch. 2017.\r\nRensin, D. K. Kubernetes - Scheduling the Future at Cloud\r\nScale. 1005 Gravenstein Highway North Sebastopol, CA\r\n95472, 2015. URL http://www.oreilly.com/\r\nwebops-perf/free/kubernetes.csp.\r\nRibeiro, M. T., Singh, S., and Guestrin, C. ”why should i\r\ntrust you?”: Explaining the predictions of any classifier.\r\nIn Proceedings of the 22Nd ACM SIGKDD International\r\nConference on Knowledge Discovery and Data Mining,\r\nKDD ’16, pp. 1135–1144, New York, NY, USA, 2016.\r\nACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.\r\n2939778. URL http://doi.acm.org/10.1145/\r\n2939672.2939778.\r\nSayfan, G. Mastering Kubernetes. Packt Publishing Ltd,\r\n2017.\r\nSeide, F. and Agarwal, A. Cntk: Microsoft’s open-source\r\ndeep-learning toolkit. In Proceedings of the 22nd ACM\r\nSIGKDD International Conference on Knowledge Dis\u0002covery and Data Mining, pp. 2135–2135. ACM, 2016.\r\nShrikumar, A., Greenside, P., and Kundaje, A. Learning\r\nimportant features through propagating activation differ\u0002ences, 2017.\r\nShvachko, K., Kuang, H., Radia, S., and Chansler, R. The\r\nhadoop distributed file system. In Mass storage systems\r\nand technologies (MSST), 2010 IEEE 26th symposium\r\non, pp. 1–10. Ieee, 2010.\r\nSill, A. The design and architecture of microservices. IEEE\r\nCloud Computing, 3(5):76–80, 2016.\r\nTaubmann, J., Sharma, K., Uulu, K. Z., Hines, J. E., and\r\nMishra, C. Status assessment of the endangered snow\r\nleopard panthera uncia and other large mammals in the\r\nkyrgyz alay, using community knowledge corrected for\r\nimperfect detection. Oryx, 50(2):220230, 2016. doi:\r\n10.1017/S0030605315000502.\r\nXin, R. Project hydrogen: Unifying state-of-the-art ai and\r\nbig data in apache spark. https://databricks.\r\ncom/session/databricks-keynote-2. Ac\u0002cessed: 2019-01-20.\r\nXin, R. S., Gonzalez, J. E., Franklin, M. J., and Stoica, I.\r\nGraphx: A resilient distributed graph system on spark.\r\nIn First International Workshop on Graph Data Manage\u0002ment Experiences and Systems, pp. 2. ACM, 2013.\r\nZaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust,\r\nM., Dave, A., Meng, X., Rosen, J., Venkataraman, S.,\r\nFranklin, M. J., Ghodsi, A., Gonzalez, J., Shenker, S.,\r\nand Stoica, I. Apache spark: A unified engine for big\r\ndata processing. Commun. ACM, 59(11):56–65, October\r\n2016. ISSN 0001-0782. doi: 10.1145/2934664. URL\r\nhttp://doi.acm.org/10.1145/2934664.\r\nZaharia, M., Chen, A., Davidson, A., Ghodsi, A., Hong,\r\nS. A., Konwinski, A., Murching, S., Nykodym, T.,\r\nOgilvie, P., Parkhe, M., et al. Accelerating the machine\r\nlearning lifecycle with mlflow. Data Engineering, pp. 39,\r\n2018.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/2d2ce734-5b73-44d0-975b-d94bcb6b1ddc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3e2d687f3a4fe3dc1aafeddc99f9f29641adc160a54e4a04abe15ccf52c3a50e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 552
      },
      {
        "segments": [
          {
            "segment_id": "2d2ce734-5b73-44d0-975b-d94bcb6b1ddc",
            "bbox": {
              "left": 0,
              "top": 0,
              "width": 612,
              "height": 792
            },
            "page_number": 10,
            "page_width": 612,
            "page_height": 792,
            "content": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\r\nKe, G., Meng, Q., Finely, T., Wang, T., Chen, W., Ma,\r\nW., Ye, Q., and Liu, T.-Y. Lightgbm: A highly efficient\r\ngradient boosting decision tree. In Advances in Neural\r\nInformation Processing Systems 30, December 2017.\r\nLiu, T., Rosenberg, C., and Rowley, H. A. Clustering bil\u0002lions of images with large scale nearest neighbor search.\r\nIn null, pp. 28. IEEE, 2007.\r\nLundberg, S. M. and Lee, S.-I. A unified approach to in\u0002terpreting model predictions. In Advances in Neural\r\nInformation Processing Systems, pp. 4765–4774, 2017.\r\nMeng, X., Bradley, J., Yavuz, B., Sparks, E., Venkataraman,\r\nS., Liu, D., Freeman, J., Tsai, D., Amde, M., Owen,\r\nS., et al. Mllib: Machine learning in apache spark. The\r\nJournal of Machine Learning Research, 17(1):1235–1241,\r\n2016.\r\nMicrosoft. Azure machine learning service documenta\u0002tion. https://docs.microsoft.com/en-us/\r\nazure/machine-learning/service/, a. Ac\u0002cessed: 2019-01-20.\r\nMicrosoft. Azure search. https://azure.\r\nmicrosoft.com/en-us/services/search/,\r\nb. Accessed: 2019-01-20.\r\nMicrosoft. Cognitive services. https://\r\nazure.microsoft.com/en-us/services/\r\ncognitive-services/, c. Accessed: 2019-01-20.\r\nNorm Estabrook, James Baker, T. M. T. W.\r\nN. S. Introduction to azure data lake storage\r\ngen2 preview. https://docs.microsoft.\r\ncom/en-us/azure/storage/blobs/\r\ndata-lake-storage-introduction. Ac\u0002cessed: 2019-01-20.\r\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\r\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\r\nA. Automatic differentiation in pytorch. 2017.\r\nRensin, D. K. Kubernetes - Scheduling the Future at Cloud\r\nScale. 1005 Gravenstein Highway North Sebastopol, CA\r\n95472, 2015. URL http://www.oreilly.com/\r\nwebops-perf/free/kubernetes.csp.\r\nRibeiro, M. T., Singh, S., and Guestrin, C. ”why should i\r\ntrust you?”: Explaining the predictions of any classifier.\r\nIn Proceedings of the 22Nd ACM SIGKDD International\r\nConference on Knowledge Discovery and Data Mining,\r\nKDD ’16, pp. 1135–1144, New York, NY, USA, 2016.\r\nACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.\r\n2939778. URL http://doi.acm.org/10.1145/\r\n2939672.2939778.\r\nSayfan, G. Mastering Kubernetes. Packt Publishing Ltd,\r\n2017.\r\nSeide, F. and Agarwal, A. Cntk: Microsoft’s open-source\r\ndeep-learning toolkit. In Proceedings of the 22nd ACM\r\nSIGKDD International Conference on Knowledge Dis\u0002covery and Data Mining, pp. 2135–2135. ACM, 2016.\r\nShrikumar, A., Greenside, P., and Kundaje, A. Learning\r\nimportant features through propagating activation differ\u0002ences, 2017.\r\nShvachko, K., Kuang, H., Radia, S., and Chansler, R. The\r\nhadoop distributed file system. In Mass storage systems\r\nand technologies (MSST), 2010 IEEE 26th symposium\r\non, pp. 1–10. Ieee, 2010.\r\nSill, A. The design and architecture of microservices. IEEE\r\nCloud Computing, 3(5):76–80, 2016.\r\nTaubmann, J., Sharma, K., Uulu, K. Z., Hines, J. E., and\r\nMishra, C. Status assessment of the endangered snow\r\nleopard panthera uncia and other large mammals in the\r\nkyrgyz alay, using community knowledge corrected for\r\nimperfect detection. Oryx, 50(2):220230, 2016. doi:\r\n10.1017/S0030605315000502.\r\nXin, R. Project hydrogen: Unifying state-of-the-art ai and\r\nbig data in apache spark. https://databricks.\r\ncom/session/databricks-keynote-2. Ac\u0002cessed: 2019-01-20.\r\nXin, R. S., Gonzalez, J. E., Franklin, M. J., and Stoica, I.\r\nGraphx: A resilient distributed graph system on spark.\r\nIn First International Workshop on Graph Data Manage\u0002ment Experiences and Systems, pp. 2. ACM, 2013.\r\nZaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust,\r\nM., Dave, A., Meng, X., Rosen, J., Venkataraman, S.,\r\nFranklin, M. J., Ghodsi, A., Gonzalez, J., Shenker, S.,\r\nand Stoica, I. Apache spark: A unified engine for big\r\ndata processing. Commun. ACM, 59(11):56–65, October\r\n2016. ISSN 0001-0782. doi: 10.1145/2934664. URL\r\nhttp://doi.acm.org/10.1145/2934664.\r\nZaharia, M., Chen, A., Davidson, A., Ghodsi, A., Hong,\r\nS. A., Konwinski, A., Murching, S., Nykodym, T.,\r\nOgilvie, P., Parkhe, M., et al. Accelerating the machine\r\nlearning lifecycle with mlflow. Data Engineering, pp. 39,\r\n2018.",
            "segment_type": "Page",
            "ocr": null,
            "image": "https://storage.googleapis.com/chunkr-bucket-dev/ccf89702-e38e-4089-bd6b-77e3ba63c341/403a9cbe-7a70-4381-b8e3-dfcbbf02e77f/images/2d2ce734-5b73-44d0-975b-d94bcb6b1ddc.jpg?x-id=GetObject&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=GOOG1E6ZKWCYPX4LV42MGE7WJ66QU2EMDPF3DJ2IFHNTQIGHNC2STOGTWF75E%2F20241112%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20241112T041629Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=3e2d687f3a4fe3dc1aafeddc99f9f29641adc160a54e4a04abe15ccf52c3a50e",
            "html": null,
            "markdown": null
          }
        ],
        "chunk_length": 552
      }
    ],
    "extracted_json": {
      "title": "Document Metadata",
      "schema_type": "object",
      "extracted_fields": [
        {
          "name": "title",
          "field_type": "string",
          "value": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales\n"
        },
        {
          "name": "author",
          "field_type": "string",
          "value": "Mark Hamilton, Sudarshan Raghunathan, Ilya Matiach, Andrew Schonhoffer, Anand Raman, Eli Barzilay, Karthik Rajendran, Dalitso Banda, Casey Jisoo Hong, Manon Knoertzer, Ben Brodsky, Minsoo Thigpen, Janhavi Suresh Mahajan, Courtney Cochrane, Abhiram Eswaran, Ari Green\n"
        },
        {
          "name": "date_published",
          "field_type": "string",
          "value": "2019\n"
        },
        {
          "name": "location",
          "field_type": "string",
          "value": "Cambridge, Massachusetts, USA\nRedmond, Washington, USA\nBoston, USA\nNew York, NY, USA\nBerkeley, CA, USA\nkyrgyz alay"
        }
      ]
    }
  }
}