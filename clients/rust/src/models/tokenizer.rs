/*
 * Chunkr API
 *
 * API service for document layout analysis and chunking to convert document into RAG/LLM-ready data.
 *
 * The version of the OpenAPI document: 1.0.0
 * Contact: ishaan@lumina.sh
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// Tokenizer : Common tokenizers used for text processing.  These values represent standard tokenization approaches and popular pre-trained tokenizers from the Hugging Face ecosystem.
/// Common tokenizers used for text processing.  These values represent standard tokenization approaches and popular pre-trained tokenizers from the Hugging Face ecosystem.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum Tokenizer {
    #[serde(rename = "Word")]
    Word,
    #[serde(rename = "Cl100kBase")]
    Cl100kBase,
    #[serde(rename = "XlmRobertaBase")]
    XlmRobertaBase,
    #[serde(rename = "BertBaseUncased")]
    BertBaseUncased,

}

impl std::fmt::Display for Tokenizer {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            Self::Word => write!(f, "Word"),
            Self::Cl100kBase => write!(f, "Cl100kBase"),
            Self::XlmRobertaBase => write!(f, "XlmRobertaBase"),
            Self::BertBaseUncased => write!(f, "BertBaseUncased"),
        }
    }
}

impl Default for Tokenizer {
    fn default() -> Tokenizer {
        Self::Word
    }
}

