/*
 * Chunkr API
 *
 * API service for document layout analysis and chunking to convert document into RAG/LLM-ready data.
 *
 * The version of the OpenAPI document: 1.0.0
 * Contact: ishaan@lumina.sh
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// LlmProcessing : Controls the LLM used for the task.
#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct LlmProcessing {
    #[serde(rename = "fallback_strategy", skip_serializing_if = "Option::is_none")]
    pub fallback_strategy: Option<Box<models::LlmProcessingFallbackStrategy>>,
    /// The maximum number of tokens to generate.
    #[serde(rename = "max_completion_tokens", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub max_completion_tokens: Option<Option<i32>>,
    /// The ID of the model to use for the task. If not provided, the default model will be used. Please check the documentation for the model you want to use.
    #[serde(rename = "model_id", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub model_id: Option<Option<String>>,
    /// The temperature to use for the LLM.
    #[serde(rename = "temperature", skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
}

impl LlmProcessing {
    /// Controls the LLM used for the task.
    pub fn new() -> LlmProcessing {
        LlmProcessing {
            fallback_strategy: None,
            max_completion_tokens: None,
            model_id: None,
            temperature: None,
        }
    }
}

