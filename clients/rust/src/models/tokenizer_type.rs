/*
 * Chunkr API
 *
 * API service for document layout analysis and chunking to convert document into RAG/LLM-ready data.
 *
 * The version of the OpenAPI document: 1.0.0
 * Contact: ishaan@lumina.sh
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// TokenizerType : Specifies which tokenizer to use for the chunking process.  This type supports two ways of specifying a tokenizer: 1. Using a predefined tokenizer from the `Tokenizer` enum 2. Using any Hugging Face tokenizer by providing its model ID as a string    (e.g. \"facebook/bart-large\", \"Qwen/Qwen-tokenizer\", etc.)  When using a string, any valid Hugging Face tokenizer ID can be specified, which will be loaded using the Hugging Face tokenizers library.
/// Specifies which tokenizer to use for the chunking process.  This type supports two ways of specifying a tokenizer: 1. Using a predefined tokenizer from the `Tokenizer` enum 2. Using any Hugging Face tokenizer by providing its model ID as a string    (e.g. \"facebook/bart-large\", \"Qwen/Qwen-tokenizer\", etc.)  When using a string, any valid Hugging Face tokenizer ID can be specified, which will be loaded using the Hugging Face tokenizers library.
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
#[serde(untagged)]
pub enum TokenizerType {
    TokenizerTypeOneOf(Box<models::TokenizerTypeOneOf>),
    TokenizerTypeOneOf1(Box<models::TokenizerTypeOneOf1>),
}

impl Default for TokenizerType {
    fn default() -> Self {
        Self::TokenizerTypeOneOf(Default::default())
    }
}

