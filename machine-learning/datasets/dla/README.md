# DLA Dataset Creation and Exploration Project

## 1. Overview

This project provides a suite of tools to build, curate, and explore a dataset for Document Layout Analysis (DLA). The primary goal is to process a collection of source documents, extract rich metadata and layout information (potentially using LLMs), and then select a diverse, representative subset of pages for manual tagging or model training.

The pipeline involves:

- Fetching initial metadata about raw pages and performing deduplication (SQLite).
- Extracting detailed page attributes (domain, complexity, segments, defects) and storing them in a central PostgreSQL database.
- Selecting a target number of pages for a tagging set using stratified sampling techniques to ensure diversity.
- Exploring the processed data and the selected tagging set using a Streamlit-based visual tool.

## 2. Key Components

- **`raw_and_dedup.db` (SQLite):** Stores initial raw page information, including document IDs, page identifiers (`dedup_id`), and paths to source files in cloud storage. Generated by `src/dla/main.py`.
- **`failed_extractions.db` (SQLite):** Logs pages that failed during the detailed feature extraction process. Used by `src/dla/extract_and_upload.py`.
- **PostgreSQL Database:** The main data store for rich, processed page profiles, including domain, complexity, layout styles, segments, and defects. Schema is defined in `sql/schema.sql`.
- **`src/dla/main.py`:** Processes source documents to identify unique pages, generates hashes, and populates `raw_and_dedup.db`.
- **`src/dla/extract_and_upload.py`:** Reads page information from `raw_and_dedup.db`, orchestrates the extraction of detailed attributes (e.g., using LLMs via API calls), and uploads this structured data to the PostgreSQL database. Handles retries for failed extractions.
- **`src/dla/create_initial_tagging_set.py`:** Performs page-level stratified sampling based on various attributes (domain, complexity, source, etc.) fetched from PostgreSQL to select a target number of pages (e.g., 10,000) for a tagging set. Stores the selected page identifiers in a dedicated table (`dla_selected_tagging_pages`) in the PostgreSQL database.
- **`src/dla/dla_explorer.py`:** A Streamlit application to visually browse and filter pages from the PostgreSQL database, including an option to view only the selected tagging set.
- **`src/dla/filter_pg_data.py`:** An optional utility script to filter data in the PostgreSQL database based on user IDs and sampling fractions. Useful for data curation or reduction.
- **`upload.py`:** A utility script to upload source document files (e.g., PDFs) to S3-compatible cloud storage. This is likely a preliminary step before running the main data processing pipeline.
- **`.env`:** Configuration file for environment variables (database URLs, API keys, cloud storage credentials).
- **`pyproject.toml` & `uv.lock`:** Python project and dependency management files.

## 3. Setup Instructions

### 3.1. Dependencies

It is recommended to use a virtual environment. This project uses `uv` for package management.

```bash
# Navigate to the project root (e.g., machine-learning/datasets/dla/)
uv pip install -r requirements.txt # Or use `uv pip sync` if a requirements.lock or pyproject.toml is set up for uv sync
```

_(Developer Note: Ensure your `pyproject.toml` is configured for `uv` or provide a `requirements.txt` if `uv pip sync` is preferred. If `uv.lock` is the primary lock file, `uv pip sync` might be more appropriate if it implies syncing to the lock file directly)._

### 3.2. Environment Variables

Create a `.env` file in the `machine-learning/datasets/dla/` directory by copying and modifying a template (if one exists) or by setting the following variables:

```env
# PostgreSQL Connection (used by multiple scripts)
SOURCE_DATABASE_URL="postgresql://user:password@host:port/dbname" # For create_initial_tagging_set.py
DATABASE_URL="postgresql://user:password@host:port/dbname"      # For dla_explorer.py, extract_and_upload.py, filter_pg_data.py (ensure this is the same DB as SOURCE_DATABASE_URL)

# SQLite Database Path (used by create_initial_tagging_set.py)
# Defaults to ./raw_and_dedup.db relative to the dla project root if not set
SQLITE_DB_PATH="raw_and_dedup.db" # Or an absolute path

# Cloud Storage Credentials (as needed by src/dla/main.py, src/dla/extract_and_upload.py, upload.py)
# Example for AWS S3 (used by upload.py and dla_explorer.py)
AWS__ACCESS_KEY="your_aws_access_key_id"
AWS__SECRET_KEY="your_aws_secret_access_key"
AWS__ENDPOINT="your_s3_endpoint_url" # e.g., https://s3.us-west-1.amazonaws.com
AWS_BUCKET_NAME="your_bucket_name_for_dla_explorer_s3_objects"
AWS__PRESIGNED_URL_ENDPOINT="your_s3_presigned_url_endpoint_if_different" # e.g., https://your_bucket.s3.amazonaws.com/ (used by upload.py)

# Example for GCS (if used by src/dla/extract_and_upload.py or src/dla/main.py for fetching source files)
GOOGLE_ACCESS_KEY="your_gcs_access_key"
GOOGLE_SECRET_KEY="your_gcs_secret_key"
GOOGLE_ENDPOINT="your_gcs_endpoint_url"
GOOGLE_BUCKET_NAME="your_gcs_bucket_name"

# LLM API Keys (if src/dla/extract_and_upload.py uses an external LLM service)
# OPENROUTER_API_KEY="your_openrouter_api_key" # Example
# Add other necessary API keys or service-specific variables
```

**Note:** Ensure the correct cloud storage (AWS S3, GCS, etc.) variables are set based on where your source documents and extracted JSON/images are stored and accessed by the scripts. The `dla_explorer.py` specifically uses AWS S3 for fetching images/JSON based on its current S3 client setup.

### 3.3. PostgreSQL Database Setup

1.  Ensure you have a running PostgreSQL server.
2.  Create the database specified in your `DATABASE_URL`.
3.  Apply the schema using a PostgreSQL client (e.g., `psql`):
    ```bash
    psql -d your_dbname -U your_user -f sql/schema.sql
    ```
    (Replace `your_dbname` and `your_user` accordingly). This will create the necessary tables (`page_profiles`, `page_segments`, `page_defects`, `upload_failures`).

## 4. Data Pipeline Workflow & Commands

Execute commands from the `machine-learning/datasets/dla/` directory. It's generally recommended to run Python scripts as modules if they are part of a package.

**Assumed Execution Environment:** Using `uv run` for Python scripts. If not using `uv`, replace `uv run -- python -m` with `python -m`.

### Step 0: Upload Source Documents (Optional - if not already in cloud storage)

This step uses `upload.py` to upload your raw document files (e.g., PDFs) to S3-compatible storage. The other scripts will expect to find these source files or their derivatives in cloud storage via keys.

```bash
# Example - this script is a utility and might need direct invocation or integration
# (Refer to upload.py for its direct usage, e.g., via its test_upload function or by importing its functions)
# Typically, you'd have a collection of PDFs to upload.
```

### Step 1: Initial Page Ingestion and Deduplication

This script processes source documents (presumably from cloud storage paths), identifies pages, generates hashes, and populates the `raw_and_dedup.db` SQLite database.

```bash
uv run -- python -m src.dla.main
```

- **Prerequisites:** Cloud storage (e.g., GCS) credentials in `.env` if `src/dla/main.py` fetches directly from there.
- **Output:** Populated `raw_and_dedup.db` with `pages_raw` and `pages_dedup` tables.

### Step 2: Detailed Feature Extraction and PostgreSQL Upload

This script takes unique pages from `raw_and_dedup.db`, extracts detailed attributes (domain, complexity, segments, defects, etc., likely using LLMs), and uploads this information to the main PostgreSQL database.

```bash
uv run -- python -m src.dla.extract_and_upload
```

- **Prerequisites:**
  - Completed Step 1 (`raw_and_dedup.db` must exist and be populated).
  - PostgreSQL database set up with the correct schema (see section 3.3).
  - `.env` file with `DATABASE_URL`.
  - Relevant API keys (e.g., `OPENROUTER_API_KEY`) and cloud storage credentials for fetching page content if needed by the extraction logic.
- **Output:** Populated `page_profiles`, `page_segments`, `page_defects` tables in PostgreSQL. `failed_extractions.db` is updated if errors occur.
- **Note:** This script might have options for rerunning failed extractions. Check its internal help or comments.

### Step 3: Create Initial Tagging Set

This script selects a representative subset of pages (e.g., 10,000) from the richly annotated data in PostgreSQL for tagging.

```bash
uv run -- python -m src.dla.create_initial_tagging_set
```

- **Prerequisites:**
  - Completed Step 1 (`raw_and_dedup.db` must exist).
  - Completed Step 2 (PostgreSQL database must be populated with detailed page profiles).
  - `.env` file with `SOURCE_DATABASE_URL` (same as `DATABASE_URL`) and `SQLITE_DB_PATH` (pointing to `raw_and_dedup.db`).
- **Output:** The `dla_selected_tagging_pages` table in PostgreSQL is cleared and populated with the identifiers of the selected pages.

### Step 4: Explore the Data (Streamlit App)

Run the DLA Explorer to visually inspect the pages, their metadata, and the selected tagging set.

```bash
streamlit run src/dla/dla_explorer.py
```

- **Prerequisites:**
  - PostgreSQL database populated (Step 2).
  - (Optional but recommended) Tagging set created (Step 3) if you want to use the "Show only Tagging Set Pages" feature.
  - `.env` file with `DATABASE_URL` and AWS S3 credentials (`AWS__ACCESS_KEY`, `AWS__SECRET_KEY`, `AWS_BUCKET_NAME`, `AWS__ENDPOINT`) as the explorer fetches images/JSON from S3.

## 5. Optional Utility Scripts

### 5.1. Filter PostgreSQL Data (`src/dla/filter_pg_data.py`)

This script can be used to reduce the data in your PostgreSQL `page_profiles` table (and related tables via CASCADE) based on specified user IDs and the fraction of their documents to keep.

```bash
uv run -- python -m src.dla.filter_pg_data
```

- **Purpose:** Data curation, reducing dataset size for specific users.
- **Prerequisites:**
  - Populated PostgreSQL database.
  - `raw_and_dedup.db` (for user_id to doc_id to page_id mapping).
  - `.env` file with `DATABASE_URL` and `SQLITE_DB_PATH`.
- **Caution:** This script performs deletions. It's highly recommended to **backup your PostgreSQL database and `raw_and_dedup.db`** before running it in live mode. It has a dry run mode.

## 6. DLA Explorer (`src/dla/dla_explorer.py`)

The DLA Explorer is a Streamlit web application designed for visual inspection and analysis of the processed page data stored in the PostgreSQL database.

### How it Works:

- Connects to the PostgreSQL database specified by the `DATABASE_URL` environment variable.
- Fetches page metadata (domain, source, complexity, etc.) from the `page_profiles` table.
- Retrieves associated page images and extracted JSON data from AWS S3 storage (credentials for S3 must be in the `.env` file).
- Allows users to:
  - Select a document domain to browse.
  - Apply various filters (source, orientation, complexity, language, etc.).
  - Navigate through pages within the selected domain/filters.
  - View the page image alongside its extracted properties, segments, and defects.
  - Optionally, filter to show only pages that are part of the `dla_selected_tagging_pages` table (if created by `create_initial_tagging_set.py`).

### Running the Explorer:

```bash
streamlit run src/dla/dla_explorer.py
```

Ensure your `.env` file is correctly configured, especially with `DATABASE_URL` and the AWS S3 credentials required by the explorer for fetching page assets.
